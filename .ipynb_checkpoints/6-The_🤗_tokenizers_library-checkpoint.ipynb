{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4e7b460",
   "metadata": {},
   "source": [
    "# The ü§ó tokenizers library\n",
    "## [Introduction](https://huggingface.co/course/chapter6/1?fw=pt)\n",
    "\n",
    "In [Chapter 3](https://huggingface.co/course/chapter3), we looked at how to fine-tune a model on a given task. When we do that, we use the same tokenizer that the model was pretrained with ‚Äî but what do we do when we want to train a model from scratch? In these cases, using a tokenizer that was pretrained on a corpus from another domain or language is typically suboptimal. For example, a tokenizer that's trained on an English corpus will perform poorly on a corpus of Japanese texts because the use of spaces and punctuation is very different in the two languages.\n",
    "\n",
    "In this chapter, you will learn how to train a brand new tokenizer on a corpus of texts, so it can then be used to pretrain a language model. This will all be done with the help of the [ü§ó Tokenizers](https://github.com/huggingface/tokenizers) library, which provides the \"fast\" tokenizers in the [ü§ó Transformers](https://github.com/huggingface/transformers) library. We'll take a close look at the features that this library provides, and explore how the fast tokenizers differ from the \"slow\" versions.\n",
    "\n",
    "Topics we will cover include:\n",
    "- How to train a new tokenizer similar to the one used by a given checkpoint on a new corpus of texts.\n",
    "- The special features of fast tokenizers.\n",
    "- The differences between the three main subword tokenization algorithms used in NLP today.\n",
    "- How to build a tokenizer from scratch with the ü§ó Tokenizers library and train it on some data.\n",
    "\n",
    "The techniques introduced in this chapter will prepare you for the section in [Chapter 7](https://huggingface.co/course/chapter7/6) where we look at creating a language model for Python source code. Let's start by looking at what it means to \"train\" a tokenizer in the first place.\n",
    "\n",
    "## [Training a new tokenizer from an old one](https://huggingface.co/course/chapter6/2?fw=pt)\n",
    "\n",
    "If a language model is not available in the language you are interested in, or if your corpus is very different from the one your language model was trained on, you will most likely want to retrain the model from scratch using a tokenizer adapted to your data. That will require training a new tokenizer on your dataset. But what exactly does that mean? When we first looked at tokenizers in [Chapter 2](https://huggingface.co/course/chapter2), we saw that most Transformer models use a *subword tokenization algorithm*. To identify which subwords are of interest and occur most frequently in the corpus at hand, the tokenizer needs to take a hard look at all the texts in the corpus ‚Äî a process we call *training*. The exact rules that govern this training depend on the type of tokenizer used, and we'll go over the three main algorithms later in this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21f329d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthias/opt/anaconda3/envs/hf/lib/python3.8/site-packages/IPython/core/display.py:717: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/DJimQynXZsQ\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/DJimQynXZsQ\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c899190",
   "metadata": {},
   "source": [
    "> <font color=\"darkred\">‚ö†Ô∏è Training a tokenizer is not the same as training a model! Model training uses stochastic gradient descent to make the loss a little bit smaller for each batch. It's randomized by nature (meaning you have to set some seeds to get the same results when doing the same training twice). Training a tokenizer is a statistical process that tries to identify which subwords are the best to pick for a given corpus, and the exact rules used to pick them depend on the tokenization algorithm. It's deterministic, meaning you always get the same results when training with the same algorithm on the same corpus.</font>\n",
    "\n",
    "### Assembling a corpus\n",
    "There's a very simple API in ü§ó Transformers that you can use to train a new tokenizer with the same characteristics as an existing one: `AutoTokenizer.train_new_from_iterator()`. To see this in action, let's say we want to train GPT-2 from scratch, but in a language other than English. Our first task will be to gather lots of data in that language in a training corpus. To provide examples everyone will be able to understand, we won't use a language like Russian or Chinese here, but rather a specialized English language: Python code.\n",
    "\n",
    "The [ü§ó Datasets](https://github.com/huggingface/datasets) library can help us assemble a corpus of Python source code. We'll use the usual `load_dataset()` function to download and cache the [CodeSearchNet](https://huggingface.co/datasets/code_search_net) dataset. This dataset was created for the [CodeSearchNet challenge](https://wandb.ai/github/CodeSearchNet/benchmark) and contains millions of functions from open source libraries on GitHub in several programming languages. Here, we will load the Python part of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6583a4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset code_search_net (/Users/matthias/.cache/huggingface/datasets/code_search_net/python/1.0.0/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e63b4f277314355a4abf13b7b4f5fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# This can take a few minutes to load, so grab a coffee or tea while you wait!\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01c06ea",
   "metadata": {},
   "source": [
    "We can have a look at the training split to see which columns we have access to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c27d31e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "    num_rows: 412178\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5a7892",
   "metadata": {},
   "source": [
    "We can see the dataset separates docstrings from code and suggests a tokenization of both. Here. we'll just use the `whole_func_string` column to train our tokenizer. We can look at an example of one these functions by indexing into the `train` split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "058f42a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def updater():\n",
      "    \"\"\"Update the current installation.\n",
      "\n",
      "    git clones the latest version and merges it with the current directory.\n",
      "    \"\"\"\n",
      "    print('%s Checking for updates' % run)\n",
      "    # Changes must be separated by ;\n",
      "    changes = '''major bug fixes;removed ninja mode;dropped python < 3.2 support;fixed unicode output;proxy support;more intels'''\n",
      "    latest_commit = requester('https://raw.githubusercontent.com/s0md3v/Photon/master/core/updater.py', host='raw.githubusercontent.com')\n",
      "    # Just a hack to see if a new version is available\n",
      "    if changes not in latest_commit:\n",
      "        changelog = re.search(r\"changes = '''(.*?)'''\", latest_commit)\n",
      "        # Splitting the changes to form a list\n",
      "        changelog = changelog.group(1).split(';')\n",
      "        print('%s A new version of Photon is available.' % good)\n",
      "        print('%s Changes:' % info)\n",
      "        for change in changelog: # print changes\n",
      "            print('%s>%s %s' % (green, end, change))\n",
      "\n",
      "        current_path = os.getcwd().split('/') # if you know it, you know it\n",
      "        folder = current_path[-1] # current directory name\n",
      "        path = '/'.join(current_path) # current directory path\n",
      "        choice = input('%s Would you like to update? [Y/n] ' % que).lower()\n",
      "\n",
      "        if choice != 'n':\n",
      "            print('%s Updating Photon' % run)\n",
      "            os.system('git clone --quiet https://github.com/s0md3v/Photon %s'\n",
      "                      % (folder))\n",
      "            os.system('cp -r %s/%s/* %s && rm -r %s/%s/ 2>/dev/null'\n",
      "                      % (path, folder, path, path, folder))\n",
      "            print('%s Update successful!' % good)\n",
      "    else:\n",
      "        print('%s Photon is up to date!' % good)\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7111e96",
   "metadata": {},
   "source": [
    "The above output differ from the one in the course (see below) because the dataset ‚Äì specifically the `whole_func_string` ‚Äì has been updated.\n",
    "\n",
    "Originally, the `whole_func_string` read as follows:\n",
    "\n",
    "```python\n",
    "def handle_simple_responses(\n",
    "      self, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):\n",
    "    \"\"\"Accepts normal responses from the device.\n",
    "\n",
    "    Args:\n",
    "      timeout_ms: Timeout in milliseconds to wait for each response.\n",
    "      info_cb: Optional callback for text sent from the bootloader.\n",
    "\n",
    "    Returns:\n",
    "      OKAY packet's message.\n",
    "    \"\"\"\n",
    "    return self._accept_responses('OKAY', info_cb, timeout_ms=timeout_ms)\n",
    "```\n",
    "\n",
    "The first thing we need to do is transform the dataset into an *iterator* of lists of texts ‚Äî for instance, a list of list of texts. Using lists of texts will enable our tokenizer to go faster (training on batches of texts instead of processing individual texts one by one), and it should be an iterator if we want to avoid having everything in memory at once. If your corpus is huge, you will want to take advantage of the fact that ü§ó Datasets does not load everything into RAM but stores the elements of the dataset on disk.\n",
    "\n",
    "Doing the following would create a list of lists of 1,000 texts each, but would load everything in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "523ccc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't uncomment the following line(s) unless your dataset is small!\n",
    "# training_corpus = [\n",
    "#     raw_datasets[\"train\"][i: i + 1000][\"whole_func_string\"] for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c2a5b6",
   "metadata": {},
   "source": [
    "Using a Python generator, we can avoid Python loading anything into memory until it's actually necessary. To create such a generator, you just to need to replace the brackets with parentheses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85bb439b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x7f7bf00cbb30>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_corpus = (\n",
    "    raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "    for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    ")\n",
    "training_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91bce0b",
   "metadata": {},
   "source": [
    "This line of code doesn't fetch any elements of the dataset; it just creates an object you can use in a Python `for` loop. The texts will only be loaded when you need them (that is, when you're at the step of the `for` loop that requires them), and only 1,000 texts at a time will be loaded. This way you won't exhaust all your memory even if you are processing a huge dataset.\n",
    "\n",
    "The problem with a generator object is that it can only be used once. So, instead of this giving us the list of the first 10 digits twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c94e1ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "gen = (i for i in range(10))\n",
    "print(list(gen))\n",
    "print(list(gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ac17c9",
   "metadata": {},
   "source": [
    "we get them once and then an empty list.\n",
    "\n",
    "That's why we define a function that returns a generator instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74f1647a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object get_training_corpus.<locals>.<genexpr> at 0x7f7c00e3c5f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    "    )\n",
    "training_corpus = get_training_corpus()\n",
    "training_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e418b1",
   "metadata": {},
   "source": [
    "You can also define your generator inside a `for` loop by using the `yield` statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c5cfec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_training_corpus()>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]\n",
    "get_training_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea6de0",
   "metadata": {},
   "source": [
    "which will produce the exact same generator as before, but allows you to use more complex logic than you can in a list comprehension.\n",
    "\n",
    "### Training a new tokenizer\n",
    "Now that we have our corpus in the form of an iterator of batches of texts, we are ready to train a new tokenizer. To do this, we first need to load the tokenizer we want to pair with our model (here, GPT-2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a2ee112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "old_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a81f1b",
   "metadata": {},
   "source": [
    "Even though we are going to train a new tokenizer, it's a good idea to do this to avoid starting entirely from scratch. This way, we won't have to specify anything about the tokenization algorithm or the special tokens we want to use; our new tokenizer will be exactly the same as GPT-2, and the only thing that will change is the vocabulary, which will be determined by the training on our corpus.\n",
    "\n",
    "First let's have a look at how this tokenizer would treat an example function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "242fe048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def',\n",
       " 'ƒ†add',\n",
       " '_',\n",
       " 'n',\n",
       " 'umbers',\n",
       " '(',\n",
       " 'a',\n",
       " ',',\n",
       " 'ƒ†b',\n",
       " '):',\n",
       " 'ƒä',\n",
       " 'ƒ†',\n",
       " 'ƒ†',\n",
       " 'ƒ†',\n",
       " 'ƒ†\"\"\"',\n",
       " 'Add',\n",
       " 'ƒ†the',\n",
       " 'ƒ†two',\n",
       " 'ƒ†numbers',\n",
       " 'ƒ†`',\n",
       " 'a',\n",
       " '`',\n",
       " 'ƒ†and',\n",
       " 'ƒ†`',\n",
       " 'b',\n",
       " '`',\n",
       " '.\"',\n",
       " '\"\"',\n",
       " 'ƒä',\n",
       " 'ƒ†',\n",
       " 'ƒ†',\n",
       " 'ƒ†',\n",
       " 'ƒ†return',\n",
       " 'ƒ†a',\n",
       " 'ƒ†+',\n",
       " 'ƒ†b']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd635093",
   "metadata": {},
   "source": [
    "This tokenizer has a few special symbols, like `ƒ†` and `ƒä`, which denote spaces and newlines, respectively. As we can see, this is not too efficient: the tokenizer returns individual tokens for each space, when it could group together indentation levels (since having sets of four or eight spaces is going to be very common in code). It also split the function name a bit weirdly, not being used to seeing words with the `_` character.\n",
    "\n",
    "Let's train a new tokenizer and see if it solves those issues. For this, we'll use the method `train_new_from_iterator()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "653f4912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='gpt2', vocab_size=52000, model_max_len=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca55cac",
   "metadata": {},
   "source": [
    "This command might take a bit of time if your corpus is very large, but for this dataset of 1.6 GB of texts it's blazing fast (1 minute 16 seconds on an AMD Ryzen 9 3900X CPU with 12 cores).\n",
    "\n",
    "Note that `AutoTokenizer.train_new_from_iterator()` only works if the tokenizer you are using is a \"fast\" tokenizer. As you'll see in the next section, the ü§ó Transformers library contains two types of tokenizers: some are written purely in Python and others (the fast ones) are backed by the ü§ó Tokenizers library, which is written in the [Rust](https://www.rust-lang.org/) programming language. Python is the language most often used for data science and deep learning applications, but when anything needs to be parallelized to be fast, it has to be written in another language. For instance, the matrix multiplications that are at the core of the model computation are written in CUDA, an optimized C library for GPUs.\n",
    "\n",
    "Training a brand new tokenizer in pure Python would be excruciatingly slow, which is why we developed the ü§ó Tokenizers library. Note that just as you didn't have to learn the CUDA language to be able to execute your model on a batch of inputs on a GPU, you won't need to learn Rust to use a fast tokenizer. The ü§ó Tokenizers library provides Python bindings for many methods that internally call some piece of code in Rust; for example, to parallelize the training of your new tokenizer or, as we saw in [Chapter 3](https://huggingface.co/course/chapter3), the tokenization of a batch of inputs.\n",
    "\n",
    "Most of the Transformer models have a fast tokenizer available (there are some exceptions that you can check [here](https://huggingface.co/transformers/#supported-frameworks)), and the `AutoTokenizer` API always selects the fast tokenizer for you if it's available. In the next section we'll take a look at some of the other special features fast tokenizers have, which will be really useful for tasks like token classification and question answering. Before diving into that, however, let's try our brand new tokenizer on the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ea21f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def',\n",
       " 'ƒ†add',\n",
       " '_',\n",
       " 'numbers',\n",
       " '(',\n",
       " 'a',\n",
       " ',',\n",
       " 'ƒ†b',\n",
       " '):',\n",
       " 'ƒäƒ†ƒ†ƒ†',\n",
       " 'ƒ†\"\"\"',\n",
       " 'Add',\n",
       " 'ƒ†the',\n",
       " 'ƒ†two',\n",
       " 'ƒ†numbers',\n",
       " 'ƒ†`',\n",
       " 'a',\n",
       " '`',\n",
       " 'ƒ†and',\n",
       " 'ƒ†`',\n",
       " 'b',\n",
       " '`.\"\"\"',\n",
       " 'ƒäƒ†ƒ†ƒ†',\n",
       " 'ƒ†return',\n",
       " 'ƒ†a',\n",
       " 'ƒ†+',\n",
       " 'ƒ†b']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3175a94",
   "metadata": {},
   "source": [
    "Here we again see the special symbols `ƒ†` and `ƒä` that denote spaces and newlines, but we can also see that our tokenizer learned some tokens that are highly specific to a corpus of Python functions: for example, there is a `ƒäƒ†ƒ†ƒ†` token that represents an indentation, and a `ƒ†\"\"\"` token that represents the three quotes that start a docstring. The tokenizer also correctly split the function name on `_`. This is quite a compact representation; comparatively, using the plain English tokenizer on the same example will give us a longer sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9340e94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(len(old_tokenizer.tokenize(example)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167932f3",
   "metadata": {},
   "source": [
    "Let's look at another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1cbd8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class',\n",
       " 'ƒ†Linear',\n",
       " 'Layer',\n",
       " '():',\n",
       " 'ƒäƒ†ƒ†ƒ†',\n",
       " 'ƒ†def',\n",
       " 'ƒ†__',\n",
       " 'init',\n",
       " '__(',\n",
       " 'self',\n",
       " ',',\n",
       " 'ƒ†input',\n",
       " '_',\n",
       " 'size',\n",
       " ',',\n",
       " 'ƒ†output',\n",
       " '_',\n",
       " 'size',\n",
       " '):',\n",
       " 'ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†',\n",
       " 'ƒ†self',\n",
       " '.',\n",
       " 'weight',\n",
       " 'ƒ†=',\n",
       " 'ƒ†torch',\n",
       " '.',\n",
       " 'randn',\n",
       " '(',\n",
       " 'input',\n",
       " '_',\n",
       " 'size',\n",
       " ',',\n",
       " 'ƒ†output',\n",
       " '_',\n",
       " 'size',\n",
       " ')',\n",
       " 'ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†',\n",
       " 'ƒ†self',\n",
       " '.',\n",
       " 'bias',\n",
       " 'ƒ†=',\n",
       " 'ƒ†torch',\n",
       " '.',\n",
       " 'zeros',\n",
       " '(',\n",
       " 'output',\n",
       " '_',\n",
       " 'size',\n",
       " ')',\n",
       " 'ƒäƒäƒ†ƒ†ƒ†',\n",
       " 'ƒ†def',\n",
       " 'ƒ†__',\n",
       " 'call',\n",
       " '__(',\n",
       " 'self',\n",
       " ',',\n",
       " 'ƒ†x',\n",
       " '):',\n",
       " 'ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†',\n",
       " 'ƒ†return',\n",
       " 'ƒ†x',\n",
       " 'ƒ†@',\n",
       " 'ƒ†self',\n",
       " '.',\n",
       " 'weights',\n",
       " 'ƒ†+',\n",
       " 'ƒ†self',\n",
       " '.',\n",
       " 'bias',\n",
       " 'ƒäƒ†ƒ†ƒ†ƒ†']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"\"\"class LinearLayer():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weight = torch.randn(input_size, output_size)\n",
    "        self.bias = torch.zeros(output_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x @ self.weights + self.bias\n",
    "    \"\"\"\n",
    "tokenizer.tokenize(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fca31f2",
   "metadata": {},
   "source": [
    "In addition to the token corresponding to an indentation, here we can also see a token for a double indentation: `ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†`. The special Python words like `class`, `init`, `call`, `self`, and `return` are each tokenized as one token, and we can see that as well as splitting on `_` and `.` the tokenizer correctly splits even camel-cased names: `LinearLayer` is tokenized as `[\"ƒ†Linear\", \"Layer\"]`.\n",
    "\n",
    "### Saving the tokenizer\n",
    "\n",
    "To make sure we can use it later, we need to save our new tokenizer. Like for models, this is done with the `save_pretrained()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "122c6d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('code-search-net-tokenizer/tokenizer_config.json',\n",
       " 'code-search-net-tokenizer/special_tokens_map.json',\n",
       " 'code-search-net-tokenizer/vocab.json',\n",
       " 'code-search-net-tokenizer/merges.txt',\n",
       " 'code-search-net-tokenizer/added_tokens.json',\n",
       " 'code-search-net-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fce6158",
   "metadata": {},
   "source": [
    "This will create a new folder named `code-search-net-tokenizer`, which will contain all the files the tokenizer needs to be reloaded. If you want to share this tokenizer with your colleagues and friends, you can upload it to the Hub by logging into your account. If you're working in a notebook, there's a convenience function to help you with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d057d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to /Users/matthias/.huggingface/token\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeafee2",
   "metadata": {},
   "source": [
    "This will display a widget where you can enter your Hugging Face login credentials. If you aren‚Äôt working in a notebook, just type the following line in your terminal:\n",
    "```bash\n",
    "huggingface-cli login\n",
    "```\n",
    "Once you've logged in, you can push your tokenizer by executing the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48387947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthias/opt/anaconda3/envs/hf/lib/python3.8/site-packages/huggingface_hub/hf_api.py:1001: FutureWarning: `create_repo` now takes `token` as an optional positional argument. Be sure to adapt your code!\n",
      "  warnings.warn(\n",
      "Cloning https://huggingface.co/mdroth/code-search-net-tokenizer into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "# use_temp_dir=True\n",
    "# https://discuss.huggingface.co/t/chapter-4-questions/6801/5\n",
    "tokenizer.push_to_hub(\"code-search-net-tokenizer\", use_temp_dir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941257e7",
   "metadata": {},
   "source": [
    "This will create a new repository in your namespace with the name `code-search-net-tokenizer`, containing the tokenizer file. You can then load the tokenizer from anywhere with the `from_pretrained()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d2e99e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"huggingface-course\" below with your actual namespace to use your own tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mdroth/code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efbd186",
   "metadata": {},
   "source": [
    "You're now all set for training a language model from scratch and fine-tuning it on your task at hand! We'll get to that in Chapter 7, but first, in the rest of this chapter we'll take a closer look at fast tokenizers and explore in detail what actually happens when we call the method `train_new_from_iterator()`.\n",
    "\n",
    "## [Fast tokenizers' special powers](https://huggingface.co/course/chapter6/3?fw=pt)\n",
    "\n",
    "In this section we will take a closer look at the capabilities of the tokenizers in ü§ó Transformers. Up to now we have only used them to tokenize inputs or decode IDs back into text, but tokenizers ‚Äî especially those backed by the ü§ó Tokenizers library ‚Äî can do a lot more. To illustrate these additional features, we will explore how to reproduce the results of the `token-classification` (that we called `ner`) and `question-answering` pipelines that we first encountered in [Chapter 1](https://huggingface.co/course/chapter1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93a4236c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthias/opt/anaconda3/envs/hf/lib/python3.8/site-packages/IPython/core/display.py:717: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/g8quOxoqhHQ\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/g8quOxoqhHQ\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54036ba3",
   "metadata": {},
   "source": [
    "In the following discussion, we will often make the distinction between \"slow\" and \"fast\" tokenizers. Slow tokenizers are those written in Python inside the ü§ó Transformers library, while the fast versions are the ones provided by ü§ó Tokenizers, which are written in Rust. If you remember the table from [Chapter 5](https://huggingface.co/course/chapter5/3) that reported how long it took a fast and a slow tokenizer to tokenize the Drug Review Dataset, you should have an idea of why we call them fast and slow:\n",
    "\n",
    "|Fast tokenizer|Slow|tokenizer|\n",
    "|:---|---|---|\n",
    "|batched=True|10.8s|4min41s|\n",
    "|batched=False|59.2s|5min3s|\n",
    "\n",
    "> <font color=\"darkred\">‚ö†Ô∏è When tokenizing a single sentence, you won't always see a difference in speed between the slow and fast versions of the same tokenizer. In fact, the fast version might actually be slower! It's only when tokenizing lots of texts in parallel at the same time that you will be able to clearly see the difference.</font>\n",
    "\n",
    "### Batch Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a331580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/3umI3tm27Vw\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/3umI3tm27Vw\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8079cc",
   "metadata": {},
   "source": [
    "The output of a tokenizer isn't a simple Python dictionary; what we get is actually a special `BatchEncoding` object. It's a subclass of a dictionary (which is why we were able to index into that result without any problem before), but with additional methods that are mostly used by fast tokenizers.\n",
    "\n",
    "Besides their parallelization capabilities, the key functionality of fast tokenizers is that they always keep track of the original span of texts the final tokens come from ‚Äî a feature we call *offset mapping*. This in turn unlocks features like mapping each word to the tokens it generated or mapping each character of the original text to the token it's inside, and vice versa.\n",
    "\n",
    "Let's take a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15bcc264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "encoding = tokenizer(example)\n",
    "print(type(encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a432dec6",
   "metadata": {},
   "source": [
    "As mentioned previously, we get a `BatchEncoding` object in the tokenizer's output (see above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6096975",
   "metadata": {},
   "source": [
    "Since the `AutoTokenizer` class picks a fast tokenizer by default, we can use the additional methods this `BatchEncoding` object provides. We have two ways to check if our tokenizer is a fast or a slow one. We can either check the attribute `is_fast` of the `tokenizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0ec8004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9d8ca",
   "metadata": {},
   "source": [
    "or check the same attribute of our `encoding`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c8fbbba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.is_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74393ad9",
   "metadata": {},
   "source": [
    "Let's see what a fast tokenizer enables us to do. First, we can access the tokens without having to convert the IDs back to tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39ac5c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BatchEncoding.tokens of {'input_ids': [101, 1422, 1271, 1110, 156, 7777, 2497, 1394, 1105, 146, 1250, 1120, 20164, 10932, 10289, 1107, 6010, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe70eb2",
   "metadata": {},
   "source": [
    "In this case the token at index 5 is `##yl`, which is part of the word \"Sylvain\" in the original sentence. We can also use the `word_ids()` method to get the index of the word each token comes from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c06e3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.word_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e137027a",
   "metadata": {},
   "source": [
    "We can see that the tokenizer's special tokens `[CLS]` and `[SEP]` are mapped to `None`, and then each token is mapped to the word it originates from. This is especially useful to determine if a token is at the start of a word or if two tokens are in the same word. We could rely on the `##` prefix for that, but it only works for BERT-like tokenizers; this method works for any type of tokenizer as long as it's a fast one. In the next chapter, we'll see how we can use this capability to apply the labels we have for each word properly to the tokens in tasks like named entity recognition (NER) and part-of-speech (POS) tagging. We can also use it to mask all the tokens coming from the same word in masked language modeling (a technique called *whole word masking*).\n",
    "\n",
    "> <font color=\"darkgreen\">The notion of what a word is is complicated. For instance, does \"I'll\" (a contraction of \"I will\") count as one or two words? It actually depends on the tokenizer and the pre-tokenization operation it applies. Some tokenizers just split on spaces, so they will consider this as one word. Others use punctuation on top of spaces, so will consider it two words.</font><br>\n",
    "‚úèÔ∏è Try it out! <font color=\"darkgreen\">Create a tokenizer from the `bert-base-cased` and `roberta-base` checkpoints and tokenize \"81s\" with them. What do you observe? What are the word IDs?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7451ebc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbc tokens:\n",
      "<bound method BatchEncoding.tokens of {'input_ids': [101, 5615, 1116, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}>\n",
      "\n",
      "bbc word IDs:\n",
      "[None, 0, 0, None]\n",
      "bert-base-cased treats '81s' as ONE word with TWO tokens:\n",
      "81\n",
      "s\n",
      "\n",
      "\n",
      "rb tokens:\n",
      "<bound method BatchEncoding.tokens of {'input_ids': [0, 6668, 29, 2], 'attention_mask': [1, 1, 1, 1]}>\n",
      "\n",
      "rb word IDs:\n",
      "[None, 0, 1, None]\n",
      "roberta-base treats '81s' as TWO words with ONE token each:\n",
      "81\n",
      "s\n"
     ]
    }
   ],
   "source": [
    "# Trying it out\n",
    "example_81s = \"81s\"\n",
    "## bert-base-cased (bbc)\n",
    "tokenizer_bbc = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "encoding_bbc = tokenizer_bbc(example_81s)\n",
    "print(f\"bbc tokens:\\n{encoding_bbc.tokens}\\n\\nbbc word IDs:\\n{encoding_bbc.word_ids()}\")\n",
    "start_bbc1, end_bbc1 = encoding_bbc.token_to_chars(1)\n",
    "start_bbc2, end_bbc2 = encoding_bbc.token_to_chars(2)\n",
    "bbc_str = f\"bert-base-cased treats '{example_81s}' as ONE word with TWO tokens:\"\n",
    "print(f\"{bbc_str}\\n{example_81s[start_bbc1:end_bbc1]}\\n{example_81s[start_bbc2:end_bbc2]}\")\n",
    "## roberta-base (rb)\n",
    "tokenizer_rb = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "encoding_rb = tokenizer_rb(example_81s)\n",
    "print(f\"\\n\\nrb tokens:\\n{encoding_rb.tokens}\\n\\nrb word IDs:\\n{encoding_rb.word_ids()}\")\n",
    "start_rb1, end_rb1 = encoding_rb.token_to_chars(1)\n",
    "start_rb2, end_rb2 = encoding_rb.token_to_chars(2)\n",
    "rb_str = f\"roberta-base treats '{example_81s}' as TWO words with ONE token each:\"\n",
    "print(f\"{rb_str}\\n{example_81s[start_rb1:end_rb1]}\\n{example_81s[start_rb2:end_rb2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c94149",
   "metadata": {},
   "source": [
    "Similarly, there is a `sentence_ids()` method that we can use to map a token to the sentence it came from (though in this case, the `token_type_ids` returned by the tokenizer can give us the same information).\n",
    "\n",
    "Lastly, we can map any word or token to characters in the original text, and vice versa, via the `word_to_chars()` or `token_to_chars()` and `char_to_word()` or `char_to_token()` methods. For instance, the `word_ids()` method told us that `##yl` is part of the word at index 3, but which word is it in the sentence? We can find out like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7673fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sylvain'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, end = encoding.word_to_chars(3)\n",
    "example[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c745082e",
   "metadata": {},
   "source": [
    "As we mentioned previously, this is all powered by the fact the fast tokenizer keeps track of the span of text each token comes from in a list of *offsets*. To illustrate their use, next we'll show you how to replicate the results of the `token-classification` pipeline manually.\n",
    "\n",
    "> ‚úèÔ∏è Try it out! <font color=\"darkgreen\">Create your own example text and see if you can understand which tokens are associated with which word ID, and also how to extract the character spans for a single word. For bonus points, try using two sentences as input and see if the sentence IDs make sense to you.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0a32542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace transformers rule NLP!\n",
      "[0, 0, 0, 0, 1, 1, 2, 3, 3, 4]\n",
      "word IDs   tokens\n",
      "\n",
      "0\t   Hu\n",
      "0\t   gging\n",
      "0\t   F\n",
      "0\t   ace\n",
      "word: HuggingFace \n",
      "\n",
      "1\t   transform\n",
      "1\t   ers\n",
      "word: transformers \n",
      "\n",
      "2\t   rule\n",
      "word: rule \n",
      "\n",
      "3\t   NL\n",
      "3\t   P\n",
      "word: NLP\n",
      "\n",
      "4\t   !\n",
      "word: !\n",
      "=> HuggingFace transformers rule NLP!\n",
      "\n",
      "\n",
      "And stable baselines rule RL.\n",
      "[0, 1, 2, 2, 3, 4, 4, 5]\n",
      "word IDs   tokens\n",
      "\n",
      "0\t   And\n",
      "word: And \n",
      "\n",
      "1\t   stable\n",
      "word: stable \n",
      "\n",
      "2\t   base\n",
      "2\t   lines\n",
      "word: baselines \n",
      "\n",
      "3\t   rule\n",
      "word: rule \n",
      "\n",
      "4\t   R\n",
      "4\t   L\n",
      "word: RL\n",
      "\n",
      "5\t   .\n",
      "word: .\n",
      "=> And stable baselines rule RL.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. DONE: add loop over tokenized inputs\n",
    "# 2. handle final word\n",
    "# 3. confirm loop works\n",
    "trytext_1 = \"HuggingFace transformers rule NLP!\"\n",
    "trytext_2 = \"And stable baselines rule RL.\"\n",
    "trytext_list = [trytext_1]\n",
    "trytext_list = [trytext_2]\n",
    "trytext_list = [trytext_1, trytext_2]\n",
    "for trytext in trytext_list:\n",
    "    print(trytext)\n",
    "    try_encoding_bbc = tokenizer_bbc(trytext)\n",
    "    normal_token_ids = try_encoding_bbc[\"input_ids\"][1:-1]\n",
    "    normal_word_ids = try_encoding_bbc.word_ids()[1:-1]\n",
    "    print(normal_word_ids)\n",
    "    # word IDs and spans\n",
    "    current_word_id = -1\n",
    "    word_spans = []\n",
    "    word_span = []\n",
    "    # loop and output\n",
    "    print(\"word IDs   tokens\\n\")\n",
    "    for i in range(len(normal_token_ids)):\n",
    "        word_id_i = normal_word_ids[i]\n",
    "        start_end_i = try_encoding_bbc.token_to_chars(i+1)\n",
    "        if word_id_i>current_word_id:\n",
    "            end_i = start_end_i[1]\n",
    "            word_span.append(start_end_i[1])\n",
    "            if word_spans!=[]:\n",
    "                prev_word_end = start_end_i[0]\n",
    "                print(f\"word: {trytext[start_i:prev_word_end]}\\n\")\n",
    "            word_spans.append(word_span)\n",
    "            word_span = [start_end_i[0]]\n",
    "            current_word_id = word_id_i\n",
    "            start_i = start_end_i[0]\n",
    "        word_span.append(start_end_i[1])\n",
    "        print(f\"{word_id_i}\\t   {trytext[start_end_i[0]:start_end_i[1]]}\")\n",
    "        prev_start_end_i = start_end_i\n",
    "    #\n",
    "    prev_word_end = start_end_i[1]\n",
    "    print(f\"word: {trytext[start_i:prev_word_end]}\\n=> {trytext}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c014057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word IDs   tokens\n",
      "\n",
      "10\n",
      "0\t   Hu\n",
      "0\t   gging\n",
      "0\t   F\n",
      "0\t   ace\n",
      "word: HuggingFace \n",
      "\n",
      "1\t   transform\n",
      "1\t   ers\n",
      "word: transformers \n",
      "\n",
      "2\t   rule\n",
      "word: rule \n",
      "\n",
      "3\t   NL\n",
      "3\t   P\n",
      "word: NLP\n",
      "\n",
      "4\t   !\n",
      "33 34\n",
      "word: !\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(33, 34)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trytext_1 = \"HuggingFace transformers rule NLP!\"\n",
    "try_encoding_bbc_1 = tokenizer_bbc(trytext_1)\n",
    "normal_token_ids = try_encoding_bbc_1[\"input_ids\"][1:-1]\n",
    "normal_word_ids = try_encoding_bbc_1.word_ids()[1:-1]\n",
    "# word IDs and spans\n",
    "current_word_id = -1\n",
    "word_spans = []\n",
    "word_span = []\n",
    "# loop and output\n",
    "print(\"word IDs   tokens\\n\")\n",
    "print(len(normal_token_ids))\n",
    "for i in range(len(normal_token_ids)):\n",
    "    #print(i, normal_word_ids)\n",
    "    word_id_i = normal_word_ids[i]\n",
    "    start_end_i = try_encoding_bbc_1.token_to_chars(i+1)\n",
    "    if word_id_i>current_word_id:\n",
    "        end_i = start_end_i[1]\n",
    "        word_span.append(start_end_i[1])\n",
    "        if word_spans!=[]:\n",
    "            prev_word_end = start_end_i[0]\n",
    "            print(f\"word: {trytext_1[start_i:prev_word_end]}\\n\")\n",
    "        word_spans.append(word_span)\n",
    "        word_span = [start_end_i[0]]\n",
    "        current_word_id = word_id_i\n",
    "        start_i = start_end_i[0]\n",
    "    word_span.append(start_end_i[1])\n",
    "    print(f\"{word_id_i}\\t   {trytext_1[start_end_i[0]:start_end_i[1]]}\")\n",
    "    prev_start_end_i = start_end_i\n",
    "#\n",
    "prev_word_end = start_end_i[1]\n",
    "print(start_i, prev_word_end)\n",
    "print(f\"word: {trytext_1[start_i:prev_word_end]}\\n\")\n",
    "start_i, prev_word_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "feef9bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "HuggingFace transformers rule NLP!\n",
      "1\n",
      "And stable baselines rule RL.\n"
     ]
    }
   ],
   "source": [
    "trytext_1 = \"HuggingFace transformers rule NLP!\"\n",
    "trytext_2 = \"And stable baselines rule RL.\"\n",
    "trytext_list = [trytext_1, trytext_2]\n",
    "for i, trytext in enumerate(trytext_list):\n",
    "    print(i)\n",
    "    print(trytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18670d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word IDs   tokens\n",
      "\n",
      "0\t   Hu\n",
      "0\t   gging\n",
      "0\t   F\n",
      "0\t   ace\n",
      "word: HuggingFace \n",
      "\n",
      "1\t   transform\n",
      "1\t   ers\n",
      "word: transformers \n",
      "\n",
      "2\t   rule\n",
      "word: rule \n",
      "\n",
      "3\t   NL\n",
      "3\t   P\n",
      "word: NLP\n",
      "\n",
      "4\t   !\n",
      "word: !\n",
      "\n",
      "word IDs   tokens\n",
      "\n",
      "0\t   Hu\n",
      "word: Hu\n",
      "\n",
      "1\t   gging\n",
      "word: gging\n",
      "\n",
      "2\t   F\n",
      "2\t   ace\n",
      "word: Face \n",
      "\n",
      "3\t   transform\n",
      "word: transform\n",
      "\n",
      "4\t   ers\n",
      "4\t   rule\n",
      "word: ers rule \n",
      "\n",
      "5\t   NL\n",
      "word: NL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. DONE: add loop over tokenized inputs\n",
    "# 2. handle final word\n",
    "# 3. confirm loop works\n",
    "trytext_1 = \"HuggingFace transformers rule NLP!\"\n",
    "trytext_2 = \"And stable baselines rule RL.\"\n",
    "trytext_list = [trytext_1]\n",
    "trytext_list = [trytext_2]\n",
    "trytext_list = [trytext_1, trytext_2]\n",
    "for trytext in trytext_list:\n",
    "    try_encoding_bbc = tokenizer_bbc(trytext)\n",
    "    normal_token_ids = try_encoding_bbc[\"input_ids\"][1:-1]\n",
    "    #print(normal_token_ids)\n",
    "    normal_word_ids = try_encoding_bbc.word_ids()[1:-1]\n",
    "    #print(normal_word_ids)\n",
    "    # word IDs and spans\n",
    "    current_word_id = -1\n",
    "    word_spans = []\n",
    "    word_span = []\n",
    "    # loop and output\n",
    "    print(\"word IDs   tokens\\n\")\n",
    "    #print(len(normal_token_ids))\n",
    "    for i in range(len(normal_token_ids)):\n",
    "        #print(i, normal_word_ids)\n",
    "        word_id_i = normal_word_ids[i]\n",
    "        start_end_i = try_encoding_bbc_1.token_to_chars(i+1)\n",
    "        if word_id_i>current_word_id:\n",
    "            end_i = start_end_i[1]\n",
    "            word_span.append(start_end_i[1])\n",
    "            if word_spans!=[]:\n",
    "                prev_word_end = start_end_i[0]\n",
    "                print(f\"word: {trytext_1[start_i:prev_word_end]}\\n\")\n",
    "            word_spans.append(word_span)\n",
    "            word_span = [start_end_i[0]]\n",
    "            current_word_id = word_id_i\n",
    "            start_i = start_end_i[0]\n",
    "        word_span.append(start_end_i[1])\n",
    "        print(f\"{word_id_i}\\t   {trytext_1[start_end_i[0]:start_end_i[1]]}\")\n",
    "        prev_start_end_i = start_end_i\n",
    "    #\n",
    "    prev_word_end = start_end_i[1]\n",
    "    print(f\"word: {trytext_1[start_i:prev_word_end]}\\n\")\n",
    "    #print(start_i, prev_word_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a511e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 1, 2, 3, 3, 4]\n",
      "word IDs   tokens\n",
      "\n",
      "0 [0, 0, 0, 0, 1, 1, 2, 3, 3, 4]\n",
      "0\t   Hu\n",
      "1 [0, 0, 0, 0, 1, 1, 2, 3, 3, 4]\n",
      "0\t   gging\n",
      "2 [0, 0, 0, 0, 1, 1, 2, 3, 3, 4]\n",
      "0\t   F\n",
      "3 [0, 0, 0, 0, 1, 1, 2, 3, 3, 4]\n",
      "0\t   ace\n",
      "4 [0, 0, 0, 0, 1, 1, 2, 3, 3, 4]\n",
      "word: HuggingFace \n",
      "\n",
      "1\t   transform\n",
      "5 [0, 0, 0, 0, 1, 1, 2, 3, 3, 4]\n",
      "1\t   ers\n",
      "6 [0, 0, 0, 0, 1, 1, 2, 3, 3, 4]\n",
      "word: transformers \n",
      "\n",
      "2\t   rule\n",
      "7 [0, 0, 0, 0, 1, 1, 2, 3, 3, 4]\n",
      "word: rule \n",
      "\n",
      "3\t   NL\n",
      "8 [0, 0, 0, 0, 1, 1, 2, 3, 3, 4]\n",
      "3\t   P\n",
      "9 [0, 0, 0, 0, 1, 1, 2, 3, 3, 4]\n",
      "word: NLP\n",
      "\n",
      "4\t   !\n",
      "10 [0, 0, 0, 0, 1, 1, 2, 3, 3, 4]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-805240f46e64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_token_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormal_word_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mword_id_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormal_word_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mstart_end_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtry_encoding_bbc_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_to_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword_id_i\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mcurrent_word_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# 1. DONE: add loop over tokenized inputs\n",
    "# 2. handle final word\n",
    "# 3. confirm loop works\n",
    "trytext_1 = \"HuggingFace transformers rule NLP!\"\n",
    "trytext_2 = \"And stable baselines rule RL.\"\n",
    "trytext_list = [trytext_1]\n",
    "#trytext_list = [trytext_2]\n",
    "#trytext_list = [trytext_1, trytext_2]\n",
    "for trytext in trytext_list:\n",
    "    try_encoding_bbc = tokenizer_bbc(trytext)\n",
    "    normal_token_ids = try_encoding_bbc[\"input_ids\"]\n",
    "    #print(normal_token_ids)\n",
    "    normal_word_ids = try_encoding_bbc.word_ids()[1:-1]\n",
    "    print(normal_word_ids)\n",
    "    # word IDs and spans\n",
    "    current_word_id = -1\n",
    "    word_spans = []\n",
    "    word_span = []\n",
    "    # loop and output\n",
    "    print(\"word IDs   tokens\\n\")\n",
    "    for i in range(len(normal_token_ids)):\n",
    "        print(i, normal_word_ids)\n",
    "        word_id_i = normal_word_ids[i]\n",
    "        start_end_i = try_encoding_bbc_1.token_to_chars(i+1)\n",
    "        if word_id_i>current_word_id:\n",
    "            end_i = start_end_i[1]\n",
    "            word_span.append(start_end_i[1])\n",
    "            if word_spans!=[]:\n",
    "                prev_word_end = start_end_i[0]\n",
    "                print(f\"word: {trytext_1[start_i:prev_word_end]}\\n\")\n",
    "            word_spans.append(word_span)\n",
    "            word_span = [start_end_i[0]]\n",
    "            current_word_id = word_id_i\n",
    "            start_i = start_end_i[0]\n",
    "        word_span.append(start_end_i[1])\n",
    "        print(f\"{word_id_i}\\t   {trytext_1[start_end_i[0]:start_end_i[1]]}\")\n",
    "        prev_start_end_i = start_end_i\n",
    "    #\n",
    "    prev_word_end = start_end_i[1]\n",
    "    print(len(trytext))\n",
    "    print(f\"word: {trytext_1[start_i:len(trytext)]}\\n\")\n",
    "    print(start_i, prev_word_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3ca28ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word IDs   tokens\n",
      "\n",
      "0\t   Hu\n",
      "0\t   gging\n",
      "0\t   F\n",
      "0\t   ace\n",
      "word: HuggingFace \n",
      "\n",
      "1\t   transform\n",
      "1\t   ers\n",
      "word: transformers \n",
      "\n",
      "2\t   rule\n",
      "word: rule \n",
      "\n",
      "3\t   NL\n",
      "3\t   P\n",
      "word: NLP\n",
      "\n",
      "4\t   !\n",
      "word: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(33, 33)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trytext_1 = \"HuggingFace transformers rule NLP!\"\n",
    "try_encoding_bbc_1 = tokenizer_bbc(trytext_1)\n",
    "normal_token_ids = try_encoding_bbc_1[\"input_ids\"][1:-1]\n",
    "normal_word_ids = try_encoding_bbc_1.word_ids()[1:-1]\n",
    "# word IDs and spans\n",
    "current_word_id = -1\n",
    "word_spans = []\n",
    "word_span = []\n",
    "# loop and output\n",
    "print(\"word IDs   tokens\\n\")\n",
    "for i in range(len(normal_token_ids)):\n",
    "    word_id_i = normal_word_ids[i]\n",
    "    start_end_i = try_encoding_bbc_1.token_to_chars(i+1)\n",
    "    if word_id_i>current_word_id:\n",
    "        end_i = start_end_i[1]\n",
    "        word_span.append(start_end_i[1])\n",
    "        if word_spans!=[]:\n",
    "            prev_word_end = start_end_i[0]\n",
    "            print(f\"word: {trytext_1[start_i:prev_word_end]}\\n\")\n",
    "        word_spans.append(word_span)\n",
    "        word_span = [start_end_i[0]]\n",
    "        current_word_id = word_id_i\n",
    "        start_i = start_end_i[0]\n",
    "    word_span.append(start_end_i[1])\n",
    "    print(f\"{word_id_i}\\t   {trytext_1[start_end_i[0]:start_end_i[1]]}\")\n",
    "    prev_start_end_i = start_end_i\n",
    "#\n",
    "prev_word_end = start_end_i[0]\n",
    "print(f\"word: {trytext_1[start_i:prev_word_end]}\\n\")\n",
    "start_i, prev_word_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d00902c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 20164, 10932, 2271, 7954, 11303, 1468, 3013, 21239, 2101, 106, 102]\n",
      "\n",
      "word IDs   tokens\n",
      "\n",
      "0\t   Hu\n",
      "0\t   gging\n",
      "0\t   F\n",
      "0\t   ace\n",
      "word: HuggingFace \n",
      "\n",
      "1\t   transform\n",
      "1\t   ers\n",
      "word: transformers \n",
      "\n",
      "2\t   rule\n",
      "word: rule \n",
      "\n",
      "3\t   NL\n",
      "3\t   P\n",
      "word: NLP\n",
      "\n",
      "4\t   !\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-e8325c59161b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nword IDs   tokens\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_token_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mword_id_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormal_word_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mstart_end_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtry_encoding_bbc_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_to_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mword_id_i\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mcurrent_word_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#print(try_encoding_bbc_1) # after tokenization, get number of lists and loop over lists\n",
    "normal_token_ids = try_encoding_bbc_1[\"input_ids\"]\n",
    "print(normal_token_ids)\n",
    "normal_word_ids = try_encoding_bbc_1.word_ids()[1:-1]\n",
    "# word IDs and spans\n",
    "current_word_id = -1\n",
    "word_spans = []\n",
    "word_span = []\n",
    "# loop and output\n",
    "print(\"\\nword IDs   tokens\\n\")\n",
    "for i in range(len(normal_token_ids)):\n",
    "    word_id_i = normal_word_ids[i]\n",
    "    start_end_i = try_encoding_bbc_1.token_to_chars(i+1)\n",
    "    if word_id_i>current_word_id:\n",
    "        end_i = start_end_i[1]\n",
    "        word_span.append(start_end_i[1])\n",
    "        if word_spans!=[]:\n",
    "            prev_word_end = start_end_i[0]\n",
    "            print(f\"word: {trytext_1[start_i:prev_word_end]}\\n\")\n",
    "        word_spans.append(word_span)\n",
    "        word_span = [start_end_i[0]]\n",
    "        current_word_id = word_id_i\n",
    "        start_i = start_end_i[0]\n",
    "    word_span.append(start_end_i[1])\n",
    "    print(f\"{word_id_i}\\t   {trytext_1[start_end_i[0]:start_end_i[1]]}\")\n",
    "    prev_start_end_i = start_end_i\n",
    "# \n",
    "prev_word_end = start_end_i[0]\n",
    "print(f\"word: {trytext_1[start_i:prev_word_end]}\\n\")\n",
    "start_i, prev_word_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69afe8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4898d4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 20164, 10932, 2271, 7954, 11303, 1468, 3013, 21239, 2101, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[101, 20164, 10932, 2271, 7954, 11303, 1468, 3013, 21239, 2101, 106, 102]\n",
      "\n",
      "word IDs   tokens\n",
      "\n",
      "0\t   Hu\n",
      "0\t   gging\n",
      "0\t   F\n",
      "0\t   ace\n",
      "word: HuggingFace \n",
      "\n",
      "1\t   transform\n",
      "1\t   ers\n",
      "word: transformers \n",
      "\n",
      "2\t   rule\n",
      "word: rule \n",
      "\n",
      "3\t   NL\n",
      "3\t   P\n",
      "word: NLP\n",
      "\n",
      "4\t   !\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-73d30797eae3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nword IDs   tokens\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_token_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mword_id_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormal_word_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mstart_end_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtry_encoding_bbc_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_to_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mword_id_i\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mcurrent_word_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "trytext_1 = \"HuggingFace transformers rule NLP!\"\n",
    "trytext_2 = \"And stable baselines rule RL.\"\n",
    "trytext_list = [trytext_1]\n",
    "trytext_list = [trytext_2]\n",
    "#trytext_list = [trytext_1, trytext_2]\n",
    "try_encoding_bbc_1 = tokenizer_bbc(trytext)\n",
    "print(try_encoding_bbc_1)\n",
    "normal_token_ids = try_encoding_bbc_1[\"input_ids\"]#[1:-1]\n",
    "print(normal_token_ids)\n",
    "normal_word_ids = try_encoding_bbc_1.word_ids()[1:-1]\n",
    "# word IDs and spans\n",
    "current_word_id = -1\n",
    "word_spans = []\n",
    "word_span = []\n",
    "# loop and output\n",
    "print(\"\\nword IDs   tokens\\n\")\n",
    "for i in range(len(normal_token_ids)):\n",
    "    word_id_i = normal_word_ids[i]\n",
    "    start_end_i = try_encoding_bbc_1.token_to_chars(i+1)\n",
    "    if word_id_i>current_word_id:\n",
    "        end_i = start_end_i[1]\n",
    "        word_span.append(start_end_i[1])\n",
    "        if word_spans!=[]:\n",
    "            prev_word_end = start_end_i[0]\n",
    "            print(f\"word: {trytext_1[start_i:prev_word_end]}\\n\")\n",
    "        word_spans.append(word_span)\n",
    "        word_span = [start_end_i[0]]\n",
    "        current_word_id = word_id_i\n",
    "        start_i = start_end_i[0]\n",
    "    word_span.append(start_end_i[1])\n",
    "    print(f\"{word_id_i}\\t   {trytext_1[start_end_i[0]:start_end_i[1]]}\")\n",
    "    prev_start_end_i = start_end_i\n",
    "# \n",
    "prev_word_end = start_end_i[0]\n",
    "print(f\"word: {trytext_1[start_i:prev_word_end]}\\n\")\n",
    "start_i, prev_word_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5683a0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 20164, 10932, 2271, 7954, 11303, 1468, 3013, 21239, 2101, 106, 102], [101, 1262, 6111, 2259, 10443, 3013, 155, 2162, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "[[101, 20164, 10932, 2271, 7954, 11303, 1468, 3013, 21239, 2101, 106, 102], [101, 1262, 6111, 2259, 10443, 3013, 155, 2162, 119, 102]]\n",
      "\n",
      "word IDs   tokens\n",
      "\n",
      "0\t   Hu\n",
      "0\t   gging\n",
      "word: Hu\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trytext_1 = \"HuggingFace transformers rule NLP!\"\n",
    "trytext_2 = \"And stable baselines rule RL.\"\n",
    "trytext = [trytext_1]\n",
    "trytext = [trytext_2]\n",
    "trytext = [trytext_1, trytext_2]\n",
    "try_encoding_bbc_1 = tokenizer_bbc(trytext)\n",
    "print(try_encoding_bbc_1)\n",
    "normal_token_ids = try_encoding_bbc_1[\"input_ids\"]#[1:-1]\n",
    "print(normal_token_ids)\n",
    "normal_word_ids = try_encoding_bbc_1.word_ids()[1:-1]\n",
    "# word IDs and spans\n",
    "current_word_id = -1\n",
    "word_spans = []\n",
    "word_span = []\n",
    "# loop and output\n",
    "print(\"\\nword IDs   tokens\\n\")\n",
    "for i in range(len(normal_token_ids)):\n",
    "    word_id_i = normal_word_ids[i]\n",
    "    start_end_i = try_encoding_bbc_1.token_to_chars(i+1)\n",
    "    if word_id_i>current_word_id:\n",
    "        end_i = start_end_i[1]\n",
    "        word_span.append(start_end_i[1])\n",
    "        if word_spans!=[]:\n",
    "            prev_word_end = start_end_i[0]\n",
    "            print(f\"word: {trytext_1[start_i:prev_word_end]}\\n\")\n",
    "        word_spans.append(word_span)\n",
    "        word_span = [start_end_i[0]]\n",
    "        current_word_id = word_id_i\n",
    "        start_i = start_end_i[0]\n",
    "    word_span.append(start_end_i[1])\n",
    "    print(f\"{word_id_i}\\t   {trytext_1[start_end_i[0]:start_end_i[1]]}\")\n",
    "    prev_start_end_i = start_end_i\n",
    "# \n",
    "prev_word_end = start_end_i[0]\n",
    "print(f\"word: {trytext_1[start_i:prev_word_end]}\\n\")\n",
    "start_i, prev_word_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c3dec72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word IDs   tokens\n",
      "\n",
      "0\t   Hu\n",
      "0\t   gging\n",
      "0\t   F\n",
      "0\t   ace\n",
      "word: HuggingFace \n",
      "\n",
      "1\t   transform\n",
      "1\t   ers\n",
      "word: transformers \n",
      "\n",
      "2\t   rule\n",
      "word: rule \n",
      "\n",
      "3\t   NL\n",
      "3\t   P\n",
      "word: NLP\n",
      "\n",
      "4\t   !\n",
      "word: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(33, 33)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trytext_1 = \"HuggingFace transformers rule NLP!\"\n",
    "try_encoding_bbc_1 = tokenizer_bbc(trytext_1)\n",
    "normal_token_ids = try_encoding_bbc_1[\"input_ids\"][1:-1]\n",
    "normal_word_ids = try_encoding_bbc_1.word_ids()[1:-1]\n",
    "# word IDs and spans\n",
    "current_word_id = -1\n",
    "word_spans = []\n",
    "word_span = []\n",
    "# loop and output\n",
    "print(\"word IDs   tokens\\n\")\n",
    "for i in range(len(normal_token_ids)):\n",
    "    word_id_i = normal_word_ids[i]\n",
    "    start_end_i = try_encoding_bbc_1.token_to_chars(i+1)\n",
    "    if word_id_i>current_word_id:\n",
    "        end_i = start_end_i[1]\n",
    "        word_span.append(start_end_i[1])\n",
    "        if word_spans!=[]:\n",
    "            prev_word_end = start_end_i[0]\n",
    "            print(f\"word: {trytext_1[start_i:prev_word_end]}\\n\")\n",
    "        word_spans.append(word_span)\n",
    "        word_span = [start_end_i[0]]\n",
    "        current_word_id = word_id_i\n",
    "        start_i = start_end_i[0]\n",
    "    word_span.append(start_end_i[1])\n",
    "    print(f\"{word_id_i}\\t   {trytext_1[start_end_i[0]:start_end_i[1]]}\")\n",
    "    prev_start_end_i = start_end_i\n",
    "#\n",
    "prev_word_end = start_end_i[0]\n",
    "print(f\"word: {trytext_1[start_i:prev_word_end]}\\n\")\n",
    "start_i, prev_word_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b89bafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word IDs   tokens\n",
      "\n",
      "0\t   Hu\n",
      "0\t   gging\n",
      "0\t   F\n",
      "0\t   ace\n",
      "word: HuggingFace \n",
      "\n",
      "1\t   transform\n",
      "1\t   ers\n",
      "word: transformers \n",
      "\n",
      "2\t   rule\n",
      "word: rule \n",
      "\n",
      "3\t   NL\n",
      "3\t   P\n",
      "word: NLP\n",
      "\n",
      "4\t   !\n",
      "word: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(33, 33)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trytext_1 = \"HuggingFace transformers rule NLP!\"\n",
    "try_encoding_bbc_1 = tokenizer_bbc(trytext_1)\n",
    "normal_token_ids = try_encoding_bbc_1[\"input_ids\"][1:-1]\n",
    "normal_word_ids = try_encoding_bbc_1.word_ids()[1:-1]\n",
    "# word IDs and spans\n",
    "current_word_id = -1\n",
    "word_spans = []\n",
    "word_span = []\n",
    "# loop and output\n",
    "print(\"word IDs   tokens\\n\")\n",
    "for i in range(len(normal_token_ids)):\n",
    "    word_id_i = normal_word_ids[i]\n",
    "    start_end_i = try_encoding_bbc_1.token_to_chars(i+1)\n",
    "    if word_id_i>current_word_id:\n",
    "        end_i = start_end_i[1]\n",
    "        word_span.append(start_end_i[1])\n",
    "        if word_spans!=[]:\n",
    "            prev_word_end = start_end_i[0]\n",
    "            print(f\"word: {trytext_1[start_i:prev_word_end]}\\n\")\n",
    "        word_spans.append(word_span)\n",
    "        word_span = [start_end_i[0]]\n",
    "        current_word_id = word_id_i\n",
    "        start_i = start_end_i[0]\n",
    "    word_span.append(start_end_i[1])\n",
    "    print(f\"{word_id_i}\\t   {trytext_1[start_end_i[0]:start_end_i[1]]}\")\n",
    "    prev_start_end_i = start_end_i\n",
    "#\n",
    "prev_word_end = start_end_i[0]\n",
    "print(f\"word: {trytext_1[start_i:prev_word_end]}\\n\")\n",
    "start_i, prev_word_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14882df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1262, 6111, 2259, 10443, 3013, 155, 2162, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "word IDs   tokens\n",
      "\n",
      "word: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(33, 33)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trytext_1 = \"HuggingFace transformers rule NLP!\"\n",
    "trytext_2 = \"And stable baselines rule RL.\"\n",
    "trytext = [trytext_1]\n",
    "trytext = [trytext_2]\n",
    "#trytext = [trytext_1, trytext_2]\n",
    "try_encoding_bbc_1 = tokenizer_bbc(trytext)\n",
    "print(try_encoding_bbc_1)\n",
    "normal_token_ids = try_encoding_bbc_1[\"input_ids\"][1:-1]\n",
    "normal_word_ids = try_encoding_bbc_1.word_ids()[1:-1]\n",
    "# word IDs and spans\n",
    "current_word_id = -1\n",
    "word_spans = []\n",
    "word_span = []\n",
    "# loop and output\n",
    "print(\"word IDs   tokens\\n\")\n",
    "for i in range(len(normal_token_ids)):\n",
    "    word_id_i = normal_word_ids[i]\n",
    "    start_end_i = try_encoding_bbc_1.token_to_chars(i+1)\n",
    "    if word_id_i>current_word_id:\n",
    "        end_i = start_end_i[1]\n",
    "        word_span.append(start_end_i[1])\n",
    "        if word_spans!=[]:\n",
    "            prev_word_end = start_end_i[0]\n",
    "            print(f\"word: {trytext_1[start_i:prev_word_end]}\\n\")\n",
    "        word_spans.append(word_span)\n",
    "        word_span = [start_end_i[0]]\n",
    "        current_word_id = word_id_i\n",
    "        start_i = start_end_i[0]\n",
    "    word_span.append(start_end_i[1])\n",
    "    print(f\"{word_id_i}\\t   {trytext_1[start_end_i[0]:start_end_i[1]]}\")\n",
    "    prev_start_end_i = start_end_i\n",
    "# \n",
    "prev_word_end = start_end_i[0]\n",
    "print(f\"word: {trytext_1[start_i:prev_word_end]}\\n\")\n",
    "start_i, prev_word_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19e2b2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word IDs   tokens\n",
      "\n",
      "0\t   Hu\n",
      "0\t   gging\n",
      "0\t   F\n",
      "0\t   ace\n",
      "word: HuggingFace \n",
      "\n",
      "1\t   transform\n",
      "1\t   ers\n",
      "word: transformers \n",
      "\n",
      "2\t   rule\n",
      "word: rule \n",
      "\n",
      "3\t   NL\n",
      "3\t   P\n",
      "word: NLP\n",
      "\n",
      "4\t   !\n",
      "word: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(33, 33)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trytext_1 = \"HuggingFace transformers rule NLP!\"\n",
    "try_encoding_bbc_1 = tokenizer_bbc(trytext_1)\n",
    "normal_token_ids = try_encoding_bbc_1[\"input_ids\"][1:-1]\n",
    "normal_word_ids = try_encoding_bbc_1.word_ids()[1:-1]\n",
    "# word IDs and spans\n",
    "current_word_id = -1\n",
    "word_spans = []\n",
    "word_span = []\n",
    "# loop and output\n",
    "print(\"word IDs   tokens\\n\")\n",
    "for i in range(len(normal_token_ids)):\n",
    "    word_id_i = normal_word_ids[i]\n",
    "    start_end_i = try_encoding_bbc_1.token_to_chars(i+1)\n",
    "    if word_id_i>current_word_id:\n",
    "        end_i = start_end_i[1]\n",
    "        word_span.append(start_end_i[1])\n",
    "        if word_spans!=[]:\n",
    "            prev_word_end = start_end_i[0]\n",
    "            print(f\"word: {trytext_1[start_i:prev_word_end]}\\n\")\n",
    "        word_spans.append(word_span)\n",
    "        word_span = [start_end_i[0]]\n",
    "        current_word_id = word_id_i\n",
    "        start_i = start_end_i[0]\n",
    "    word_span.append(start_end_i[1])\n",
    "    print(f\"{word_id_i}\\t   {trytext_1[start_end_i[0]:start_end_i[1]]}\")\n",
    "    prev_start_end_i = start_end_i\n",
    "#\n",
    "prev_word_end = start_end_i[0]\n",
    "print(f\"word: {trytext_1[start_i:prev_word_end]}\\n\")\n",
    "start_i, prev_word_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6042fad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 33)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_i, prev_word_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b2c18e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word IDs   tokens\n",
      "0\t   Hu\n",
      "0\t   gging\n",
      "0\t   F\n",
      "0\t   ace\n",
      "word: HuggingFace\n",
      "1\t   transform\n",
      "1\t   ers\n",
      "word: transformers\n",
      "2\t   rule\n",
      "word: rule\n",
      "3\t   NL\n",
      "3\t   P\n",
      "word: NL\n",
      "4\t   !\n",
      "end of string\n"
     ]
    }
   ],
   "source": [
    "trytext_1 = \"HuggingFace transformers rule NLP!\"\n",
    "try_encoding_bbc_1 = tokenizer_bbc(trytext_1)\n",
    "normal_token_ids = try_encoding_bbc_1[\"input_ids\"][1:-1]\n",
    "normal_word_ids = try_encoding_bbc_1.word_ids()[1:-1]\n",
    "# word IDs and spans\n",
    "current_word_id = -1\n",
    "word_spans = []\n",
    "word_span = []\n",
    "# loop and output\n",
    "print(\"word IDs   tokens\")\n",
    "for i in range(len(normal_token_ids)): # loop over tokens\n",
    "    word_id_i = normal_word_ids[i]\n",
    "    start_end_i = try_encoding_bbc_1.token_to_chars(i+1)\n",
    "    #\n",
    "    if word_id_i>current_word_id:      # new word\n",
    "        end_i = start_end_i[1]\n",
    "        #print(start_end_i)\n",
    "        word_span.append(start_end_i[1])\n",
    "        # neu\n",
    "        if word_spans!=[]:\n",
    "            #print(f\"prev_end_i: {prev_start_end_i[1]}\")\n",
    "            prev_word_end = start_end_i[0]-1\n",
    "            #print(f\"end position of previous word is {prev_word_end}\")\n",
    "            print(f\"word: {trytext_1[start_i:prev_word_end]}\")\n",
    "        #\n",
    "        word_spans.append(word_span)\n",
    "        #print(f\"\\nword starts at {start_end_i[0]}\")\n",
    "        #print(f\"prev_end_i: {prev_start_end_i[1]}\")\n",
    "        word_span = [start_end_i[0]]\n",
    "        current_word_id = word_id_i\n",
    "        start_i = start_end_i[0]\n",
    "    word_span.append(start_end_i[1])\n",
    "    print(f\"{word_id_i}\\t   {trytext_1[start_end_i[0]:start_end_i[1]]}\")\n",
    "    prev_start_end_i = start_end_i\n",
    "print(\"end of string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c008dcc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33, 34]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c72c96df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NLP'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trytext_1[30:33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "84788b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word IDs   tokens\n",
      "CharSpan(start=0, end=2)\n",
      "word starts at 0\n",
      "0\t   Hu\n",
      "0\t   gging\n",
      "0\t   F\n",
      "0\t   ace\n",
      "CharSpan(start=12, end=21)\n",
      "word starts at 12\n",
      "1\t   transform\n",
      "1\t   ers\n",
      "CharSpan(start=25, end=29)\n",
      "word starts at 25\n",
      "2\t   rule\n",
      "CharSpan(start=30, end=32)\n",
      "word starts at 30\n",
      "3\t   NL\n",
      "3\t   P\n",
      "CharSpan(start=33, end=34)\n",
      "word starts at 33\n",
      "4\t   !\n",
      "end of string\n"
     ]
    }
   ],
   "source": [
    "trytext_1 = \"HuggingFace transformers rule NLP!\"\n",
    "try_encoding_bbc_1 = tokenizer_bbc(trytext_1)\n",
    "normal_token_ids = try_encoding_bbc_1[\"input_ids\"][1:-1]\n",
    "normal_word_ids = try_encoding_bbc_1.word_ids()[1:-1]\n",
    "# word IDs and spans\n",
    "current_word_id = -1\n",
    "word_spans = []\n",
    "word_span = []\n",
    "# loop and output\n",
    "print(\"word IDs   tokens\")\n",
    "for i in range(len(normal_token_ids)): # loop over tokens\n",
    "    word_id_i = normal_word_ids[i]\n",
    "    start_end_i = try_encoding_bbc_1.token_to_chars(i+1)\n",
    "    # start of new word => ...\n",
    "    # ... 1. finish word_span and append it ...\n",
    "    # ... 2. start new word_span ...\n",
    "    # ... 3. update current_word_id\n",
    "    if word_id_i>current_word_id:      # new word\n",
    "        end_i = start_end_i[1]\n",
    "        print(start_end_i)\n",
    "        word_span.append(start_end_i[1])\n",
    "        if word_spans!=[]:\n",
    "            print(f\"word ends at {start_end_i[0]}\")\n",
    "            word_spans.append(word_span)\n",
    "        print(f\"word starts at {start_end_i[0]}\")\n",
    "        word_span = [start_end_i[0]]\n",
    "        current_word_id = word_id_i\n",
    "        start_i = start_end_i[0]\n",
    "    word_span.append(start_end_i[1])\n",
    "    \n",
    "    print(f\"{word_id_i}\\t   {trytext_1[start_end_i[0]:start_end_i[1]]}\")\n",
    "print(\"end of string\")\n",
    "#########\n",
    "# for token_id in token_ids:\n",
    "#     get word_id\n",
    "#     get word_start from start_token_start\n",
    "#     get word_end from end_token_end # put this line where the next word_id is handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "008be23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a108622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token start: 0\ttoken end: 2\tword id: 0\n",
      "\n",
      "new word\n",
      "word start: 0\n",
      "token start: 2\ttoken end: 7\tword id: 0\n",
      "token start: 7\ttoken end: 8\tword id: 0\n",
      "token start: 8\ttoken end: 11\tword id: 0\n",
      "token start: 12\ttoken end: 21\tword id: 1\n",
      "previous word exists: its end position is 11\n",
      "previous word: HuggingFace\n",
      "\n",
      "new word\n",
      "word start: 12\n",
      "token start: 21\ttoken end: 24\tword id: 1\n",
      "token start: 25\ttoken end: 29\tword id: 2\n",
      "previous word exists: its end position is 24\n",
      "previous word: transformers\n",
      "\n",
      "new word\n",
      "word start: 25\n",
      "token start: 30\ttoken end: 32\tword id: 3\n",
      "previous word exists: its end position is 29\n",
      "previous word: rule\n",
      "\n",
      "new word\n",
      "word start: 30\n",
      "token start: 32\ttoken end: 33\tword id: 3\n",
      "token start: 33\ttoken end: 34\tword id: 4\n",
      "previous word exists: its end position is 32\n",
      "previous word: NL\n",
      "\n",
      "new word\n",
      "word start: 33\n",
      "last word exists: its end position is 33\n",
      "last word: P!\n"
     ]
    }
   ],
   "source": [
    "trytext_1 = \"HuggingFace transformers rule NLP!\"\n",
    "try_encoding_bbc_1 = tokenizer_bbc(trytext_1)\n",
    "normal_token_ids = try_encoding_bbc_1[\"input_ids\"][1:-1]\n",
    "normal_word_ids = try_encoding_bbc_1.word_ids()[1:-1]\n",
    "# word IDs and spans\n",
    "current_word_id = -1\n",
    "word_spans = []\n",
    "word_span = []\n",
    "# loop and output\n",
    "for i in range(len(normal_token_ids)): # loop over tokens\n",
    "    word_id_i = normal_word_ids[i]\n",
    "    start_end_i = try_encoding_bbc_1.token_to_chars(i+1)\n",
    "    print(f\"token start: {start_end_i[0]}\\ttoken end: {start_end_i[1]}\\tword id: {word_id_i}\")\n",
    "    if word_id_i > current_word_id:\n",
    "        # get end position of previous word (if there is a previous word)\n",
    "        if word_spans!=[]:\n",
    "            prev_word_end = start_end_i[0]-1\n",
    "            print(f\"previous word exists: its end position is {prev_word_end}\")\n",
    "            print(f\"previous word: {trytext_1[word_start:prev_word_end]}\")\n",
    "        word_spans.append(word_span)\n",
    "        # get start position of current word\n",
    "        print(\"\\nnew word\")\n",
    "        word_start = start_end_i[0]\n",
    "        print(f\"word start: {word_start}\")\n",
    "        current_word_id = word_id_i\n",
    "# last word\n",
    "last_word_end = start_end_i[0]\n",
    "print(f\"last word exists: its end position is {last_word_end}\")\n",
    "print(f\"last word: {trytext_1[word_start-1:last_word_end+2]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########\n",
    "#  H u g g i n g F a c  e     t  r  a  n  s  f  o  r  m  e  r  s     r  u  l  e     N  L  P  !\"\n",
    "# 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 \"\n",
    "# 0                      11 12                                  24 25          29 30    32 33 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6676af20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10db04db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token start: 0\ttoken end: 2\tword id: 0\n",
      "\n",
      "new word\n",
      "word start: 0\n",
      "token start: 2\ttoken end: 7\tword id: 0\n",
      "token start: 7\ttoken end: 8\tword id: 0\n",
      "token start: 8\ttoken end: 11\tword id: 0\n",
      "token start: 12\ttoken end: 21\tword id: 1\n",
      "previous word exists: its end position is 11\n",
      "previous word: HuggingFace\n",
      "\n",
      "new word\n",
      "word start: 12\n",
      "token start: 21\ttoken end: 24\tword id: 1\n",
      "token start: 25\ttoken end: 29\tword id: 2\n",
      "previous word exists: its end position is 24\n",
      "previous word: transformers\n",
      "\n",
      "new word\n",
      "word start: 25\n",
      "token start: 30\ttoken end: 32\tword id: 3\n",
      "previous word exists: its end position is 29\n",
      "previous word: rule\n",
      "\n",
      "new word\n",
      "word start: 30\n",
      "token start: 32\ttoken end: 33\tword id: 3\n",
      "token start: 33\ttoken end: 34\tword id: 4\n",
      "previous word exists: its end position is 32\n",
      "previous word: NL\n",
      "\n",
      "new word\n",
      "word start: 33\n"
     ]
    }
   ],
   "source": [
    "trytext_1 = \"HuggingFace transformers rule NLP!\"\n",
    "try_encoding_bbc_1 = tokenizer_bbc(trytext_1)\n",
    "normal_token_ids = try_encoding_bbc_1[\"input_ids\"][1:-1]\n",
    "normal_word_ids = try_encoding_bbc_1.word_ids()[1:-1]\n",
    "# word IDs and spans\n",
    "current_word_id = -1\n",
    "word_spans = []\n",
    "word_span = []\n",
    "# loop and output\n",
    "for i in range(len(normal_token_ids)): # loop over tokens\n",
    "    word_id_i = normal_word_ids[i]\n",
    "    start_end_i = try_encoding_bbc_1.token_to_chars(i+1)\n",
    "    print(f\"token start: {start_end_i[0]}\\ttoken end: {start_end_i[1]}\\tword id: {word_id_i}\")\n",
    "    if word_id_i > current_word_id:\n",
    "        # get end position of previous word (if there is a previous word)\n",
    "        if word_spans!=[]:\n",
    "            prev_word_end = start_end_i[0]-1\n",
    "            print(f\"previous word exists: its end position is {prev_word_end}\")\n",
    "            print(f\"previous word: {trytext_1[word_start:prev_word_end]}\")\n",
    "        word_spans.append(word_span)\n",
    "        # get start position of current word\n",
    "        print(\"\\nnew word\")\n",
    "        word_start = start_end_i[0]\n",
    "        print(f\"word start: {word_start}\")\n",
    "        current_word_id = word_id_i\n",
    "#########\n",
    "# for token_id in token_ids:\n",
    "#     get word_id\n",
    "#     get word_start from start_token_start\n",
    "#     get word_end from end_token_end # put this line where the next word_id is handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2760b200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_spans==[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "262b0730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"abcdefghij\"[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d97285be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word IDs   tokens\n",
      "CharSpan(start=0, end=2)\n",
      "word starts at 0\n",
      "0\t   Hu\n",
      "0\t   gging\n",
      "0\t   F\n",
      "0\t   ace\n",
      "CharSpan(start=12, end=21)\n",
      "word starts at 12\n",
      "1\t   transform\n",
      "1\t   ers\n",
      "CharSpan(start=25, end=29)\n",
      "word starts at 25\n",
      "2\t   rule\n",
      "CharSpan(start=30, end=32)\n",
      "word starts at 30\n",
      "3\t   NL\n",
      "3\t   P\n",
      "CharSpan(start=33, end=34)\n",
      "word starts at 33\n",
      "4\t   !\n",
      "word ends\n"
     ]
    }
   ],
   "source": [
    "trytext_1 = \"HuggingFace transformers rule NLP!\"\n",
    "try_encoding_bbc_1 = tokenizer_bbc(trytext_1)\n",
    "normal_token_ids = try_encoding_bbc_1[\"input_ids\"][1:-1]\n",
    "normal_word_ids = try_encoding_bbc_1.word_ids()[1:-1]\n",
    "# word IDs and spans\n",
    "current_word_id = -1\n",
    "word_spans = []\n",
    "word_span = []\n",
    "# loop and output\n",
    "print(\"word IDs   tokens\")\n",
    "for i in range(len(normal_token_ids)): # loop over tokens\n",
    "    word_id_i = normal_word_ids[i]\n",
    "    start_end_i = try_encoding_bbc_1.token_to_chars(i+1)\n",
    "    # start of new word => ...\n",
    "    # ... 1. finish word_span and append it ...\n",
    "    # ... 2. start new word_span ...\n",
    "    # ... 3. update current_word_id\n",
    "    if word_id_i>current_word_id:      # new word\n",
    "        end_i = start_end_i[1]\n",
    "        print(start_end_i)\n",
    "        word_span.append(start_end_i[1])\n",
    "        if word_spans!=[]:\n",
    "            print(f\"word ends at {start_end_i[0]}\")\n",
    "            word_spans.append(word_span)\n",
    "        print(f\"word starts at {start_end_i[0]}\")\n",
    "        word_span = [start_end_i[0]]\n",
    "        current_word_id = word_id_i\n",
    "        start_i = start_end_i[0]\n",
    "    word_span.append(start_end_i[1])\n",
    "    \n",
    "    print(f\"{word_id_i}\\t   {trytext_1[start_end_i[0]:start_end_i[1]]}\")\n",
    "print(\"word ends\")\n",
    "#########\n",
    "# for token_id in token_ids:\n",
    "#     get word_id\n",
    "#     get word_start from start_token_start\n",
    "#     get word_end from end_token_end # put this line where the next word_id is handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c273a81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word IDs   tokens\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "trytext_1 = \"HuggingFace transformers rule NLP!\"\n",
    "try_encoding_bbc_1 = tokenizer_bbc(trytext_1)\n",
    "normal_token_ids = try_encoding_bbc_1[\"input_ids\"][1:-1]\n",
    "normal_word_ids = try_encoding_bbc_1.word_ids()[1:-1]\n",
    "# word IDs and spans\n",
    "current_word_id = -1\n",
    "word_spans = []\n",
    "word_span = []\n",
    "# loop and output\n",
    "print(\"word IDs   tokens\")\n",
    "for i in range(len(normal_token_ids)):\n",
    "    word_id_i = normal_word_ids[i]\n",
    "    print(word_id_i)\n",
    "    start_end_i = try_encoding_bbc_1.token_to_chars(i+1)\n",
    "    if word_id_i>current_word_id:\n",
    "        end_i = start_end_i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d272745c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1, 1, 2, 3, 3, 4]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_encoding_bbc_1.word_ids()[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fa49ca4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_spans!=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2e89bf73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 20164, 10932, 2271, 7954, 11303, 1468, 3013, 21239, 2101, 106, 102]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trytext_1 = \"HuggingFace transformers rule NLP!\"\n",
    "try_encoding_bbc_1 = tokenizer_bbc(trytext_1)\n",
    "try_encoding_bbc_1[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f511074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] HuggingFace transformers rule NLP! [SEP]'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bbc.decode(try_encoding_bbc_1[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6b009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d71d9644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word ids:\n",
      "[None, 0, 0, 0, 0, 1, 1, 2, 3, 3, 4, None]\n"
     ]
    }
   ],
   "source": [
    "# Trying it out\n",
    "trytext_2 = \"And stable baselines rule RL.\"\n",
    "## \n",
    "trytext_1 = \"HuggingFace transformers rule NLP!\"\n",
    "## get word ids, associated tokens, and character spans (=> [start:end]) for each word => print these\n",
    "\n",
    "try_encoding_bbc_1 = tokenizer_bbc(trytext_1)\n",
    "trytext_1_word_ids = try_encoding_bbc_1.word_ids()\n",
    "print(f\"word ids:\\n{trytext_1_word_ids}\") # word ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e36cdb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_normal_tokens = len(trytext_1_word_ids) - 2\n",
    "n_normal_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5423a66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gin'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trytext_1[3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fa59f6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hu\n",
      "gging\n",
      "F\n",
      "ace\n",
      "transform\n",
      "ers\n",
      "rule\n",
      "NL\n",
      "P\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_normal_tokens):\n",
    "    start_end_i = try_encoding_bbc_1.token_to_chars(i+1)\n",
    "    print(trytext_1[start_end_i[0]:start_end_i[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "05f987a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'81'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_rb1, end_rb1 = encoding_rb.token_to_chars(1)\n",
    "example_81s[start_rb1:end_rb1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "47568618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharSpan(start=33, end=34)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_encoding_bbc_1.token_to_chars(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "42dfa72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "7\n",
      "[101, 20164, 10932, 2271, 7954, 11303, 1468, 3013, 21239, 2101, 106, 102]\n",
      "\n",
      "<bound method BatchEncoding.token_to_chars of {'input_ids': [101, 20164, 10932, 2271, 7954, 11303, 1468, 3013, 21239, 2101, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}>\n",
      "101\n",
      "20164\n",
      "10932\n",
      "2271\n",
      "7954\n",
      "11303\n",
      "1468\n",
      "3013\n",
      "21239\n",
      "2101\n",
      "106\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "# try_encoding_bbc_1.decode(try_1_word_ids[3])\n",
    "\n",
    "# try_encoding_bbc_1.token_to_chars(2) # works!\n",
    "\n",
    "for chars in try_encoding_bbc_1.token_to_chars(2): # [0:2]\n",
    "    print(chars)\n",
    "# input ids\n",
    "input_ids = try_encoding_bbc_1[\"input_ids\"]\n",
    "print(input_ids)\n",
    "print()\n",
    "print(try_encoding_bbc_1.token_to_chars)\n",
    "for input_id in input_ids:\n",
    "    print(input_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27d13e0",
   "metadata": {},
   "source": [
    "### Inside the `token-classification` pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
