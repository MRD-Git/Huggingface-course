{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9efa82d1",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/main_classes/output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bde7eb",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/debarshichanda/bert-multi-label-text-classification/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edbe7d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "#import emoji\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from bs4 import BeautifulSoup\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, AutoTokenizer, BertModel, BertConfig, AutoModel, AdamW\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea4bfab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                    Text       ID  anger  \\\n",
       " 0                         WHY THE FUCK IS BAYLESS ISOING  eezlygj      1   \n",
       " 1                            To make her feel threatened  ed7ypvh      0   \n",
       " 2                                 Dirty Southern Wankers  ed0bdzj      1   \n",
       " 3      OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...  edvnz26      0   \n",
       " 4      Yes I heard abt the f bombs! That has to be wh...  ee3b6wu      0   \n",
       " ...                                                  ...      ...    ...   \n",
       " 28422  Added you mate well I’ve just got the bow and ...  edsb738      0   \n",
       " 28423  Always thought that was funny but is it a refe...  ee7fdou      0   \n",
       " 28424  What are you talking about? Anything bad that ...  efgbhks      1   \n",
       " 28425            More like a baptism, with sexy results!  ed1naf8      0   \n",
       " 28426                                    Enjoy the ride!  eecwmbq      0   \n",
       " \n",
       "        fear  joy  sadness  surprise  \n",
       " 0         0    0        0         0  \n",
       " 1         1    0        0         0  \n",
       " 2         0    0        0         0  \n",
       " 3         0    0        0         1  \n",
       " 4         0    1        0         0  \n",
       " ...     ...  ...      ...       ...  \n",
       " 28422     0    1        0         0  \n",
       " 28423     0    0        0         1  \n",
       " 28424     0    0        0         0  \n",
       " 28425     0    1        0         0  \n",
       " 28426     0    1        0         0  \n",
       " \n",
       " [28427 rows x 7 columns],\n",
       "                                                    Text       ID  anger  fear  \\\n",
       " 0                  I've never been this sad in my life!  edcu99z      0     0   \n",
       " 1     He could have easily taken a real camera from ...  eepig6r      0     0   \n",
       " 2     Thank you for your vote of confidence, but we ...  eczm50f      0     0   \n",
       " 3     Wah Mum other people call me on my bullshit an...  ed4yr9r      1     0   \n",
       " 4     At least now [NAME] has more time to gain his ...  eekez9p      0     0   \n",
       " ...                                                 ...      ...    ...   ...   \n",
       " 3559  It's pretty dangerous when the state decides w...  edyrazk      0     1   \n",
       " 3560  I filed for divorce this morning. Hoping he mo...  edi2z3y      0     0   \n",
       " 3561  The last time it happened I just said, \"No\" an...  eewbqtx      1     0   \n",
       " 3562  I can’t stand this arrogant prick he’s no bett...  eefx57m      1     0   \n",
       " 3563              ::but I like baby bangs:: /tiny voice  ed5h3jh      0     0   \n",
       " \n",
       "       joy  sadness  surprise  \n",
       " 0       0        1         0  \n",
       " 1       1        0         0  \n",
       " 2       1        0         0  \n",
       " 3       0        0         0  \n",
       " 4       1        0         0  \n",
       " ...   ...      ...       ...  \n",
       " 3559    0        0         0  \n",
       " 3560    1        0         0  \n",
       " 3561    0        0         0  \n",
       " 3562    0        0         0  \n",
       " 3563    1        0         0  \n",
       " \n",
       " [3564 rows x 7 columns])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_dev   = pd.read_csv(\"val.csv\")\n",
    "#df_train = pd.read_csv(\"train.csv\", header=None, names=['Text', 'Class', 'ID'])\n",
    "#df_dev = pd.read_csv(\"val.csv\", sep='\\t', header=None, names=['Text', 'Class', 'ID'])\n",
    "df_train, df_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efa3790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def list_of_classes(row):\n",
    "#    classes = [\"anger\", \"fear\", \"joy\", \"sadness\", \"surprise\"]\n",
    "#    arr = [1 if emotion==\"1\" else 0 for emotion in classes]\n",
    "#    print(arr)\n",
    "#    return arr\n",
    "#df_train = df_train.apply(list_of_classes, axis=1, result_type='expand')\n",
    "#df_train['Len of classes'] = df_train['List of classes'].apply(lambda x: len(x))\n",
    "#df_dev['List of classes'] = df_dev['Class'].apply(lambda x: x.split(','))\n",
    "#df_dev['Len of classes'] = df_dev['List of classes'].apply(lambda x: len(x))\n",
    "#df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04f734fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9dd2080a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger', 'fear', 'joy', 'sadness', 'surprise']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 200\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "VALID_BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 2e-5\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "target_cols = [col for col in df_train.columns if col not in ['Text', 'ID']]\n",
    "target_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2db70f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.text = df.Text\n",
    "        self.tokenizer = tokenizer\n",
    "        self.targets = df[target_cols].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0783c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BERTDataset(df_train, tokenizer, MAX_LEN)\n",
    "valid_dataset = BERTDataset(df_dev, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e02fcff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=4, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, num_workers=4, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16ed98b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535872ac4c274adc9f5a1b3da6fd4c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained('roberta-base')\n",
    "#         self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.fc = torch.nn.Linear(768,5)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, features = self.roberta(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
    "#         output_2 = self.l2(output_1)\n",
    "        output = self.fc(features)\n",
    "        return output\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "379def4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "optimizer = AdamW(params=model.parameters(), lr=LEARNING_RATE, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df49fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_break = 2\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        print(f\"Epoch {epoch}, step {i}\")\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        if i%i_break==0:\n",
    "            print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
    "            break\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcbf301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation():\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(valid_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
