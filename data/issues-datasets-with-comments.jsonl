{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4336","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4336\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4336\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4336\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4336","id":1234446174,"node_id":"PR_kwDODunzps43vpqG","number":4336,"title":"Eval metadata batch 2 : Health Fact, Jigsaw Toxicity, LIAR, LJ Speech, MSRA NER, Multi News, NCBI Disease, PiQA, Poem Sentiment, QAsper","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Summary of CircleCI errors:\r\n- **Jjigsaw_toxicity_pred**:  `Citation Information` but it is empty.\r\n- **LIAR** :  `Data Instances`,`Data Fields`,  `Data Splits`,  `Citation Information` are empty.\r\n- **MSRA NER** : Dataset Summary`, `Data Instances`, `Data Fields`,  `Data Splits`, `Citation Information` are empty.\r\n"],"created_at":1652387085000,"updated_at":1652387085000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4336","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4336","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4336.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4336.patch","merged_at":null},"body":"Adding evaluation metadata for Health Fact, Jigsaw Toxicity, LIAR, LJ Speech, MSRA NER, Multi News, NCBI Disease, PiQA, Poem Sentiment, QAsper","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4336\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4336\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4335","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4335\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4335\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4335\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4335","id":1234157123,"node_id":"PR_kwDODunzps43usJP","number":4335,"title":"Eval metadata batch 1: BillSum, BoolQ, CoNLL2003, CoNLLPP, CUAD, Emotion, GigaWord, GLUE, Hate Speech 18,  Hate Speech","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Summary of CircleCI errors:\r\n- **BoolQ**: missing 8 required positional arguments: 'annotations_creators', 'language_creators', 'licenses', 'multilinguality', 'size_categories', 'source_datasets', 'task_categories', and 'task_ids'\r\n- **Conllpp**: expected some content in section `Citation Information` but it is empty.\r\n- **GLUE**:   'annotations_creators', 'language_creators', 'source_datasets' :['unknown'] are not registered tags\r\n- **ConLL2003**:  field 'task_ids':  ['part-of-speech-tagging'] are not registered tags for 'task_ids'\r\n-  **Hate_speech18:**   Expected some content in section `Data Instances` but it is empty, Expected some content in section `Data Splits` but it is empty"],"created_at":1652369296000,"updated_at":1652382954000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4335","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4335","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4335.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4335.patch","merged_at":null},"body":"adding evaluation metadata for the BillSum, BoolQ, CoNLL2003, CoNLLPP, CUAD, Emotion, GigaWord, GLUE, Hate Speech 18 and Hate Speech datasets","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4335\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4335\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4334","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4334\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4334\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4334\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4334","id":1234103477,"node_id":"PR_kwDODunzps43uguB","number":4334,"title":"Adding eval metadata for billsum","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1652366948000,"updated_at":1652366964000,"closed_at":1652366964000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":true,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4334","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4334","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4334.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4334.patch","merged_at":null},"body":"Adding eval metadata for billsum","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4334\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4334\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4333","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4333\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4333\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4333\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4333","id":1234038705,"node_id":"PR_kwDODunzps43uSuj","number":4333,"title":"Adding eval metadata for Banking 77","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["@lhoestq , Circle CI is giving me an error, saying that ['extended'] is a key that shouldn't be in the dataset metadata, but it was there before my modification (so I don't want to remove it)"],"created_at":1652364305000,"updated_at":1652367466000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4333","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4333","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4333.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4333.patch","merged_at":null},"body":"Adding eval metadata for Banking 77","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4333\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4333\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4332","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4332\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4332\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4332\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4332","id":1234021188,"node_id":"PR_kwDODunzps43uO8S","number":4332,"title":"Adding eval metadata for arabic speech corpus","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1652363498000,"updated_at":1652366342000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4332","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4332","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4332.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4332.patch","merged_at":null},"body":"Adding eval metadata for arabic speech corpus","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4332\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4332\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4331","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4331\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4331\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4331\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4331","id":1234016110,"node_id":"PR_kwDODunzps43uN2R","number":4331,"title":"Adding eval metadata to Amazon Polarity","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1652363279000,"updated_at":1652366375000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4331","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4331","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4331.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4331.patch","merged_at":null},"body":"Adding eval metadata to Amazon Polarity","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4331\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4331\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4330","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4330\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4330\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4330\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4330","id":1233992681,"node_id":"PR_kwDODunzps43uIwm","number":4330,"title":"Adding eval metadata to Allocin\u00e9 dataset","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1652362299000,"updated_at":1652366420000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4330","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4330","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4330.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4330.patch","merged_at":null},"body":"Adding eval metadata to Allocin\u00e9 dataset","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4330\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4330\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4329","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4329\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4329\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4329\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4329","id":1233991207,"node_id":"PR_kwDODunzps43uIcF","number":4329,"title":"Adding eval metadata for AG News","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1652362232000,"updated_at":1652366457000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4329","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4329","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4329.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4329.patch","merged_at":null},"body":"Adding eval metadata for AG News","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4329\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4329\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4328","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4328\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4328\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4328\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4328","id":1233856690,"node_id":"PR_kwDODunzps43trrd","number":4328,"title":"Fix and clean Apache Beam functionality","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4328). All of your documentation changes will be reflected on that endpoint."],"created_at":1652355667000,"updated_at":1652356669000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4328","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4328","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4328.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4328.patch","merged_at":null},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4328\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4328\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4327","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4327\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4327\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4327\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4327","id":1233840020,"node_id":"I_kwDODunzps5JiueU","number":4327,"title":"`wikipedia` pre-processed datasets","user":{"login":"vpj","id":81152,"node_id":"MDQ6VXNlcjgxMTUy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/81152?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/vpj","html_url":"https:\/\/github.com\/vpj","followers_url":"https:\/\/api.github.com\/users\/vpj\/followers","following_url":"https:\/\/api.github.com\/users\/vpj\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/vpj\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/vpj\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/vpj\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/vpj\/orgs","repos_url":"https:\/\/api.github.com\/users\/vpj\/repos","events_url":"https:\/\/api.github.com\/users\/vpj\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/vpj\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi @vpj, thanks for reporting.\r\n\r\nI'm sorry, but I can't reproduce your bug: I load \"20220301.simple\"in 9 seconds:\r\n```shell\r\ntime python -c \"from datasets import load_dataset; load_dataset('wikipedia', '20220301.simple')\"\r\n\r\nDownloading and preparing dataset wikipedia\/20220301.simple (download: 228.58 MiB, generated: 224.18 MiB, post-processed: Unknown size, total: 452.76 MiB) to ...\/.cache\/huggingface\/datasets\/wikipedia\/20220301.simple\/2.0.0\/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559...\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.66k\/1.66k [00:00<00:00, 1.02MB\/s]\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 235M\/235M [00:02<00:00, 82.8MB\/s]\r\nDataset wikipedia downloaded and prepared to ...\/.cache\/huggingface\/datasets\/wikipedia\/20220301.simple\/2.0.0\/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559. Subsequent calls will reuse this data.\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:00<00:00, 290.75it\/s]\r\n\r\nreal\t0m9.693s\r\nuser\t0m6.002s\r\nsys\t0m3.260s\r\n```\r\n\r\nCould you please check your environment info, as requested when opening this issue?\r\n```\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:\r\n- Platform:\r\n- Python version:\r\n- PyArrow version:\r\n```\r\nMaybe you are using an old version of `datasets`..."],"created_at":1652354742000,"updated_at":1652363921000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\n[Wikipedia](https:\/\/huggingface.co\/datasets\/wikipedia) dataset readme says that certain subsets are preprocessed. However it seems like they are not available. When I try to load them it takes a really long time, and it seems like it's processing them.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nload_dataset(\"wikipedia\", \"20220301.en\")\r\n```\r\n\r\n## Expected results\r\nTo load the dataset\r\n\r\n## Actual results\r\nTakes a very long time to load (after downloading)\r\n\r\nAfter `Downloading data files: 100%`. It takes hours and gets killed.\r\nTried `wikipedia.simple` and it got processed after ~30mins.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4327\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4327\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4326","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4326\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4326\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4326\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4326","id":1233818489,"node_id":"PR_kwDODunzps43tjWy","number":4326,"title":"Fix type hint and documentation for `new_fingerprint`","user":{"login":"fxmarty","id":9808326,"node_id":"MDQ6VXNlcjk4MDgzMjY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/9808326?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/fxmarty","html_url":"https:\/\/github.com\/fxmarty","followers_url":"https:\/\/api.github.com\/users\/fxmarty\/followers","following_url":"https:\/\/api.github.com\/users\/fxmarty\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/fxmarty\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/fxmarty\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/fxmarty\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/fxmarty\/orgs","repos_url":"https:\/\/api.github.com\/users\/fxmarty\/repos","events_url":"https:\/\/api.github.com\/users\/fxmarty\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/fxmarty\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4326). All of your documentation changes will be reflected on that endpoint."],"created_at":1652353508000,"updated_at":1652354339000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4326","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4326","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4326.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4326.patch","merged_at":null},"body":"Currently, there are no type hints nor `Optional` for the argument `new_fingerprint` in several methods of `datasets.arrow_dataset.Dataset`.\r\n\r\nThere was some documentation missing as well.\r\n\r\nNote that pylance is happy with the type hints, but pyright does not detect that `new_fingerprint` is set within the decorator.\r\n\r\nThe modifications in this PR are fine since here https:\/\/github.com\/huggingface\/datasets\/blob\/aa743886221d76afb409d263e1b136e7a71fe2b4\/src\/datasets\/fingerprint.py#L446-L454\r\n\r\nfor the non-inplace case we make sure to auto-generate a new fingerprint (as indicated in the doc).","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4326\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4326\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4325","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4325\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4325\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4325\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4325","id":1233812191,"node_id":"I_kwDODunzps5Jinrf","number":4325,"title":"Dataset Viewer issue for strombergnlp\/offenseval_2020, strombergnlp\/polstance","user":{"login":"leondz","id":121934,"node_id":"MDQ6VXNlcjEyMTkzNA==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/121934?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/leondz","html_url":"https:\/\/github.com\/leondz","followers_url":"https:\/\/api.github.com\/users\/leondz\/followers","following_url":"https:\/\/api.github.com\/users\/leondz\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/leondz\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/leondz\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/leondz\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/leondz\/orgs","repos_url":"https:\/\/api.github.com\/users\/leondz\/repos","events_url":"https:\/\/api.github.com\/users\/leondz\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/leondz\/received_events","type":"User","site_admin":false},"labels":[{"id":3470211881,"node_id":"LA_kwDODunzps7O1zsp","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer","name":"dataset-viewer","color":"E5583E","default":false,"description":"Related to the dataset viewer on huggingface.co"}],"state":"open","locked":false,"assignee":{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},"assignees":[{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Not sure if it's related... I was going to raise an issue for https:\/\/huggingface.co\/datasets\/domenicrosati\/TruthfulQA which also has the same issue... https:\/\/huggingface.co\/datasets\/domenicrosati\/TruthfulQA\/viewer\/domenicrosati--TruthfulQA\/train \r\n\r\n","Yes, it's related. The backend behind the dataset viewer is currently under too much load, and these datasets are still in the jobs queue. We're actively working on this issue, and we expect to fix the issue permanently soon. Thanks for your patience \ud83d\ude4f \u00a0","Thanks @severo and no worries! - a suggestion for a UI usability thing maybe is to indicate that the dataset processing is in the job queue (rather than no data?)"],"created_at":1652353148000,"updated_at":1652378145000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"### Link\n\nhttps:\/\/huggingface.co\/datasets\/strombergnlp\/offenseval_2020\/viewer\/ar\/train\n\n### Description\n\nThe viewer isn't running for these two datasets. I left it overnight because a wait sometimes helps things get loaded, and the error messages have all gone, but the datasets are still turning up blank in viewer. Maybe it needs a bit more time.\r\n\r\n* https:\/\/huggingface.co\/datasets\/strombergnlp\/polstance\/viewer\/PolStance\/train\r\n\r\n* https:\/\/huggingface.co\/datasets\/strombergnlp\/offenseval_2020\/viewer\/ar\/train\r\n\r\nWhile offenseval_2020 is gated w. prompt, the other gated previews I have run fine in Viewer, e.g. https:\/\/huggingface.co\/datasets\/strombergnlp\/shaj , so I'm a bit stumped!\n\n### Owner\n\nYes","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4325\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4325\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4324","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4324\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4324\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4324\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4324","id":1233780870,"node_id":"I_kwDODunzps5JigCG","number":4324,"title":"Support >1 PWC dataset per dataset card","user":{"login":"leondz","id":121934,"node_id":"MDQ6VXNlcjEyMTkzNA==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/121934?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/leondz","html_url":"https:\/\/github.com\/leondz","followers_url":"https:\/\/api.github.com\/users\/leondz\/followers","following_url":"https:\/\/api.github.com\/users\/leondz\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/leondz\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/leondz\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/leondz\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/leondz\/orgs","repos_url":"https:\/\/api.github.com\/users\/leondz\/repos","events_url":"https:\/\/api.github.com\/users\/leondz\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/leondz\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1652351347000,"updated_at":1652352522000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"**Is your feature request related to a problem? Please describe.**\r\nSome datasets cover more than one dataset on PapersWithCode. For example, the OffensEval 2020 challenge involved five languages, and there's one dataset to cover all five datasets, `strombergnlp\/offenseval_2020`. However, the yaml `paperswithcode_id:` dataset card entry only supports one value; when multiple are added, the PWC link disappears from the dataset page.\r\n\r\nBecause the link from a PapersWithCode dataset to a Hugging Face Hub entry can't be entered manually and seems to be scraped, this means end users don't have a way of getting a dataset reader link to appear on all the PWC datasets supported by one HF Hub Dataset reader.\r\n\r\nIt's not super unusual to have papers introduce multiple parallel variants of a dataset and would be handy to reflect this, so e.g. dataset maintainers can DRY, and so dataset users can keep what they're doing simple.\r\n\r\n**Describe the solution you'd like**\r\nI'd like `paperswithcode_id:` to support lists and be able to connect with multiple PWC datasets.\r\n\r\n**Describe alternatives you've considered**\r\nDe-normalising the datasets on HF Hub to create multiple readers for each variation on a task, i.e. instead of a single `offenseval_2020`, having `offenseval_2020_ar`, `offenseval_2020_da`, `offenseval_2020_gr`, ...\r\n\r\n**Additional context**\r\nHope that's enough\r\n\r\n**Priority**\r\nLow","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4324\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4324\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4323","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4323\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4323\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4323\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4323","id":1233634928,"node_id":"I_kwDODunzps5Jh8Zw","number":4323,"title":"Audio can not find value[\"bytes\"]","user":{"login":"YooSungHyun","id":34292279,"node_id":"MDQ6VXNlcjM0MjkyMjc5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/34292279?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/YooSungHyun","html_url":"https:\/\/github.com\/YooSungHyun","followers_url":"https:\/\/api.github.com\/users\/YooSungHyun\/followers","following_url":"https:\/\/api.github.com\/users\/YooSungHyun\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/YooSungHyun\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/YooSungHyun\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/YooSungHyun\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/YooSungHyun\/orgs","repos_url":"https:\/\/api.github.com\/users\/YooSungHyun\/repos","events_url":"https:\/\/api.github.com\/users\/YooSungHyun\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/YooSungHyun\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["![image](https:\/\/user-images.githubusercontent.com\/34292279\/168063684-fff5c12a-8b1e-4c65-b18b-36100ab8a1af.png)\r\n\r\nthat is reason my bytes`s empty\r\nbut i have some confused why path prior is higher than bytes?\r\n\r\nif you can make bytes in _generate_examples , you don`t have to make bytes to path?\r\nbecause we have path and bytes already"],"created_at":1652344318000,"updated_at":1652354920000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nI wrote down _generate_examples like:\r\n![image](https:\/\/user-images.githubusercontent.com\/34292279\/168027186-2fe8b255-2cd8-4b9b-ab1e-8d5a7182979b.png)\r\n\r\nbut where is the bytes?\r\n![image](https:\/\/user-images.githubusercontent.com\/34292279\/168027330-f2496dd0-1d99-464c-b15c-bc57eee0415a.png)\r\n\r\n\r\n## Expected results\r\nvalue[\"bytes\"] is not None, so i can make datasets with bytes, not path\r\n\r\n## bytes looks like:\r\nblah blah~~\r\n\\xfe\\x03\\x00\\xfb\\x06\\x1c\\x0bo\\x074\\x03\\xaf\\x01\\x13\\x04\\xbc\\x06\\x8c\\x05y\\x05,\\t7\\x08\\xaf\\x03\\xc0\\xfe\\xe8\\xfc\\x94\\xfe\\xb7\\xfd\\xea\\xfa\\xd5\\xf9$\\xf9>\\xf9\\x1f\\xf8\\r\\xf5F\\xf49\\xf4\\xda\\xf5-\\xf8\\n\\xf8k\\xf8\\x07\\xfb\\x18\\xfd\\xd9\\xfdv\\xfd\"\\xfe\\xcc\\x01\\x1c\\x04\\x08\\x04@\\x04{\\x06^\\tf\\t\\x1e\\x07\\x8b\\x06\\x02\\x08\\x13\\t\\x07\\x08 \\x06g\\x06\"\\x06\\xa0\\x03\\xc6\\x002\\xff \\xff\\x1d\\xff\\x19\\xfd?\\xfb\\xdb\\xfa\\xfc\\xfa$\\xfb}\\xf9\\xe5\\xf7\\xf9\\xf7\\xce\\xf8.\\xf9b\\xf9\\xc5\\xf9\\xc0\\xfb\\xfa\\xfcP\\xfc\\xba\\xfbQ\\xfc1\\xfe\\x9f\\xff\\x12\\x00\\xa2\\x00\\x18\\x02Z\\x03\\x02\\x04\\xb1\\x03\\xc5\\x03W\\x04\\x82\\x04\\x8f\\x04U\\x04\\xb6\\x04\\x10\\x05{\\x04\\x83\\x02\\x17\\x01\\x1d\\x00\\xa0\\xff\\xec\\xfe\\x03\\xfe#\\xfe\\xc2\\xfe2\\xff\\xe6\\xfe\\x9a\\xfe~\\x01\\x91\\x08\\xb3\\tU\\x05\\x10\\x024\\x02\\xe4\\x05\\xa8\\x07\\xa7\\x053\\x07I\\n\\x91\\x07v\\x02\\x95\\xfd\\xbb\\xfd\\x96\\xff\\x01\\xfe\\x1e\\xfb\\xbb\\xf9S\\xf8!\\xf8\\xf4\\xf5\\xd6\\xf3\\xf7\\xf3l\\xf4d\\xf6l\\xf7d\\xf6b\\xf7\\xc1\\xfa(\\xfd\\xcf\\xfd*\\xfdq\\xfe\\xe9\\x01\\xa8\\x03t\\x03\\x17\\x04B\\x07\\xce\\t\\t\\t\\xeb\\x06\\x0c\\x07\\x95\\x08\\x92\\t\\xbc\\x07O\\x06\\xfb\\x06\\xd2\\x06U\\x04\\x00\\x02\\x92\\x00\\xdc\\x00\\x84\\x00 \\xfeT\\xfc\\xf1\\xfb\\x82\\xfc\\x97\\xfb}\\xf9\\x00\\xf8_\\xf8\\x0b\\xf9\\xe5\\xf8\\xe2\\xf7\\xaa\\xf8\\xb2\\xfa\\x10\\xfbl\\xfa\\xf5\\xf9Y\\xfb\\xc0\\xfd\\xe8\\xfe\\xec\\xfe1\\x00\\xad\\x01\\xec\\x02E\\x03\\x13\\x03\\x9b\\x03o\\x04\\xce\\x04\\xa8\\x04\\xb2\\x04\\x1b\\x05\\xc0\\x05\\xd2\\x04\\xe8\\x02z\\x01\\xbe\\x00\\xae\\x00\\x07\\x00$\\xff|\\xff\\x8e\\x00\\x13\\x00\\x10\\xff\\x98\\xff0\\x05{\\x0b\\x05\\t\\xaa\\x03\\x82\\x01n\\x03\r\nblah blah~~\r\n\r\nthat function not return None\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:2.2.1\r\n- Platform:ubuntu 18.04\r\n- Python version:3.6.9\r\n- PyArrow version:6.0.1\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4323\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4323\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4322","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4322\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4322\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4322\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4322","id":1233596947,"node_id":"PR_kwDODunzps43s1wy","number":4322,"title":"Added stratify option to train_test_split function.","user":{"login":"nandwalritik","id":48522685,"node_id":"MDQ6VXNlcjQ4NTIyNjg1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/48522685?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nandwalritik","html_url":"https:\/\/github.com\/nandwalritik","followers_url":"https:\/\/api.github.com\/users\/nandwalritik\/followers","following_url":"https:\/\/api.github.com\/users\/nandwalritik\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nandwalritik\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nandwalritik\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nandwalritik\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nandwalritik\/orgs","repos_url":"https:\/\/api.github.com\/users\/nandwalritik\/repos","events_url":"https:\/\/api.github.com\/users\/nandwalritik\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nandwalritik\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["> Nice thank you ! This will be super useful :)\r\n> \r\n> Could you also add some tests in test_arrow_dataset.py and add an example of usage in the `Example:` section of the `train_test_split` docstring ?\r\n\r\nI will try to do it, is there any documentation for adding test cases? I have never done it before.","Thanks for the changes !\r\n\r\n> I will try to do it, is there any documentation for adding test cases? I have never done it before.\r\n\r\nYou can just add a function `test_train_test_split_startify` in `test_arrow_dataset.py`.\r\n\r\nIn this function you can define a dataset and make sure that `train_test_split` with the `stratify` argument works as expected.\r\n\r\nYou can do `pytest tests\/test_arrow_dataset.py::test_train_test_split_startify` to run your test.\r\n\r\nFeel free to get some inspiration from other tests like `test_interleave_datasets` for example"],"created_at":1652342431000,"updated_at":1652356193000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4322","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4322","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4322.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4322.patch","merged_at":null},"body":"This PR adds `stratify` option to `train_test_split` method. I took reference from scikit-learn's `StratifiedShuffleSplit` class for implementing stratified split and integrated the changes as were suggested by @lhoestq.\r\n\r\nIt fixes #3452.\r\n\r\n@lhoestq Please review and let me know, if any changes are required.\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4322\/reactions","total_count":2,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4322\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4321","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4321\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4321\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4321\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4321","id":1233273351,"node_id":"PR_kwDODunzps43ryW7","number":4321,"title":"Adding dataset enwik8","user":{"login":"HallerPatrick","id":22773355,"node_id":"MDQ6VXNlcjIyNzczMzU1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/22773355?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/HallerPatrick","html_url":"https:\/\/github.com\/HallerPatrick","followers_url":"https:\/\/api.github.com\/users\/HallerPatrick\/followers","following_url":"https:\/\/api.github.com\/users\/HallerPatrick\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/HallerPatrick\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/HallerPatrick\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/HallerPatrick\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/HallerPatrick\/orgs","repos_url":"https:\/\/api.github.com\/users\/HallerPatrick\/repos","events_url":"https:\/\/api.github.com\/users\/HallerPatrick\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/HallerPatrick\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1652311502000,"updated_at":1652312866000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4321","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4321","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4321.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4321.patch","merged_at":null},"body":"Because I regularly work with enwik8, I would like to contribute the dataset loader \ud83e\udd17 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4321\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4321\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4320","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4320\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4320\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4320\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4320","id":1233208864,"node_id":"I_kwDODunzps5JgUYg","number":4320,"title":"Multi-news dataset loader attempts to strip wrong character from beginning of summaries","user":{"login":"JohnGiorgi","id":8917831,"node_id":"MDQ6VXNlcjg5MTc4MzE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8917831?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/JohnGiorgi","html_url":"https:\/\/github.com\/JohnGiorgi","followers_url":"https:\/\/api.github.com\/users\/JohnGiorgi\/followers","following_url":"https:\/\/api.github.com\/users\/JohnGiorgi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/JohnGiorgi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/JohnGiorgi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/JohnGiorgi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/JohnGiorgi\/orgs","repos_url":"https:\/\/api.github.com\/users\/JohnGiorgi\/repos","events_url":"https:\/\/api.github.com\/users\/JohnGiorgi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/JohnGiorgi\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1652305001000,"updated_at":1652305062000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\n\r\nThe `multi_news.py` data loader has [a line which attempts to strip `\"- \"` from the beginning of summaries](https:\/\/github.com\/huggingface\/datasets\/blob\/aa743886221d76afb409d263e1b136e7a71fe2b4\/datasets\/multi_news\/multi_news.py#L97). The actual character in the multi-news dataset, however, is `\"\u2013 \"`, which is different, e.g. `\"\u2013 \" != \"- \"`.\r\n\r\nI would have just opened a PR to fix the mistake, but I am wondering what the motivation for stripping this character is? AFAICT most approaches just leave it in, e.g. the current SOTA on this dataset, [PRIMERA](https:\/\/huggingface.co\/allenai\/PRIMERA-multinews) (you can see its in the generated summaries of the model in their [example notebook](https:\/\/github.com\/allenai\/PRIMER\/blob\/main\/Evaluation_Example.ipynb)).\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.2.0\r\n- Platform: Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.3.5\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4320\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4320\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4319","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4319\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4319\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4319\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4319","id":1232982023,"node_id":"PR_kwDODunzps43q0UY","number":4319,"title":"Adding eval metadata for ade v2","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1652290580000,"updated_at":1652362191000,"closed_at":1652361739000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4319","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4319","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4319.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4319.patch","merged_at":1652361739000},"body":"Adding metadata to allow evaluation","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4319\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4319\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4318","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4318\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4318\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4318\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4318","id":1232905488,"node_id":"PR_kwDODunzps43qkkQ","number":4318,"title":"Don't check f.loc in _get_extraction_protocol_with_magic_number","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1652286429000,"updated_at":1652288222000,"closed_at":1652287591000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4318","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4318","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4318.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4318.patch","merged_at":1652287591000},"body":"`f.loc` doesn't always exist for file-like objects in python. I removed it since it was not necessary anyway (we always seek the file to 0 after reading the magic number)\r\n\r\nFix https:\/\/github.com\/huggingface\/datasets\/issues\/4310","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4318\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4318\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4317","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4317\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4317\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4317\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4317","id":1232737401,"node_id":"PR_kwDODunzps43qBzh","number":4317,"title":"Fix cnn_dailymail (dm stories were ignored)","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1652279125000,"updated_at":1652284809000,"closed_at":1652284357000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4317","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4317","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4317.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4317.patch","merged_at":1652284357000},"body":"https:\/\/github.com\/huggingface\/datasets\/pull\/4188 introduced a bug in `datasets` 2.2.0: DailyMail stories are ignored when generating the dataset.\r\n\r\nI fixed that, and removed the google drive link (it has annoying quota limitations issues)\r\n\r\nWe can do a patch release after this is merged","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4317\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4317\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4316","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4316\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4316\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4316\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4316","id":1232681207,"node_id":"PR_kwDODunzps43p1Za","number":4316,"title":"Support passing config_kwargs to CLI run_beam","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1652277217000,"updated_at":1652279809000,"closed_at":1652279311000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4316","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4316","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4316.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4316.patch","merged_at":1652279311000},"body":"This PR supports passing `config_kwargs` to CLI run_beam, so that for example for \"wikipedia\" dataset, we can pass:\r\n```\r\n--date 20220501 --language ca\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4316\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4316\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4315","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4315\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4315\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4315\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4315","id":1232549330,"node_id":"PR_kwDODunzps43pZ6p","number":4315,"title":"Fix CLI run_beam namespace","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1652271660000,"updated_at":1652274780000,"closed_at":1652274308000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4315","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4315","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4315.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4315.patch","merged_at":1652274308000},"body":"Currently, it raises TypeError:\r\n```\r\nTypeError: __init__() got an unexpected keyword argument 'namespace'\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4315\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4315\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4314","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4314\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4314\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4314\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4314","id":1232326726,"node_id":"PR_kwDODunzps43oqXD","number":4314,"title":"Catch pull error when mirroring","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1652261915000,"updated_at":1652273647000,"closed_at":1652273202000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4314","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4314","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4314.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4314.patch","merged_at":1652273202000},"body":"Catch pull errors when mirroring so that the script continues to update the other datasets.\r\n\r\nThe error will still be printed at the end of the job. In this case the job also fails, and asks to manually update the datasets that failed.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4314\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4314\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4313","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4313\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4313\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4313\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4313","id":1231764100,"node_id":"PR_kwDODunzps43m4qB","number":4313,"title":"Add API code examples for Builder classes","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1652221352000,"updated_at":1652374963000,"closed_at":1652359017000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4313","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4313","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4313.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4313.patch","merged_at":1652359017000},"body":"This PR adds API code examples for the Builder classes.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4313\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4313\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4312","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4312\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4312\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4312\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4312","id":1231662775,"node_id":"PR_kwDODunzps43mlug","number":4312,"title":"added TR-News dataset","user":{"login":"batubayk","id":25901065,"node_id":"MDQ6VXNlcjI1OTAxMDY1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/25901065?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/batubayk","html_url":"https:\/\/github.com\/batubayk","followers_url":"https:\/\/api.github.com\/users\/batubayk\/followers","following_url":"https:\/\/api.github.com\/users\/batubayk\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/batubayk\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/batubayk\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/batubayk\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/batubayk\/orgs","repos_url":"https:\/\/api.github.com\/users\/batubayk\/repos","events_url":"https:\/\/api.github.com\/users\/batubayk\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/batubayk\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1652214780000,"updated_at":1652214780000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4312","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4312","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4312.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4312.patch","merged_at":null},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4312\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4312\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4311","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4311\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4311\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4311\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4311","id":1231369438,"node_id":"PR_kwDODunzps43ln8-","number":4311,"title":"[Imagefolder] Docs + Don't infer labels from file names when there are metadata + Error messages when metadata and images aren't linked correctly","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Merging this one since mario is off, I took care of adding some tests to make sure everything is fine. Will do the release after it"],"created_at":1652197935000,"updated_at":1652203182000,"closed_at":1652202707000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4311","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4311","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4311.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4311.patch","merged_at":1652202707000},"body":"I updated the `docs\/source\/image_process.mdx` documentation and added an example for image captioning and object detection using `ImageFolder`.\r\n\r\nWhile doing so I also improved a few aspects:\r\n- we don't need to infer labels from file names when there are metadata - they can just be in the metadata if necessary\r\n- raise informative error messages when metadata and images aren't linked correctly:\r\n  - when an image is missing a metadata file\r\n  - when a metadata file is missing an image\r\n\r\nI added some tests for these changes as well\r\n\r\ncc @mariosasko ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4311\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4311\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4310","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4310\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4310\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4310\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4310","id":1231319815,"node_id":"I_kwDODunzps5JZHMH","number":4310,"title":"Loading dataset with streaming: '_io.BufferedReader' object has no attribute 'loc'","user":{"login":"milmin","id":72745467,"node_id":"MDQ6VXNlcjcyNzQ1NDY3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/72745467?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/milmin","html_url":"https:\/\/github.com\/milmin","followers_url":"https:\/\/api.github.com\/users\/milmin\/followers","following_url":"https:\/\/api.github.com\/users\/milmin\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/milmin\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/milmin\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/milmin\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/milmin\/orgs","repos_url":"https:\/\/api.github.com\/users\/milmin\/repos","events_url":"https:\/\/api.github.com\/users\/milmin\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/milmin\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"assignees":[{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":[],"created_at":1652195573000,"updated_at":1652287591000,"closed_at":1652287591000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nLoading a datasets with `load_dataset` and `streaming=True` returns `AttributeError: '_io.BufferedReader' object has no attribute 'loc'`. Notice that loading with `streaming=False` works fine.\r\n\r\nIn the following steps we load parquet files but the same happens with pickle files. The problem seems to come from `fsspec` lib, I put in the environment info also `s3fs` and `fsspec` versions since I'm loading from an s3 bucket.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n# path is the path to parquet files\r\ndata_files = {\"train\": path + \"meta_train.parquet.gzip\", \"test\": path + \"meta_test.parquet.gzip\"}\r\ndataset = load_dataset(\"parquet\", data_files=data_files, streaming=True)\r\n```\r\n\r\n## Expected results\r\nA dataset object `datasets.dataset_dict.DatasetDict`\r\n\r\n## Actual results\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n<command-562086> in <module>\r\n     11 \r\n     12 data_files = {\"train\": path + \"meta_train.parquet.gzip\", \"test\": path + \"meta_test.parquet.gzip\"}\r\n---> 13 dataset = load_dataset(\"parquet\", data_files=data_files, streaming=True)\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\r\n   1679     if streaming:\r\n   1680         extend_dataset_builder_for_streaming(builder_instance, use_auth_token=use_auth_token)\r\n-> 1681         return builder_instance.as_streaming_dataset(\r\n   1682             split=split,\r\n   1683             use_auth_token=use_auth_token,\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/builder.py in as_streaming_dataset(self, split, base_path, use_auth_token)\r\n    904         )\r\n    905         self._check_manual_download(dl_manager)\r\n--> 906         splits_generators = {sg.name: sg for sg in self._split_generators(dl_manager)}\r\n    907         # By default, return all splits\r\n    908         if split is None:\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/packaged_modules\/parquet\/parquet.py in _split_generators(self, dl_manager)\r\n     30         if not self.config.data_files:\r\n     31             raise ValueError(f\"At least one data file must be specified, but got data_files={self.config.data_files}\")\r\n---> 32         data_files = dl_manager.download_and_extract(self.config.data_files)\r\n     33         if isinstance(data_files, (str, list, tuple)):\r\n     34             files = data_files\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/utils\/streaming_download_manager.py in download_and_extract(self, url_or_urls)\r\n    798 \r\n    799     def download_and_extract(self, url_or_urls):\r\n--> 800         return self.extract(self.download(url_or_urls))\r\n    801 \r\n    802     def iter_archive(self, urlpath_or_buf: Union[str, io.BufferedReader]) -> Iterable[Tuple]:\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/utils\/streaming_download_manager.py in extract(self, path_or_paths)\r\n    776 \r\n    777     def extract(self, path_or_paths):\r\n--> 778         urlpaths = map_nested(self._extract, path_or_paths, map_tuple=True)\r\n    779         return urlpaths\r\n    780 \r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/utils\/py_utils.py in map_nested(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, types, disable_tqdm, desc)\r\n    312         num_proc = 1\r\n    313     if num_proc <= 1 or len(iterable) <= num_proc:\r\n--> 314         mapped = [\r\n    315             _single_map_nested((function, obj, types, None, True, None))\r\n    316             for obj in logging.tqdm(iterable, disable=disable_tqdm, desc=desc)\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/utils\/py_utils.py in <listcomp>(.0)\r\n    313     if num_proc <= 1 or len(iterable) <= num_proc:\r\n    314         mapped = [\r\n--> 315             _single_map_nested((function, obj, types, None, True, None))\r\n    316             for obj in logging.tqdm(iterable, disable=disable_tqdm, desc=desc)\r\n    317         ]\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/utils\/py_utils.py in _single_map_nested(args)\r\n    267         return {k: _single_map_nested((function, v, types, None, True, None)) for k, v in pbar}\r\n    268     else:\r\n--> 269         mapped = [_single_map_nested((function, v, types, None, True, None)) for v in pbar]\r\n    270         if isinstance(data_struct, list):\r\n    271             return mapped\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/utils\/py_utils.py in <listcomp>(.0)\r\n    267         return {k: _single_map_nested((function, v, types, None, True, None)) for k, v in pbar}\r\n    268     else:\r\n--> 269         mapped = [_single_map_nested((function, v, types, None, True, None)) for v in pbar]\r\n    270         if isinstance(data_struct, list):\r\n    271             return mapped\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/utils\/py_utils.py in _single_map_nested(args)\r\n    249     # Singleton first to spare some computation\r\n    250     if not isinstance(data_struct, dict) and not isinstance(data_struct, types):\r\n--> 251         return function(data_struct)\r\n    252 \r\n    253     # Reduce logging to keep things readable in multiprocessing with tqdm\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/utils\/streaming_download_manager.py in _extract(self, urlpath)\r\n    781     def _extract(self, urlpath: str) -> str:\r\n    782         urlpath = str(urlpath)\r\n--> 783         protocol = _get_extraction_protocol(urlpath, use_auth_token=self.download_config.use_auth_token)\r\n    784         if protocol is None:\r\n    785             # no extraction\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/utils\/streaming_download_manager.py in _get_extraction_protocol(urlpath, use_auth_token)\r\n    371         urlpath, kwargs = urlpath, {}\r\n    372     with fsspec.open(urlpath, **kwargs) as f:\r\n--> 373         return _get_extraction_protocol_with_magic_number(f)\r\n    374 \r\n    375 \r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/utils\/streaming_download_manager.py in _get_extraction_protocol_with_magic_number(f)\r\n    335 def _get_extraction_protocol_with_magic_number(f) -> Optional[str]:\r\n    336     \"\"\"read the magic number from a file-like object and return the compression protocol\"\"\"\r\n--> 337     prev_loc = f.loc\r\n    338     magic_number = f.read(MAGIC_NUMBER_MAX_LENGTH)\r\n    339     f.seek(prev_loc)\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/fsspec\/implementations\/local.py in __getattr__(self, item)\r\n    337 \r\n    338     def __getattr__(self, item):\r\n--> 339         return getattr(self.f, item)\r\n    340 \r\n    341     def __enter__(self):\r\n\r\nAttributeError: '_io.BufferedReader' object has no attribute 'loc'\r\n```\r\n## Environment info\r\n- `datasets` version: 2.1.0\r\n- Platform: Linux-5.4.0-1071-aws-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.2\r\n- `fsspec` version: 2021.08.1\r\n- `s3fs` version: 2021.08.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4310\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4310\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4309","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4309\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4309\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4309\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4309","id":1231232935,"node_id":"PR_kwDODunzps43lKpm","number":4309,"title":"[WIP] Add TEDLIUM dataset","user":{"login":"sanchit-gandhi","id":93869735,"node_id":"U_kgDOBZhWpw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/93869735?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sanchit-gandhi","html_url":"https:\/\/github.com\/sanchit-gandhi","followers_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/followers","following_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/orgs","repos_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/repos","events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/received_events","type":"User","site_admin":false},"labels":[{"id":2067376369,"node_id":"MDU6TGFiZWwyMDY3Mzc2MzY5","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20request","name":"dataset request","color":"e99695","default":false,"description":"Requesting to add a new dataset"},{"id":2725241052,"node_id":"MDU6TGFiZWwyNzI1MjQxMDUy","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/speech","name":"speech","color":"d93f0b","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4309). All of your documentation changes will be reflected on that endpoint.","```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset('.\/datasets\/tedlium', 'release1', cache_dir='\/home\/sanchitgandhi\/cache')\r\n```\r\n\r\n```\r\nDownloading and preparing dataset tedlium\/release1 to \/home\/sanchitgandhi\/cache\/tedlium\/release1\/1.0.1\/5a9fcb97b4b52d5a1c9dc7bde4b1d5994cd89c4a3425ea36c789bf6096fee4f0...\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"\/home\/sanchit_huggingface_co\/datasets\/src\/datasets\/load.py\", line 1703, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/home\/sanchit_huggingface_co\/datasets\/src\/datasets\/builder.py\", line 605, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/home\/sanchit_huggingface_co\/datasets\/src\/datasets\/builder.py\", line 1240, in _download_and_prepare\r\n    raise MissingBeamOptions(\r\ndatasets.builder.MissingBeamOptions: Trying to generate a dataset using Apache Beam, yet no Beam Runner or PipelineOptions() has been provided in `load_dataset` or in the builder arguments. For big datasets it has to run on large-scale data processing tools like Dataflow, Spark, etc. More information about Apache Beam runners at https:\/\/beam.apache.org\/documentation\/runners\/capability-matrix\/\r\nIf you really want to run it locally because you feel like the Dataset is small enough, you can use the local beam runner called `DirectRunner` (you may run out of memory). \r\nExample of usage: \r\n        `load_dataset('tedlium', 'release1', beam_runner='DirectRunner')`\r\n```\r\nSpecifying the `beam_runner='DirectRunner'` works:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset('.\/datasets\/tedlium', 'release1', cache_dir='\/home\/sanchitgandhi\/cache', beam_runner='DirectRunner')\r\n```","Extra Python imports\/Linux packages:\r\n```\r\npip install pydub\r\nsudo apt install ffmpeg\r\n```","Script heavily inspired by the TF datasets script at: https:\/\/github.com\/tensorflow\/datasets\/blob\/master\/tensorflow_datasets\/audio\/tedlium.py\r\n\r\nThe TF datasets script uses the module AudioSegment from the package `pydub` (https:\/\/github.com\/jiaaro\/pydub), which is used to to open the audio files (stored in .sph format):\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/61bf6123634bf6e7c7287cd6097909eb26118c58\/datasets\/tedlium\/tedlium.py#L167-L170\r\nThis package requires the pip install of `pydub` and the system installation of `ffmpeg`: https:\/\/github.com\/jiaaro\/pydub#installation\r\nIs it ok to use these packages? Or do we tend to avoid introducing additional dependencies?\r\n\r\nThe TF datasets script also uses `_build_pcollection`:\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/8afbbb6fe66b40d05574e2e72e65e974c72ae769\/datasets\/tedlium\/tedlium.py#L200-L206\r\nHowever, I was advised against using `beam` logic. Thus, I have reverted to generating the examples file-by-file: https:\/\/github.com\/huggingface\/datasets\/blob\/61bf6123634bf6e7c7287cd6097909eb26118c58\/datasets\/tedlium\/tedlium.py#L112-L138\r\n\r\nI am now able to generate examples by running the `load_dataset` command:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset('.\/datasets\/tedlium', 'release1', cache_dir='\/home\/sanchitgandhi\/cache')\r\n```\r\n\r\nHere, generating examples is **extremely** slow: it takes ~1 second per example, so ~60k seconds for the train set (~16 hours). Is there a way of paralleling this to make it faster?","> This package requires the pip install of pydub and the system installation of ffmpeg: https:\/\/github.com\/jiaaro\/pydub#installation\r\nIs it ok to use these packages? Or do we tend to avoid introducing additional dependencies?\r\n\r\nIt's ok, windows users will have have a bad time but I'm not sure we can do much about it.\r\n\r\n> Here, generating examples is extremely slow: it takes ~1 second per example, so ~60k seconds for the train set (~16 hours). Is there a way of paralleling this to make it faster?\r\n\r\nNot at the moment. For such cases we advise hosting the dataset ourselves in a processed format. The license doesn't allow this since the license is \"NoDerivatives\". Currently the only way to parallelize it is by keeping is as a beam dataset and let users pay Google Dataflow to process it (or use spark or whatever).","Thanks for your super speedy reply @lhoestq!\r\n\r\nI\u2019ve uploaded the script and README.md to the org here: https:\/\/huggingface.co\/datasets\/LIUM\/tedlium\r\nIs any modification of the script required to be able to use it from the Hub? When I run:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ntedlium = load_dataset(\"LIUM\/tedlium\", \"release1\") # for Release 1\r\n```\r\nI get the following error:\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nInput In [2], in <cell line: 1>()\r\n----> 1 load_dataset(\"LIUM\/tedlium\", \"release1\")\r\n\r\nFile ~\/datasets\/src\/datasets\/load.py:1676, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\r\n   1673 ignore_verifications = ignore_verifications or save_infos\r\n   1675 # Create a dataset builder\r\n-> 1676 builder_instance = load_dataset_builder(\r\n   1677     path=path,\r\n   1678     name=name,\r\n   1679     data_dir=data_dir,\r\n   1680     data_files=data_files,\r\n   1681     cache_dir=cache_dir,\r\n   1682     features=features,\r\n   1683     download_config=download_config,\r\n   1684     download_mode=download_mode,\r\n   1685     revision=revision,\r\n   1686     use_auth_token=use_auth_token,\r\n   1687     **config_kwargs,\r\n   1688 )\r\n   1690 # Return iterable dataset in case of streaming\r\n   1691 if streaming:\r\n\r\nFile ~\/datasets\/src\/datasets\/load.py:1502, in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\r\n   1500     download_config = download_config.copy() if download_config else DownloadConfig()\r\n   1501     download_config.use_auth_token = use_auth_token\r\n-> 1502 dataset_module = dataset_module_factory(\r\n   1503     path,\r\n   1504     revision=revision,\r\n   1505     download_config=download_config,\r\n   1506     download_mode=download_mode,\r\n   1507     data_dir=data_dir,\r\n   1508     data_files=data_files,\r\n   1509 )\r\n   1511 # Get dataset builder class from the processing script\r\n   1512 builder_cls = import_main_class(dataset_module.module_path)\r\n\r\nFile ~\/datasets\/src\/datasets\/load.py:1254, in dataset_module_factory(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1249             if isinstance(e1, FileNotFoundError):\r\n   1250                 raise FileNotFoundError(\r\n   1251                     f\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory. \"\r\n   1252                     f\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\r\n   1253                 ) from None\r\n-> 1254             raise e1 from None\r\n   1255 else:\r\n   1256     raise FileNotFoundError(\r\n   1257         f\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory.\"\r\n   1258     )\r\n\r\nFile ~\/datasets\/src\/datasets\/load.py:1227, in dataset_module_factory(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1225         raise e\r\n   1226 if filename in [sibling.rfilename for sibling in dataset_info.siblings]:\r\n-> 1227     return HubDatasetModuleFactoryWithScript(\r\n   1228         path,\r\n   1229         revision=revision,\r\n   1230         download_config=download_config,\r\n   1231         download_mode=download_mode,\r\n   1232         dynamic_modules_path=dynamic_modules_path,\r\n   1233     ).get_module()\r\n   1234 else:\r\n   1235     return HubDatasetModuleFactoryWithoutScript(\r\n   1236         path,\r\n   1237         revision=revision,\r\n   (...)\r\n   1241         download_mode=download_mode,\r\n   1242     ).get_module()\r\n\r\nFile ~\/datasets\/src\/datasets\/load.py:940, in HubDatasetModuleFactoryWithScript.get_module(self)\r\n    938 def get_module(self) -> DatasetModule:\r\n    939     # get script and other files\r\n--> 940     local_path = self.download_loading_script()\r\n    941     dataset_infos_path = self.download_dataset_infos_file()\r\n    942     imports = get_imports(local_path)\r\n\r\nFile ~\/datasets\/src\/datasets\/load.py:918, in HubDatasetModuleFactoryWithScript.download_loading_script(self)\r\n    917 def download_loading_script(self) -> str:\r\n--> 918     file_path = hf_hub_url(path=self.name, name=self.name.split(\"\/\")[1] + \".py\", revision=self.revision)\r\n    919     download_config = self.download_config.copy()\r\n    920     if download_config.download_desc is None:\r\n\r\nTypeError: hf_hub_url() got an unexpected keyword argument 'name'\r\n```\r\n\r\nNote that I am able to load the dataset from the `datasets` repo with the following lines of code:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset('.\/datasets\/tedlium', 'release1', cache_dir='\/home\/sanchitgandhi\/cache')\r\n```","What version of `datasets` do you have ?\r\nUpdating `datasets` should fix the error ;)\r\n"],"created_at":1652191967000,"updated_at":1652282379000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4309","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4309","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4309.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4309.patch","merged_at":null},"body":"Adds the TED-LIUM dataset https:\/\/www.tensorflow.org\/datasets\/catalog\/tedlium#tedliumrelease3 \r\n\r\nTODO:\r\n\r\n- [x] Port `tedium.py` from TF datasets using `convert_dataset.sh` script\r\n- [ ] Make `load_dataset` work\r\n- [ ] Run `datasets-cli` command to generate `dataset_infos.json`\r\n- [ ] Create dummy data for continuous testing\r\n- [ ] Dummy data tests\r\n- [ ] Real data tests\r\n- [ ] Create the metadata JSON\r\n- [ ] Close PR and add directly to the Hub under LIUM org","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4309\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4309\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4308","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4308\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4308\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4308\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4308","id":1231217783,"node_id":"PR_kwDODunzps43lHdP","number":4308,"title":"Remove unused multiprocessing args from test CLI","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1652191335000,"updated_at":1652273905000,"closed_at":1652273443000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4308","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4308","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4308.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4308.patch","merged_at":1652273442000},"body":"Multiprocessing is not used in the test CLI.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4308\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4308\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4307","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4307\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4307\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4307\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4307","id":1231175639,"node_id":"PR_kwDODunzps43k-Wo","number":4307,"title":"Add packaged builder configs to the documentation","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1652189659000,"updated_at":1652191430000,"closed_at":1652190954000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4307","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4307","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4307.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4307.patch","merged_at":1652190954000},"body":"Add the packaged builders configurations to the docs reference is useful to show the list of all parameters one can use when loading data in many formats: CSV, JSON, etc.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4307\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4307\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4306","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4306\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4306\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4306\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4306","id":1231137204,"node_id":"I_kwDODunzps5JYam0","number":4306,"title":"`load_dataset` does not work with certain filename.","user":{"login":"wusuowei60","id":57242693,"node_id":"MDQ6VXNlcjU3MjQyNjkz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/57242693?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/wusuowei60","html_url":"https:\/\/github.com\/wusuowei60","followers_url":"https:\/\/api.github.com\/users\/wusuowei60\/followers","following_url":"https:\/\/api.github.com\/users\/wusuowei60\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/wusuowei60\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/wusuowei60\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/wusuowei60\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/wusuowei60\/orgs","repos_url":"https:\/\/api.github.com\/users\/wusuowei60\/repos","events_url":"https:\/\/api.github.com\/users\/wusuowei60\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/wusuowei60\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Never mind. It is because of the caching of datasets..."],"created_at":1652188444000,"updated_at":1652209116000,"closed_at":1652209089000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nThis is a weird bug that took me some time to find out.\r\n\r\nI have a JSON dataset that I want to load with `load_dataset` like this:\r\n\r\n```\r\ndata_files = dict(train=\"train.json.zip\", val=\"val.json.zip\")\r\ndataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\r\n```\r\n\r\n## Expected results\r\nNo error.\r\n\r\n## Actual results\r\nThe val file is loaded as expected, but the train file throws JSON decoding error:\r\n\r\n```\r\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 <ipython-input-74-97947e92c100>:5 in <module>                                             \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 \/home\/tiankang\/software\/anaconda3\/lib\/python3.8\/site-packages\/datasets\/load.py:1687 in    \u2502\r\n\u2502 load_dataset                                                                              \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   1684 \u2502   try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES                       \u2502\r\n\u2502   1685 \u2502                                                                                  \u2502\r\n\u2502   1686 \u2502   # Download and prepare data                                                    \u2502\r\n\u2502 \u2771 1687 \u2502   builder_instance.download_and_prepare(                                         \u2502\r\n\u2502   1688 \u2502   \u2502   download_config=download_config,                                           \u2502\r\n\u2502   1689 \u2502   \u2502   download_mode=download_mode,                                               \u2502\r\n\u2502   1690 \u2502   \u2502   ignore_verifications=ignore_verifications,                                 \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 \/home\/tiankang\/software\/anaconda3\/lib\/python3.8\/site-packages\/datasets\/builder.py:605 in  \u2502\r\n\u2502 download_and_prepare                                                                      \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502    602 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   except ConnectionError:                                    \u2502\r\n\u2502    603 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   logger.warning(\"HF google storage unreachable. Downloa \u2502\r\n\u2502    604 \u2502   \u2502   \u2502   \u2502   \u2502   if not downloaded_from_gcs:                                    \u2502\r\n\u2502 \u2771  605 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   self._download_and_prepare(                                \u2502\r\n\u2502    606 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   dl_manager=dl_manager, verify_infos=verify_infos, **do \u2502\r\n\u2502    607 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   )                                                          \u2502\r\n\u2502    608 \u2502   \u2502   \u2502   \u2502   \u2502   # Sync info                                                    \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 \/home\/tiankang\/software\/anaconda3\/lib\/python3.8\/site-packages\/datasets\/builder.py:694 in  \u2502\r\n\u2502 _download_and_prepare                                                                     \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502    691 \u2502   \u2502   \u2502                                                                          \u2502\r\n\u2502    692 \u2502   \u2502   \u2502   try:                                                                   \u2502\r\n\u2502    693 \u2502   \u2502   \u2502   \u2502   # Prepare split will record examples associated to the split       \u2502\r\n\u2502 \u2771  694 \u2502   \u2502   \u2502   \u2502   self._prepare_split(split_generator, **prepare_split_kwargs)       \u2502\r\n\u2502    695 \u2502   \u2502   \u2502   except OSError as e:                                                   \u2502\r\n\u2502    696 \u2502   \u2502   \u2502   \u2502   raise OSError(                                                     \u2502\r\n\u2502    697 \u2502   \u2502   \u2502   \u2502   \u2502   \"Cannot find data file. \"                                      \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 \/home\/tiankang\/software\/anaconda3\/lib\/python3.8\/site-packages\/datasets\/builder.py:1151 in \u2502\r\n\u2502 _prepare_split                                                                            \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   1148 \u2502   \u2502                                                                              \u2502\r\n\u2502   1149 \u2502   \u2502   generator = self._generate_tables(**split_generator.gen_kwargs)            \u2502\r\n\u2502   1150 \u2502   \u2502   with ArrowWriter(features=self.info.features, path=fpath) as writer:       \u2502\r\n\u2502 \u2771 1151 \u2502   \u2502   \u2502   for key, table in logging.tqdm(                                        \u2502\r\n\u2502   1152 \u2502   \u2502   \u2502   \u2502   generator, unit=\" tables\", leave=False, disable=True  # not loggin \u2502\r\n\u2502   1153 \u2502   \u2502   \u2502   ):                                                                     \u2502\r\n\u2502   1154 \u2502   \u2502   \u2502   \u2502   writer.write_table(table)                                          \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 \/home\/tiankang\/software\/anaconda3\/lib\/python3.8\/site-packages\/tqdm\/notebook.py:257 in     \u2502\r\n\u2502 __iter__                                                                                  \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   254 \u2502                                                                                   \u2502\r\n\u2502   255 \u2502   def __iter__(self):                                                             \u2502\r\n\u2502   256 \u2502   \u2502   try:                                                                        \u2502\r\n\u2502 \u2771 257 \u2502   \u2502   \u2502   for obj in super(tqdm_notebook, self).__iter__():                       \u2502\r\n\u2502   258 \u2502   \u2502   \u2502   \u2502   # return super(tqdm...) will not catch exception                    \u2502\r\n\u2502   259 \u2502   \u2502   \u2502   \u2502   yield obj                                                           \u2502\r\n\u2502   260 \u2502   \u2502   # NB: except ... [ as ...] breaks IPython async KeyboardInterrupt           \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 \/home\/tiankang\/software\/anaconda3\/lib\/python3.8\/site-packages\/tqdm\/std.py:1183 in         \u2502\r\n\u2502 __iter__                                                                                  \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   1180 \u2502   \u2502   # If the bar is disabled, then just walk the iterable                      \u2502\r\n\u2502   1181 \u2502   \u2502   # (note: keep this check outside the loop for performance)                 \u2502\r\n\u2502   1182 \u2502   \u2502   if self.disable:                                                           \u2502\r\n\u2502 \u2771 1183 \u2502   \u2502   \u2502   for obj in iterable:                                                   \u2502\r\n\u2502   1184 \u2502   \u2502   \u2502   \u2502   yield obj                                                          \u2502\r\n\u2502   1185 \u2502   \u2502   \u2502   return                                                                 \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 \/home\/tiankang\/software\/anaconda3\/lib\/python3.8\/site-packages\/datasets\/packaged_modules\/j \u2502\r\n\u2502 son\/json.py:90 in _generate_tables                                                        \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502    87 \u2502   \u2502   \u2502   # If the file is one json object and if we need to look at the list of  \u2502\r\n\u2502    88 \u2502   \u2502   \u2502   if self.config.field is not None:                                       \u2502\r\n\u2502    89 \u2502   \u2502   \u2502   \u2502   with open(file, encoding=\"utf-8\") as f:                             \u2502\r\n\u2502 \u2771  90 \u2502   \u2502   \u2502   \u2502   \u2502   dataset = json.load(f)                                          \u2502\r\n\u2502    91 \u2502   \u2502   \u2502   \u2502                                                                       \u2502\r\n\u2502    92 \u2502   \u2502   \u2502   \u2502   # We keep only the field we are interested in                       \u2502\r\n\u2502    93 \u2502   \u2502   \u2502   \u2502   dataset = dataset[self.config.field]                                \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 \/home\/tiankang\/software\/anaconda3\/lib\/python3.8\/json\/__init__.py:293 in load              \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   290 \u2502   To use a custom ``JSONDecoder`` subclass, specify it with the ``cls``           \u2502\r\n\u2502   291 \u2502   kwarg; otherwise ``JSONDecoder`` is used.                                       \u2502\r\n\u2502   292 \u2502   \"\"\"                                                                             \u2502\r\n\u2502 \u2771 293 \u2502   return loads(fp.read(),                                                         \u2502\r\n\u2502   294 \u2502   \u2502   cls=cls, object_hook=object_hook,                                           \u2502\r\n\u2502   295 \u2502   \u2502   parse_float=parse_float, parse_int=parse_int,                               \u2502\r\n\u2502   296 \u2502   \u2502   parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)   \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 \/home\/tiankang\/software\/anaconda3\/lib\/python3.8\/json\/__init__.py:357 in loads             \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   354 \u2502   if (cls is None and object_hook is None and                                     \u2502\r\n\u2502   355 \u2502   \u2502   \u2502   parse_int is None and parse_float is None and                           \u2502\r\n\u2502   356 \u2502   \u2502   \u2502   parse_constant is None and object_pairs_hook is None and not kw):       \u2502\r\n\u2502 \u2771 357 \u2502   \u2502   return _default_decoder.decode(s)                                           \u2502\r\n\u2502   358 \u2502   if cls is None:                                                                 \u2502\r\n\u2502   359 \u2502   \u2502   cls = JSONDecoder                                                           \u2502\r\n\u2502   360 \u2502   if object_hook is not None:                                                     \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 \/home\/tiankang\/software\/anaconda3\/lib\/python3.8\/json\/decoder.py:337 in decode             \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   334 \u2502   \u2502   containing a JSON document).                                                \u2502\r\n\u2502   335 \u2502   \u2502                                                                               \u2502\r\n\u2502   336 \u2502   \u2502   \"\"\"                                                                         \u2502\r\n\u2502 \u2771 337 \u2502   \u2502   obj, end = self.raw_decode(s, idx=_w(s, 0).end())                           \u2502\r\n\u2502   338 \u2502   \u2502   end = _w(s, end).end()                                                      \u2502\r\n\u2502   339 \u2502   \u2502   if end != len(s):                                                           \u2502\r\n\u2502   340 \u2502   \u2502   \u2502   raise JSONDecodeError(\"Extra data\", s, end)                             \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 \/home\/tiankang\/software\/anaconda3\/lib\/python3.8\/json\/decoder.py:353 in raw_decode         \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   350 \u2502   \u2502                                                                               \u2502\r\n\u2502   351 \u2502   \u2502   \"\"\"                                                                         \u2502\r\n\u2502   352 \u2502   \u2502   try:                                                                        \u2502\r\n\u2502 \u2771 353 \u2502   \u2502   \u2502   obj, end = self.scan_once(s, idx)                                       \u2502\r\n\u2502   354 \u2502   \u2502   except StopIteration as err:                                                \u2502\r\n\u2502   355 \u2502   \u2502   \u2502   raise JSONDecodeError(\"Expecting value\", s, err.value) from None        \u2502\r\n\u2502   356 \u2502   \u2502   return obj, end                                                             \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nJSONDecodeError: Unterminated string starting at: line 85 column 20 (char 60051)\r\n```\r\n\r\nHowever, when I rename the `train.json.zip` to other names (like `training.json.zip`, or even to `train.json`), everything works fine; when I unzip the file to `train.json`, it works as well.\r\n\r\n## Environment info\r\n```\r\n- `datasets` version: 2.1.0\r\n- Platform: Linux-4.4.0-131-generic-x86_64-with-glibc2.10\r\n- Python version: 3.8.5\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.2\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4306\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4306\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4305","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4305\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4305\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4305\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4305","id":1231099934,"node_id":"PR_kwDODunzps43kt4P","number":4305,"title":"Fixes FrugalScore","user":{"login":"moussaKam","id":28675016,"node_id":"MDQ6VXNlcjI4Njc1MDE2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/28675016?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/moussaKam","html_url":"https:\/\/github.com\/moussaKam","followers_url":"https:\/\/api.github.com\/users\/moussaKam\/followers","following_url":"https:\/\/api.github.com\/users\/moussaKam\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/moussaKam\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/moussaKam\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/moussaKam\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/moussaKam\/orgs","repos_url":"https:\/\/api.github.com\/users\/moussaKam\/repos","events_url":"https:\/\/api.github.com\/users\/moussaKam\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/moussaKam\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4305). All of your documentation changes will be reflected on that endpoint.","> predictions and references are swapped. Basically Frugalscore is commutative, however some tiny differences can occur if we swap the references and the predictions. I decided to swap them just to obtain the exact results as reported in the paper.\r\n\r\nWhat is the order of magnitude of the difference ? Do you know what causes this ?\r\n\r\n> I switched to dynamic padding that was was used in the training, forcing the padding to max_length introduces errors for some reason that I ignore.\r\n\r\nWhat error ?"],"created_at":1652186646000,"updated_at":1652263399000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4305","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4305","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4305.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4305.patch","merged_at":null},"body":"There are two minor modifications in this PR:\r\n1) `predictions` and `references` are swapped. Basically Frugalscore is commutative, however some tiny differences can occur if we swap the references and the predictions. I decided to swap them just to obtain the exact results as reported in the paper.\r\n2) I switched to dynamic padding that was was used in the training, forcing the padding to `max_length`  introduces errors for some reason that I ignore.\r\n\r\n@lhoestq ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4305\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4305\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4304","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4304\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4304\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4304\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4304","id":1231047051,"node_id":"I_kwDODunzps5JYEmL","number":4304,"title":"Language code search does direct matches","user":{"login":"leondz","id":121934,"node_id":"MDQ6VXNlcjEyMTkzNA==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/121934?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/leondz","html_url":"https:\/\/github.com\/leondz","followers_url":"https:\/\/api.github.com\/users\/leondz\/followers","following_url":"https:\/\/api.github.com\/users\/leondz\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/leondz\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/leondz\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/leondz\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/leondz\/orgs","repos_url":"https:\/\/api.github.com\/users\/leondz\/repos","events_url":"https:\/\/api.github.com\/users\/leondz\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/leondz\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Thanks for reporting ! I forwarded the issue to the front-end team :)\r\n\r\nWill keep you posted !\r\n\r\nI also changed the tagging app to suggest two letters code for now."],"created_at":1652183956000,"updated_at":1652186322000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\n\r\nHi. Searching for bcp47 tags that are just the language prefix (e.g. `sq` or `da`) excludes datasets that have added extra information in their language metadata (e.g. `sq-AL` or `da-bornholm`). The example codes given in the [tagging app](https:\/\/huggingface.co\/spaces\/huggingface\/datasets-tagging) encourages addition of the additional codes (\"_expected format is BCP47 tags separated for ';' e.g. 'en-US;fr-FR'_\") but this would lead to those datasets being hidden in datasets search.\r\n\r\n## Steps to reproduce the bug\r\n1. Add a dataset using a variant tag (e.g. [`sq-AL`](https:\/\/huggingface.co\/datasets?languages=languages:sq-AL))\r\n2. Look for datasets using the full code \r\n3. Note that they're missing when just the language is searched for (e.g. [`sq`](https:\/\/huggingface.co\/datasets?languages=languages:sq))\r\n\r\nSome datasets are already affected by this - e.g. `AmazonScience\/massive` is listed under `sq-AL` but not `sq`.\r\n\r\nOne workaround is for dataset creators to add an additional root language tag to dataset YAML metadata, but it's unclear how to communicate this. It might be possible to index the search on `languagecode.split('-')[0]` but I wanted to float this issue before trying to write any code :)\r\n\r\n## Expected results\r\nDatasets using longer bcp47 tags also appear under searches for just the language code; e.g. Quebecois datasets (`fr-CA`) would come up when looking for French datasets with no region specification (`fr`), or US English (`en-US`) datasets would come up when searching for English datasets (`en`).\r\n\r\n## Actual results\r\nThe language codes seem to be directly string matched, excluding datasets with specific language tags from non-specific searches.\r\n\r\n## Environment info\r\n(web app)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4304\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4304\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4303","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4303\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4303\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4303\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4303","id":1230867728,"node_id":"PR_kwDODunzps43j8cH","number":4303,"title":"Fix: Add missing comma","user":{"login":"mrm8488","id":3653789,"node_id":"MDQ6VXNlcjM2NTM3ODk=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3653789?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mrm8488","html_url":"https:\/\/github.com\/mrm8488","followers_url":"https:\/\/api.github.com\/users\/mrm8488\/followers","following_url":"https:\/\/api.github.com\/users\/mrm8488\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mrm8488\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mrm8488\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mrm8488\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mrm8488\/orgs","repos_url":"https:\/\/api.github.com\/users\/mrm8488\/repos","events_url":"https:\/\/api.github.com\/users\/mrm8488\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mrm8488\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The CI failure is unrelated to this PR and fixed on master, merging :)"],"created_at":1652174498000,"updated_at":1652259015000,"closed_at":1652259014000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4303","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4303","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4303.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4303.patch","merged_at":1652259014000},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4303\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4303\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4302","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4302\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4302\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4302\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4302","id":1230651117,"node_id":"PR_kwDODunzps43jPE5","number":4302,"title":"Remove hacking license tags when mirroring datasets on the Hub","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4302). All of your documentation changes will be reflected on that endpoint.","The Hub doesn't allow these characters in the YAML tags, and git push fails if you want to push a dataset card containing these characters.","Ok, let me rename the bad config names :) I think I can also keep backward compatibility with a warning","Almost done with it btw, will submit a PR that shows all the configuration name changes (from a bit more than 20 datasets)"],"created_at":1652161966000,"updated_at":1652362157000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4302","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4302","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4302.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4302.patch","merged_at":null},"body":"Currently, when mirroring datasets on the Hub, the license tags are hacked: removed of characters \".\" and \"$\". On the contrary, this hacking is not applied to community datasets on the Hub. This generates multiple variants of the same tag on the Hub. \r\n\r\nI guess this hacking is no longer necessary:\r\n- it is not applied to community datasets\r\n- all canonical datasets are validated by maintainers before being merged: CI + maintainers make sure license tags are the right ones\r\n\r\nFix #4298.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4302\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4302\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4301","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4301\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4301\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4301\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4301","id":1230401256,"node_id":"PR_kwDODunzps43idlE","number":4301,"title":"Add ImageNet-Sketch dataset","user":{"login":"nateraw","id":32437151,"node_id":"MDQ6VXNlcjMyNDM3MTUx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/32437151?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nateraw","html_url":"https:\/\/github.com\/nateraw","followers_url":"https:\/\/api.github.com\/users\/nateraw\/followers","following_url":"https:\/\/api.github.com\/users\/nateraw\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nateraw\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nateraw\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nateraw\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nateraw\/orgs","repos_url":"https:\/\/api.github.com\/users\/nateraw\/repos","events_url":"https:\/\/api.github.com\/users\/nateraw\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nateraw\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4301). All of your documentation changes will be reflected on that endpoint."],"created_at":1652139525000,"updated_at":1652376654000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4301","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4301","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4301.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4301.patch","merged_at":null},"body":"This PR adds the ImageNet-Sketch dataset and resolves #3953 .","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4301\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4301\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4300","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4300\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4300\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4300\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4300","id":1230272761,"node_id":"PR_kwDODunzps43iA86","number":4300,"title":"Add API code examples for loading methods","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4300). All of your documentation changes will be reflected on that endpoint."],"created_at":1652131826000,"updated_at":1652376548000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4300","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4300","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4300.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4300.patch","merged_at":null},"body":"This PR adds API code examples for loading methods, let me know if I've missed any important parameters we should showcase :)\r\n\r\nI was a bit confused about `inspect_dataset` and `inspect_metric`. The `path` parameter says it will accept a dataset identifier from the Hub. But when I try the identifier `rotten_tomatoes`, it gives me:\r\n\r\n```py\r\nfrom datasets import inspect_dataset\r\ninspect_dataset('rotten_tomatoes', local_path='\/content\/rotten_tomatoes')\r\n\r\nFileNotFoundError: Couldn't find a dataset script at \/content\/rotten_tomatoes\/rotten_tomatoes.py or any data file in the same directory.\r\n```\r\n\r\nDoes the user need to have an existing copy of `rotten_tomatoes.py` on their local drive (in which case, it seems like the same option as the first option in `path`)?","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4300\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4300\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4299","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4299\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4299\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4299\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4299","id":1230236782,"node_id":"PR_kwDODunzps43h5RP","number":4299,"title":"Remove manual download from imagenet-1k","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4299). All of your documentation changes will be reflected on that endpoint."],"created_at":1652129358000,"updated_at":1652130131000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4299","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4299","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4299.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4299.patch","merged_at":null},"body":"Remove the manual download code from `imagenet-1k` to make it a regular dataset.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4299\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4299\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4298","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4298\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4298\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4298\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4298","id":1229748006,"node_id":"I_kwDODunzps5JTHcm","number":4298,"title":"Normalise license names","user":{"login":"leondz","id":121934,"node_id":"MDQ6VXNlcjEyMTkzNA==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/121934?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/leondz","html_url":"https:\/\/github.com\/leondz","followers_url":"https:\/\/api.github.com\/users\/leondz\/followers","following_url":"https:\/\/api.github.com\/users\/leondz\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/leondz\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/leondz\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/leondz\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/leondz\/orgs","repos_url":"https:\/\/api.github.com\/users\/leondz\/repos","events_url":"https:\/\/api.github.com\/users\/leondz\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/leondz\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["we'll add the same server-side metadata validation system as for hf.co\/models soon-ish\r\n\r\n(you can check on hf.co\/models that licenses are \"clean\")"],"created_at":1652104292000,"updated_at":1652204645000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"**Is your feature request related to a problem? Please describe.**\r\nWhen browsing datasets, the Licenses tag cloud (bottom left of e.g. https:\/\/huggingface.co\/datasets) has multiple variants of the same license. This means the options exclude datasets arbitrarily, giving users artificially low recall. The cause of the dupes is probably due to a bit of variation in metadata.\r\n\r\n**Describe the solution you'd like**\r\nI'd like the licenses in metadata to follow the same standard as much as possible, to remove this problem. I'd like to go ahead and normalise the dataset metadata to follow the format & values given in [src\/datasets\/utils\/resources\/licenses.json](https:\/\/github.com\/huggingface\/datasets\/blob\/master\/src\/datasets\/utils\/resources\/licenses.json) .\r\n\r\n**Describe alternatives you've considered**\r\nNone\r\n\r\n**Additional context**\r\nNone\r\n\r\n**Priority** \r\nLow\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4298\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4298\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4297","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4297\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4297\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4297\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4297","id":1229735498,"node_id":"I_kwDODunzps5JTEZK","number":4297,"title":"Datasets YAML tagging space is down","user":{"login":"leondz","id":121934,"node_id":"MDQ6VXNlcjEyMTkzNA==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/121934?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/leondz","html_url":"https:\/\/github.com\/leondz","followers_url":"https:\/\/api.github.com\/users\/leondz\/followers","following_url":"https:\/\/api.github.com\/users\/leondz\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/leondz\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/leondz\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/leondz\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/leondz\/orgs","repos_url":"https:\/\/api.github.com\/users\/leondz\/repos","events_url":"https:\/\/api.github.com\/users\/leondz\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/leondz\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"assignees":[{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["@lhoestq @albertvillanova `update-task-list` branch does not exist anymore, should point to `main` now i guess","Thanks for reporting, fixing it now","It's up again :)"],"created_at":1652103905000,"updated_at":1652107465000,"closed_at":1652107465000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nThe neat hf spaces app for generating YAML tags for dataset `README.md`s is down\r\n\r\n## Steps to reproduce the bug\r\n1. Visit https:\/\/huggingface.co\/spaces\/huggingface\/datasets-tagging\r\n\r\n## Expected results\r\nThere'll be a HF spaces web app for generating dataset metadata YAML\r\n\r\n## Actual results\r\nThere's an error message; here's the step where it breaks:\r\n\r\n```\r\nStep 18\/29 : RUN pip install -r requirements.txt\r\n ---> Running in e88bfe7e7e0c\r\nDefaulting to user installation because normal site-packages is not writeable\r\nCollecting git+https:\/\/github.com\/huggingface\/datasets.git@update-task-list (from -r requirements.txt (line 4))\r\n  Cloning https:\/\/github.com\/huggingface\/datasets.git (to revision update-task-list) to \/tmp\/pip-req-build-bm8t0r0k\r\n  Running command git clone --filter=blob:none --quiet https:\/\/github.com\/huggingface\/datasets.git \/tmp\/pip-req-build-bm8t0r0k\r\n  WARNING: Did not find branch or tag 'update-task-list', assuming revision or ref.\r\n  Running command git checkout -q update-task-list\r\n  error: pathspec 'update-task-list' did not match any file(s) known to git\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 git checkout -q update-task-list did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> See above for output.\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: subprocess-exited-with-error\r\n\r\n\u00d7 git checkout -q update-task-list did not run successfully.\r\n\u2502 exit code: 1\r\n\u2570\u2500> See above for output.\r\n```\r\n\r\n## Environment info\r\n\r\n- Platform: Linux \/ Brave\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4297\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4297\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4296","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4296\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4296\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4296\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4296","id":1229554645,"node_id":"PR_kwDODunzps43foZ-","number":4296,"title":"Fix URL query parameters in compression hop path when streaming","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4296). All of your documentation changes will be reflected on that endpoint."],"created_at":1652095102000,"updated_at":1652096518000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4296","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4296","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4296.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4296.patch","merged_at":null},"body":"Fix #3488.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4296\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4296\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4295","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4295\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4295\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4295\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4295","id":1229527283,"node_id":"PR_kwDODunzps43fieR","number":4295,"title":"Fix missing lz4 dependency for tests","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1652093600000,"updated_at":1652095282000,"closed_at":1652094824000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4295","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4295","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4295.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4295.patch","merged_at":1652094824000},"body":"Currently, `lz4` is not defined as a dependency for tests. Therefore, all tests marked with `@require_lz4` are skipped.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4295\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4295\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4294","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4294\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4294\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4294\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4294","id":1229455582,"node_id":"PR_kwDODunzps43fTXA","number":4294,"title":"Fix CLI run_beam save_infos","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1652089663000,"updated_at":1652166244000,"closed_at":1652165770000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4294","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4294","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4294.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4294.patch","merged_at":1652165770000},"body":"Currently, it raises TypeError:\r\n```\r\nTypeError: _download_and_prepare() got an unexpected keyword argument 'save_infos'\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4294\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4294\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4293","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4293\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4293\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4293\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4293","id":1228815477,"node_id":"PR_kwDODunzps43dRt9","number":4293,"title":"Fix wrong map parameter name in cache docs","user":{"login":"h4iku","id":3812788,"node_id":"MDQ6VXNlcjM4MTI3ODg=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3812788?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/h4iku","html_url":"https:\/\/github.com\/h4iku","followers_url":"https:\/\/api.github.com\/users\/h4iku\/followers","following_url":"https:\/\/api.github.com\/users\/h4iku\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/h4iku\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/h4iku\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/h4iku\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/h4iku\/orgs","repos_url":"https:\/\/api.github.com\/users\/h4iku\/repos","events_url":"https:\/\/api.github.com\/users\/h4iku\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/h4iku\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4293). All of your documentation changes will be reflected on that endpoint."],"created_at":1651994866000,"updated_at":1651995554000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4293","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4293","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4293.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4293.patch","merged_at":null},"body":"The `load_from_cache` parameter of `map` should be `load_from_cache_file`.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4293\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4293\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4292","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4292\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4292\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4292\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4292","id":1228216788,"node_id":"PR_kwDODunzps43bhrp","number":4292,"title":"Add API code examples for remaining main classes","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4292). All of your documentation changes will be reflected on that endpoint."],"created_at":1651860931000,"updated_at":1652306365000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4292","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4292","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4292.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4292.patch","merged_at":null},"body":"This PR adds API code examples for the remaining functions in the Main classes. I wasn't too familiar with some of the functions (`decode_batch`, `decode_column`, `decode_example`, etc.) so please feel free to add an example of usage and I can fill in the rest :)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4292\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4292\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4291","id":1227777500,"node_id":"I_kwDODunzps5JLmXc","number":4291,"title":"Dataset Viewer issue for strombergnlp\/ipm_nel : preview is empty, no error message","user":{"login":"leondz","id":121934,"node_id":"MDQ6VXNlcjEyMTkzNA==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/121934?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/leondz","html_url":"https:\/\/github.com\/leondz","followers_url":"https:\/\/api.github.com\/users\/leondz\/followers","following_url":"https:\/\/api.github.com\/users\/leondz\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/leondz\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/leondz\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/leondz\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/leondz\/orgs","repos_url":"https:\/\/api.github.com\/users\/leondz\/repos","events_url":"https:\/\/api.github.com\/users\/leondz\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/leondz\/received_events","type":"User","site_admin":false},"labels":[{"id":3470211881,"node_id":"LA_kwDODunzps7O1zsp","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer","name":"dataset-viewer","color":"E5583E","default":false,"description":"Related to the dataset viewer on huggingface.co"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi @leondz, thanks for reporting.\r\n\r\nIndeed, the dataset viewer relies on the dataset being streamable (passing `streaming=True` to `load_dataset`). Whereas most of the datastes are streamable out of the box (thanks to our implementation of streaming), there are still some exceptions.\r\n\r\nIn particular, in your case, that is due to the data file being TAR. This format is not streamable out of the box (it does not allow random access to the archived files), but we use a trick to allow streaming: using `dl_manager.iter_archive`.\r\n\r\nLet me know if you need some help: I could push a commit to your repo with the fix.","Ah, right! The preview is working now, but this explanation is good to know, thank you. I'll prefer formats with random file access supported in datasets.utils.extract in future, and try out this fix for the tarfiles :)"],"created_at":1651838607000,"updated_at":1652084758000,"closed_at":1652084758000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"### Link\n\nhttps:\/\/huggingface.co\/datasets\/strombergnlp\/ipm_nel\/viewer\/ipm_nel\/train\n\n### Description\n\nThe viewer is blank. I tried my best to emulate a dataset with a working viewer, but this one just doesn't seem to want to come up. What did I miss?\n\n### Owner\n\nYes","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4290","id":1227592826,"node_id":"PR_kwDODunzps43Zr08","number":4290,"title":"Update README.md","user":{"login":"monk1337","id":17107749,"node_id":"MDQ6VXNlcjE3MTA3NzQ5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17107749?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/monk1337","html_url":"https:\/\/github.com\/monk1337","followers_url":"https:\/\/api.github.com\/users\/monk1337\/followers","following_url":"https:\/\/api.github.com\/users\/monk1337\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/monk1337\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/monk1337\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/monk1337\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/monk1337\/orgs","repos_url":"https:\/\/api.github.com\/users\/monk1337\/repos","events_url":"https:\/\/api.github.com\/users\/monk1337\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/monk1337\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4290). All of your documentation changes will be reflected on that endpoint."],"created_at":1651827171000,"updated_at":1651827848000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4290","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4290","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4290.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4290.patch","merged_at":null},"body":"Updating readme in medmcqa dataset.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4288","id":1226821732,"node_id":"PR_kwDODunzps43XLKi","number":4288,"title":"Add missing `faiss` import to fix https:\/\/github.com\/huggingface\/datasets\/issues\/4287","user":{"login":"alvarobartt","id":36760800,"node_id":"MDQ6VXNlcjM2NzYwODAw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36760800?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alvarobartt","html_url":"https:\/\/github.com\/alvarobartt","followers_url":"https:\/\/api.github.com\/users\/alvarobartt\/followers","following_url":"https:\/\/api.github.com\/users\/alvarobartt\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alvarobartt\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alvarobartt\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alvarobartt\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alvarobartt\/orgs","repos_url":"https:\/\/api.github.com\/users\/alvarobartt\/repos","events_url":"https:\/\/api.github.com\/users\/alvarobartt\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alvarobartt\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1651764109000,"updated_at":1652187306000,"closed_at":1652184588000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4288","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4288","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4288.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4288.patch","merged_at":1652184588000},"body":"This PR fixes the issue recently mentioned in https:\/\/github.com\/huggingface\/datasets\/issues\/4287 \ud83e\udd17 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4287","id":1226806652,"node_id":"I_kwDODunzps5JH5V8","number":4287,"title":"\"NameError: name 'faiss' is not defined\" on `.add_faiss_index` when `device` is not None","user":{"login":"alvarobartt","id":36760800,"node_id":"MDQ6VXNlcjM2NzYwODAw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36760800?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alvarobartt","html_url":"https:\/\/github.com\/alvarobartt","followers_url":"https:\/\/api.github.com\/users\/alvarobartt\/followers","following_url":"https:\/\/api.github.com\/users\/alvarobartt\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alvarobartt\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alvarobartt\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alvarobartt\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alvarobartt\/orgs","repos_url":"https:\/\/api.github.com\/users\/alvarobartt\/repos","events_url":"https:\/\/api.github.com\/users\/alvarobartt\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alvarobartt\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["So I managed to solve this by adding a missing `import faiss` in the `@staticmethod` defined in https:\/\/github.com\/huggingface\/datasets\/blob\/f51b6994db27ea69261ef919fb7775928f9ec10b\/src\/datasets\/search.py#L305, triggered from https:\/\/github.com\/huggingface\/datasets\/blob\/f51b6994db27ea69261ef919fb7775928f9ec10b\/src\/datasets\/search.py#L249 when trying to `ds_with_embeddings.add_faiss_index(column='embeddings', device=0)` with the code above.\r\n\r\nAs it seems that the `@staticmethod` doesn't recognize the `import faiss` defined in https:\/\/github.com\/huggingface\/datasets\/blob\/f51b6994db27ea69261ef919fb7775928f9ec10b\/src\/datasets\/search.py#L261, so whenever the value of `device` is not None in https:\/\/github.com\/huggingface\/datasets\/blob\/71f76e0bdeaddadedc4f9c8d15cfff5a36d62f66\/src\/datasets\/search.py#L438, that exception is triggered.\r\n\r\nSo on, adding `import faiss` inside https:\/\/github.com\/huggingface\/datasets\/blob\/71f76e0bdeaddadedc4f9c8d15cfff5a36d62f66\/src\/datasets\/search.py#L305 right after the check of `device`'s value, solves the issue and lets you calculate the indices in GPU.\r\n\r\nI'll add the code in a PR linked to this issue in case you want to merge it!","Adding here the complete error traceback!\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/home\/alvarobartt\/lol.py\", line 12, in <module>\r\n    ds_with_embeddings.add_faiss_index(column='embeddings', device=0) # default `device=None`\r\n  File \"\/home\/alvarobartt\/.local\/lib\/python3.9\/site-packages\/datasets\/arrow_dataset.py\", line 3656, in add_faiss_index\r\n    super().add_faiss_index(\r\n  File \"\/home\/alvarobartt\/.local\/lib\/python3.9\/site-packages\/datasets\/search.py\", line 478, in add_faiss_index\r\n    faiss_index.add_vectors(self, column=column, train_size=train_size, faiss_verbose=True)\r\n  File \"\/home\/alvarobartt\/.local\/lib\/python3.9\/site-packages\/datasets\/search.py\", line 281, in add_vectors\r\n    self.faiss_index = self._faiss_index_to_device(index, self.device)\r\n  File \"\/home\/alvarobartt\/.local\/lib\/python3.9\/site-packages\/datasets\/search.py\", line 327, in _faiss_index_to_device\r\n    faiss_res = faiss.StandardGpuResources()\r\nNameError: name 'faiss' is not defined\r\n```","Closed as https:\/\/github.com\/huggingface\/datasets\/pull\/4288 already merged! :hugs:"],"created_at":1651763385000,"updated_at":1652190799000,"closed_at":1652190799000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\n\r\nWhen using `datasets` to calculate the FAISS indices of a dataset, the exception `NameError: name 'faiss' is not defined` is triggered when trying to calculate those on a device (GPU), so `.add_faiss_index(..., device=0)` fails with that exception.\r\n\r\nAll that assuming that `datasets` is properly installed and `faiss-gpu` too, as well as all the CUDA drivers required.\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\n# Sample code to reproduce the bug\r\nfrom transformers import DPRContextEncoder, DPRContextEncoderTokenizer\r\nimport torch\r\ntorch.set_grad_enabled(False)\r\nctx_encoder = DPRContextEncoder.from_pretrained(\"facebook\/dpr-ctx_encoder-single-nq-base\")\r\nctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook\/dpr-ctx_encoder-single-nq-base\")\r\n\r\nfrom datasets import load_dataset\r\nds = load_dataset('crime_and_punish', split='train[:100]')\r\nds_with_embeddings = ds.map(lambda example: {'embeddings': ctx_encoder(**ctx_tokenizer(example[\"line\"], return_tensors=\"pt\"))[0][0].numpy()})\r\n\r\nds_with_embeddings.add_faiss_index(column='embeddings', device=0) # default `device=None`\r\n```\r\n\r\n## Expected results\r\n\r\nA new column named `embeddings` in the dataset that we're adding the index to.\r\n\r\n## Actual results\r\n\r\nAn exception is triggered with the following message `NameError: name 'faiss' is not defined`.\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 2.1.0\r\n- Platform: Linux-5.13.0-1022-azure-x86_64-with-glibc2.31\r\n- Python version: 3.9.12\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.2\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4286","id":1226758621,"node_id":"PR_kwDODunzps43W-DI","number":4286,"title":"Add Lahnda language tag","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651761260000,"updated_at":1652184604000,"closed_at":1652184158000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4286","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4286","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4286.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4286.patch","merged_at":1652184157000},"body":"This language is present in [Wikimedia's WIT](https:\/\/huggingface.co\/datasets\/wikimedia\/wit_base) dataset.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4285","id":1226374831,"node_id":"PR_kwDODunzps43VtEa","number":4285,"title":"Update LexGLUE README.md","user":{"login":"iliaschalkidis","id":1626984,"node_id":"MDQ6VXNlcjE2MjY5ODQ=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1626984?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/iliaschalkidis","html_url":"https:\/\/github.com\/iliaschalkidis","followers_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/followers","following_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/orgs","repos_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/repos","events_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651739810000,"updated_at":1651757944000,"closed_at":1651757615000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4285","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4285","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4285.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4285.patch","merged_at":1651757615000},"body":"Update the leaderboard based on the latest results presented in the ACL 2022 version of the article.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4284","id":1226200727,"node_id":"I_kwDODunzps5JFlaX","number":4284,"title":"Issues in processing very large datasets","user":{"login":"sajastu","id":10419055,"node_id":"MDQ6VXNlcjEwNDE5MDU1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10419055?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sajastu","html_url":"https:\/\/github.com\/sajastu","followers_url":"https:\/\/api.github.com\/users\/sajastu\/followers","following_url":"https:\/\/api.github.com\/users\/sajastu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sajastu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sajastu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sajastu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sajastu\/orgs","repos_url":"https:\/\/api.github.com\/users\/sajastu\/repos","events_url":"https:\/\/api.github.com\/users\/sajastu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sajastu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi ! `datasets` doesn't load the dataset in memory. Instead it uses memory mapping to load your dataset from your disk (it is stored as arrow files). Do you know at what point you have RAM issues exactly ?\r\n\r\nHow big are your graph_data_train dictionaries btw ?"],"created_at":1651726869000,"updated_at":1652184923000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nI'm trying to add a feature called \"subgraph\" to CNN\/DM dataset (modifications on run_summarization.py of Huggingface Transformers script) --- I'm not quite sure if I'm doing it the right way, though--- but the main problem appears when the training starts where the error ` [OSError: [Errno 12] Cannot allocate memory]`  appears. I suppose this problem roots in RAM issues and how the dataset is loaded during training, but I have no clue of what I can do to fix it.  Observing the dataset's cache directory, I see that it takes ~600GB of memory and that's why I believe special care is needed when loading it into the memory. \r\n\r\n\r\nHere are my modifications to `run_summarization.py` code. \r\n\r\n\r\n```\r\n# loading pre-computed dictionary where keys are 'id' of article and values are corresponding subgraph\r\ngraph_data_train = get_graph_data('train') \r\ngraph_data_validation = get_graph_data('val')\r\n...\r\n...\r\n\r\n\r\nwith training_args.main_process_first(desc=\"train dataset map pre-processing\"):\r\n    train_dataset = train_dataset.map(\r\n        preprocess_function_train,\r\n        batched=True,\r\n        num_proc=data_args.preprocessing_num_workers,\r\n        remove_columns=column_names,\r\n        load_from_cache_file=not data_args.overwrite_cache,\r\n        desc=\"Running tokenizer on train dataset\",\r\n    )\r\n\r\n```\r\n\r\n\r\nAnd here is the modified preprocessed function:\r\n\r\n```\r\ndef preprocess_function_train(examples):\r\n        inputs, targets, sub_graphs, ids = [], [], [], []\r\n        for i in range(len(examples[text_column])):\r\n            if examples[text_column][i] is not None and examples[summary_column][i] is not None:\r\n                # if examples['doc_id'][i] in graph_data.keys():\r\n                inputs.append(examples[text_column][i])\r\n                targets.append(examples[summary_column][i])\r\n                sub_graphs.append(graph_data_train[examples['id'][i]])\r\n                ids.append(examples['id'][i])\r\n\r\n        inputs = [prefix + inp for inp in inputs]\r\n        model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True,\r\n                                 sub_graphs=sub_graphs, ids=ids)\r\n\r\n            # Setup the tokenizer for targets\r\n        with tokenizer.as_target_tokenizer():\r\n            labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\r\n\r\n        # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\r\n        # padding in the loss.\r\n        if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\r\n            labels[\"input_ids\"] = [\r\n                [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\r\n            ]\r\n\r\n        model_inputs[\"labels\"] = labels[\"input_ids\"]\r\n        return model_inputs\r\n```\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:  2.1.0\r\n- Platform: Linux Ubuntu\r\n- Python version: 3.6\r\n- PyArrow version: 6.0.1\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4283","id":1225686988,"node_id":"PR_kwDODunzps43Tnxo","number":4283,"title":"Fix filesystem docstring","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651686162000,"updated_at":1651854722000,"closed_at":1651818137000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4283","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4283","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4283.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4283.patch","merged_at":1651818137000},"body":"This PR untangles the `S3FileSystem` docstring so the [parameters](https:\/\/huggingface.co\/docs\/datasets\/master\/en\/package_reference\/main_classes#parameters) are properly displayed.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4282","id":1225616545,"node_id":"PR_kwDODunzps43TZYL","number":4282,"title":"Don't do unnecessary list type casting to avoid replacing None values by empty lists","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Quick question about the message in the warning. You say \"will be fixed in a future major version\" but don't you mean \"will raise an error in a future major version\"?","Right ! Good catch, thanks, I updated the message to say \"will raise an error in a future major version\""],"created_at":1651682221000,"updated_at":1651833838000,"closed_at":1651833420000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4282","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4282","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4282.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4282.patch","merged_at":1651833420000},"body":"In certain cases, `None` values are replaced by empty lists when casting feature types.\r\n\r\nIt happens every time you cast an array of nested lists like [None, [0, 1, 2, 3]] to a different type (to change the integer precision for example). In this case you'd get [[], [0, 1, 2, 3]] for example. This issue comes from PyArrow, see the discussion in https:\/\/github.com\/huggingface\/datasets\/issues\/3676\r\n\r\nThis issue also happens when no type casting is needed, because casting is supposed to be a no-op in this case. But as https:\/\/github.com\/huggingface\/datasets\/issues\/3676 shown, it's not the case and `None` are replaced by empty lists even if we cast to the exact same type.\r\n\r\nIn this PR I just workaround this bug in the case where no type casting is needed. In particular, I only call `pa.ListArray.from_arrays` only when necessary.\r\n\r\nI also added a warning when some `None` are effectively replaced by empty lists. I wanted to raise an error in this case, but maybe we should wait a major update to do so\r\n\r\nThis PR fixes this particular case, that is occurring in `run_qa.py` in `transformers`:\r\n```python\r\nfrom datasets import Dataset\r\n\r\nds = Dataset.from_dict({\"a\": range(4)})\r\nds = ds.map(lambda x: {\"b\": [[None, [0]]]}, batched=True, batch_size=1, remove_columns=[\"a\"])\r\nprint(ds.to_pandas())\r\n# before:\r\n#              b\r\n# 0  [None, [0]]\r\n# 1    [[], [0]]\r\n# 2    [[], [0]]\r\n# 3    [[], [0]]\r\n#\r\n# now:\r\n#              b\r\n# 0  [None, [0]]\r\n# 1  [None, [0]]\r\n# 2  [None, [0]]\r\n# 3  [None, [0]]\r\n```\r\n\r\ncc @sgugger ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4281","id":1225556939,"node_id":"PR_kwDODunzps43TNBm","number":4281,"title":"Remove a copy-paste sentence in dataset cards","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","The non-passing tests have nothing to do with this PR."],"created_at":1651678915000,"updated_at":1651826283000,"closed_at":1651689196000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4281","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4281","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4281.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4281.patch","merged_at":1651689196000},"body":"Remove the following copy-paste sentence from dataset cards:\r\n```\r\nWe show detailed information for up to 5 configurations of the dataset.\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4280","id":1225446844,"node_id":"PR_kwDODunzps43S2xg","number":4280,"title":"Add missing features to commonsense_qa dataset","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","@albertvillanova it adds question_concept and id which is great. I suppose we'll talk about staying true to the format on another PR. ","Yes, let's merge this PR as it is: it adds missing features.\r\n\r\nA subsequent PR may address the request on changing the dataset feature structure."],"created_at":1651674266000,"updated_at":1651847037000,"closed_at":1651846606000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4280","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4280","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4280.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4280.patch","merged_at":1651846606000},"body":"Fix partially #4275.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4279","id":1225300273,"node_id":"PR_kwDODunzps43SXw5","number":4279,"title":"Update minimal PyArrow version warning","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651667169000,"updated_at":1651740658000,"closed_at":1651740227000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4279","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4279","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4279.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4279.patch","merged_at":1651740227000},"body":"Update the minimal PyArrow version warning (should've been part of #4250). ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4278","id":1225122123,"node_id":"PR_kwDODunzps43RyTs","number":4278,"title":"Add missing features to openbookqa dataset for additional config","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Let's merge this PR as it is: it adds missing features.\r\n\r\nA subsequent PR may address the request on changing the data feature structure."],"created_at":1651656170000,"updated_at":1651842800000,"closed_at":1651842361000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4278","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4278","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4278.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4278.patch","merged_at":1651842361000},"body":"Fix partially #4276.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4277","id":1225002286,"node_id":"PR_kwDODunzps43RZV9","number":4277,"title":"Enable label alignment for token classification datasets","user":{"login":"lewtun","id":26859204,"node_id":"MDQ6VXNlcjI2ODU5MjA0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/26859204?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lewtun","html_url":"https:\/\/github.com\/lewtun","followers_url":"https:\/\/api.github.com\/users\/lewtun\/followers","following_url":"https:\/\/api.github.com\/users\/lewtun\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lewtun\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lewtun\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lewtun\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lewtun\/orgs","repos_url":"https:\/\/api.github.com\/users\/lewtun\/repos","events_url":"https:\/\/api.github.com\/users\/lewtun\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lewtun\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Hmm, not sure why the Windows tests are failing with:\r\n\r\n```\r\nDid not find path entry C:\\tools\\miniconda3\\bin\r\nC:\\tools\\miniconda3\\envs\\py37\\python.exe: No module named pytest\r\n```\r\n\r\nEdit: running the CI again fixed the problem \ud83d\ude43 ","> One last nit and we can merge then\r\n\r\nThanks, done!"],"created_at":1651648516000,"updated_at":1651851735000,"closed_at":1651851391000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4277","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4277","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4277.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4277.patch","merged_at":1651851391000},"body":"This PR extends the `Dataset.align_labels_with_mapping()` method to support alignment of label mappings between datasets and models for token classification (e.g. NER).\r\n\r\nExample of usage:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nner_ds = load_dataset(\"conll2003\", split=\"train\")\r\n# returns [3, 0, 7, 0, 0, 0, 7, 0, 0]\r\nner_ds[0][\"ner_tags\"]\r\n# hypothetical model mapping with O <--> B-LOC\r\nlabel2id = {\r\n    \"B-LOC\": \"0\",\r\n    \"B-MISC\": \"7\",\r\n    \"B-ORG\": \"3\",\r\n    \"B-PER\": \"1\",\r\n    \"I-LOC\": \"6\",\r\n    \"I-MISC\": \"8\",\r\n    \"I-ORG\": \"4\",\r\n    \"I-PER\": \"2\",\r\n    \"O\": \"5\"\r\n  }\r\nner_aligned_ds = ner_ds.align_labels_with_mapping(label2id, \"ner_tags\")\r\n# returns [3, 5, 7, 5, 5, 5, 7, 5, 5]\r\nner_aligned_ds[0][\"ner_tags\"]\r\n```\r\n\r\nContext: we need this in AutoTrain to automatically align datasets \/ models during evaluation. cc @abhishekkrthakur ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4276","id":1224949252,"node_id":"I_kwDODunzps5JAz4E","number":4276,"title":"OpenBookQA has missing and inconsistent field names","user":{"login":"vblagoje","id":458335,"node_id":"MDQ6VXNlcjQ1ODMzNQ==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/458335?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/vblagoje","html_url":"https:\/\/github.com\/vblagoje","followers_url":"https:\/\/api.github.com\/users\/vblagoje\/followers","following_url":"https:\/\/api.github.com\/users\/vblagoje\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/vblagoje\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/vblagoje\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/vblagoje\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/vblagoje\/orgs","repos_url":"https:\/\/api.github.com\/users\/vblagoje\/repos","events_url":"https:\/\/api.github.com\/users\/vblagoje\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/vblagoje\/received_events","type":"User","site_admin":false},"labels":[{"id":2067388877,"node_id":"MDU6TGFiZWwyMDY3Mzg4ODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20bug","name":"dataset bug","color":"2edb81","default":false,"description":"A bug in a dataset script provided in the library"}],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Thanks for reporting, @vblagoje.\r\n\r\nIndeed, I noticed some of these issues while reviewing this PR:\r\n- #4259 \r\n\r\nThis is in my TODO list. ","Ok, awesome @albertvillanova How about #4275 ?","On the other hand, I am not sure if we should always preserve the original nested structure. I think we should also consider other factors as convenience or consistency.\r\n\r\nFor example, other datasets also flatten \"question.stem\" into \"question\":\r\n- ai2_arc:\r\n  ```python\r\n  question = data[\"question\"][\"stem\"]\r\n  choices = data[\"question\"][\"choices\"]\r\n  text_choices = [choice[\"text\"] for choice in choices]\r\n  label_choices = [choice[\"label\"] for choice in choices]\r\n  yield id_, {\r\n      \"id\": id_,\r\n      \"answerKey\": answerkey,\r\n      \"question\": question,\r\n      \"choices\": {\"text\": text_choices, \"label\": label_choices},\r\n  }\r\n  ```\r\n- commonsense_qa:\r\n  ```python\r\n  question = data[\"question\"]\r\n  stem = question[\"stem\"]\r\n  yield id_, {\r\n      \"answerKey\": answerkey,\r\n      \"question\": stem,\r\n      \"choices\": {\"label\": labels, \"text\": texts},\r\n  }\r\n  ```\r\n- cos_e:\r\n  ```python\r\n  \"question\": cqa[\"question\"][\"stem\"],\r\n  ```\r\n- qasc\r\n- quartz\r\n- wiqa\r\n\r\nExceptions:\r\n- exams\r\n\r\nI think we should agree on a CONVENIENT format for QA and use always CONSISTENTLY the same.","@albertvillanova I agree that we should be consistent. In the last month, I have come across tons of code that deals with OpenBookQA and CommonSenseQA and all of that code relies on the original data format structure. We can't expect users to adopt HF Datasets if we arbitrarily change the structure of the format just because we think something makes more sense. I am in that position now (downloading original data rather than using HF Datasets) and undoubtedly it hinders HF Datasets' widespread use and adoption. Missing fields like in the case of #4275 is definitely bad and not even up for a discussion IMHO! cc @lhoestq ","I'm opening a PR that adds the missing fields.\r\n\r\nLet's agree on the feature structure: @lhoestq @mariosasko @polinaeterna ","IMO we should always try to preserve the original structure unless there is a good reason not to (and I don't see one in this case).","I agree with @mariosasko . The transition to the original format could be done in one PR for the next minor release, clearly documenting all dataset changes just as @albertvillanova outlined them above and perhaps even providing a per dataset util method to convert the new valid format to the old for backward compatibility. Users who relied on the old format will update their code with either the util method for a quick fix or slightly more elaborate for the new. ","I don't have a strong opinion on this, besides the fact that whatever decision we agree on, should be applied to all datasets.\r\n\r\nThere is always the tension between:\r\n- preserving each dataset original structure (which has the advantage of not forcing users to learn other structure for the same dataset),\r\n-  and on the other hand performing some king of standardization\/harmonization depending on the task (this has the advantage that once learnt, the same structure applies to all datasets; this has been done for e.g.  POS tagging: all datasets have been adapted to a certain \"standard\" structure).\r\n   - Another advantage: datasets can easily be interchanged (or joined) to be used by the same model\r\n\r\nRecently, in the BigScience BioMedical hackathon, they adopted a different approach:\r\n- they implement a \"source\" config, respecting the original structure as much as possible\r\n- they implement additional config for each task, with a \"standard\" nested structure per task, which is most useful for users.","@albertvillanova, thanks for the detailed answer and the new perspectives. I understand the friction for the best design approach much better now.  Ultimately, it is essential to include all the missing fields and the correct data first. Whatever approach is determined to be optimal is important but not as crucial once all the data is there, and users can create lambda functions to create whatever structure serves them best. "],"created_at":1651643512000,"updated_at":1652013223000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nOpenBookQA implementation is inconsistent with the original dataset.\r\n\r\nWe need to:\r\n\r\n1. The dataset field [question][stem] is flattened into question_stem. Unflatten it to match the original format.\r\n2. Add missing additional fields:\r\n    - 'fact1': row['fact1'],\r\n    - 'humanScore': row['humanScore'],\r\n    - 'clarity': row['clarity'],\r\n    - 'turkIdAnonymized': row['turkIdAnonymized']\r\n3. Ensure the structure and every data item in the original OpenBookQA matches our OpenBookQA version.\r\n\r\n## Expected results\r\nThe structure and every data item in the original OpenBookQA matches our OpenBookQA version.\r\n\r\n## Actual results\r\nTBD\r\n\r\n## Environment info\r\n- `datasets` version: 2.1.0\r\n- Platform: macOS-10.15.7-x86_64-i386-64bit\r\n- Python version: 3.8.13\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4275","id":1224943414,"node_id":"I_kwDODunzps5JAyc2","number":4275,"title":"CommonSenseQA has missing and inconsistent field names","user":{"login":"vblagoje","id":458335,"node_id":"MDQ6VXNlcjQ1ODMzNQ==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/458335?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/vblagoje","html_url":"https:\/\/github.com\/vblagoje","followers_url":"https:\/\/api.github.com\/users\/vblagoje\/followers","following_url":"https:\/\/api.github.com\/users\/vblagoje\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/vblagoje\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/vblagoje\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/vblagoje\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/vblagoje\/orgs","repos_url":"https:\/\/api.github.com\/users\/vblagoje\/repos","events_url":"https:\/\/api.github.com\/users\/vblagoje\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/vblagoje\/received_events","type":"User","site_admin":false},"labels":[{"id":2067388877,"node_id":"MDU6TGFiZWwyMDY3Mzg4ODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20bug","name":"dataset bug","color":"2edb81","default":false,"description":"A bug in a dataset script provided in the library"}],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Thanks for reporting, @vblagoje.\r\n\r\nI'm opening a PR to address this. "],"created_at":1651642739000,"updated_at":1651664478000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nIn short, CommonSenseQA implementation is inconsistent with the original dataset.\r\n\r\nMore precisely, we need to:\r\n\r\n1. Add the dataset matching \"id\" field. The current dataset, instead, regenerates monotonically increasing id.  \r\n2. The [\u201cquestion\u201d][\u201cstem\u201d] field is flattened into \"question\". We should match the original dataset and unflatten it\r\n3. Add the missing \"question_concept\" field in the question tree node\r\n4. Anything else? Go over the data structure of the newly repaired CommonSenseQA and make sure it matches the original\r\n\r\n## Expected results\r\nEvery data item of the CommonSenseQA should structurally and data-wise match the original CommonSenseQA dataset.\r\n\r\n## Actual results\r\nTBD\r\n\r\n## Environment info\r\n- `datasets` version: 2.1.0\r\n- Platform: macOS-10.15.7-x86_64-i386-64bit\r\n- Python version: 3.8.13\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4274","id":1224740303,"node_id":"PR_kwDODunzps43Qm2w","number":4274,"title":"Add API code examples for IterableDataset","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651617857000,"updated_at":1651681772000,"closed_at":1651681324000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4274","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4274","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4274.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4274.patch","merged_at":1651681324000},"body":"This PR adds API code examples for `IterableDataset` and `IterableDatasetDicts`.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4273","id":1224681036,"node_id":"PR_kwDODunzps43QaA6","number":4273,"title":"leadboard info added for TNE","user":{"login":"yanaiela","id":8031035,"node_id":"MDQ6VXNlcjgwMzEwMzU=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8031035?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yanaiela","html_url":"https:\/\/github.com\/yanaiela","followers_url":"https:\/\/api.github.com\/users\/yanaiela\/followers","following_url":"https:\/\/api.github.com\/users\/yanaiela\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yanaiela\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yanaiela\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yanaiela\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yanaiela\/orgs","repos_url":"https:\/\/api.github.com\/users\/yanaiela\/repos","events_url":"https:\/\/api.github.com\/users\/yanaiela\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yanaiela\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651613741000,"updated_at":1651757124000,"closed_at":1651756693000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4273","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4273","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4273.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4273.patch","merged_at":1651756693000},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4272","id":1224635660,"node_id":"PR_kwDODunzps43QQQt","number":4272,"title":"Fix typo in logging docs","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","> This PR fixes #4271.\r\n\r\nThings have not changed when searching \"tqdm\" in the Dataset document. The second result still performs as \"Enable\".","Hi @jiangwy99, the fix will appear on the `main` version of the docs:\r\n\r\n![Screen Shot 2022-05-04 at 8 38 29 AM](https:\/\/user-images.githubusercontent.com\/59462357\/166718225-6848ab91-87d1-4572-9912-40a909af6cb9.png)\r\n","Fixed now, thanks."],"created_at":1651610877000,"updated_at":1651678947000,"closed_at":1651647516000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4272","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4272","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4272.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4272.patch","merged_at":1651647515000},"body":"This PR fixes #4271.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4271","id":1224404403,"node_id":"I_kwDODunzps5I-u2z","number":4271,"title":"A typo in docs of datasets.disable_progress_bar","user":{"login":"jiangwy99","id":39762734,"node_id":"MDQ6VXNlcjM5NzYyNzM0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/39762734?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jiangwy99","html_url":"https:\/\/github.com\/jiangwy99","followers_url":"https:\/\/api.github.com\/users\/jiangwy99\/followers","following_url":"https:\/\/api.github.com\/users\/jiangwy99\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jiangwy99\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jiangwy99\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jiangwy99\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jiangwy99\/orgs","repos_url":"https:\/\/api.github.com\/users\/jiangwy99\/repos","events_url":"https:\/\/api.github.com\/users\/jiangwy99\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jiangwy99\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"assignees":[{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi! Thanks for catching and reporting the typo, a PR has been opened to fix it :)"],"created_at":1651599896000,"updated_at":1651647515000,"closed_at":1651647515000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nin the docs of V2.1.0 datasets.disable_progress_bar, we should replace \"enable\" with \"disable\".","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4270","id":1224244460,"node_id":"PR_kwDODunzps43PC5V","number":4270,"title":"Fix style in openbookqa dataset","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651591294000,"updated_at":1651826286000,"closed_at":1651594852000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4270","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4270","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4270.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4270.patch","merged_at":1651594852000},"body":"CI in PR:\r\n- #4259 \r\n\r\nwas green, but after merging it to master, a code quality error appeared.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4269","id":1223865145,"node_id":"PR_kwDODunzps43Nzwh","number":4269,"title":"Add license and point of contact to big_patent dataset","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651569847000,"updated_at":1651826289000,"closed_at":1651576579000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4269","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4269","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4269.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4269.patch","merged_at":1651576579000},"body":"Update metadata of big_patent dataset with:\r\n- license\r\n- point of contact","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4268","id":1223331964,"node_id":"I_kwDODunzps5I6pB8","number":4268,"title":"error downloading bigscience-catalogue-lm-data\/lm_en_wiktionary_filtered","user":{"login":"i-am-neo","id":102043285,"node_id":"U_kgDOBhUOlQ","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/102043285?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/i-am-neo","html_url":"https:\/\/github.com\/i-am-neo","followers_url":"https:\/\/api.github.com\/users\/i-am-neo\/followers","following_url":"https:\/\/api.github.com\/users\/i-am-neo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/i-am-neo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/i-am-neo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/i-am-neo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/i-am-neo\/orgs","repos_url":"https:\/\/api.github.com\/users\/i-am-neo\/repos","events_url":"https:\/\/api.github.com\/users\/i-am-neo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/i-am-neo\/received_events","type":"User","site_admin":false},"labels":[{"id":2067388877,"node_id":"MDU6TGFiZWwyMDY3Mzg4ODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20bug","name":"dataset bug","color":"2edb81","default":false,"description":"A bug in a dataset script provided in the library"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["It would help a lot to be able to preview the dataset - I'd like to see if the pronunciations are in the dataset, eg. for [\"word\"](https:\/\/en.wiktionary.org\/wiki\/word),\r\n\r\nPronunciation\r\n([Received Pronunciation](https:\/\/en.wikipedia.org\/wiki\/Received_Pronunciation)) [IPA](https:\/\/en.wiktionary.org\/wiki\/Wiktionary:International_Phonetic_Alphabet)([key](https:\/\/en.wiktionary.org\/wiki\/Appendix:English_pronunciation)): \/w\u025c\u02d0d\/\r\n([General American](https:\/\/en.wikipedia.org\/wiki\/General_American)) [enPR](https:\/\/en.wiktionary.org\/wiki\/Appendix:English_pronunciation): w\u00fbrd, [IPA](https:\/\/en.wiktionary.org\/wiki\/Wiktionary:International_Phonetic_Alphabet)([key](https:\/\/en.wiktionary.org\/wiki\/Appendix:English_pronunciation)): \/w\u025dd\/","Hi @i-am-neo, thanks for reporting.\r\n\r\nNormally this dataset should be private and not accessible for public use. @cakiki, @lvwerra, any reason why is it public? I see many other Wikimedia datasets are also public.\r\n\r\nAlso note that last commit \"Add metadata\" (https:\/\/huggingface.co\/datasets\/bigscience-catalogue-lm-data\/lm_en_wiktionary_filtered\/commit\/dc2f458dab50e00f35c94efb3cd4009996858609) introduced buggy data files (`data\/file-01.jsonl.gz.lock`, `data\/file-01.jsonl.gz.lock.lock`). The same bug appears in other datasets as well.\r\n\r\n@i-am-neo, please note that in the near future we are planning to make public all datasets used for the BigScience project (at least all of them whose license allows to do that). Once public, they will be accessible for all the NLP community.","Ah this must be a bug introduced at creation time since the repos were created programmatically; I'll go ahead and make them private; sorry about that!","All datasets are private now. \r\n\r\nRe:that bug I think we're currently avoiding it by avoiding verifications. (i.e. `ignore_verifications=True`)","Thanks a lot, @cakiki.\r\n\r\n@i-am-neo, I'm closing this issue for now because the dataset is not publicly available yet. Just stay tuned, as we will soon release all the BigScience open-license datasets.  ","Thanks for letting me know, @albertvillanova @cakiki.\r\nAny chance of having a subset alpha version in the meantime? \r\nI only need two dicts out of wiktionary: 1) phoneme(as key): word, and 2) word(as key): its phonemes.\r\n\r\nWould like to use it for a mini-poc [Robust ASR](https:\/\/github.com\/huggingface\/transformers\/issues\/13162#issuecomment-1096881290) decoding, cc @patrickvonplaten. \r\n\r\n(Patrick, possible to email you so as not to litter github with comments? I have some observations after experiments training hubert on some YT AMI-like data (11.44% wer).  Also wonder if a robust ASR is on your\/HG's roadmap).  Thanks!","Hey @i-am-neo,\r\n\r\nCool to hear that you're working on Robust ASR! Feel free to drop me a mail :-)","@i-am-neo This particular subset of the dataset was taken from the [CirrusSearch dumps](https:\/\/dumps.wikimedia.org\/other\/cirrussearch\/current\/)\r\nYou're specifically after the [enwiktionary-20220425-cirrussearch-content.json.gz](https:\/\/dumps.wikimedia.org\/other\/cirrussearch\/current\/enwiktionary-20220425-cirrussearch-content.json.gz) file","thanks @cakiki !  <del>I could access the gz file yesterday (but neglected to tuck it away somewhere safe), and today the link is throwing a 404. Can you help? <\/del>  Never mind, got it!","thanks @patrickvonplaten.  will do - getting my observations together."],"created_at":1651523665000,"updated_at":1651852410000,"closed_at":1651577028000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nError generated when attempting to download dataset\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"bigscience-catalogue-lm-data\/lm_en_wiktionary_filtered\")\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\n```\r\nExpectedMoreDownloadedFiles               Traceback (most recent call last)\r\n\r\n[<ipython-input-62-4ac5cf959477>](https:\/\/localhost:8080\/#) in <module>()\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 dataset = load_dataset(\"bigscience-catalogue-lm-data\/lm_en_wiktionary_filtered\")\r\n\r\n3 frames\r\n\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/utils\/info_utils.py](https:\/\/localhost:8080\/#) in verify_checksums(expected_checksums, recorded_checksums, verification_name)\r\n     31         return\r\n     32     if len(set(expected_checksums) - set(recorded_checksums)) > 0:\r\n---> 33         raise ExpectedMoreDownloadedFiles(str(set(expected_checksums) - set(recorded_checksums)))\r\n     34     if len(set(recorded_checksums) - set(expected_checksums)) > 0:\r\n     35         raise UnexpectedDownloadedFile(str(set(recorded_checksums) - set(expected_checksums)))\r\n\r\nExpectedMoreDownloadedFiles: {'\/home\/leandro\/catalogue_data\/datasets\/lm_en_wiktionary_filtered\/data\/file-01.jsonl.gz', '\/home\/leandro\/catalogue_data\/datasets\/lm_en_wiktionary_filtered\/data\/file-01.jsonl.gz.lock'}\r\n```\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.3\r\n- Platform: Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- PyArrow version: 6.0.1\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4267","id":1223214275,"node_id":"PR_kwDODunzps43LzOR","number":4267,"title":"Replace data URL in SAMSum dataset within the same repository","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651516688000,"updated_at":1651826293000,"closed_at":1651518229000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4267","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4267","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4267.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4267.patch","merged_at":1651518229000},"body":"Replace data URL with one in the same repository.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4266","id":1223116436,"node_id":"PR_kwDODunzps43LeXK","number":4266,"title":"Add HF Speech Bench to Librispeech Dataset Card","user":{"login":"sanchit-gandhi","id":93869735,"node_id":"U_kgDOBZhWpw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/93869735?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sanchit-gandhi","html_url":"https:\/\/github.com\/sanchit-gandhi","followers_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/followers","following_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/orgs","repos_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/repos","events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651510771000,"updated_at":1651740440000,"closed_at":1651740009000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4266","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4266","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4266.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4266.patch","merged_at":1651740009000},"body":"Adds the HF Speech Bench to Librispeech Dataset Card in place of the Papers With Code Leaderboard. Should improve usage and visibility of this leaderboard! Wondering whether this can also be done for [Common Voice 7](https:\/\/huggingface.co\/datasets\/mozilla-foundation\/common_voice_7_0) and [8](https:\/\/huggingface.co\/datasets\/mozilla-foundation\/common_voice_8_0) through someone with permissions? \r\n\r\ncc @patrickvonplaten: more leaderboard promotion!","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4263","id":1222723083,"node_id":"PR_kwDODunzps43KLnD","number":4263,"title":"Rename imagenet2012 -> imagenet-1k","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["> Later we can add imagenet-21k as a new dataset if we want.\r\n\r\nisn't it what models refer to as `imagenet` already?","> isn't it what models refer to as imagenet already?\r\n\r\nI wasn't sure, but it looks like it indeed. Therefore having a dataset `imagenet` for ImageNet 21k makes sense actually.\r\n\r\nEDIT: actually not all `imagenet` tag refer to ImageNet 21k - we will need to correct some of them","_The documentation is not available anymore as the PR was closed or merged._","should we remove the repo mirror on the hub side or will you do it?"],"created_at":1651487181000,"updated_at":1651513846000,"closed_at":1651509177000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4263","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4263","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4263.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4263.patch","merged_at":1651509177000},"body":"On the Hugging Face Hub, users refer to imagenet2012 (from #4178 ) as imagenet-1k in their model tags.\r\n\r\nTo correctly link models to imagenet, we should rename this dataset `imagenet-1k`.\r\n\r\nLater we can add `imagenet-21k` as a new dataset if we want.\r\n\r\nOnce this one is merged we can delete the `imagenet2012` dataset repository on the Hub.\r\n\r\nEDIT: to complete the rationale on why we should name it `imagenet-1k`:\r\nIf users specifically added the tag `imagenet-1k` , then it could be for two reasons (not sure which one is predominant), either they\r\n- wanted to make it explicit that it\u2019s not 21k -> the distinction is important for the community\r\n- or they have been following this convention from other models -> the convention implicitly exists already","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263\/reactions","total_count":4,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":1,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4262","id":1222130749,"node_id":"PR_kwDODunzps43IOye","number":4262,"title":"Add YAML tags to Dataset Card rotten tomatoes","user":{"login":"mo6zes","id":10004251,"node_id":"MDQ6VXNlcjEwMDA0MjUx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10004251?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mo6zes","html_url":"https:\/\/github.com\/mo6zes","followers_url":"https:\/\/api.github.com\/users\/mo6zes\/followers","following_url":"https:\/\/api.github.com\/users\/mo6zes\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mo6zes\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mo6zes\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mo6zes\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mo6zes\/orgs","repos_url":"https:\/\/api.github.com\/users\/mo6zes\/repos","events_url":"https:\/\/api.github.com\/users\/mo6zes\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mo6zes\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651406348000,"updated_at":1651588053000,"closed_at":1651587635000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4262","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4262","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4262.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4262.patch","merged_at":1651587635000},"body":"The dataset card for the rotten tomatoes \/ MR movie review dataset had some missing YAML tags. Hopefully, this also improves the visibility of this dataset now that paperswithcode and huggingface link to eachother.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262\/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4261","id":1221883779,"node_id":"I_kwDODunzps5I1HeD","number":4261,"title":"data leakage in `webis\/conclugen` dataset","user":{"login":"xflashxx","id":54585776,"node_id":"MDQ6VXNlcjU0NTg1Nzc2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/54585776?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/xflashxx","html_url":"https:\/\/github.com\/xflashxx","followers_url":"https:\/\/api.github.com\/users\/xflashxx\/followers","following_url":"https:\/\/api.github.com\/users\/xflashxx\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/xflashxx\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/xflashxx\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/xflashxx\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/xflashxx\/orgs","repos_url":"https:\/\/api.github.com\/users\/xflashxx\/repos","events_url":"https:\/\/api.github.com\/users\/xflashxx\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/xflashxx\/received_events","type":"User","site_admin":false},"labels":[{"id":2067388877,"node_id":"MDU6TGFiZWwyMDY3Mzg4ODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20bug","name":"dataset bug","color":"2edb81","default":false,"description":"A bug in a dataset script provided in the library"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi @xflashxx, thanks for reporting.\r\n\r\nPlease note that this dataset was generated and shared by Webis Group: https:\/\/huggingface.co\/webis\r\n\r\nWe are contacting the dataset owners to inform them about the issue you found. We'll keep you updated of their reply.","i'd suggest just pinging the authors here in the issue if possible?","Thanks for reporting this @xflashxx. I'll have a look and get back to you on this.","Hi @xflashxx and @albertvillanova,\r\n\r\nI have updated the files with de-duplicated splits. Apparently the debate portals from which part of the examples were sourced had unique timestamps for some examples (up to 6%; updated counts in the README) without any actual content updated that lead to \"new\" items. The length of `ids_validation` and `ids_testing` is zero.\r\n\r\nRegarding impact on scores:\r\n1. We employed automatic evaluation (on a separate set of 1000 examples) only to justify the exclusion of the smaller models for manual evaluation (due to budget constraints). I am confident the ranking still stands (unsurprisingly, the bigger models doing better than those trained on the smaller splits). We also highlight this in the paper. \r\n\r\n2. The examples used for manual evaluation have no overlap with any splits (also because they do not have any ground truth as we applied the trained models on an unlabeled sample to test its practical usage). I've added these two files to the dataset repository.\r\n\r\nHope this helps!","Thanks @shahbazsyed for your fast fix.\r\n\r\nAs a side note:\r\n- Your email appearing as Point of Contact in the dataset README has a typo: @uni.leipzig.de instead of @uni-leipzig.de\r\n- Your commits on the Hub are not linked to your profile on the Hub: this is because we use the email address to make this link; the email address used in your commit author and the email address set on your Hub account settings."],"created_at":1651340617000,"updated_at":1651557866000,"closed_at":1651557866000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nSome samples (argument-conclusion pairs) in the *training* split of the `webis\/conclugen` dataset are present in both the *validation* and *test* splits, creating data leakage and distorting model results.\r\nFurthermore, all splits contain duplicate samples.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ntraining = load_dataset(\"webis\/conclugen\", \"base\", split=\"train\")\r\nvalidation = load_dataset(\"webis\/conclugen\", \"base\", split=\"validation\")\r\ntesting = load_dataset(\"webis\/conclugen\", \"base\", split=\"test\")\r\n\r\n# collect which sample id's are present in the training split\r\nids_validation = list()\r\nids_testing = list()\r\n\r\nfor train_sample in training:\r\n    train_argument = train_sample[\"argument\"]\r\n    train_conclusion = train_sample[\"conclusion\"]\r\n    train_id = train_sample[\"id\"]\r\n    \r\n    # test if current sample is in validation split\r\n    if train_argument in validation[\"argument\"]:\r\n        for validation_sample in validation:\r\n            validation_argument = validation_sample[\"argument\"]\r\n            validation_conclusion = validation_sample[\"conclusion\"]\r\n            validation_id = validation_sample[\"id\"]\r\n            if train_argument == validation_argument and train_conclusion == validation_conclusion:\r\n                ids_validation.append(validation_id)\r\n    \r\n    # test if current sample is in test split\r\n    if train_argument in testing[\"argument\"]:\r\n        for testing_sample in testing:\r\n            testing_argument = testing_sample[\"argument\"]\r\n            testing_conclusion = testing_sample[\"conclusion\"]\r\n            testing_id = testing_sample[\"id\"]\r\n            if train_argument == testing_argument and train_conclusion == testing_conclusion:\r\n                ids_testing.append(testing_id)\r\n```\r\n\r\n## Expected results\r\nLength of both lists `ids_validation` and `ids_testing` should be zero.\r\n\r\n## Actual results\r\nLength of `ids_validation` = `2556`\r\nLength of `ids_testing` = `287`\r\n\r\nFurthermore, there seems to be duplicate samples in (at least) the *training* split, since:\r\n`print(len(set(ids_validation)))` = `950`\r\n`print(len(set(ids_testing)))` = `101`\r\n\r\nAll in all, around 7% of the samples of each the *validation* and *test* split seems to be present in the *training* split.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.4\r\n- Platform: macOS-12.3.1-arm64-arm-64bit\r\n- Python version: 3.9.10\r\n- PyArrow version: 7.0.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4260","id":1221830292,"node_id":"PR_kwDODunzps43HSfs","number":4260,"title":"Add mr_polarity movie review sentiment classification","user":{"login":"mo6zes","id":10004251,"node_id":"MDQ6VXNlcjEwMDA0MjUx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10004251?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mo6zes","html_url":"https:\/\/github.com\/mo6zes","followers_url":"https:\/\/api.github.com\/users\/mo6zes\/followers","following_url":"https:\/\/api.github.com\/users\/mo6zes\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mo6zes\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mo6zes\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mo6zes\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mo6zes\/orgs","repos_url":"https:\/\/api.github.com\/users\/mo6zes\/repos","events_url":"https:\/\/api.github.com\/users\/mo6zes\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mo6zes\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["whoops just found https:\/\/huggingface.co\/datasets\/rotten_tomatoes"],"created_at":1651324773000,"updated_at":1651328185000,"closed_at":1651328185000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4260","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4260","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4260.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4260.patch","merged_at":null},"body":"Add the MR (Movie Review) dataset. The original dataset contains sentences from Rotten Tomatoes labeled as either \"positive\" or  \"negative\". \r\n\r\nHomepage: [https:\/\/www.cs.cornell.edu\/people\/pabo\/movie-review-data\/](https:\/\/www.cs.cornell.edu\/people\/pabo\/movie-review-data\/)\r\npaperswithcode: [https:\/\/paperswithcode.com\/dataset\/mr](https:\/\/paperswithcode.com\/dataset\/mr)\r\n\r\n- [ ] I was not able to generate dummy data, the original dataset files have \".pos\" and \".neg\" as file extensions so the auto-generator does not work. Is it fine like this or should dummy data be added?\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4259","id":1221768025,"node_id":"PR_kwDODunzps43HHGc","number":4259,"title":"Fix bug in choices labels in openbookqa dataset","user":{"login":"manandey","id":6687858,"node_id":"MDQ6VXNlcjY2ODc4NTg=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/6687858?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/manandey","html_url":"https:\/\/github.com\/manandey","followers_url":"https:\/\/api.github.com\/users\/manandey\/followers","following_url":"https:\/\/api.github.com\/users\/manandey\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/manandey\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/manandey\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/manandey\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/manandey\/orgs","repos_url":"https:\/\/api.github.com\/users\/manandey\/repos","events_url":"https:\/\/api.github.com\/users\/manandey\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/manandey\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651304499000,"updated_at":1651645891000,"closed_at":1651590861000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4259","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4259","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4259.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4259.patch","merged_at":1651590861000},"body":"This PR fixes the Bug in the openbookqa dataset as mentioned in this issue #3550.\r\n\r\nFix #3550.\r\n\r\ncc. @lhoestq @mariosasko ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4258","id":1221637727,"node_id":"PR_kwDODunzps43Gstg","number":4258,"title":"Fix\/start token mask issue and update documentation","user":{"login":"TristanThrush","id":20826878,"node_id":"MDQ6VXNlcjIwODI2ODc4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/20826878?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/TristanThrush","html_url":"https:\/\/github.com\/TristanThrush","followers_url":"https:\/\/api.github.com\/users\/TristanThrush\/followers","following_url":"https:\/\/api.github.com\/users\/TristanThrush\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/TristanThrush\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/TristanThrush\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/TristanThrush\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/TristanThrush\/orgs","repos_url":"https:\/\/api.github.com\/users\/TristanThrush\/repos","events_url":"https:\/\/api.github.com\/users\/TristanThrush\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/TristanThrush\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","> Good catch ! Thanks :)\r\n> \r\n> Next time can you describe your fix in the Pull Request description please ?\r\n\r\nThanks. Also whoops, sorry about not being very descriptive. I updated the pull request description, and will keep this in mind for future PRs."],"created_at":1651272164000,"updated_at":1651509200000,"closed_at":1651508772000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4258","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4258","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4258.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4258.patch","merged_at":1651508772000},"body":"This pr fixes a couple bugs:\r\n\r\n1) the perplexity was calculated with a 0 in the attention mask for the start token, which was causing high perplexity scores that were not correct\r\n2) the documentation was not updated","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4257","id":1221393137,"node_id":"PR_kwDODunzps43GATC","number":4257,"title":"Create metric card for Mahalanobis Distance","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651257447000,"updated_at":1651503018000,"closed_at":1651502604000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4257","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4257","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4257.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4257.patch","merged_at":1651502604000},"body":"proposing a metric card to better explain how Mahalanobis distance works (last one for now :sweat_smile:","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4256","id":1221379625,"node_id":"PR_kwDODunzps43F9Zw","number":4256,"title":"Create metric card for MSE","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651256482000,"updated_at":1651503342000,"closed_at":1651502927000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4256","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4256","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4256.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4256.patch","merged_at":1651502927000},"body":"Proposing a metric card for Mean Squared Error","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4255","id":1221142899,"node_id":"PR_kwDODunzps43FHgR","number":4255,"title":"No google drive URL for pubmed_qa","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","CI is failing because some sections are missing in the dataset card, but this is unrelated to this PR - Merging !"],"created_at":1651247746000,"updated_at":1651249495000,"closed_at":1651249136000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4255","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4255","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4255.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4255.patch","merged_at":1651249136000},"body":"I hosted the data files in https:\/\/huggingface.co\/datasets\/pubmed_qa. This is allowed because the data is under the MIT license.\r\n\r\ncc @stas00 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4254","id":1220204395,"node_id":"PR_kwDODunzps43Bwnj","number":4254,"title":"Replace data URL in SAMSum dataset and support streaming","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651220503000,"updated_at":1651826296000,"closed_at":1651249569000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4254","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4254","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4254.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4254.patch","merged_at":1651249568000},"body":"This PR replaces data URL in SAMSum dataset:\r\n- original host (arxiv.org) does not allow HTTP Range requests\r\n- we have hosted the data on the Hub (license: CC BY-NC-ND 4.0)\r\n\r\nMoreover, it implements support for streaming.\r\n\r\nFix #4146.\r\nRelated to: #4236.\r\n\r\nCC: @severo ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4253","id":1219286408,"node_id":"PR_kwDODunzps42-c8Q","number":4253,"title":"Create metric cards for mean IOU","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651179507000,"updated_at":1651254287000,"closed_at":1651253886000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4253","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4253","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4253.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4253.patch","merged_at":1651253886000},"body":"Proposing a metric card for mIoU :rocket:\r\n\r\nsorry for spamming you with review requests, @albertvillanova ! :hugs: ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4252","id":1219151100,"node_id":"PR_kwDODunzps429--I","number":4252,"title":"Creating metric card for MAE","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651172673000,"updated_at":1651251551000,"closed_at":1651251150000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4252","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4252","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4252.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4252.patch","merged_at":1651251150000},"body":"Initial proposal for MAE metric card","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4251","id":1219116354,"node_id":"PR_kwDODunzps4293dB","number":4251,"title":"Metric card for the XTREME-S dataset","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651170739000,"updated_at":1651250771000,"closed_at":1651250326000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4251","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4251","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4251.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4251.patch","merged_at":1651250326000},"body":"Proposing a metric card for the XTREME-S dataset :hugs:","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4250","id":1219093830,"node_id":"PR_kwDODunzps429yjN","number":4250,"title":"Bump PyArrow Version to 6","user":{"login":"dnaveenr","id":17746528,"node_id":"MDQ6VXNlcjE3NzQ2NTI4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17746528?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/dnaveenr","html_url":"https:\/\/github.com\/dnaveenr","followers_url":"https:\/\/api.github.com\/users\/dnaveenr\/followers","following_url":"https:\/\/api.github.com\/users\/dnaveenr\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/dnaveenr\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/dnaveenr\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/dnaveenr\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/dnaveenr\/orgs","repos_url":"https:\/\/api.github.com\/users\/dnaveenr\/repos","events_url":"https:\/\/api.github.com\/users\/dnaveenr\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/dnaveenr\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Updated meta.yaml as well. Thanks.","I'm OK with bumping PyArrow to version 6 to match the version in Colab, but maybe a better solution would be to stop using extension types in our codebase to avoid similar issues.","> but maybe a better solution would be to stop using extension types in our codebase to avoid similar issues.\r\n\r\nI agree, not much attention has been payed to extension arrays in the latest developments of Arrow anyway.\r\n\r\nLet's not use them more that what we do right now, and try to remove them at one point"],"created_at":1651169450000,"updated_at":1651657012000,"closed_at":1651656586000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4250","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4250","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4250.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4250.patch","merged_at":1651656586000},"body":"Fixes #4152 \r\n\r\nThis PR updates the PyArrow version to 6 in setup.py, CI job files .circleci\/config.yaml and .github\/workflows\/benchmarks.yaml files.\r\nThis will fix ArrayND error which exists in pyarrow 5.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4249","id":1218524424,"node_id":"PR_kwDODunzps42742y","number":4249,"title":"Support streaming XGLUE dataset","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651141643000,"updated_at":1651826301000,"closed_at":1651162083000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4249","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4249","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4249.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4249.patch","merged_at":1651162083000},"body":"Support streaming XGLUE dataset.\r\n\r\nFix #4247.\r\n\r\nCC: @severo ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4248","id":1218460444,"node_id":"I_kwDODunzps5IoDsc","number":4248,"title":"conll2003 dataset loads original data.","user":{"login":"sue991","id":26458611,"node_id":"MDQ6VXNlcjI2NDU4NjEx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/26458611?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sue991","html_url":"https:\/\/github.com\/sue991","followers_url":"https:\/\/api.github.com\/users\/sue991\/followers","following_url":"https:\/\/api.github.com\/users\/sue991\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sue991\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sue991\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sue991\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sue991\/orgs","repos_url":"https:\/\/api.github.com\/users\/sue991\/repos","events_url":"https:\/\/api.github.com\/users\/sue991\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sue991\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Thanks for reporting @sue99.\r\n\r\nUnfortunately. I'm not able to reproduce your problem:\r\n```python\r\nIn [1]: import datasets\r\n   ...: from datasets import load_dataset\r\n   ...: dataset = load_dataset(\"conll2003\")\r\n\r\nIn [2]: dataset\r\nOut[2]: \r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\r\n        num_rows: 14042\r\n    })\r\n    validation: Dataset({\r\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\r\n        num_rows: 3251\r\n    })\r\n    test: Dataset({\r\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\r\n        num_rows: 3454\r\n    })\r\n})\r\n\r\nIn [3]: dataset[\"train\"][0]\r\nOut[3]: \r\n{'id': '0',\r\n 'tokens': ['EU',\r\n  'rejects',\r\n  'German',\r\n  'call',\r\n  'to',\r\n  'boycott',\r\n  'British',\r\n  'lamb',\r\n  '.'],\r\n 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\r\n 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\r\n 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\r\n```\r\n\r\nJust guessing: might be the case that you are calling `load_dataset` from a working directory that contains a local folder named `conll2003` (containing the raw data files)? If that is the case, `datasets` library gives precedence to the local folder over the dataset on the Hub. "],"created_at":1651138411000,"updated_at":1651163473000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nI load `conll2003` dataset to use refined data like [this](https:\/\/huggingface.co\/datasets\/conll2003\/viewer\/conll2003\/train)  preview, but it is original data that contains `'-DOCSTART- -X- -X- O'` text.\r\n\r\nIs this a bug or should I use another dataset_name like `lhoestq\/conll2003` ?\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"conll2003\")\r\n```\r\n\r\n## Expected results\r\n{\r\n    \"chunk_tags\": [11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0],\r\n    \"id\": \"0\",\r\n    \"ner_tags\": [0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\r\n    \"pos_tags\": [12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7],\r\n    \"tokens\": [\"The\", \"European\", \"Commission\", \"said\", \"on\", \"Thursday\", \"it\", \"disagreed\", \"with\", \"German\", \"advice\", \"to\", \"consumers\", \"to\", \"shun\", \"British\", \"lamb\", \"until\", \"scientists\", \"determine\", \"whether\", \"mad\", \"cow\", \"disease\", \"can\", \"be\", \"transmitted\", \"to\", \"sheep\", \".\"]\r\n}\r\n\r\n## Actual results\r\n```python\r\nprint(dataset)\r\n\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['text'],\r\n        num_rows: 219554\r\n    })\r\n    test: Dataset({\r\n        features: ['text'],\r\n        num_rows: 50350\r\n    })\r\n    validation: Dataset({\r\n        features: ['text'],\r\n        num_rows: 55044\r\n    })\r\n})\r\n```\r\n\r\n```python\r\nfor i in range(20):\r\n    print(dataset['train'][i])\r\n\r\n{'text': '-DOCSTART- -X- -X- O'}\r\n{'text': ''}\r\n{'text': 'EU NNP B-NP B-ORG'}\r\n{'text': 'rejects VBZ B-VP O'}\r\n{'text': 'German JJ B-NP B-MISC'}\r\n{'text': 'call NN I-NP O'}\r\n{'text': 'to TO B-VP O'}\r\n{'text': 'boycott VB I-VP O'}\r\n{'text': 'British JJ B-NP B-MISC'}\r\n{'text': 'lamb NN I-NP O'}\r\n{'text': '. . O O'}\r\n{'text': ''}\r\n{'text': 'Peter NNP B-NP B-PER'}\r\n{'text': 'Blackburn NNP I-NP I-PER'}\r\n{'text': ''}\r\n{'text': 'BRUSSELS NNP B-NP B-LOC'}\r\n{'text': '1996-08-22 CD I-NP O'}\r\n{'text': ''}\r\n{'text': 'The DT B-NP O'}\r\n{'text': 'European NNP I-NP B-ORG'}\r\n```\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4247","id":1218320882,"node_id":"I_kwDODunzps5Inhny","number":4247,"title":"The data preview of XGLUE","user":{"login":"czq1999","id":49108847,"node_id":"MDQ6VXNlcjQ5MTA4ODQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/49108847?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/czq1999","html_url":"https:\/\/github.com\/czq1999","followers_url":"https:\/\/api.github.com\/users\/czq1999\/followers","following_url":"https:\/\/api.github.com\/users\/czq1999\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/czq1999\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/czq1999\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/czq1999\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/czq1999\/orgs","repos_url":"https:\/\/api.github.com\/users\/czq1999\/repos","events_url":"https:\/\/api.github.com\/users\/czq1999\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/czq1999\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["![image](https:\/\/user-images.githubusercontent.com\/49108847\/165700611-915b4343-766f-4b81-bdaa-b31950250f06.png)\r\n","Thanks for reporting @czq1999.\r\n\r\nNote that the dataset viewer uses the dataset in streaming mode and that not all datasets support streaming yet.\r\n\r\nThat is the case for XGLUE dataset (as the error message points out): this must be refactored to support streaming. ","Fixed, thanks @albertvillanova !\r\n\r\nhttps:\/\/huggingface.co\/datasets\/xglue\r\n\r\n<img width=\"824\" alt=\"Capture d\u2019e\u0301cran 2022-04-29 a\u0300 10 23 14\" src=\"https:\/\/user-images.githubusercontent.com\/1676121\/165909391-9f98d98a-665a-4e57-822d-8baa2dc9b7c9.png\">\r\n"],"created_at":1651131050000,"updated_at":1651220608000,"closed_at":1651162083000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"It seems that something wrong with the data previvew of XGLUE","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4246","id":1218320293,"node_id":"PR_kwDODunzps427NiD","number":4246,"title":"Support to load dataset with TSV files by passing only dataset name","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651131015000,"updated_at":1651826308000,"closed_at":1651824847000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4246","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4246","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4246.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4246.patch","merged_at":1651824847000},"body":"This PR implements support to load a dataset (w\/o script) containing TSV files by passing only the dataset name (no need to pass `sep='\\t'`):\r\n```python\r\nds = load_dataset(\"dataset\/name\")\r\n```\r\n\r\nThe refactoring allows for future builder kwargs customizations based on file extension.\r\n\r\nRelated to #4238.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4245","id":1217959400,"node_id":"PR_kwDODunzps426AUR","number":4245,"title":"Add code examples for DatasetDict","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651099942000,"updated_at":1651256374000,"closed_at":1651255983000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4245","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4245","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4245.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4245.patch","merged_at":1651255983000},"body":"This PR adds code examples for `DatasetDict` in the API reference :)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4244","id":1217732221,"node_id":"PR_kwDODunzps425Po6","number":4244,"title":"task id update","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Reverted the multi-input-text-classification tag from task_categories and added it as task_ids @lhoestq ","_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651084094000,"updated_at":1651661033000,"closed_at":1651660597000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4244","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4244","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4244.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4244.patch","merged_at":1651660597000},"body":"changed multi input text classification as task id instead of category","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4243","id":1217689909,"node_id":"PR_kwDODunzps425Gkn","number":4243,"title":"WIP: Initial shades loading script and readme","user":{"login":"shayne-longpre","id":69018523,"node_id":"MDQ6VXNlcjY5MDE4NTIz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/69018523?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/shayne-longpre","html_url":"https:\/\/github.com\/shayne-longpre","followers_url":"https:\/\/api.github.com\/users\/shayne-longpre\/followers","following_url":"https:\/\/api.github.com\/users\/shayne-longpre\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/shayne-longpre\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/shayne-longpre\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/shayne-longpre\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/shayne-longpre\/orgs","repos_url":"https:\/\/api.github.com\/users\/shayne-longpre\/repos","events_url":"https:\/\/api.github.com\/users\/shayne-longpre\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/shayne-longpre\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1651081543000,"updated_at":1651081543000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4243","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4243","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4243.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4243.patch","merged_at":null},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4242","id":1217665960,"node_id":"PR_kwDODunzps425BYf","number":4242,"title":"Update auth when mirroring datasets on the hub","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651080151000,"updated_at":1651081024000,"closed_at":1651080642000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4242","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4242","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4242.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4242.patch","merged_at":1651080642000},"body":"We don't need to use extraHeaders anymore for rate limits anymore. Anyway extraHeaders was not working with git LFS because it was passing the wrong auth to S3.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4241","id":1217423686,"node_id":"I_kwDODunzps5IkGlG","number":4241,"title":"NonMatchingChecksumError when attempting to download GLUE","user":{"login":"drussellmrichie","id":9650729,"node_id":"MDQ6VXNlcjk2NTA3Mjk=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/9650729?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/drussellmrichie","html_url":"https:\/\/github.com\/drussellmrichie","followers_url":"https:\/\/api.github.com\/users\/drussellmrichie\/followers","following_url":"https:\/\/api.github.com\/users\/drussellmrichie\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/drussellmrichie\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/drussellmrichie\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/drussellmrichie\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/drussellmrichie\/orgs","repos_url":"https:\/\/api.github.com\/users\/drussellmrichie\/repos","events_url":"https:\/\/api.github.com\/users\/drussellmrichie\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/drussellmrichie\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi :)\r\n\r\nI think your issue may be related to the older `nlp` library. I was able to download `glue` with the latest version of `datasets`. Can you try updating with:\r\n\r\n```py\r\npip install -U datasets\r\n```\r\n\r\nThen you can download:\r\n\r\n```py\r\nfrom datasets import load_dataset\r\nds = load_dataset(\"glue\", \"rte\")\r\n```","This appears to work. Thank you!\n\nOn Wed, Apr 27, 2022, 1:18 PM Steven Liu ***@***.***> wrote:\n\n> Hi :)\n>\n> I think your issue may be related to the older nlp library. I was able to\n> download glue with the latest version of datasets. Can you try updating\n> with:\n>\n> pip install -U datasets\n>\n> Then you can download:\n>\n> from datasets import load_datasetds = load_dataset(\"glue\", \"rte\")\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/huggingface\/datasets\/issues\/4241#issuecomment-1111267650>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/ACJUEKLUP2EL7ES3RRWJRPTVHFZHBANCNFSM5UPJBYXA>\n> .\n> You are receiving this because you authored the thread.Message ID:\n> ***@***.***>\n>\n"],"created_at":1651068861000,"updated_at":1651131927000,"closed_at":1651131927000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nI am trying to download the GLUE dataset from the NLP module but get an error (see below).\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport nlp\r\nnlp.__version__ # '0.2.0'\r\nnlp.load_dataset('glue', name=\"rte\", download_mode=\"force_redownload\")\r\n```\r\n\r\n## Expected results\r\nI expect the dataset to download without an error.\r\n\r\n## Actual results\r\n```\r\nINFO:nlp.load:Checking \/home\/richier\/.cache\/huggingface\/datasets\/5fe6ab0df8a32a3371b2e6a969d31d855a19563724fb0d0f163748c270c0ac60.2ea96febf19981fae5f13f0a43d4e2aa58bc619bc23acf06de66675f425a5538.py for additional imports.\r\nINFO:nlp.load:Found main folder for dataset https:\/\/s3.amazonaws.com\/datasets.huggingface.co\/nlp\/datasets\/glue\/glue.py at \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\r\nINFO:nlp.load:Found specific version folder for dataset https:\/\/s3.amazonaws.com\/datasets.huggingface.co\/nlp\/datasets\/glue\/glue.py at \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4\r\nINFO:nlp.load:Found script file from https:\/\/s3.amazonaws.com\/datasets.huggingface.co\/nlp\/datasets\/glue\/glue.py to \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4\/glue.py\r\nINFO:nlp.load:Found dataset infos file from https:\/\/s3.amazonaws.com\/datasets.huggingface.co\/nlp\/datasets\/glue\/dataset_infos.json to \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4\/dataset_infos.json\r\nINFO:nlp.load:Found metadata file for dataset https:\/\/s3.amazonaws.com\/datasets.huggingface.co\/nlp\/datasets\/glue\/glue.py at \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4\/glue.json\r\nINFO:nlp.info:Loading Dataset Infos from \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4\r\nINFO:nlp.builder:Generating dataset glue (\/home\/richier\/.cache\/huggingface\/datasets\/glue\/rte\/1.0.0)\r\nINFO:nlp.builder:Dataset not on Hf google storage. Downloading and preparing it from source\r\nINFO:nlp.utils.file_utils:Couldn't get ETag version for url https:\/\/firebasestorage.googleapis.com\/v0\/b\/mtl-sentence-representations.appspot.com\/o\/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb\r\nINFO:nlp.utils.file_utils:https:\/\/firebasestorage.googleapis.com\/v0\/b\/mtl-sentence-representations.appspot.com\/o\/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb not found in cache or force_download set to True, downloading to \/home\/richier\/.cache\/huggingface\/datasets\/downloads\/tmpldt3n805\r\nDownloading and preparing dataset glue\/rte (download: 680.81 KiB, generated: 1.83 MiB, total: 2.49 MiB) to \/home\/richier\/.cache\/huggingface\/datasets\/glue\/rte\/1.0.0...\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 73.0\/73.0 [00:00<00:00, 73.9kB\/s]\r\nINFO:nlp.utils.file_utils:storing https:\/\/firebasestorage.googleapis.com\/v0\/b\/mtl-sentence-representations.appspot.com\/o\/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb in cache at \/home\/richier\/.cache\/huggingface\/datasets\/downloads\/e8b62ee44e6f8b6aea761935928579ffe1aa55d161808c482e0725abbdcf9c64\r\nINFO:nlp.utils.file_utils:creating metadata file for \/home\/richier\/.cache\/huggingface\/datasets\/downloads\/e8b62ee44e6f8b6aea761935928579ffe1aa55d161808c482e0725abbdcf9c64\r\n---------------------------------------------------------------------------\r\nNonMatchingChecksumError                  Traceback (most recent call last)\r\n<ipython-input-7-669a8343dcc1> in <module>\r\n----> 1 nlp.load_dataset('glue', name=\"rte\", download_mode=\"force_redownload\")\r\n\r\n~\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    518         download_mode=download_mode,\r\n    519         ignore_verifications=ignore_verifications,\r\n--> 520         save_infos=save_infos,\r\n    521     )\r\n    522 \r\n\r\n~\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\r\n    418                 verify_infos = not save_infos and not ignore_verifications\r\n    419                 self._download_and_prepare(\r\n--> 420                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    421                 )\r\n    422                 # Sync info\r\n\r\n~\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    458         # Checksums verification\r\n    459         if verify_infos:\r\n--> 460             verify_checksums(self.info.download_checksums, dl_manager.get_recorded_sizes_checksums())\r\n    461         for split_generator in split_generators:\r\n    462             if str(split_generator.split_info.name).lower() == \"all\":\r\n\r\n~\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/utils\/info_utils.py in verify_checksums(expected_checksums, recorded_checksums)\r\n     34     bad_urls = [url for url in expected_checksums if expected_checksums[url] != recorded_checksums[url]]\r\n     35     if len(bad_urls) > 0:\r\n---> 36         raise NonMatchingChecksumError(str(bad_urls))\r\n     37     logger.info(\"All the checksums matched successfully.\")\r\n     38 \r\n\r\nNonMatchingChecksumError: ['https:\/\/firebasestorage.googleapis.com\/v0\/b\/mtl-sentence-representations.appspot.com\/o\/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb']\r\n```\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0\r\n- Platform: Linux-4.18.0-348.20.1.el8_5.x86_64-x86_64-with-redhat-8.5-Ootpa\r\n- Python version: 3.6.13\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.1.5\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4240","id":1217287594,"node_id":"PR_kwDODunzps423xRl","number":4240,"title":"Fix yield for crd3","user":{"login":"shanyas10","id":21066979,"node_id":"MDQ6VXNlcjIxMDY2OTc5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/21066979?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/shanyas10","html_url":"https:\/\/github.com\/shanyas10","followers_url":"https:\/\/api.github.com\/users\/shanyas10\/followers","following_url":"https:\/\/api.github.com\/users\/shanyas10\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/shanyas10\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/shanyas10\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/shanyas10\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/shanyas10\/orgs","repos_url":"https:\/\/api.github.com\/users\/shanyas10\/repos","events_url":"https:\/\/api.github.com\/users\/shanyas10\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/shanyas10\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["I don't think you need to generate new dummy data, since they're in the same format as the original data.\r\n\r\nThe CI is failing because of this error:\r\n```python\r\n>                       turn[\"names\"] = turn[\"NAMES\"]\r\nE                       TypeError: tuple indices must be integers or slices, not str\r\n```\r\n\r\nDo you know what could cause this ? If I understand correctly, `turn` is supposed to be a list of dictionaries right ?","> ```  \r\n>   \r\n> Do you know what could cause this ? If I understand correctly, turn is supposed to be a list of dictionaries right ?\r\n> ```\r\n\r\nThis is strange. Let me look into this. As per https:\/\/github.com\/RevanthRameshkumar\/CRD3\/blob\/master\/data\/aligned%20data\/c%3D2\/C1E001_2_0.json TURNS is a list of dictionaries. So when we iterate over `row[\"TURNS]\"` each `turn` is essentially a dictionary. Not sure why it's being considered a tuple here."],"created_at":1651062696000,"updated_at":1651236101000,"closed_at":1651236101000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4240","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4240","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4240.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4240.patch","merged_at":1651236101000},"body":"Modified the `_generate_examples` function to consider all the turns for a chunk id as a single example\r\nModified the features accordingly\r\n\r\n```\r\n\"turns\": [\r\n                        {\r\n                            \"names\": datasets.features.Sequence(datasets.Value(\"string\")),\r\n                            \"utterances\": datasets.features.Sequence(datasets.Value(\"string\")),\r\n                            \"number\": datasets.Value(\"int32\"),\r\n                        }\r\n                    ],\r\n                }\r\n```\r\n\r\nI wasn't able to run `datasets-cli dummy_data datasets` command. Is there a workaround for this? ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4239","id":1217269689,"node_id":"PR_kwDODunzps423tZr","number":4239,"title":"Small fixes in ROC AUC docs","user":{"login":"wschella","id":9478856,"node_id":"MDQ6VXNlcjk0Nzg4NTY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/9478856?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/wschella","html_url":"https:\/\/github.com\/wschella","followers_url":"https:\/\/api.github.com\/users\/wschella\/followers","following_url":"https:\/\/api.github.com\/users\/wschella\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/wschella\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/wschella\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/wschella\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/wschella\/orgs","repos_url":"https:\/\/api.github.com\/users\/wschella\/repos","events_url":"https:\/\/api.github.com\/users\/wschella\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/wschella\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651061750000,"updated_at":1651498137000,"closed_at":1651497723000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4239","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4239","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4239.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4239.patch","merged_at":1651497723000},"body":"The list of use cases did not render on GitHub with the prepended spacing.\r\nAdditionally, some typo's we're fixed.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4238","id":1217168123,"node_id":"I_kwDODunzps5IjIL7","number":4238,"title":"Dataset caching policy","user":{"login":"loretoparisi","id":163333,"node_id":"MDQ6VXNlcjE2MzMzMw==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/163333?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/loretoparisi","html_url":"https:\/\/github.com\/loretoparisi","followers_url":"https:\/\/api.github.com\/users\/loretoparisi\/followers","following_url":"https:\/\/api.github.com\/users\/loretoparisi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/loretoparisi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/loretoparisi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/loretoparisi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/loretoparisi\/orgs","repos_url":"https:\/\/api.github.com\/users\/loretoparisi\/repos","events_url":"https:\/\/api.github.com\/users\/loretoparisi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/loretoparisi\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi @loretoparisi, thanks for reporting.\r\n\r\nThere is an option to force the redownload of the data files (and thus not using previously download and cached data files): `load_dataset(..., download_mode=\"force_redownload\")`.\r\n\r\nPlease, let me know if this fixes your problem.\r\n\r\nI can confirm you that your dataset loads without any problem for me:\r\n```python\r\nIn [2]: ds = load_dataset(\"loretoparisi\/tatoeba-sentences\", data_files={\"train\": \"train.csv\", \"test\": \"test.csv\"}, delimiter=\"\\t\", column_names=['label', 'text'])\r\n\r\nIn [3]: ds\r\nOut[3]: \r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['label', 'text'],\r\n        num_rows: 8256449\r\n    })\r\n    test: Dataset({\r\n        features: ['label', 'text'],\r\n        num_rows: 2061204\r\n    })\r\n})\r\n``` ","@albertvillanova thank you, it seems it still does not work using:\r\n\r\n```python\r\nsentences = load_dataset(\r\n     \"loretoparisi\/tatoeba-sentences\",\r\n     data_files=data_files,\r\n     delimiter='\\t', \r\n     column_names=['label', 'text'],\r\n     download_mode=\"force_redownload\"\r\n)\r\n```\r\n[This](https:\/\/colab.research.google.com\/drive\/1EA6FWo5pHxU8rPHHRn24NlHqRPiOlPTr?usp=sharing) is my notebook!\r\n\r\nThe problem is that the download file's revision for `test.csv` is not correctly parsed\r\n\r\n![Schermata 2022-04-27 alle 18 09 41](https:\/\/user-images.githubusercontent.com\/163333\/165563507-0be53eb6-8f61-49b0-b959-306e59281de3.png)\r\n\r\nIf you download that file `test.csv` from the repo, the line `\\\\N` is not there anymore (it was there at the first file upload).\r\n\r\nMy impression is that the Apache Arrow file is still cached - so server side, despite of enabling a forced download. For what I can see I get those two arrow files, but I cannot grep the bad line (`\\\\N`) since are binary files:\r\n\r\n```\r\n!ls -l \/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-efeff8965c730a2c\/0.0.0\/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\r\n!ls -l \/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-efeff8965c730a2c\/0.0.0\/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\/csv-test.arrow\r\n!head \/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-efeff8965c730a2c\/0.0.0\/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\/dataset_info.json\r\n```\r\n","SOLVED! The problem was the with the file itself, using caching parameter helped indeed.\r\nThanks for helping!"],"created_at":1651056131000,"updated_at":1651076965000,"closed_at":1651076930000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nI cannot clean cache of my datasets files, despite I have updated the `csv` files on the repository [here](https:\/\/huggingface.co\/datasets\/loretoparisi\/tatoeba-sentences). The original file had a line with bad characters, causing the following error\r\n\r\n```\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/features\/features.py](https:\/\/localhost:8080\/#) in str2int(self, values)\r\n    852                 if value not in self._str2int:\r\n    853                     value = str(value).strip()\r\n--> 854                 output.append(self._str2int[str(value)])\r\n    855             else:\r\n    856                 # No names provided, try to integerize\r\n\r\nKeyError: '\\\\N'\r\n```\r\n\r\nThe file now is cleanup up, but I still get the error. This happens even if I inspect the local cached contents, and cleanup the files locally:\r\n\r\n```python\r\nfrom datasets import load_dataset_builder\r\ndataset_builder = load_dataset_builder(\"loretoparisi\/tatoeba-sentences\")\r\nprint(dataset_builder.cache_dir)\r\nprint(dataset_builder.info.features)\r\nprint(dataset_builder.info.splits)\r\n```\r\n\r\n```\r\nUsing custom data configuration loretoparisi--tatoeba-sentences-e59b8ad92f1bb8dd\r\n\/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-e59b8ad92f1bb8dd\/0.0.0\/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\r\nNone\r\nNone\r\n```\r\n\r\nand removing files located at `\/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-*`.\r\n Is there any remote file caching policy in place? If so, is it possibile to programmatically disable it? \r\nCurrently it seems that the file `test.csv` on the repo [here](https:\/\/huggingface.co\/datasets\/loretoparisi\/tatoeba-sentences\/blob\/main\/test.csv) is cached remotely. In fact I download locally the file from raw link, the file is up-to-date; but If I use it within `datasets` as shown above, it gives to me always the first revision of the file, not the last.\r\n\r\nThank you.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset,Features,Value,ClassLabel\r\n\r\nclass_names = [\"cmn\",\"deu\",\"rus\",\"fra\",\"eng\",\"jpn\",\"spa\",\"ita\",\"kor\",\"vie\",\"nld\",\"epo\",\"por\",\"tur\",\"heb\",\"hun\",\"ell\",\"ind\",\"ara\",\"arz\",\"fin\",\"bul\",\"yue\",\"swe\",\"ukr\",\"bel\",\"que\",\"ces\",\"swh\",\"nno\",\"wuu\",\"nob\",\"zsm\",\"est\",\"kat\",\"pol\",\"lat\",\"urd\",\"sqi\",\"isl\",\"fry\",\"afr\",\"ron\",\"fao\",\"san\",\"bre\",\"tat\",\"yid\",\"uig\",\"uzb\",\"srp\",\"qya\",\"dan\",\"pes\",\"slk\",\"eus\",\"cycl\",\"acm\",\"tgl\",\"lvs\",\"kaz\",\"hye\",\"hin\",\"lit\",\"ben\",\"cat\",\"bos\",\"hrv\",\"tha\",\"orv\",\"cha\",\"mon\",\"lzh\",\"scn\",\"gle\",\"mkd\",\"slv\",\"frm\",\"glg\",\"vol\",\"ain\",\"jbo\",\"tok\",\"ina\",\"nds\",\"mal\",\"tlh\",\"roh\",\"ltz\",\"oss\",\"ido\",\"gla\",\"mlt\",\"sco\",\"ast\",\"jav\",\"oci\",\"ile\",\"ota\",\"xal\",\"tel\",\"sjn\",\"nov\",\"khm\",\"tpi\",\"ang\",\"aze\",\"tgk\",\"tuk\",\"chv\",\"hsb\",\"dsb\",\"bod\",\"sme\",\"cym\",\"mri\",\"ksh\",\"kmr\",\"ewe\",\"kab\",\"ber\",\"tpw\",\"udm\",\"lld\",\"pms\",\"lad\",\"grn\",\"mlg\",\"xho\",\"pnb\",\"grc\",\"hat\",\"lao\",\"npi\",\"cor\",\"nah\",\"avk\",\"mar\",\"guj\",\"pan\",\"kir\",\"myv\",\"prg\",\"sux\",\"crs\",\"ckt\",\"bak\",\"zlm\",\"hil\",\"cbk\",\"chr\",\"nav\",\"lkt\",\"enm\",\"arq\",\"lin\",\"abk\",\"pcd\",\"rom\",\"gsw\",\"tam\",\"zul\",\"awa\",\"wln\",\"amh\",\"bar\",\"hbo\",\"mhr\",\"bho\",\"mrj\",\"ckb\",\"osx\",\"pfl\",\"mgm\",\"sna\",\"mah\",\"hau\",\"kan\",\"nog\",\"sin\",\"glv\",\"dng\",\"kal\",\"liv\",\"vro\",\"apc\",\"jdt\",\"fur\",\"che\",\"haw\",\"yor\",\"crh\",\"pdc\",\"ppl\",\"kin\",\"shs\",\"mnw\",\"tet\",\"sah\",\"kum\",\"ngt\",\"nya\",\"pus\",\"hif\",\"mya\",\"moh\",\"wol\",\"tir\",\"ton\",\"lzz\",\"oar\",\"lug\",\"brx\",\"non\",\"mww\",\"hak\",\"nlv\",\"ngu\",\"bua\",\"aym\",\"vec\",\"ibo\",\"tkl\",\"bam\",\"kha\",\"ceb\",\"lou\",\"fuc\",\"smo\",\"gag\",\"lfn\",\"arg\",\"umb\",\"tyv\",\"kjh\",\"oji\",\"cyo\",\"urh\",\"kzj\",\"pam\",\"srd\",\"lmo\",\"swg\",\"mdf\",\"gil\",\"snd\",\"tso\",\"sot\",\"zza\",\"tsn\",\"pau\",\"som\",\"egl\",\"ady\",\"asm\",\"ori\",\"dtp\",\"cho\",\"max\",\"kam\",\"niu\",\"sag\",\"ilo\",\"kaa\",\"fuv\",\"nch\",\"hoc\",\"iba\",\"gbm\",\"sun\",\"war\",\"mvv\",\"pap\",\"ary\",\"kxi\",\"csb\",\"pag\",\"cos\",\"rif\",\"kek\",\"krc\",\"aii\",\"ban\",\"ssw\",\"tvl\",\"mfe\",\"tah\",\"bvy\",\"bcl\",\"hnj\",\"nau\",\"nst\",\"afb\",\"quc\",\"min\",\"tmw\",\"mad\",\"bjn\",\"mai\",\"cjy\",\"got\",\"hsn\",\"gan\",\"tzl\",\"dws\",\"ldn\",\"afh\",\"sgs\",\"krl\",\"vep\",\"rue\",\"tly\",\"mic\",\"ext\",\"izh\",\"sma\",\"jam\",\"cmo\",\"mwl\",\"kpv\",\"koi\",\"bis\",\"ike\",\"run\",\"evn\",\"ryu\",\"mnc\",\"aoz\",\"otk\",\"kas\",\"aln\",\"akl\",\"yua\",\"shy\",\"fkv\",\"gos\",\"fij\",\"thv\",\"zgh\",\"gcf\",\"cay\",\"xmf\",\"tig\",\"div\",\"lij\",\"rap\",\"hrx\",\"cpi\",\"tts\",\"gaa\",\"tmr\",\"iii\",\"ltg\",\"bzt\",\"syc\",\"emx\",\"gom\",\"chg\",\"osp\",\"stq\",\"frr\",\"fro\",\"nys\",\"toi\",\"new\",\"phn\",\"jpa\",\"rel\",\"drt\",\"chn\",\"pli\",\"laa\",\"bal\",\"hdn\",\"hax\",\"mik\",\"ajp\",\"xqa\",\"pal\",\"crk\",\"mni\",\"lut\",\"ayl\",\"ood\",\"sdh\",\"ofs\",\"nus\",\"kiu\",\"diq\",\"qxq\",\"alt\",\"bfz\",\"klj\",\"mus\",\"srn\",\"guc\",\"lim\",\"zea\",\"shi\",\"mnr\",\"bom\",\"sat\",\"szl\"]\r\nfeatures = Features({ 'label': ClassLabel(names=class_names), 'text': Value('string')})\r\nnum_labels = features['label'].num_classes\r\ndata_files = { \"train\": \"train.csv\", \"test\": \"test.csv\" }\r\nsentences = load_dataset(\r\n     \"loretoparisi\/tatoeba-sentences\",\r\n     data_files=data_files,\r\n     delimiter='\\t', \r\n     column_names=['label', 'text'],\r\n)\r\n# You can make this part faster with num_proc=<some int>\r\nsentences = sentences.map(lambda ex: {\"label\" : features[\"label\"].str2int(ex[\"label\"]) if ex[\"label\"] is not None else None}, features=features)\r\nsentences = sentences.shuffle()\r\n```\r\n\r\n## Expected results\r\nProperly tokenize dataset file `test.csv` without issues.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n```\r\nDownloading data files: 100%\r\n2\/2 [00:16<00:00, 7.34s\/it]\r\nDownloading data: 100%\r\n391M\/391M [00:12<00:00, 36.6MB\/s]\r\nDownloading data: 100%\r\n92.4M\/92.4M [00:02<00:00, 40.0MB\/s]\r\nExtracting data files: 100%\r\n2\/2 [00:00<00:00, 47.66it\/s]\r\nDataset csv downloaded and prepared to \/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-efeff8965c730a2c\/0.0.0\/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\r\n100%\r\n2\/2 [00:00<00:00, 25.94it\/s]\r\n11%\r\n942339\/8256449 [01:55<13:11, 9245.85ex\/s]\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n[<ipython-input-3-6a9867fad8d6>](https:\/\/localhost:8080\/#) in <module>()\r\n     12 )\r\n     13 # You can make this part faster with num_proc=<some int>\r\n---> 14 sentences = sentences.map(lambda ex: {\"label\" : features[\"label\"].str2int(ex[\"label\"]) if ex[\"label\"] is not None else None}, features=features)\r\n     15 sentences = sentences.shuffle()\r\n\r\n10 frames\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/features\/features.py](https:\/\/localhost:8080\/#) in str2int(self, values)\r\n    852                 if value not in self._str2int:\r\n    853                     value = str(value).strip()\r\n--> 854                 output.append(self._str2int[str(value)])\r\n    855             else:\r\n    856                 # No names provided, try to integerize\r\n\r\nKeyError: '\\\\N'\r\n```\r\n\r\n## Environment info\r\n```\r\n- `datasets` version: 2.1.0\r\n- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.3.5\r\n- ```\r\n\r\n\r\n```\r\n- `transformers` version: 4.18.0\r\n- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- Huggingface_hub version: 0.5.1\r\n- PyTorch version (GPU?): 1.11.0+cu113 (True)\r\n- Tensorflow version (GPU?): 2.8.0 (True)\r\n- Flax version (CPU?\/GPU?\/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n- ```\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4237","id":1217121044,"node_id":"I_kwDODunzps5Ii8sU","number":4237,"title":"Common Voice 8 doesn't show datasets viewer","user":{"login":"patrickvonplaten","id":23423619,"node_id":"MDQ6VXNlcjIzNDIzNjE5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/23423619?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/patrickvonplaten","html_url":"https:\/\/github.com\/patrickvonplaten","followers_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/followers","following_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/orgs","repos_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/repos","events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/received_events","type":"User","site_admin":false},"labels":[{"id":3470211881,"node_id":"LA_kwDODunzps7O1zsp","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer","name":"dataset-viewer","color":"E5583E","default":false,"description":"Related to the dataset viewer on huggingface.co"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Thanks for reporting. I understand it's an error in the dataset script. To reproduce:\r\n\r\n```python\r\n>>> import datasets as ds\r\n>>> split_names = ds.get_dataset_split_names(\"mozilla-foundation\/common_voice_8_0\", use_auth_token=\"**********\")\r\nDownloading builder script: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.9k\/10.9k [00:00<00:00, 10.9MB\/s]\r\nDownloading extra modules: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.98k\/2.98k [00:00<00:00, 3.36MB\/s]\r\nDownloading extra modules: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 53.1k\/53.1k [00:00<00:00, 650kB\/s]\r\nNo config specified, defaulting to: common_voice\/en\r\nTraceback (most recent call last):\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/libs\/libmodels\/.venv\/lib\/python3.9\/site-packages\/datasets\/inspect.py\", line 280, in get_dataset_config_info\r\n    for split_generator in builder._split_generators(\r\n  File \"\/home\/slesage\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/mozilla-foundation--common_voice_8_0\/720589e6e5ad674019008b719053303a71716db1b27e63c9846df02fdf93f2f3\/common_voice_8_0.py\", line 153, in _split_generators\r\n    self._log_download(self.config.name, bundle_version, hf_auth_token)\r\n  File \"\/home\/slesage\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/mozilla-foundation--common_voice_8_0\/720589e6e5ad674019008b719053303a71716db1b27e63c9846df02fdf93f2f3\/common_voice_8_0.py\", line 139, in _log_download\r\n    email = HfApi().whoami(auth_token)[\"email\"]\r\nKeyError: 'email'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/libs\/libmodels\/.venv\/lib\/python3.9\/site-packages\/datasets\/inspect.py\", line 323, in get_dataset_split_names\r\n    info = get_dataset_config_info(\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/libs\/libmodels\/.venv\/lib\/python3.9\/site-packages\/datasets\/inspect.py\", line 285, in get_dataset_config_info\r\n    raise SplitsNotFoundError(\"The split names could not be parsed from the dataset config.\") from err\r\ndatasets.inspect.SplitsNotFoundError: The split names could not be parsed from the dataset config.\r\n```","Thanks for reporting @patrickvonplaten and thanks for the investigation @severo.\r\n\r\nUnfortunately I'm not able to reproduce the error.\r\n\r\nI think the error has to do with authentication with `huggingface_hub`, because the exception is thrown from these code lines: https:\/\/huggingface.co\/datasets\/mozilla-foundation\/common_voice_8_0\/blob\/main\/common_voice_8_0.py#L137-L139\r\n```python\r\nfrom huggingface_hub import HfApi, HfFolder\r\n\r\nif isinstance(auth_token, bool):\r\n    email = HfApi().whoami(auth_token)\r\nemail = HfApi().whoami(auth_token)[\"email\"]\r\n```\r\n\r\nCould you please verify the previous code with the `auth_token` you pass to `load_dataset(..., use_auth_token=auth_token,...`?","OK, thanks for digging a bit into it. Indeed, the error occurs with the dataset-viewer, but not with a normal user token, because we use an app token, and it does not have a related email!\r\n\r\n```python\r\n>>> from huggingface_hub import HfApi, HfFolder\r\n>>> auth_token = \"hf_app_******\"\r\n>>> t = HfApi().whoami(auth_token)\r\n>>> t\r\n{'type': 'app', 'name': 'dataset-preview-backend'}\r\n>>> t[\"email\"]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nKeyError: 'email'\r\n```\r\n\r\nNote also that the doc (https:\/\/huggingface.co\/docs\/huggingface_hub\/package_reference\/hf_api#huggingface_hub.HfApi.whoami) does not state that `whoami` should return an `email` key.\r\n\r\n@SBrandeis @julien-c: do you think the app token should have an email associated, like the users?","We can workaround this with\r\n```python\r\nemail = HfApi().whoami(auth_token).get(\"email\", \"system@huggingface.co\")\r\n```\r\nin the common voice scripts","Hmmm, does this mean that any person who downloads the common voice dataset will be logged as \"system@huggingface.co\"? If so, it would defeat the purpose of sending the user's email to the commonvoice API, right?","I agree with @severo: we cannot set our system email as default, allowing anybody not authenticated to by-pass the Common Voice usage policy.\r\n\r\nAdditionally, looking at the code, I think we should implement a more robust way to send user email to Common Voice: currently anybody can tweak the script and send somebody else email instead.\r\n\r\nCC: @patrickvonplaten @lhoestq @SBrandeis @julien-c ","Hmm I don't agree here. \r\n\r\nAnybody can always just bypass the system by setting whatever email. As soon as someone has access to the downloading script it's trivial to tweak the code to not send the \"correct\" email but to just whatever and it would work.\r\n\r\nNote that someone only has visibility on the code after having \"signed\" the access-mechanism so I think we can expect the users to have agreed to not do anything malicious. \r\n\r\nI'm fine with both @lhoestq's solution or we find a way that forces the user to be logged in + being able to load the data for the datasets viewer. Wdyt @lhoestq @severo @albertvillanova ?","> Additionally, looking at the code, I think we should implement a more robust way to send user email to Common Voice: currently anybody can tweak the script and send somebody else email instead.\r\n\r\nYes, I agree we can forget about this @patrickvonplaten. After having had a look at Common Voice website, I've seen they only require sending an email (no auth is inplace on their side, contrary to what I had previously thought). Therefore, currently we impose stronger requirements than them: we require the user having logged in and accepted the access mechanism.\r\n\r\nCurrently the script as it is already requires the user being logged in:\r\n```python\r\nHfApi().whoami(auth_token)\r\n```\r\nthrows an exception if None\/invalid auth_token is passed.\r\n\r\nOn the other hand, we should agree on the way to allow the viewer to stream the data.","The preview is back now, thanks !"],"created_at":1651053920000,"updated_at":1652185025000,"closed_at":1652185024000,"author_association":"MEMBER","active_lock_reason":null,"draft":null,"pull_request":null,"body":"https:\/\/huggingface.co\/datasets\/mozilla-foundation\/common_voice_8_0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4236","id":1217115691,"node_id":"PR_kwDODunzps423MOc","number":4236,"title":"Replace data URL in big_patent dataset and support streaming","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","I first uploaded the data files to the Hub: I think it is a good option because we have git lfs to track versions and changes. Moreover people will be able to make PRs to propose updates on the data files.\r\n- I would have preferred to upload it it to the \"data\" org namespace, but it is already taken (although not used): might be possible to take it?\r\n\r\nAs an alternative (and to be consistent with previous datasets), I also uploaded the data files to our AWS bucket.\r\n\r\nWe should decide which to use (now and for future datasets) and set it here before merging. We should remove the data files for the non-chosen option.\r\n\r\nCC: @lhoestq @mariosasko @polinaeterna ","Would it make sense to make the dataset a community one (so, create an organization for it) and store the script and the data in a single repository? Just as it is for most of the datasets. That way we can also access the data using a relative path inside the repo (that's not the point though). The point is that to me it seems a bit more straightforward to store everything in one place. \r\n\r\nI guess the strong argument against this logic is that in this case the canonical version won't work... But maybe there is some redirecting mechanism I don't know about? :)\r\n\r\nAnyway, I'm in favor of hosting data on the Hub instead of AWS :) ","I also think storing everything in one place\/single repository is the best option.\r\n\r\n@polinaeterna Canonical datasets also support data files (see the [`red_caps` repo](https:\/\/huggingface.co\/datasets\/red_caps\/tree\/main) for instance) ","Thanks @polinaeterna and @mariosasko for your comments.\r\n\r\nYes, definitely it is much better to have everything in the same repo. \r\n\r\nI'm transferring their data files to the Hub under \"big_patent\" and deleting them from the other repo and AWS."],"created_at":1651053673000,"updated_at":1651826314000,"closed_at":1651515675000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4236","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4236","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4236.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4236.patch","merged_at":1651515675000},"body":"This PR replaces the Google Drive URL with by our Hub one, once the data owners have approved to host their data on the Hub.\r\n\r\nMoreover, this PR makes the dataset streamable.\r\n\r\nFix #4217.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4235","id":1216952640,"node_id":"I_kwDODunzps5IiTlA","number":4235,"title":"How to load VERY LARGE dataset?","user":{"login":"CaoYiqingT","id":45160643,"node_id":"MDQ6VXNlcjQ1MTYwNjQz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/45160643?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/CaoYiqingT","html_url":"https:\/\/github.com\/CaoYiqingT","followers_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/followers","following_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/orgs","repos_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/repos","events_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The `Trainer` support `IterableDataset`, not just datasets."],"created_at":1651045813000,"updated_at":1651059857000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"### System Info\n\n```shell\nI am using transformer trainer while meeting the issue.\r\nThe trainer requests torch.utils.data.Dataset as input, which loads the whole dataset into the memory at once. Therefore, when the dataset is too large to load, there's nothing I can do except using IterDataset, which loads samples of data seperately, and results in low efficiency. \r\nI wonder if there are any tricks like Sharding in huggingface trainer.\r\nLooking forward to your reply.\n```\n\n\n### Who can help?\n\nTrainer: @sgugger\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nNone\n\n### Expected behavior\n\n```shell\nI wonder if there are any tricks like fairseq Sharding very large datasets https:\/\/fairseq.readthedocs.io\/en\/latest\/getting_started.html.\r\nThanks a lot!\n```\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4234","id":1216818846,"node_id":"PR_kwDODunzps422Mwn","number":4234,"title":"Autoeval config","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Related to: https:\/\/github.com\/huggingface\/autonlp-backend\/issues\/414 and https:\/\/github.com\/huggingface\/autonlp-backend\/issues\/424","The tests are failing due to the changed metadata:\r\n\r\n```\r\ngot an unexpected keyword argument 'train-eval-index'\r\n```\r\n\r\nI think you can fix this by updating the `DatasetMetadata` class and implementing an appropriate `validate_train_eval_index()` function\r\n\r\n@lhoestq we are working with an arbitrary set of tags for `autoeval config`. See https:\/\/github.com\/huggingface\/autonlp-backend\/issues\/414\r\nI need to add a validator function though for the tests to pass. Our set is not well-defined as in the rest https:\/\/github.com\/huggingface\/datasets\/tree\/master\/src\/datasets\/utils\/resources. What's a workaround for this?","On the question of validating the `train-eval-index` metadata, I think the simplest approach would be to validate that the required fields exist and not worry about their values (which are open-ended).\r\n\r\nFor me, the required fields include:\r\n\r\n* `config`\r\n* `task`\r\n* `task_id`\r\n* `splits` (train \/ validation \/ eval)\r\n* `col_mapping`\r\n* `metrics` (checking that each one has `type`, `name`) \r\n\r\nHere I'm using the spec defined in https:\/\/github.com\/huggingface\/autonlp-backend\/issues\/414 as a guide.\r\n\r\nWDYT @lhoestq ?","Makes sense ! Currently the metadata type validator doesn't support subfields - let me open a PR to add it","I ended up improving the metadata validation in this PR x)\r\n\r\nIn particular:\r\n- I added support YAML keys with dashes instead of underscores for `train-eval-index`\r\n- I added `train-eval-index` validation  with `validate_train_eval_index`. It does nothing fancy, it just checks that it is a list if it exists in the YAML, but feel free to improve it if you want\r\n\r\nLet me know if it sounds good to you ! I think we can improve `validate_train_eval_index` in another PR","Come on windows... I didn't do anything advanced...\r\n\r\nAnyway, will try to fix this when I get back home x)","> Come on windows... I didn't do anything advanced...\r\n> \r\n> Anyway, will try to fix this when I get back home x)\r\n\r\nHehe, thanks!","Thanks, @lhoestq this is great! ","Did I just fix it for windows and now it fails on linux ? xD","> Did I just fix it for windows and now it fails on linux ? xD\r\n\r\nLooks like the Heisenberg uncertainty principle is at play here - you cannot simultaneously have unit tests passing in both Linux and Windows \ud83d\ude05 ","The worst is that the tests pass locally both on my windows and my linux x)","Ok fixed it, the issue came from python 3.6 that doesn't return the right `__origin__` for Dict and List types","> Alright thanks for adding the first Autoeval config ! :D\r\n\r\nWoohoo! Thank you so much \ud83e\udd17 ","This is cool!"],"created_at":1651037530000,"updated_at":1651843231000,"closed_at":1651774858000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4234","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4234","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4234.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4234.patch","merged_at":1651774858000},"body":"Added autoeval config to imdb as pilot","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4336","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4336\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4336\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4336\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4336","id":1234446174,"node_id":"PR_kwDODunzps43vpqG","number":4336,"title":"Eval metadata batch 2 : Health Fact, Jigsaw Toxicity, LIAR, LJ Speech, MSRA NER, Multi News, NCBI Disease, PiQA, Poem Sentiment, QAsper","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Summary of CircleCI errors:\r\n- **Jjigsaw_toxicity_pred**:  `Citation Information` but it is empty.\r\n- **LIAR** :  `Data Instances`,`Data Fields`,  `Data Splits`,  `Citation Information` are empty.\r\n- **MSRA NER** : Dataset Summary`, `Data Instances`, `Data Fields`,  `Data Splits`, `Citation Information` are empty.\r\n"],"created_at":1652387085000,"updated_at":1652387085000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4336","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4336","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4336.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4336.patch","merged_at":null},"body":"Adding evaluation metadata for Health Fact, Jigsaw Toxicity, LIAR, LJ Speech, MSRA NER, Multi News, NCBI Disease, PiQA, Poem Sentiment, QAsper","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4336\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4336\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4335","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4335\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4335\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4335\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4335","id":1234157123,"node_id":"PR_kwDODunzps43usJP","number":4335,"title":"Eval metadata batch 1: BillSum, BoolQ, CoNLL2003, CoNLLPP, CUAD, Emotion, GigaWord, GLUE, Hate Speech 18,  Hate Speech","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Summary of CircleCI errors:\r\n- **BoolQ**: missing 8 required positional arguments: 'annotations_creators', 'language_creators', 'licenses', 'multilinguality', 'size_categories', 'source_datasets', 'task_categories', and 'task_ids'\r\n- **Conllpp**: expected some content in section `Citation Information` but it is empty.\r\n- **GLUE**:   'annotations_creators', 'language_creators', 'source_datasets' :['unknown'] are not registered tags\r\n- **ConLL2003**:  field 'task_ids':  ['part-of-speech-tagging'] are not registered tags for 'task_ids'\r\n-  **Hate_speech18:**   Expected some content in section `Data Instances` but it is empty, Expected some content in section `Data Splits` but it is empty"],"created_at":1652369296000,"updated_at":1652382954000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4335","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4335","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4335.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4335.patch","merged_at":null},"body":"adding evaluation metadata for the BillSum, BoolQ, CoNLL2003, CoNLLPP, CUAD, Emotion, GigaWord, GLUE, Hate Speech 18 and Hate Speech datasets","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4335\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4335\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4334","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4334\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4334\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4334\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4334","id":1234103477,"node_id":"PR_kwDODunzps43uguB","number":4334,"title":"Adding eval metadata for billsum","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1652366948000,"updated_at":1652366964000,"closed_at":1652366964000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":true,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4334","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4334","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4334.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4334.patch","merged_at":null},"body":"Adding eval metadata for billsum","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4334\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4334\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4333","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4333\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4333\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4333\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4333","id":1234038705,"node_id":"PR_kwDODunzps43uSuj","number":4333,"title":"Adding eval metadata for Banking 77","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["@lhoestq , Circle CI is giving me an error, saying that ['extended'] is a key that shouldn't be in the dataset metadata, but it was there before my modification (so I don't want to remove it)"],"created_at":1652364305000,"updated_at":1652367466000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4333","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4333","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4333.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4333.patch","merged_at":null},"body":"Adding eval metadata for Banking 77","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4333\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4333\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4332","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4332\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4332\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4332\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4332","id":1234021188,"node_id":"PR_kwDODunzps43uO8S","number":4332,"title":"Adding eval metadata for arabic speech corpus","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1652363498000,"updated_at":1652366342000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4332","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4332","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4332.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4332.patch","merged_at":null},"body":"Adding eval metadata for arabic speech corpus","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4332\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4332\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4331","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4331\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4331\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4331\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4331","id":1234016110,"node_id":"PR_kwDODunzps43uN2R","number":4331,"title":"Adding eval metadata to Amazon Polarity","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1652363279000,"updated_at":1652366375000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4331","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4331","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4331.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4331.patch","merged_at":null},"body":"Adding eval metadata to Amazon Polarity","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4331\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4331\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4330","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4330\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4330\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4330\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4330","id":1233992681,"node_id":"PR_kwDODunzps43uIwm","number":4330,"title":"Adding eval metadata to Allocin\u00e9 dataset","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1652362299000,"updated_at":1652366420000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4330","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4330","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4330.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4330.patch","merged_at":null},"body":"Adding eval metadata to Allocin\u00e9 dataset","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4330\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4330\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4329","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4329\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4329\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4329\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4329","id":1233991207,"node_id":"PR_kwDODunzps43uIcF","number":4329,"title":"Adding eval metadata for AG News","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1652362232000,"updated_at":1652366457000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4329","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4329","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4329.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4329.patch","merged_at":null},"body":"Adding eval metadata for AG News","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4329\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4329\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4328","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4328\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4328\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4328\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4328","id":1233856690,"node_id":"PR_kwDODunzps43trrd","number":4328,"title":"Fix and clean Apache Beam functionality","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4328). All of your documentation changes will be reflected on that endpoint."],"created_at":1652355667000,"updated_at":1652356669000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4328","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4328","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4328.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4328.patch","merged_at":null},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4328\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4328\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4327","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4327\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4327\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4327\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4327","id":1233840020,"node_id":"I_kwDODunzps5JiueU","number":4327,"title":"`wikipedia` pre-processed datasets","user":{"login":"vpj","id":81152,"node_id":"MDQ6VXNlcjgxMTUy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/81152?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/vpj","html_url":"https:\/\/github.com\/vpj","followers_url":"https:\/\/api.github.com\/users\/vpj\/followers","following_url":"https:\/\/api.github.com\/users\/vpj\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/vpj\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/vpj\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/vpj\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/vpj\/orgs","repos_url":"https:\/\/api.github.com\/users\/vpj\/repos","events_url":"https:\/\/api.github.com\/users\/vpj\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/vpj\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi @vpj, thanks for reporting.\r\n\r\nI'm sorry, but I can't reproduce your bug: I load \"20220301.simple\"in 9 seconds:\r\n```shell\r\ntime python -c \"from datasets import load_dataset; load_dataset('wikipedia', '20220301.simple')\"\r\n\r\nDownloading and preparing dataset wikipedia\/20220301.simple (download: 228.58 MiB, generated: 224.18 MiB, post-processed: Unknown size, total: 452.76 MiB) to ...\/.cache\/huggingface\/datasets\/wikipedia\/20220301.simple\/2.0.0\/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559...\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.66k\/1.66k [00:00<00:00, 1.02MB\/s]\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 235M\/235M [00:02<00:00, 82.8MB\/s]\r\nDataset wikipedia downloaded and prepared to ...\/.cache\/huggingface\/datasets\/wikipedia\/20220301.simple\/2.0.0\/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559. Subsequent calls will reuse this data.\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:00<00:00, 290.75it\/s]\r\n\r\nreal\t0m9.693s\r\nuser\t0m6.002s\r\nsys\t0m3.260s\r\n```\r\n\r\nCould you please check your environment info, as requested when opening this issue?\r\n```\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:\r\n- Platform:\r\n- Python version:\r\n- PyArrow version:\r\n```\r\nMaybe you are using an old version of `datasets`..."],"created_at":1652354742000,"updated_at":1652363921000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\n[Wikipedia](https:\/\/huggingface.co\/datasets\/wikipedia) dataset readme says that certain subsets are preprocessed. However it seems like they are not available. When I try to load them it takes a really long time, and it seems like it's processing them.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nload_dataset(\"wikipedia\", \"20220301.en\")\r\n```\r\n\r\n## Expected results\r\nTo load the dataset\r\n\r\n## Actual results\r\nTakes a very long time to load (after downloading)\r\n\r\nAfter `Downloading data files: 100%`. It takes hours and gets killed.\r\nTried `wikipedia.simple` and it got processed after ~30mins.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4327\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4327\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4326","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4326\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4326\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4326\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4326","id":1233818489,"node_id":"PR_kwDODunzps43tjWy","number":4326,"title":"Fix type hint and documentation for `new_fingerprint`","user":{"login":"fxmarty","id":9808326,"node_id":"MDQ6VXNlcjk4MDgzMjY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/9808326?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/fxmarty","html_url":"https:\/\/github.com\/fxmarty","followers_url":"https:\/\/api.github.com\/users\/fxmarty\/followers","following_url":"https:\/\/api.github.com\/users\/fxmarty\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/fxmarty\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/fxmarty\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/fxmarty\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/fxmarty\/orgs","repos_url":"https:\/\/api.github.com\/users\/fxmarty\/repos","events_url":"https:\/\/api.github.com\/users\/fxmarty\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/fxmarty\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4326). All of your documentation changes will be reflected on that endpoint."],"created_at":1652353508000,"updated_at":1652354339000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4326","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4326","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4326.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4326.patch","merged_at":null},"body":"Currently, there are no type hints nor `Optional` for the argument `new_fingerprint` in several methods of `datasets.arrow_dataset.Dataset`.\r\n\r\nThere was some documentation missing as well.\r\n\r\nNote that pylance is happy with the type hints, but pyright does not detect that `new_fingerprint` is set within the decorator.\r\n\r\nThe modifications in this PR are fine since here https:\/\/github.com\/huggingface\/datasets\/blob\/aa743886221d76afb409d263e1b136e7a71fe2b4\/src\/datasets\/fingerprint.py#L446-L454\r\n\r\nfor the non-inplace case we make sure to auto-generate a new fingerprint (as indicated in the doc).","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4326\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4326\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4325","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4325\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4325\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4325\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4325","id":1233812191,"node_id":"I_kwDODunzps5Jinrf","number":4325,"title":"Dataset Viewer issue for strombergnlp\/offenseval_2020, strombergnlp\/polstance","user":{"login":"leondz","id":121934,"node_id":"MDQ6VXNlcjEyMTkzNA==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/121934?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/leondz","html_url":"https:\/\/github.com\/leondz","followers_url":"https:\/\/api.github.com\/users\/leondz\/followers","following_url":"https:\/\/api.github.com\/users\/leondz\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/leondz\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/leondz\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/leondz\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/leondz\/orgs","repos_url":"https:\/\/api.github.com\/users\/leondz\/repos","events_url":"https:\/\/api.github.com\/users\/leondz\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/leondz\/received_events","type":"User","site_admin":false},"labels":[{"id":3470211881,"node_id":"LA_kwDODunzps7O1zsp","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer","name":"dataset-viewer","color":"E5583E","default":false,"description":"Related to the dataset viewer on huggingface.co"}],"state":"open","locked":false,"assignee":{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},"assignees":[{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Not sure if it's related... I was going to raise an issue for https:\/\/huggingface.co\/datasets\/domenicrosati\/TruthfulQA which also has the same issue... https:\/\/huggingface.co\/datasets\/domenicrosati\/TruthfulQA\/viewer\/domenicrosati--TruthfulQA\/train \r\n\r\n","Yes, it's related. The backend behind the dataset viewer is currently under too much load, and these datasets are still in the jobs queue. We're actively working on this issue, and we expect to fix the issue permanently soon. Thanks for your patience \ud83d\ude4f \u00a0","Thanks @severo and no worries! - a suggestion for a UI usability thing maybe is to indicate that the dataset processing is in the job queue (rather than no data?)"],"created_at":1652353148000,"updated_at":1652378145000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"### Link\n\nhttps:\/\/huggingface.co\/datasets\/strombergnlp\/offenseval_2020\/viewer\/ar\/train\n\n### Description\n\nThe viewer isn't running for these two datasets. I left it overnight because a wait sometimes helps things get loaded, and the error messages have all gone, but the datasets are still turning up blank in viewer. Maybe it needs a bit more time.\r\n\r\n* https:\/\/huggingface.co\/datasets\/strombergnlp\/polstance\/viewer\/PolStance\/train\r\n\r\n* https:\/\/huggingface.co\/datasets\/strombergnlp\/offenseval_2020\/viewer\/ar\/train\r\n\r\nWhile offenseval_2020 is gated w. prompt, the other gated previews I have run fine in Viewer, e.g. https:\/\/huggingface.co\/datasets\/strombergnlp\/shaj , so I'm a bit stumped!\n\n### Owner\n\nYes","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4325\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4325\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4324","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4324\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4324\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4324\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4324","id":1233780870,"node_id":"I_kwDODunzps5JigCG","number":4324,"title":"Support >1 PWC dataset per dataset card","user":{"login":"leondz","id":121934,"node_id":"MDQ6VXNlcjEyMTkzNA==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/121934?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/leondz","html_url":"https:\/\/github.com\/leondz","followers_url":"https:\/\/api.github.com\/users\/leondz\/followers","following_url":"https:\/\/api.github.com\/users\/leondz\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/leondz\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/leondz\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/leondz\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/leondz\/orgs","repos_url":"https:\/\/api.github.com\/users\/leondz\/repos","events_url":"https:\/\/api.github.com\/users\/leondz\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/leondz\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1652351347000,"updated_at":1652352522000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"**Is your feature request related to a problem? Please describe.**\r\nSome datasets cover more than one dataset on PapersWithCode. For example, the OffensEval 2020 challenge involved five languages, and there's one dataset to cover all five datasets, `strombergnlp\/offenseval_2020`. However, the yaml `paperswithcode_id:` dataset card entry only supports one value; when multiple are added, the PWC link disappears from the dataset page.\r\n\r\nBecause the link from a PapersWithCode dataset to a Hugging Face Hub entry can't be entered manually and seems to be scraped, this means end users don't have a way of getting a dataset reader link to appear on all the PWC datasets supported by one HF Hub Dataset reader.\r\n\r\nIt's not super unusual to have papers introduce multiple parallel variants of a dataset and would be handy to reflect this, so e.g. dataset maintainers can DRY, and so dataset users can keep what they're doing simple.\r\n\r\n**Describe the solution you'd like**\r\nI'd like `paperswithcode_id:` to support lists and be able to connect with multiple PWC datasets.\r\n\r\n**Describe alternatives you've considered**\r\nDe-normalising the datasets on HF Hub to create multiple readers for each variation on a task, i.e. instead of a single `offenseval_2020`, having `offenseval_2020_ar`, `offenseval_2020_da`, `offenseval_2020_gr`, ...\r\n\r\n**Additional context**\r\nHope that's enough\r\n\r\n**Priority**\r\nLow","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4324\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4324\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4323","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4323\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4323\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4323\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4323","id":1233634928,"node_id":"I_kwDODunzps5Jh8Zw","number":4323,"title":"Audio can not find value[\"bytes\"]","user":{"login":"YooSungHyun","id":34292279,"node_id":"MDQ6VXNlcjM0MjkyMjc5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/34292279?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/YooSungHyun","html_url":"https:\/\/github.com\/YooSungHyun","followers_url":"https:\/\/api.github.com\/users\/YooSungHyun\/followers","following_url":"https:\/\/api.github.com\/users\/YooSungHyun\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/YooSungHyun\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/YooSungHyun\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/YooSungHyun\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/YooSungHyun\/orgs","repos_url":"https:\/\/api.github.com\/users\/YooSungHyun\/repos","events_url":"https:\/\/api.github.com\/users\/YooSungHyun\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/YooSungHyun\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["![image](https:\/\/user-images.githubusercontent.com\/34292279\/168063684-fff5c12a-8b1e-4c65-b18b-36100ab8a1af.png)\r\n\r\nthat is reason my bytes`s empty\r\nbut i have some confused why path prior is higher than bytes?\r\n\r\nif you can make bytes in _generate_examples , you don`t have to make bytes to path?\r\nbecause we have path and bytes already"],"created_at":1652344318000,"updated_at":1652354920000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nI wrote down _generate_examples like:\r\n![image](https:\/\/user-images.githubusercontent.com\/34292279\/168027186-2fe8b255-2cd8-4b9b-ab1e-8d5a7182979b.png)\r\n\r\nbut where is the bytes?\r\n![image](https:\/\/user-images.githubusercontent.com\/34292279\/168027330-f2496dd0-1d99-464c-b15c-bc57eee0415a.png)\r\n\r\n\r\n## Expected results\r\nvalue[\"bytes\"] is not None, so i can make datasets with bytes, not path\r\n\r\n## bytes looks like:\r\nblah blah~~\r\n\\xfe\\x03\\x00\\xfb\\x06\\x1c\\x0bo\\x074\\x03\\xaf\\x01\\x13\\x04\\xbc\\x06\\x8c\\x05y\\x05,\\t7\\x08\\xaf\\x03\\xc0\\xfe\\xe8\\xfc\\x94\\xfe\\xb7\\xfd\\xea\\xfa\\xd5\\xf9$\\xf9>\\xf9\\x1f\\xf8\\r\\xf5F\\xf49\\xf4\\xda\\xf5-\\xf8\\n\\xf8k\\xf8\\x07\\xfb\\x18\\xfd\\xd9\\xfdv\\xfd\"\\xfe\\xcc\\x01\\x1c\\x04\\x08\\x04@\\x04{\\x06^\\tf\\t\\x1e\\x07\\x8b\\x06\\x02\\x08\\x13\\t\\x07\\x08 \\x06g\\x06\"\\x06\\xa0\\x03\\xc6\\x002\\xff \\xff\\x1d\\xff\\x19\\xfd?\\xfb\\xdb\\xfa\\xfc\\xfa$\\xfb}\\xf9\\xe5\\xf7\\xf9\\xf7\\xce\\xf8.\\xf9b\\xf9\\xc5\\xf9\\xc0\\xfb\\xfa\\xfcP\\xfc\\xba\\xfbQ\\xfc1\\xfe\\x9f\\xff\\x12\\x00\\xa2\\x00\\x18\\x02Z\\x03\\x02\\x04\\xb1\\x03\\xc5\\x03W\\x04\\x82\\x04\\x8f\\x04U\\x04\\xb6\\x04\\x10\\x05{\\x04\\x83\\x02\\x17\\x01\\x1d\\x00\\xa0\\xff\\xec\\xfe\\x03\\xfe#\\xfe\\xc2\\xfe2\\xff\\xe6\\xfe\\x9a\\xfe~\\x01\\x91\\x08\\xb3\\tU\\x05\\x10\\x024\\x02\\xe4\\x05\\xa8\\x07\\xa7\\x053\\x07I\\n\\x91\\x07v\\x02\\x95\\xfd\\xbb\\xfd\\x96\\xff\\x01\\xfe\\x1e\\xfb\\xbb\\xf9S\\xf8!\\xf8\\xf4\\xf5\\xd6\\xf3\\xf7\\xf3l\\xf4d\\xf6l\\xf7d\\xf6b\\xf7\\xc1\\xfa(\\xfd\\xcf\\xfd*\\xfdq\\xfe\\xe9\\x01\\xa8\\x03t\\x03\\x17\\x04B\\x07\\xce\\t\\t\\t\\xeb\\x06\\x0c\\x07\\x95\\x08\\x92\\t\\xbc\\x07O\\x06\\xfb\\x06\\xd2\\x06U\\x04\\x00\\x02\\x92\\x00\\xdc\\x00\\x84\\x00 \\xfeT\\xfc\\xf1\\xfb\\x82\\xfc\\x97\\xfb}\\xf9\\x00\\xf8_\\xf8\\x0b\\xf9\\xe5\\xf8\\xe2\\xf7\\xaa\\xf8\\xb2\\xfa\\x10\\xfbl\\xfa\\xf5\\xf9Y\\xfb\\xc0\\xfd\\xe8\\xfe\\xec\\xfe1\\x00\\xad\\x01\\xec\\x02E\\x03\\x13\\x03\\x9b\\x03o\\x04\\xce\\x04\\xa8\\x04\\xb2\\x04\\x1b\\x05\\xc0\\x05\\xd2\\x04\\xe8\\x02z\\x01\\xbe\\x00\\xae\\x00\\x07\\x00$\\xff|\\xff\\x8e\\x00\\x13\\x00\\x10\\xff\\x98\\xff0\\x05{\\x0b\\x05\\t\\xaa\\x03\\x82\\x01n\\x03\r\nblah blah~~\r\n\r\nthat function not return None\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:2.2.1\r\n- Platform:ubuntu 18.04\r\n- Python version:3.6.9\r\n- PyArrow version:6.0.1\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4323\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4323\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4322","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4322\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4322\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4322\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4322","id":1233596947,"node_id":"PR_kwDODunzps43s1wy","number":4322,"title":"Added stratify option to train_test_split function.","user":{"login":"nandwalritik","id":48522685,"node_id":"MDQ6VXNlcjQ4NTIyNjg1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/48522685?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nandwalritik","html_url":"https:\/\/github.com\/nandwalritik","followers_url":"https:\/\/api.github.com\/users\/nandwalritik\/followers","following_url":"https:\/\/api.github.com\/users\/nandwalritik\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nandwalritik\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nandwalritik\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nandwalritik\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nandwalritik\/orgs","repos_url":"https:\/\/api.github.com\/users\/nandwalritik\/repos","events_url":"https:\/\/api.github.com\/users\/nandwalritik\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nandwalritik\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["> Nice thank you ! This will be super useful :)\r\n> \r\n> Could you also add some tests in test_arrow_dataset.py and add an example of usage in the `Example:` section of the `train_test_split` docstring ?\r\n\r\nI will try to do it, is there any documentation for adding test cases? I have never done it before.","Thanks for the changes !\r\n\r\n> I will try to do it, is there any documentation for adding test cases? I have never done it before.\r\n\r\nYou can just add a function `test_train_test_split_startify` in `test_arrow_dataset.py`.\r\n\r\nIn this function you can define a dataset and make sure that `train_test_split` with the `stratify` argument works as expected.\r\n\r\nYou can do `pytest tests\/test_arrow_dataset.py::test_train_test_split_startify` to run your test.\r\n\r\nFeel free to get some inspiration from other tests like `test_interleave_datasets` for example"],"created_at":1652342431000,"updated_at":1652356193000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4322","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4322","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4322.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4322.patch","merged_at":null},"body":"This PR adds `stratify` option to `train_test_split` method. I took reference from scikit-learn's `StratifiedShuffleSplit` class for implementing stratified split and integrated the changes as were suggested by @lhoestq.\r\n\r\nIt fixes #3452.\r\n\r\n@lhoestq Please review and let me know, if any changes are required.\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4322\/reactions","total_count":2,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4322\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4321","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4321\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4321\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4321\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4321","id":1233273351,"node_id":"PR_kwDODunzps43ryW7","number":4321,"title":"Adding dataset enwik8","user":{"login":"HallerPatrick","id":22773355,"node_id":"MDQ6VXNlcjIyNzczMzU1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/22773355?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/HallerPatrick","html_url":"https:\/\/github.com\/HallerPatrick","followers_url":"https:\/\/api.github.com\/users\/HallerPatrick\/followers","following_url":"https:\/\/api.github.com\/users\/HallerPatrick\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/HallerPatrick\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/HallerPatrick\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/HallerPatrick\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/HallerPatrick\/orgs","repos_url":"https:\/\/api.github.com\/users\/HallerPatrick\/repos","events_url":"https:\/\/api.github.com\/users\/HallerPatrick\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/HallerPatrick\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1652311502000,"updated_at":1652312866000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4321","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4321","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4321.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4321.patch","merged_at":null},"body":"Because I regularly work with enwik8, I would like to contribute the dataset loader \ud83e\udd17 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4321\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4321\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4320","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4320\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4320\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4320\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4320","id":1233208864,"node_id":"I_kwDODunzps5JgUYg","number":4320,"title":"Multi-news dataset loader attempts to strip wrong character from beginning of summaries","user":{"login":"JohnGiorgi","id":8917831,"node_id":"MDQ6VXNlcjg5MTc4MzE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8917831?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/JohnGiorgi","html_url":"https:\/\/github.com\/JohnGiorgi","followers_url":"https:\/\/api.github.com\/users\/JohnGiorgi\/followers","following_url":"https:\/\/api.github.com\/users\/JohnGiorgi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/JohnGiorgi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/JohnGiorgi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/JohnGiorgi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/JohnGiorgi\/orgs","repos_url":"https:\/\/api.github.com\/users\/JohnGiorgi\/repos","events_url":"https:\/\/api.github.com\/users\/JohnGiorgi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/JohnGiorgi\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1652305001000,"updated_at":1652305062000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\n\r\nThe `multi_news.py` data loader has [a line which attempts to strip `\"- \"` from the beginning of summaries](https:\/\/github.com\/huggingface\/datasets\/blob\/aa743886221d76afb409d263e1b136e7a71fe2b4\/datasets\/multi_news\/multi_news.py#L97). The actual character in the multi-news dataset, however, is `\"\u2013 \"`, which is different, e.g. `\"\u2013 \" != \"- \"`.\r\n\r\nI would have just opened a PR to fix the mistake, but I am wondering what the motivation for stripping this character is? AFAICT most approaches just leave it in, e.g. the current SOTA on this dataset, [PRIMERA](https:\/\/huggingface.co\/allenai\/PRIMERA-multinews) (you can see its in the generated summaries of the model in their [example notebook](https:\/\/github.com\/allenai\/PRIMER\/blob\/main\/Evaluation_Example.ipynb)).\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.2.0\r\n- Platform: Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.3.5\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4320\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4320\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4319","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4319\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4319\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4319\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4319","id":1232982023,"node_id":"PR_kwDODunzps43q0UY","number":4319,"title":"Adding eval metadata for ade v2","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1652290580000,"updated_at":1652362191000,"closed_at":1652361739000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4319","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4319","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4319.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4319.patch","merged_at":1652361739000},"body":"Adding metadata to allow evaluation","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4319\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4319\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4318","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4318\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4318\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4318\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4318","id":1232905488,"node_id":"PR_kwDODunzps43qkkQ","number":4318,"title":"Don't check f.loc in _get_extraction_protocol_with_magic_number","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1652286429000,"updated_at":1652288222000,"closed_at":1652287591000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4318","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4318","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4318.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4318.patch","merged_at":1652287591000},"body":"`f.loc` doesn't always exist for file-like objects in python. I removed it since it was not necessary anyway (we always seek the file to 0 after reading the magic number)\r\n\r\nFix https:\/\/github.com\/huggingface\/datasets\/issues\/4310","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4318\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4318\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4317","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4317\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4317\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4317\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4317","id":1232737401,"node_id":"PR_kwDODunzps43qBzh","number":4317,"title":"Fix cnn_dailymail (dm stories were ignored)","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1652279125000,"updated_at":1652284809000,"closed_at":1652284357000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4317","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4317","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4317.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4317.patch","merged_at":1652284357000},"body":"https:\/\/github.com\/huggingface\/datasets\/pull\/4188 introduced a bug in `datasets` 2.2.0: DailyMail stories are ignored when generating the dataset.\r\n\r\nI fixed that, and removed the google drive link (it has annoying quota limitations issues)\r\n\r\nWe can do a patch release after this is merged","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4317\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4317\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4316","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4316\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4316\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4316\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4316","id":1232681207,"node_id":"PR_kwDODunzps43p1Za","number":4316,"title":"Support passing config_kwargs to CLI run_beam","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1652277217000,"updated_at":1652279809000,"closed_at":1652279311000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4316","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4316","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4316.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4316.patch","merged_at":1652279311000},"body":"This PR supports passing `config_kwargs` to CLI run_beam, so that for example for \"wikipedia\" dataset, we can pass:\r\n```\r\n--date 20220501 --language ca\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4316\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4316\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4315","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4315\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4315\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4315\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4315","id":1232549330,"node_id":"PR_kwDODunzps43pZ6p","number":4315,"title":"Fix CLI run_beam namespace","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1652271660000,"updated_at":1652274780000,"closed_at":1652274308000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4315","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4315","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4315.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4315.patch","merged_at":1652274308000},"body":"Currently, it raises TypeError:\r\n```\r\nTypeError: __init__() got an unexpected keyword argument 'namespace'\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4315\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4315\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4314","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4314\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4314\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4314\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4314","id":1232326726,"node_id":"PR_kwDODunzps43oqXD","number":4314,"title":"Catch pull error when mirroring","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1652261915000,"updated_at":1652273647000,"closed_at":1652273202000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4314","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4314","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4314.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4314.patch","merged_at":1652273202000},"body":"Catch pull errors when mirroring so that the script continues to update the other datasets.\r\n\r\nThe error will still be printed at the end of the job. In this case the job also fails, and asks to manually update the datasets that failed.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4314\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4314\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4313","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4313\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4313\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4313\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4313","id":1231764100,"node_id":"PR_kwDODunzps43m4qB","number":4313,"title":"Add API code examples for Builder classes","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1652221352000,"updated_at":1652374963000,"closed_at":1652359017000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4313","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4313","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4313.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4313.patch","merged_at":1652359017000},"body":"This PR adds API code examples for the Builder classes.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4313\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4313\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4312","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4312\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4312\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4312\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4312","id":1231662775,"node_id":"PR_kwDODunzps43mlug","number":4312,"title":"added TR-News dataset","user":{"login":"batubayk","id":25901065,"node_id":"MDQ6VXNlcjI1OTAxMDY1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/25901065?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/batubayk","html_url":"https:\/\/github.com\/batubayk","followers_url":"https:\/\/api.github.com\/users\/batubayk\/followers","following_url":"https:\/\/api.github.com\/users\/batubayk\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/batubayk\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/batubayk\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/batubayk\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/batubayk\/orgs","repos_url":"https:\/\/api.github.com\/users\/batubayk\/repos","events_url":"https:\/\/api.github.com\/users\/batubayk\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/batubayk\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1652214780000,"updated_at":1652214780000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4312","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4312","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4312.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4312.patch","merged_at":null},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4312\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4312\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4311","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4311\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4311\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4311\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4311","id":1231369438,"node_id":"PR_kwDODunzps43ln8-","number":4311,"title":"[Imagefolder] Docs + Don't infer labels from file names when there are metadata + Error messages when metadata and images aren't linked correctly","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Merging this one since mario is off, I took care of adding some tests to make sure everything is fine. Will do the release after it"],"created_at":1652197935000,"updated_at":1652203182000,"closed_at":1652202707000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4311","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4311","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4311.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4311.patch","merged_at":1652202707000},"body":"I updated the `docs\/source\/image_process.mdx` documentation and added an example for image captioning and object detection using `ImageFolder`.\r\n\r\nWhile doing so I also improved a few aspects:\r\n- we don't need to infer labels from file names when there are metadata - they can just be in the metadata if necessary\r\n- raise informative error messages when metadata and images aren't linked correctly:\r\n  - when an image is missing a metadata file\r\n  - when a metadata file is missing an image\r\n\r\nI added some tests for these changes as well\r\n\r\ncc @mariosasko ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4311\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4311\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4310","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4310\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4310\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4310\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4310","id":1231319815,"node_id":"I_kwDODunzps5JZHMH","number":4310,"title":"Loading dataset with streaming: '_io.BufferedReader' object has no attribute 'loc'","user":{"login":"milmin","id":72745467,"node_id":"MDQ6VXNlcjcyNzQ1NDY3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/72745467?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/milmin","html_url":"https:\/\/github.com\/milmin","followers_url":"https:\/\/api.github.com\/users\/milmin\/followers","following_url":"https:\/\/api.github.com\/users\/milmin\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/milmin\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/milmin\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/milmin\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/milmin\/orgs","repos_url":"https:\/\/api.github.com\/users\/milmin\/repos","events_url":"https:\/\/api.github.com\/users\/milmin\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/milmin\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"assignees":[{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":[],"created_at":1652195573000,"updated_at":1652287591000,"closed_at":1652287591000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nLoading a datasets with `load_dataset` and `streaming=True` returns `AttributeError: '_io.BufferedReader' object has no attribute 'loc'`. Notice that loading with `streaming=False` works fine.\r\n\r\nIn the following steps we load parquet files but the same happens with pickle files. The problem seems to come from `fsspec` lib, I put in the environment info also `s3fs` and `fsspec` versions since I'm loading from an s3 bucket.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n# path is the path to parquet files\r\ndata_files = {\"train\": path + \"meta_train.parquet.gzip\", \"test\": path + \"meta_test.parquet.gzip\"}\r\ndataset = load_dataset(\"parquet\", data_files=data_files, streaming=True)\r\n```\r\n\r\n## Expected results\r\nA dataset object `datasets.dataset_dict.DatasetDict`\r\n\r\n## Actual results\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n<command-562086> in <module>\r\n     11 \r\n     12 data_files = {\"train\": path + \"meta_train.parquet.gzip\", \"test\": path + \"meta_test.parquet.gzip\"}\r\n---> 13 dataset = load_dataset(\"parquet\", data_files=data_files, streaming=True)\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\r\n   1679     if streaming:\r\n   1680         extend_dataset_builder_for_streaming(builder_instance, use_auth_token=use_auth_token)\r\n-> 1681         return builder_instance.as_streaming_dataset(\r\n   1682             split=split,\r\n   1683             use_auth_token=use_auth_token,\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/builder.py in as_streaming_dataset(self, split, base_path, use_auth_token)\r\n    904         )\r\n    905         self._check_manual_download(dl_manager)\r\n--> 906         splits_generators = {sg.name: sg for sg in self._split_generators(dl_manager)}\r\n    907         # By default, return all splits\r\n    908         if split is None:\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/packaged_modules\/parquet\/parquet.py in _split_generators(self, dl_manager)\r\n     30         if not self.config.data_files:\r\n     31             raise ValueError(f\"At least one data file must be specified, but got data_files={self.config.data_files}\")\r\n---> 32         data_files = dl_manager.download_and_extract(self.config.data_files)\r\n     33         if isinstance(data_files, (str, list, tuple)):\r\n     34             files = data_files\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/utils\/streaming_download_manager.py in download_and_extract(self, url_or_urls)\r\n    798 \r\n    799     def download_and_extract(self, url_or_urls):\r\n--> 800         return self.extract(self.download(url_or_urls))\r\n    801 \r\n    802     def iter_archive(self, urlpath_or_buf: Union[str, io.BufferedReader]) -> Iterable[Tuple]:\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/utils\/streaming_download_manager.py in extract(self, path_or_paths)\r\n    776 \r\n    777     def extract(self, path_or_paths):\r\n--> 778         urlpaths = map_nested(self._extract, path_or_paths, map_tuple=True)\r\n    779         return urlpaths\r\n    780 \r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/utils\/py_utils.py in map_nested(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, types, disable_tqdm, desc)\r\n    312         num_proc = 1\r\n    313     if num_proc <= 1 or len(iterable) <= num_proc:\r\n--> 314         mapped = [\r\n    315             _single_map_nested((function, obj, types, None, True, None))\r\n    316             for obj in logging.tqdm(iterable, disable=disable_tqdm, desc=desc)\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/utils\/py_utils.py in <listcomp>(.0)\r\n    313     if num_proc <= 1 or len(iterable) <= num_proc:\r\n    314         mapped = [\r\n--> 315             _single_map_nested((function, obj, types, None, True, None))\r\n    316             for obj in logging.tqdm(iterable, disable=disable_tqdm, desc=desc)\r\n    317         ]\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/utils\/py_utils.py in _single_map_nested(args)\r\n    267         return {k: _single_map_nested((function, v, types, None, True, None)) for k, v in pbar}\r\n    268     else:\r\n--> 269         mapped = [_single_map_nested((function, v, types, None, True, None)) for v in pbar]\r\n    270         if isinstance(data_struct, list):\r\n    271             return mapped\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/utils\/py_utils.py in <listcomp>(.0)\r\n    267         return {k: _single_map_nested((function, v, types, None, True, None)) for k, v in pbar}\r\n    268     else:\r\n--> 269         mapped = [_single_map_nested((function, v, types, None, True, None)) for v in pbar]\r\n    270         if isinstance(data_struct, list):\r\n    271             return mapped\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/utils\/py_utils.py in _single_map_nested(args)\r\n    249     # Singleton first to spare some computation\r\n    250     if not isinstance(data_struct, dict) and not isinstance(data_struct, types):\r\n--> 251         return function(data_struct)\r\n    252 \r\n    253     # Reduce logging to keep things readable in multiprocessing with tqdm\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/utils\/streaming_download_manager.py in _extract(self, urlpath)\r\n    781     def _extract(self, urlpath: str) -> str:\r\n    782         urlpath = str(urlpath)\r\n--> 783         protocol = _get_extraction_protocol(urlpath, use_auth_token=self.download_config.use_auth_token)\r\n    784         if protocol is None:\r\n    785             # no extraction\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/utils\/streaming_download_manager.py in _get_extraction_protocol(urlpath, use_auth_token)\r\n    371         urlpath, kwargs = urlpath, {}\r\n    372     with fsspec.open(urlpath, **kwargs) as f:\r\n--> 373         return _get_extraction_protocol_with_magic_number(f)\r\n    374 \r\n    375 \r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/datasets\/utils\/streaming_download_manager.py in _get_extraction_protocol_with_magic_number(f)\r\n    335 def _get_extraction_protocol_with_magic_number(f) -> Optional[str]:\r\n    336     \"\"\"read the magic number from a file-like object and return the compression protocol\"\"\"\r\n--> 337     prev_loc = f.loc\r\n    338     magic_number = f.read(MAGIC_NUMBER_MAX_LENGTH)\r\n    339     f.seek(prev_loc)\r\n\r\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d\/lib\/python3.8\/site-packages\/fsspec\/implementations\/local.py in __getattr__(self, item)\r\n    337 \r\n    338     def __getattr__(self, item):\r\n--> 339         return getattr(self.f, item)\r\n    340 \r\n    341     def __enter__(self):\r\n\r\nAttributeError: '_io.BufferedReader' object has no attribute 'loc'\r\n```\r\n## Environment info\r\n- `datasets` version: 2.1.0\r\n- Platform: Linux-5.4.0-1071-aws-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.2\r\n- `fsspec` version: 2021.08.1\r\n- `s3fs` version: 2021.08.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4310\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4310\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4309","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4309\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4309\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4309\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4309","id":1231232935,"node_id":"PR_kwDODunzps43lKpm","number":4309,"title":"[WIP] Add TEDLIUM dataset","user":{"login":"sanchit-gandhi","id":93869735,"node_id":"U_kgDOBZhWpw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/93869735?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sanchit-gandhi","html_url":"https:\/\/github.com\/sanchit-gandhi","followers_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/followers","following_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/orgs","repos_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/repos","events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/received_events","type":"User","site_admin":false},"labels":[{"id":2067376369,"node_id":"MDU6TGFiZWwyMDY3Mzc2MzY5","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20request","name":"dataset request","color":"e99695","default":false,"description":"Requesting to add a new dataset"},{"id":2725241052,"node_id":"MDU6TGFiZWwyNzI1MjQxMDUy","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/speech","name":"speech","color":"d93f0b","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4309). All of your documentation changes will be reflected on that endpoint.","```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset('.\/datasets\/tedlium', 'release1', cache_dir='\/home\/sanchitgandhi\/cache')\r\n```\r\n\r\n```\r\nDownloading and preparing dataset tedlium\/release1 to \/home\/sanchitgandhi\/cache\/tedlium\/release1\/1.0.1\/5a9fcb97b4b52d5a1c9dc7bde4b1d5994cd89c4a3425ea36c789bf6096fee4f0...\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"\/home\/sanchit_huggingface_co\/datasets\/src\/datasets\/load.py\", line 1703, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/home\/sanchit_huggingface_co\/datasets\/src\/datasets\/builder.py\", line 605, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/home\/sanchit_huggingface_co\/datasets\/src\/datasets\/builder.py\", line 1240, in _download_and_prepare\r\n    raise MissingBeamOptions(\r\ndatasets.builder.MissingBeamOptions: Trying to generate a dataset using Apache Beam, yet no Beam Runner or PipelineOptions() has been provided in `load_dataset` or in the builder arguments. For big datasets it has to run on large-scale data processing tools like Dataflow, Spark, etc. More information about Apache Beam runners at https:\/\/beam.apache.org\/documentation\/runners\/capability-matrix\/\r\nIf you really want to run it locally because you feel like the Dataset is small enough, you can use the local beam runner called `DirectRunner` (you may run out of memory). \r\nExample of usage: \r\n        `load_dataset('tedlium', 'release1', beam_runner='DirectRunner')`\r\n```\r\nSpecifying the `beam_runner='DirectRunner'` works:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset('.\/datasets\/tedlium', 'release1', cache_dir='\/home\/sanchitgandhi\/cache', beam_runner='DirectRunner')\r\n```","Extra Python imports\/Linux packages:\r\n```\r\npip install pydub\r\nsudo apt install ffmpeg\r\n```","Script heavily inspired by the TF datasets script at: https:\/\/github.com\/tensorflow\/datasets\/blob\/master\/tensorflow_datasets\/audio\/tedlium.py\r\n\r\nThe TF datasets script uses the module AudioSegment from the package `pydub` (https:\/\/github.com\/jiaaro\/pydub), which is used to to open the audio files (stored in .sph format):\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/61bf6123634bf6e7c7287cd6097909eb26118c58\/datasets\/tedlium\/tedlium.py#L167-L170\r\nThis package requires the pip install of `pydub` and the system installation of `ffmpeg`: https:\/\/github.com\/jiaaro\/pydub#installation\r\nIs it ok to use these packages? Or do we tend to avoid introducing additional dependencies?\r\n\r\nThe TF datasets script also uses `_build_pcollection`:\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/8afbbb6fe66b40d05574e2e72e65e974c72ae769\/datasets\/tedlium\/tedlium.py#L200-L206\r\nHowever, I was advised against using `beam` logic. Thus, I have reverted to generating the examples file-by-file: https:\/\/github.com\/huggingface\/datasets\/blob\/61bf6123634bf6e7c7287cd6097909eb26118c58\/datasets\/tedlium\/tedlium.py#L112-L138\r\n\r\nI am now able to generate examples by running the `load_dataset` command:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset('.\/datasets\/tedlium', 'release1', cache_dir='\/home\/sanchitgandhi\/cache')\r\n```\r\n\r\nHere, generating examples is **extremely** slow: it takes ~1 second per example, so ~60k seconds for the train set (~16 hours). Is there a way of paralleling this to make it faster?","> This package requires the pip install of pydub and the system installation of ffmpeg: https:\/\/github.com\/jiaaro\/pydub#installation\r\nIs it ok to use these packages? Or do we tend to avoid introducing additional dependencies?\r\n\r\nIt's ok, windows users will have have a bad time but I'm not sure we can do much about it.\r\n\r\n> Here, generating examples is extremely slow: it takes ~1 second per example, so ~60k seconds for the train set (~16 hours). Is there a way of paralleling this to make it faster?\r\n\r\nNot at the moment. For such cases we advise hosting the dataset ourselves in a processed format. The license doesn't allow this since the license is \"NoDerivatives\". Currently the only way to parallelize it is by keeping is as a beam dataset and let users pay Google Dataflow to process it (or use spark or whatever).","Thanks for your super speedy reply @lhoestq!\r\n\r\nI\u2019ve uploaded the script and README.md to the org here: https:\/\/huggingface.co\/datasets\/LIUM\/tedlium\r\nIs any modification of the script required to be able to use it from the Hub? When I run:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ntedlium = load_dataset(\"LIUM\/tedlium\", \"release1\") # for Release 1\r\n```\r\nI get the following error:\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nInput In [2], in <cell line: 1>()\r\n----> 1 load_dataset(\"LIUM\/tedlium\", \"release1\")\r\n\r\nFile ~\/datasets\/src\/datasets\/load.py:1676, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\r\n   1673 ignore_verifications = ignore_verifications or save_infos\r\n   1675 # Create a dataset builder\r\n-> 1676 builder_instance = load_dataset_builder(\r\n   1677     path=path,\r\n   1678     name=name,\r\n   1679     data_dir=data_dir,\r\n   1680     data_files=data_files,\r\n   1681     cache_dir=cache_dir,\r\n   1682     features=features,\r\n   1683     download_config=download_config,\r\n   1684     download_mode=download_mode,\r\n   1685     revision=revision,\r\n   1686     use_auth_token=use_auth_token,\r\n   1687     **config_kwargs,\r\n   1688 )\r\n   1690 # Return iterable dataset in case of streaming\r\n   1691 if streaming:\r\n\r\nFile ~\/datasets\/src\/datasets\/load.py:1502, in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\r\n   1500     download_config = download_config.copy() if download_config else DownloadConfig()\r\n   1501     download_config.use_auth_token = use_auth_token\r\n-> 1502 dataset_module = dataset_module_factory(\r\n   1503     path,\r\n   1504     revision=revision,\r\n   1505     download_config=download_config,\r\n   1506     download_mode=download_mode,\r\n   1507     data_dir=data_dir,\r\n   1508     data_files=data_files,\r\n   1509 )\r\n   1511 # Get dataset builder class from the processing script\r\n   1512 builder_cls = import_main_class(dataset_module.module_path)\r\n\r\nFile ~\/datasets\/src\/datasets\/load.py:1254, in dataset_module_factory(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1249             if isinstance(e1, FileNotFoundError):\r\n   1250                 raise FileNotFoundError(\r\n   1251                     f\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory. \"\r\n   1252                     f\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\r\n   1253                 ) from None\r\n-> 1254             raise e1 from None\r\n   1255 else:\r\n   1256     raise FileNotFoundError(\r\n   1257         f\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory.\"\r\n   1258     )\r\n\r\nFile ~\/datasets\/src\/datasets\/load.py:1227, in dataset_module_factory(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1225         raise e\r\n   1226 if filename in [sibling.rfilename for sibling in dataset_info.siblings]:\r\n-> 1227     return HubDatasetModuleFactoryWithScript(\r\n   1228         path,\r\n   1229         revision=revision,\r\n   1230         download_config=download_config,\r\n   1231         download_mode=download_mode,\r\n   1232         dynamic_modules_path=dynamic_modules_path,\r\n   1233     ).get_module()\r\n   1234 else:\r\n   1235     return HubDatasetModuleFactoryWithoutScript(\r\n   1236         path,\r\n   1237         revision=revision,\r\n   (...)\r\n   1241         download_mode=download_mode,\r\n   1242     ).get_module()\r\n\r\nFile ~\/datasets\/src\/datasets\/load.py:940, in HubDatasetModuleFactoryWithScript.get_module(self)\r\n    938 def get_module(self) -> DatasetModule:\r\n    939     # get script and other files\r\n--> 940     local_path = self.download_loading_script()\r\n    941     dataset_infos_path = self.download_dataset_infos_file()\r\n    942     imports = get_imports(local_path)\r\n\r\nFile ~\/datasets\/src\/datasets\/load.py:918, in HubDatasetModuleFactoryWithScript.download_loading_script(self)\r\n    917 def download_loading_script(self) -> str:\r\n--> 918     file_path = hf_hub_url(path=self.name, name=self.name.split(\"\/\")[1] + \".py\", revision=self.revision)\r\n    919     download_config = self.download_config.copy()\r\n    920     if download_config.download_desc is None:\r\n\r\nTypeError: hf_hub_url() got an unexpected keyword argument 'name'\r\n```\r\n\r\nNote that I am able to load the dataset from the `datasets` repo with the following lines of code:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset('.\/datasets\/tedlium', 'release1', cache_dir='\/home\/sanchitgandhi\/cache')\r\n```","What version of `datasets` do you have ?\r\nUpdating `datasets` should fix the error ;)\r\n"],"created_at":1652191967000,"updated_at":1652282379000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4309","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4309","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4309.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4309.patch","merged_at":null},"body":"Adds the TED-LIUM dataset https:\/\/www.tensorflow.org\/datasets\/catalog\/tedlium#tedliumrelease3 \r\n\r\nTODO:\r\n\r\n- [x] Port `tedium.py` from TF datasets using `convert_dataset.sh` script\r\n- [ ] Make `load_dataset` work\r\n- [ ] Run `datasets-cli` command to generate `dataset_infos.json`\r\n- [ ] Create dummy data for continuous testing\r\n- [ ] Dummy data tests\r\n- [ ] Real data tests\r\n- [ ] Create the metadata JSON\r\n- [ ] Close PR and add directly to the Hub under LIUM org","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4309\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4309\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4308","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4308\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4308\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4308\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4308","id":1231217783,"node_id":"PR_kwDODunzps43lHdP","number":4308,"title":"Remove unused multiprocessing args from test CLI","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1652191335000,"updated_at":1652273905000,"closed_at":1652273443000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4308","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4308","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4308.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4308.patch","merged_at":1652273442000},"body":"Multiprocessing is not used in the test CLI.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4308\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4308\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4307","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4307\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4307\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4307\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4307","id":1231175639,"node_id":"PR_kwDODunzps43k-Wo","number":4307,"title":"Add packaged builder configs to the documentation","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1652189659000,"updated_at":1652191430000,"closed_at":1652190954000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4307","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4307","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4307.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4307.patch","merged_at":1652190954000},"body":"Add the packaged builders configurations to the docs reference is useful to show the list of all parameters one can use when loading data in many formats: CSV, JSON, etc.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4307\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4307\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4306","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4306\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4306\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4306\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4306","id":1231137204,"node_id":"I_kwDODunzps5JYam0","number":4306,"title":"`load_dataset` does not work with certain filename.","user":{"login":"wusuowei60","id":57242693,"node_id":"MDQ6VXNlcjU3MjQyNjkz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/57242693?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/wusuowei60","html_url":"https:\/\/github.com\/wusuowei60","followers_url":"https:\/\/api.github.com\/users\/wusuowei60\/followers","following_url":"https:\/\/api.github.com\/users\/wusuowei60\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/wusuowei60\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/wusuowei60\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/wusuowei60\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/wusuowei60\/orgs","repos_url":"https:\/\/api.github.com\/users\/wusuowei60\/repos","events_url":"https:\/\/api.github.com\/users\/wusuowei60\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/wusuowei60\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Never mind. It is because of the caching of datasets..."],"created_at":1652188444000,"updated_at":1652209116000,"closed_at":1652209089000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nThis is a weird bug that took me some time to find out.\r\n\r\nI have a JSON dataset that I want to load with `load_dataset` like this:\r\n\r\n```\r\ndata_files = dict(train=\"train.json.zip\", val=\"val.json.zip\")\r\ndataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\r\n```\r\n\r\n## Expected results\r\nNo error.\r\n\r\n## Actual results\r\nThe val file is loaded as expected, but the train file throws JSON decoding error:\r\n\r\n```\r\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 <ipython-input-74-97947e92c100>:5 in <module>                                             \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 \/home\/tiankang\/software\/anaconda3\/lib\/python3.8\/site-packages\/datasets\/load.py:1687 in    \u2502\r\n\u2502 load_dataset                                                                              \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   1684 \u2502   try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES                       \u2502\r\n\u2502   1685 \u2502                                                                                  \u2502\r\n\u2502   1686 \u2502   # Download and prepare data                                                    \u2502\r\n\u2502 \u2771 1687 \u2502   builder_instance.download_and_prepare(                                         \u2502\r\n\u2502   1688 \u2502   \u2502   download_config=download_config,                                           \u2502\r\n\u2502   1689 \u2502   \u2502   download_mode=download_mode,                                               \u2502\r\n\u2502   1690 \u2502   \u2502   ignore_verifications=ignore_verifications,                                 \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 \/home\/tiankang\/software\/anaconda3\/lib\/python3.8\/site-packages\/datasets\/builder.py:605 in  \u2502\r\n\u2502 download_and_prepare                                                                      \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502    602 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   except ConnectionError:                                    \u2502\r\n\u2502    603 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   logger.warning(\"HF google storage unreachable. Downloa \u2502\r\n\u2502    604 \u2502   \u2502   \u2502   \u2502   \u2502   if not downloaded_from_gcs:                                    \u2502\r\n\u2502 \u2771  605 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   self._download_and_prepare(                                \u2502\r\n\u2502    606 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   dl_manager=dl_manager, verify_infos=verify_infos, **do \u2502\r\n\u2502    607 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   )                                                          \u2502\r\n\u2502    608 \u2502   \u2502   \u2502   \u2502   \u2502   # Sync info                                                    \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 \/home\/tiankang\/software\/anaconda3\/lib\/python3.8\/site-packages\/datasets\/builder.py:694 in  \u2502\r\n\u2502 _download_and_prepare                                                                     \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502    691 \u2502   \u2502   \u2502                                                                          \u2502\r\n\u2502    692 \u2502   \u2502   \u2502   try:                                                                   \u2502\r\n\u2502    693 \u2502   \u2502   \u2502   \u2502   # Prepare split will record examples associated to the split       \u2502\r\n\u2502 \u2771  694 \u2502   \u2502   \u2502   \u2502   self._prepare_split(split_generator, **prepare_split_kwargs)       \u2502\r\n\u2502    695 \u2502   \u2502   \u2502   except OSError as e:                                                   \u2502\r\n\u2502    696 \u2502   \u2502   \u2502   \u2502   raise OSError(                                                     \u2502\r\n\u2502    697 \u2502   \u2502   \u2502   \u2502   \u2502   \"Cannot find data file. \"                                      \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 \/home\/tiankang\/software\/anaconda3\/lib\/python3.8\/site-packages\/datasets\/builder.py:1151 in \u2502\r\n\u2502 _prepare_split                                                                            \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   1148 \u2502   \u2502                                                                              \u2502\r\n\u2502   1149 \u2502   \u2502   generator = self._generate_tables(**split_generator.gen_kwargs)            \u2502\r\n\u2502   1150 \u2502   \u2502   with ArrowWriter(features=self.info.features, path=fpath) as writer:       \u2502\r\n\u2502 \u2771 1151 \u2502   \u2502   \u2502   for key, table in logging.tqdm(                                        \u2502\r\n\u2502   1152 \u2502   \u2502   \u2502   \u2502   generator, unit=\" tables\", leave=False, disable=True  # not loggin \u2502\r\n\u2502   1153 \u2502   \u2502   \u2502   ):                                                                     \u2502\r\n\u2502   1154 \u2502   \u2502   \u2502   \u2502   writer.write_table(table)                                          \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 \/home\/tiankang\/software\/anaconda3\/lib\/python3.8\/site-packages\/tqdm\/notebook.py:257 in     \u2502\r\n\u2502 __iter__                                                                                  \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   254 \u2502                                                                                   \u2502\r\n\u2502   255 \u2502   def __iter__(self):                                                             \u2502\r\n\u2502   256 \u2502   \u2502   try:                                                                        \u2502\r\n\u2502 \u2771 257 \u2502   \u2502   \u2502   for obj in super(tqdm_notebook, self).__iter__():                       \u2502\r\n\u2502   258 \u2502   \u2502   \u2502   \u2502   # return super(tqdm...) will not catch exception                    \u2502\r\n\u2502   259 \u2502   \u2502   \u2502   \u2502   yield obj                                                           \u2502\r\n\u2502   260 \u2502   \u2502   # NB: except ... [ as ...] breaks IPython async KeyboardInterrupt           \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 \/home\/tiankang\/software\/anaconda3\/lib\/python3.8\/site-packages\/tqdm\/std.py:1183 in         \u2502\r\n\u2502 __iter__                                                                                  \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   1180 \u2502   \u2502   # If the bar is disabled, then just walk the iterable                      \u2502\r\n\u2502   1181 \u2502   \u2502   # (note: keep this check outside the loop for performance)                 \u2502\r\n\u2502   1182 \u2502   \u2502   if self.disable:                                                           \u2502\r\n\u2502 \u2771 1183 \u2502   \u2502   \u2502   for obj in iterable:                                                   \u2502\r\n\u2502   1184 \u2502   \u2502   \u2502   \u2502   yield obj                                                          \u2502\r\n\u2502   1185 \u2502   \u2502   \u2502   return                                                                 \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 \/home\/tiankang\/software\/anaconda3\/lib\/python3.8\/site-packages\/datasets\/packaged_modules\/j \u2502\r\n\u2502 son\/json.py:90 in _generate_tables                                                        \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502    87 \u2502   \u2502   \u2502   # If the file is one json object and if we need to look at the list of  \u2502\r\n\u2502    88 \u2502   \u2502   \u2502   if self.config.field is not None:                                       \u2502\r\n\u2502    89 \u2502   \u2502   \u2502   \u2502   with open(file, encoding=\"utf-8\") as f:                             \u2502\r\n\u2502 \u2771  90 \u2502   \u2502   \u2502   \u2502   \u2502   dataset = json.load(f)                                          \u2502\r\n\u2502    91 \u2502   \u2502   \u2502   \u2502                                                                       \u2502\r\n\u2502    92 \u2502   \u2502   \u2502   \u2502   # We keep only the field we are interested in                       \u2502\r\n\u2502    93 \u2502   \u2502   \u2502   \u2502   dataset = dataset[self.config.field]                                \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 \/home\/tiankang\/software\/anaconda3\/lib\/python3.8\/json\/__init__.py:293 in load              \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   290 \u2502   To use a custom ``JSONDecoder`` subclass, specify it with the ``cls``           \u2502\r\n\u2502   291 \u2502   kwarg; otherwise ``JSONDecoder`` is used.                                       \u2502\r\n\u2502   292 \u2502   \"\"\"                                                                             \u2502\r\n\u2502 \u2771 293 \u2502   return loads(fp.read(),                                                         \u2502\r\n\u2502   294 \u2502   \u2502   cls=cls, object_hook=object_hook,                                           \u2502\r\n\u2502   295 \u2502   \u2502   parse_float=parse_float, parse_int=parse_int,                               \u2502\r\n\u2502   296 \u2502   \u2502   parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)   \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 \/home\/tiankang\/software\/anaconda3\/lib\/python3.8\/json\/__init__.py:357 in loads             \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   354 \u2502   if (cls is None and object_hook is None and                                     \u2502\r\n\u2502   355 \u2502   \u2502   \u2502   parse_int is None and parse_float is None and                           \u2502\r\n\u2502   356 \u2502   \u2502   \u2502   parse_constant is None and object_pairs_hook is None and not kw):       \u2502\r\n\u2502 \u2771 357 \u2502   \u2502   return _default_decoder.decode(s)                                           \u2502\r\n\u2502   358 \u2502   if cls is None:                                                                 \u2502\r\n\u2502   359 \u2502   \u2502   cls = JSONDecoder                                                           \u2502\r\n\u2502   360 \u2502   if object_hook is not None:                                                     \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 \/home\/tiankang\/software\/anaconda3\/lib\/python3.8\/json\/decoder.py:337 in decode             \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   334 \u2502   \u2502   containing a JSON document).                                                \u2502\r\n\u2502   335 \u2502   \u2502                                                                               \u2502\r\n\u2502   336 \u2502   \u2502   \"\"\"                                                                         \u2502\r\n\u2502 \u2771 337 \u2502   \u2502   obj, end = self.raw_decode(s, idx=_w(s, 0).end())                           \u2502\r\n\u2502   338 \u2502   \u2502   end = _w(s, end).end()                                                      \u2502\r\n\u2502   339 \u2502   \u2502   if end != len(s):                                                           \u2502\r\n\u2502   340 \u2502   \u2502   \u2502   raise JSONDecodeError(\"Extra data\", s, end)                             \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 \/home\/tiankang\/software\/anaconda3\/lib\/python3.8\/json\/decoder.py:353 in raw_decode         \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   350 \u2502   \u2502                                                                               \u2502\r\n\u2502   351 \u2502   \u2502   \"\"\"                                                                         \u2502\r\n\u2502   352 \u2502   \u2502   try:                                                                        \u2502\r\n\u2502 \u2771 353 \u2502   \u2502   \u2502   obj, end = self.scan_once(s, idx)                                       \u2502\r\n\u2502   354 \u2502   \u2502   except StopIteration as err:                                                \u2502\r\n\u2502   355 \u2502   \u2502   \u2502   raise JSONDecodeError(\"Expecting value\", s, err.value) from None        \u2502\r\n\u2502   356 \u2502   \u2502   return obj, end                                                             \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nJSONDecodeError: Unterminated string starting at: line 85 column 20 (char 60051)\r\n```\r\n\r\nHowever, when I rename the `train.json.zip` to other names (like `training.json.zip`, or even to `train.json`), everything works fine; when I unzip the file to `train.json`, it works as well.\r\n\r\n## Environment info\r\n```\r\n- `datasets` version: 2.1.0\r\n- Platform: Linux-4.4.0-131-generic-x86_64-with-glibc2.10\r\n- Python version: 3.8.5\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.2\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4306\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4306\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4305","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4305\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4305\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4305\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4305","id":1231099934,"node_id":"PR_kwDODunzps43kt4P","number":4305,"title":"Fixes FrugalScore","user":{"login":"moussaKam","id":28675016,"node_id":"MDQ6VXNlcjI4Njc1MDE2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/28675016?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/moussaKam","html_url":"https:\/\/github.com\/moussaKam","followers_url":"https:\/\/api.github.com\/users\/moussaKam\/followers","following_url":"https:\/\/api.github.com\/users\/moussaKam\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/moussaKam\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/moussaKam\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/moussaKam\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/moussaKam\/orgs","repos_url":"https:\/\/api.github.com\/users\/moussaKam\/repos","events_url":"https:\/\/api.github.com\/users\/moussaKam\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/moussaKam\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4305). All of your documentation changes will be reflected on that endpoint.","> predictions and references are swapped. Basically Frugalscore is commutative, however some tiny differences can occur if we swap the references and the predictions. I decided to swap them just to obtain the exact results as reported in the paper.\r\n\r\nWhat is the order of magnitude of the difference ? Do you know what causes this ?\r\n\r\n> I switched to dynamic padding that was was used in the training, forcing the padding to max_length introduces errors for some reason that I ignore.\r\n\r\nWhat error ?"],"created_at":1652186646000,"updated_at":1652263399000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4305","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4305","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4305.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4305.patch","merged_at":null},"body":"There are two minor modifications in this PR:\r\n1) `predictions` and `references` are swapped. Basically Frugalscore is commutative, however some tiny differences can occur if we swap the references and the predictions. I decided to swap them just to obtain the exact results as reported in the paper.\r\n2) I switched to dynamic padding that was was used in the training, forcing the padding to `max_length`  introduces errors for some reason that I ignore.\r\n\r\n@lhoestq ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4305\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4305\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4304","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4304\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4304\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4304\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4304","id":1231047051,"node_id":"I_kwDODunzps5JYEmL","number":4304,"title":"Language code search does direct matches","user":{"login":"leondz","id":121934,"node_id":"MDQ6VXNlcjEyMTkzNA==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/121934?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/leondz","html_url":"https:\/\/github.com\/leondz","followers_url":"https:\/\/api.github.com\/users\/leondz\/followers","following_url":"https:\/\/api.github.com\/users\/leondz\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/leondz\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/leondz\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/leondz\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/leondz\/orgs","repos_url":"https:\/\/api.github.com\/users\/leondz\/repos","events_url":"https:\/\/api.github.com\/users\/leondz\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/leondz\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Thanks for reporting ! I forwarded the issue to the front-end team :)\r\n\r\nWill keep you posted !\r\n\r\nI also changed the tagging app to suggest two letters code for now."],"created_at":1652183956000,"updated_at":1652186322000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\n\r\nHi. Searching for bcp47 tags that are just the language prefix (e.g. `sq` or `da`) excludes datasets that have added extra information in their language metadata (e.g. `sq-AL` or `da-bornholm`). The example codes given in the [tagging app](https:\/\/huggingface.co\/spaces\/huggingface\/datasets-tagging) encourages addition of the additional codes (\"_expected format is BCP47 tags separated for ';' e.g. 'en-US;fr-FR'_\") but this would lead to those datasets being hidden in datasets search.\r\n\r\n## Steps to reproduce the bug\r\n1. Add a dataset using a variant tag (e.g. [`sq-AL`](https:\/\/huggingface.co\/datasets?languages=languages:sq-AL))\r\n2. Look for datasets using the full code \r\n3. Note that they're missing when just the language is searched for (e.g. [`sq`](https:\/\/huggingface.co\/datasets?languages=languages:sq))\r\n\r\nSome datasets are already affected by this - e.g. `AmazonScience\/massive` is listed under `sq-AL` but not `sq`.\r\n\r\nOne workaround is for dataset creators to add an additional root language tag to dataset YAML metadata, but it's unclear how to communicate this. It might be possible to index the search on `languagecode.split('-')[0]` but I wanted to float this issue before trying to write any code :)\r\n\r\n## Expected results\r\nDatasets using longer bcp47 tags also appear under searches for just the language code; e.g. Quebecois datasets (`fr-CA`) would come up when looking for French datasets with no region specification (`fr`), or US English (`en-US`) datasets would come up when searching for English datasets (`en`).\r\n\r\n## Actual results\r\nThe language codes seem to be directly string matched, excluding datasets with specific language tags from non-specific searches.\r\n\r\n## Environment info\r\n(web app)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4304\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4304\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4303","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4303\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4303\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4303\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4303","id":1230867728,"node_id":"PR_kwDODunzps43j8cH","number":4303,"title":"Fix: Add missing comma","user":{"login":"mrm8488","id":3653789,"node_id":"MDQ6VXNlcjM2NTM3ODk=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3653789?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mrm8488","html_url":"https:\/\/github.com\/mrm8488","followers_url":"https:\/\/api.github.com\/users\/mrm8488\/followers","following_url":"https:\/\/api.github.com\/users\/mrm8488\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mrm8488\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mrm8488\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mrm8488\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mrm8488\/orgs","repos_url":"https:\/\/api.github.com\/users\/mrm8488\/repos","events_url":"https:\/\/api.github.com\/users\/mrm8488\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mrm8488\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The CI failure is unrelated to this PR and fixed on master, merging :)"],"created_at":1652174498000,"updated_at":1652259015000,"closed_at":1652259014000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4303","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4303","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4303.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4303.patch","merged_at":1652259014000},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4303\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4303\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4302","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4302\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4302\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4302\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4302","id":1230651117,"node_id":"PR_kwDODunzps43jPE5","number":4302,"title":"Remove hacking license tags when mirroring datasets on the Hub","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4302). All of your documentation changes will be reflected on that endpoint.","The Hub doesn't allow these characters in the YAML tags, and git push fails if you want to push a dataset card containing these characters.","Ok, let me rename the bad config names :) I think I can also keep backward compatibility with a warning","Almost done with it btw, will submit a PR that shows all the configuration name changes (from a bit more than 20 datasets)"],"created_at":1652161966000,"updated_at":1652362157000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4302","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4302","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4302.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4302.patch","merged_at":null},"body":"Currently, when mirroring datasets on the Hub, the license tags are hacked: removed of characters \".\" and \"$\". On the contrary, this hacking is not applied to community datasets on the Hub. This generates multiple variants of the same tag on the Hub. \r\n\r\nI guess this hacking is no longer necessary:\r\n- it is not applied to community datasets\r\n- all canonical datasets are validated by maintainers before being merged: CI + maintainers make sure license tags are the right ones\r\n\r\nFix #4298.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4302\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4302\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4301","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4301\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4301\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4301\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4301","id":1230401256,"node_id":"PR_kwDODunzps43idlE","number":4301,"title":"Add ImageNet-Sketch dataset","user":{"login":"nateraw","id":32437151,"node_id":"MDQ6VXNlcjMyNDM3MTUx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/32437151?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nateraw","html_url":"https:\/\/github.com\/nateraw","followers_url":"https:\/\/api.github.com\/users\/nateraw\/followers","following_url":"https:\/\/api.github.com\/users\/nateraw\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nateraw\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nateraw\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nateraw\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nateraw\/orgs","repos_url":"https:\/\/api.github.com\/users\/nateraw\/repos","events_url":"https:\/\/api.github.com\/users\/nateraw\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nateraw\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4301). All of your documentation changes will be reflected on that endpoint."],"created_at":1652139525000,"updated_at":1652376654000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4301","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4301","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4301.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4301.patch","merged_at":null},"body":"This PR adds the ImageNet-Sketch dataset and resolves #3953 .","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4301\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4301\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4300","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4300\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4300\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4300\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4300","id":1230272761,"node_id":"PR_kwDODunzps43iA86","number":4300,"title":"Add API code examples for loading methods","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4300). All of your documentation changes will be reflected on that endpoint."],"created_at":1652131826000,"updated_at":1652376548000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4300","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4300","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4300.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4300.patch","merged_at":null},"body":"This PR adds API code examples for loading methods, let me know if I've missed any important parameters we should showcase :)\r\n\r\nI was a bit confused about `inspect_dataset` and `inspect_metric`. The `path` parameter says it will accept a dataset identifier from the Hub. But when I try the identifier `rotten_tomatoes`, it gives me:\r\n\r\n```py\r\nfrom datasets import inspect_dataset\r\ninspect_dataset('rotten_tomatoes', local_path='\/content\/rotten_tomatoes')\r\n\r\nFileNotFoundError: Couldn't find a dataset script at \/content\/rotten_tomatoes\/rotten_tomatoes.py or any data file in the same directory.\r\n```\r\n\r\nDoes the user need to have an existing copy of `rotten_tomatoes.py` on their local drive (in which case, it seems like the same option as the first option in `path`)?","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4300\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4300\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4299","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4299\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4299\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4299\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4299","id":1230236782,"node_id":"PR_kwDODunzps43h5RP","number":4299,"title":"Remove manual download from imagenet-1k","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4299). All of your documentation changes will be reflected on that endpoint."],"created_at":1652129358000,"updated_at":1652130131000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4299","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4299","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4299.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4299.patch","merged_at":null},"body":"Remove the manual download code from `imagenet-1k` to make it a regular dataset.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4299\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4299\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4298","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4298\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4298\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4298\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4298","id":1229748006,"node_id":"I_kwDODunzps5JTHcm","number":4298,"title":"Normalise license names","user":{"login":"leondz","id":121934,"node_id":"MDQ6VXNlcjEyMTkzNA==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/121934?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/leondz","html_url":"https:\/\/github.com\/leondz","followers_url":"https:\/\/api.github.com\/users\/leondz\/followers","following_url":"https:\/\/api.github.com\/users\/leondz\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/leondz\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/leondz\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/leondz\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/leondz\/orgs","repos_url":"https:\/\/api.github.com\/users\/leondz\/repos","events_url":"https:\/\/api.github.com\/users\/leondz\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/leondz\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["we'll add the same server-side metadata validation system as for hf.co\/models soon-ish\r\n\r\n(you can check on hf.co\/models that licenses are \"clean\")"],"created_at":1652104292000,"updated_at":1652204645000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"**Is your feature request related to a problem? Please describe.**\r\nWhen browsing datasets, the Licenses tag cloud (bottom left of e.g. https:\/\/huggingface.co\/datasets) has multiple variants of the same license. This means the options exclude datasets arbitrarily, giving users artificially low recall. The cause of the dupes is probably due to a bit of variation in metadata.\r\n\r\n**Describe the solution you'd like**\r\nI'd like the licenses in metadata to follow the same standard as much as possible, to remove this problem. I'd like to go ahead and normalise the dataset metadata to follow the format & values given in [src\/datasets\/utils\/resources\/licenses.json](https:\/\/github.com\/huggingface\/datasets\/blob\/master\/src\/datasets\/utils\/resources\/licenses.json) .\r\n\r\n**Describe alternatives you've considered**\r\nNone\r\n\r\n**Additional context**\r\nNone\r\n\r\n**Priority** \r\nLow\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4298\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4298\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4297","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4297\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4297\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4297\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4297","id":1229735498,"node_id":"I_kwDODunzps5JTEZK","number":4297,"title":"Datasets YAML tagging space is down","user":{"login":"leondz","id":121934,"node_id":"MDQ6VXNlcjEyMTkzNA==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/121934?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/leondz","html_url":"https:\/\/github.com\/leondz","followers_url":"https:\/\/api.github.com\/users\/leondz\/followers","following_url":"https:\/\/api.github.com\/users\/leondz\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/leondz\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/leondz\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/leondz\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/leondz\/orgs","repos_url":"https:\/\/api.github.com\/users\/leondz\/repos","events_url":"https:\/\/api.github.com\/users\/leondz\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/leondz\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"assignees":[{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["@lhoestq @albertvillanova `update-task-list` branch does not exist anymore, should point to `main` now i guess","Thanks for reporting, fixing it now","It's up again :)"],"created_at":1652103905000,"updated_at":1652107465000,"closed_at":1652107465000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nThe neat hf spaces app for generating YAML tags for dataset `README.md`s is down\r\n\r\n## Steps to reproduce the bug\r\n1. Visit https:\/\/huggingface.co\/spaces\/huggingface\/datasets-tagging\r\n\r\n## Expected results\r\nThere'll be a HF spaces web app for generating dataset metadata YAML\r\n\r\n## Actual results\r\nThere's an error message; here's the step where it breaks:\r\n\r\n```\r\nStep 18\/29 : RUN pip install -r requirements.txt\r\n ---> Running in e88bfe7e7e0c\r\nDefaulting to user installation because normal site-packages is not writeable\r\nCollecting git+https:\/\/github.com\/huggingface\/datasets.git@update-task-list (from -r requirements.txt (line 4))\r\n  Cloning https:\/\/github.com\/huggingface\/datasets.git (to revision update-task-list) to \/tmp\/pip-req-build-bm8t0r0k\r\n  Running command git clone --filter=blob:none --quiet https:\/\/github.com\/huggingface\/datasets.git \/tmp\/pip-req-build-bm8t0r0k\r\n  WARNING: Did not find branch or tag 'update-task-list', assuming revision or ref.\r\n  Running command git checkout -q update-task-list\r\n  error: pathspec 'update-task-list' did not match any file(s) known to git\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 git checkout -q update-task-list did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> See above for output.\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: subprocess-exited-with-error\r\n\r\n\u00d7 git checkout -q update-task-list did not run successfully.\r\n\u2502 exit code: 1\r\n\u2570\u2500> See above for output.\r\n```\r\n\r\n## Environment info\r\n\r\n- Platform: Linux \/ Brave\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4297\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4297\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4296","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4296\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4296\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4296\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4296","id":1229554645,"node_id":"PR_kwDODunzps43foZ-","number":4296,"title":"Fix URL query parameters in compression hop path when streaming","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4296). All of your documentation changes will be reflected on that endpoint."],"created_at":1652095102000,"updated_at":1652096518000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4296","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4296","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4296.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4296.patch","merged_at":null},"body":"Fix #3488.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4296\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4296\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4295","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4295\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4295\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4295\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4295","id":1229527283,"node_id":"PR_kwDODunzps43fieR","number":4295,"title":"Fix missing lz4 dependency for tests","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1652093600000,"updated_at":1652095282000,"closed_at":1652094824000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4295","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4295","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4295.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4295.patch","merged_at":1652094824000},"body":"Currently, `lz4` is not defined as a dependency for tests. Therefore, all tests marked with `@require_lz4` are skipped.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4295\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4295\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4294","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4294\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4294\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4294\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4294","id":1229455582,"node_id":"PR_kwDODunzps43fTXA","number":4294,"title":"Fix CLI run_beam save_infos","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1652089663000,"updated_at":1652166244000,"closed_at":1652165770000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4294","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4294","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4294.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4294.patch","merged_at":1652165770000},"body":"Currently, it raises TypeError:\r\n```\r\nTypeError: _download_and_prepare() got an unexpected keyword argument 'save_infos'\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4294\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4294\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4293","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4293\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4293\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4293\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4293","id":1228815477,"node_id":"PR_kwDODunzps43dRt9","number":4293,"title":"Fix wrong map parameter name in cache docs","user":{"login":"h4iku","id":3812788,"node_id":"MDQ6VXNlcjM4MTI3ODg=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3812788?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/h4iku","html_url":"https:\/\/github.com\/h4iku","followers_url":"https:\/\/api.github.com\/users\/h4iku\/followers","following_url":"https:\/\/api.github.com\/users\/h4iku\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/h4iku\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/h4iku\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/h4iku\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/h4iku\/orgs","repos_url":"https:\/\/api.github.com\/users\/h4iku\/repos","events_url":"https:\/\/api.github.com\/users\/h4iku\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/h4iku\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4293). All of your documentation changes will be reflected on that endpoint."],"created_at":1651994866000,"updated_at":1651995554000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4293","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4293","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4293.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4293.patch","merged_at":null},"body":"The `load_from_cache` parameter of `map` should be `load_from_cache_file`.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4293\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4293\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4292","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4292\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4292\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4292\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4292","id":1228216788,"node_id":"PR_kwDODunzps43bhrp","number":4292,"title":"Add API code examples for remaining main classes","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4292). All of your documentation changes will be reflected on that endpoint."],"created_at":1651860931000,"updated_at":1652306365000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4292","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4292","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4292.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4292.patch","merged_at":null},"body":"This PR adds API code examples for the remaining functions in the Main classes. I wasn't too familiar with some of the functions (`decode_batch`, `decode_column`, `decode_example`, etc.) so please feel free to add an example of usage and I can fill in the rest :)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4292\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4292\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4291","id":1227777500,"node_id":"I_kwDODunzps5JLmXc","number":4291,"title":"Dataset Viewer issue for strombergnlp\/ipm_nel : preview is empty, no error message","user":{"login":"leondz","id":121934,"node_id":"MDQ6VXNlcjEyMTkzNA==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/121934?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/leondz","html_url":"https:\/\/github.com\/leondz","followers_url":"https:\/\/api.github.com\/users\/leondz\/followers","following_url":"https:\/\/api.github.com\/users\/leondz\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/leondz\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/leondz\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/leondz\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/leondz\/orgs","repos_url":"https:\/\/api.github.com\/users\/leondz\/repos","events_url":"https:\/\/api.github.com\/users\/leondz\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/leondz\/received_events","type":"User","site_admin":false},"labels":[{"id":3470211881,"node_id":"LA_kwDODunzps7O1zsp","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer","name":"dataset-viewer","color":"E5583E","default":false,"description":"Related to the dataset viewer on huggingface.co"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi @leondz, thanks for reporting.\r\n\r\nIndeed, the dataset viewer relies on the dataset being streamable (passing `streaming=True` to `load_dataset`). Whereas most of the datastes are streamable out of the box (thanks to our implementation of streaming), there are still some exceptions.\r\n\r\nIn particular, in your case, that is due to the data file being TAR. This format is not streamable out of the box (it does not allow random access to the archived files), but we use a trick to allow streaming: using `dl_manager.iter_archive`.\r\n\r\nLet me know if you need some help: I could push a commit to your repo with the fix.","Ah, right! The preview is working now, but this explanation is good to know, thank you. I'll prefer formats with random file access supported in datasets.utils.extract in future, and try out this fix for the tarfiles :)"],"created_at":1651838607000,"updated_at":1652084758000,"closed_at":1652084758000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"### Link\n\nhttps:\/\/huggingface.co\/datasets\/strombergnlp\/ipm_nel\/viewer\/ipm_nel\/train\n\n### Description\n\nThe viewer is blank. I tried my best to emulate a dataset with a working viewer, but this one just doesn't seem to want to come up. What did I miss?\n\n### Owner\n\nYes","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4290","id":1227592826,"node_id":"PR_kwDODunzps43Zr08","number":4290,"title":"Update README.md","user":{"login":"monk1337","id":17107749,"node_id":"MDQ6VXNlcjE3MTA3NzQ5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17107749?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/monk1337","html_url":"https:\/\/github.com\/monk1337","followers_url":"https:\/\/api.github.com\/users\/monk1337\/followers","following_url":"https:\/\/api.github.com\/users\/monk1337\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/monk1337\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/monk1337\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/monk1337\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/monk1337\/orgs","repos_url":"https:\/\/api.github.com\/users\/monk1337\/repos","events_url":"https:\/\/api.github.com\/users\/monk1337\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/monk1337\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4290). All of your documentation changes will be reflected on that endpoint."],"created_at":1651827171000,"updated_at":1651827848000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4290","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4290","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4290.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4290.patch","merged_at":null},"body":"Updating readme in medmcqa dataset.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4288","id":1226821732,"node_id":"PR_kwDODunzps43XLKi","number":4288,"title":"Add missing `faiss` import to fix https:\/\/github.com\/huggingface\/datasets\/issues\/4287","user":{"login":"alvarobartt","id":36760800,"node_id":"MDQ6VXNlcjM2NzYwODAw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36760800?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alvarobartt","html_url":"https:\/\/github.com\/alvarobartt","followers_url":"https:\/\/api.github.com\/users\/alvarobartt\/followers","following_url":"https:\/\/api.github.com\/users\/alvarobartt\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alvarobartt\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alvarobartt\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alvarobartt\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alvarobartt\/orgs","repos_url":"https:\/\/api.github.com\/users\/alvarobartt\/repos","events_url":"https:\/\/api.github.com\/users\/alvarobartt\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alvarobartt\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1651764109000,"updated_at":1652187306000,"closed_at":1652184588000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4288","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4288","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4288.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4288.patch","merged_at":1652184588000},"body":"This PR fixes the issue recently mentioned in https:\/\/github.com\/huggingface\/datasets\/issues\/4287 \ud83e\udd17 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4287","id":1226806652,"node_id":"I_kwDODunzps5JH5V8","number":4287,"title":"\"NameError: name 'faiss' is not defined\" on `.add_faiss_index` when `device` is not None","user":{"login":"alvarobartt","id":36760800,"node_id":"MDQ6VXNlcjM2NzYwODAw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36760800?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alvarobartt","html_url":"https:\/\/github.com\/alvarobartt","followers_url":"https:\/\/api.github.com\/users\/alvarobartt\/followers","following_url":"https:\/\/api.github.com\/users\/alvarobartt\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alvarobartt\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alvarobartt\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alvarobartt\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alvarobartt\/orgs","repos_url":"https:\/\/api.github.com\/users\/alvarobartt\/repos","events_url":"https:\/\/api.github.com\/users\/alvarobartt\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alvarobartt\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["So I managed to solve this by adding a missing `import faiss` in the `@staticmethod` defined in https:\/\/github.com\/huggingface\/datasets\/blob\/f51b6994db27ea69261ef919fb7775928f9ec10b\/src\/datasets\/search.py#L305, triggered from https:\/\/github.com\/huggingface\/datasets\/blob\/f51b6994db27ea69261ef919fb7775928f9ec10b\/src\/datasets\/search.py#L249 when trying to `ds_with_embeddings.add_faiss_index(column='embeddings', device=0)` with the code above.\r\n\r\nAs it seems that the `@staticmethod` doesn't recognize the `import faiss` defined in https:\/\/github.com\/huggingface\/datasets\/blob\/f51b6994db27ea69261ef919fb7775928f9ec10b\/src\/datasets\/search.py#L261, so whenever the value of `device` is not None in https:\/\/github.com\/huggingface\/datasets\/blob\/71f76e0bdeaddadedc4f9c8d15cfff5a36d62f66\/src\/datasets\/search.py#L438, that exception is triggered.\r\n\r\nSo on, adding `import faiss` inside https:\/\/github.com\/huggingface\/datasets\/blob\/71f76e0bdeaddadedc4f9c8d15cfff5a36d62f66\/src\/datasets\/search.py#L305 right after the check of `device`'s value, solves the issue and lets you calculate the indices in GPU.\r\n\r\nI'll add the code in a PR linked to this issue in case you want to merge it!","Adding here the complete error traceback!\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/home\/alvarobartt\/lol.py\", line 12, in <module>\r\n    ds_with_embeddings.add_faiss_index(column='embeddings', device=0) # default `device=None`\r\n  File \"\/home\/alvarobartt\/.local\/lib\/python3.9\/site-packages\/datasets\/arrow_dataset.py\", line 3656, in add_faiss_index\r\n    super().add_faiss_index(\r\n  File \"\/home\/alvarobartt\/.local\/lib\/python3.9\/site-packages\/datasets\/search.py\", line 478, in add_faiss_index\r\n    faiss_index.add_vectors(self, column=column, train_size=train_size, faiss_verbose=True)\r\n  File \"\/home\/alvarobartt\/.local\/lib\/python3.9\/site-packages\/datasets\/search.py\", line 281, in add_vectors\r\n    self.faiss_index = self._faiss_index_to_device(index, self.device)\r\n  File \"\/home\/alvarobartt\/.local\/lib\/python3.9\/site-packages\/datasets\/search.py\", line 327, in _faiss_index_to_device\r\n    faiss_res = faiss.StandardGpuResources()\r\nNameError: name 'faiss' is not defined\r\n```","Closed as https:\/\/github.com\/huggingface\/datasets\/pull\/4288 already merged! :hugs:"],"created_at":1651763385000,"updated_at":1652190799000,"closed_at":1652190799000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\n\r\nWhen using `datasets` to calculate the FAISS indices of a dataset, the exception `NameError: name 'faiss' is not defined` is triggered when trying to calculate those on a device (GPU), so `.add_faiss_index(..., device=0)` fails with that exception.\r\n\r\nAll that assuming that `datasets` is properly installed and `faiss-gpu` too, as well as all the CUDA drivers required.\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\n# Sample code to reproduce the bug\r\nfrom transformers import DPRContextEncoder, DPRContextEncoderTokenizer\r\nimport torch\r\ntorch.set_grad_enabled(False)\r\nctx_encoder = DPRContextEncoder.from_pretrained(\"facebook\/dpr-ctx_encoder-single-nq-base\")\r\nctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook\/dpr-ctx_encoder-single-nq-base\")\r\n\r\nfrom datasets import load_dataset\r\nds = load_dataset('crime_and_punish', split='train[:100]')\r\nds_with_embeddings = ds.map(lambda example: {'embeddings': ctx_encoder(**ctx_tokenizer(example[\"line\"], return_tensors=\"pt\"))[0][0].numpy()})\r\n\r\nds_with_embeddings.add_faiss_index(column='embeddings', device=0) # default `device=None`\r\n```\r\n\r\n## Expected results\r\n\r\nA new column named `embeddings` in the dataset that we're adding the index to.\r\n\r\n## Actual results\r\n\r\nAn exception is triggered with the following message `NameError: name 'faiss' is not defined`.\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 2.1.0\r\n- Platform: Linux-5.13.0-1022-azure-x86_64-with-glibc2.31\r\n- Python version: 3.9.12\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.2\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4286","id":1226758621,"node_id":"PR_kwDODunzps43W-DI","number":4286,"title":"Add Lahnda language tag","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651761260000,"updated_at":1652184604000,"closed_at":1652184158000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4286","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4286","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4286.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4286.patch","merged_at":1652184157000},"body":"This language is present in [Wikimedia's WIT](https:\/\/huggingface.co\/datasets\/wikimedia\/wit_base) dataset.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4285","id":1226374831,"node_id":"PR_kwDODunzps43VtEa","number":4285,"title":"Update LexGLUE README.md","user":{"login":"iliaschalkidis","id":1626984,"node_id":"MDQ6VXNlcjE2MjY5ODQ=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1626984?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/iliaschalkidis","html_url":"https:\/\/github.com\/iliaschalkidis","followers_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/followers","following_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/orgs","repos_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/repos","events_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651739810000,"updated_at":1651757944000,"closed_at":1651757615000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4285","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4285","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4285.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4285.patch","merged_at":1651757615000},"body":"Update the leaderboard based on the latest results presented in the ACL 2022 version of the article.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4284","id":1226200727,"node_id":"I_kwDODunzps5JFlaX","number":4284,"title":"Issues in processing very large datasets","user":{"login":"sajastu","id":10419055,"node_id":"MDQ6VXNlcjEwNDE5MDU1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10419055?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sajastu","html_url":"https:\/\/github.com\/sajastu","followers_url":"https:\/\/api.github.com\/users\/sajastu\/followers","following_url":"https:\/\/api.github.com\/users\/sajastu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sajastu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sajastu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sajastu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sajastu\/orgs","repos_url":"https:\/\/api.github.com\/users\/sajastu\/repos","events_url":"https:\/\/api.github.com\/users\/sajastu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sajastu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi ! `datasets` doesn't load the dataset in memory. Instead it uses memory mapping to load your dataset from your disk (it is stored as arrow files). Do you know at what point you have RAM issues exactly ?\r\n\r\nHow big are your graph_data_train dictionaries btw ?"],"created_at":1651726869000,"updated_at":1652184923000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nI'm trying to add a feature called \"subgraph\" to CNN\/DM dataset (modifications on run_summarization.py of Huggingface Transformers script) --- I'm not quite sure if I'm doing it the right way, though--- but the main problem appears when the training starts where the error ` [OSError: [Errno 12] Cannot allocate memory]`  appears. I suppose this problem roots in RAM issues and how the dataset is loaded during training, but I have no clue of what I can do to fix it.  Observing the dataset's cache directory, I see that it takes ~600GB of memory and that's why I believe special care is needed when loading it into the memory. \r\n\r\n\r\nHere are my modifications to `run_summarization.py` code. \r\n\r\n\r\n```\r\n# loading pre-computed dictionary where keys are 'id' of article and values are corresponding subgraph\r\ngraph_data_train = get_graph_data('train') \r\ngraph_data_validation = get_graph_data('val')\r\n...\r\n...\r\n\r\n\r\nwith training_args.main_process_first(desc=\"train dataset map pre-processing\"):\r\n    train_dataset = train_dataset.map(\r\n        preprocess_function_train,\r\n        batched=True,\r\n        num_proc=data_args.preprocessing_num_workers,\r\n        remove_columns=column_names,\r\n        load_from_cache_file=not data_args.overwrite_cache,\r\n        desc=\"Running tokenizer on train dataset\",\r\n    )\r\n\r\n```\r\n\r\n\r\nAnd here is the modified preprocessed function:\r\n\r\n```\r\ndef preprocess_function_train(examples):\r\n        inputs, targets, sub_graphs, ids = [], [], [], []\r\n        for i in range(len(examples[text_column])):\r\n            if examples[text_column][i] is not None and examples[summary_column][i] is not None:\r\n                # if examples['doc_id'][i] in graph_data.keys():\r\n                inputs.append(examples[text_column][i])\r\n                targets.append(examples[summary_column][i])\r\n                sub_graphs.append(graph_data_train[examples['id'][i]])\r\n                ids.append(examples['id'][i])\r\n\r\n        inputs = [prefix + inp for inp in inputs]\r\n        model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True,\r\n                                 sub_graphs=sub_graphs, ids=ids)\r\n\r\n            # Setup the tokenizer for targets\r\n        with tokenizer.as_target_tokenizer():\r\n            labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\r\n\r\n        # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\r\n        # padding in the loss.\r\n        if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\r\n            labels[\"input_ids\"] = [\r\n                [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\r\n            ]\r\n\r\n        model_inputs[\"labels\"] = labels[\"input_ids\"]\r\n        return model_inputs\r\n```\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:  2.1.0\r\n- Platform: Linux Ubuntu\r\n- Python version: 3.6\r\n- PyArrow version: 6.0.1\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4283","id":1225686988,"node_id":"PR_kwDODunzps43Tnxo","number":4283,"title":"Fix filesystem docstring","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651686162000,"updated_at":1651854722000,"closed_at":1651818137000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4283","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4283","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4283.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4283.patch","merged_at":1651818137000},"body":"This PR untangles the `S3FileSystem` docstring so the [parameters](https:\/\/huggingface.co\/docs\/datasets\/master\/en\/package_reference\/main_classes#parameters) are properly displayed.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4282","id":1225616545,"node_id":"PR_kwDODunzps43TZYL","number":4282,"title":"Don't do unnecessary list type casting to avoid replacing None values by empty lists","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Quick question about the message in the warning. You say \"will be fixed in a future major version\" but don't you mean \"will raise an error in a future major version\"?","Right ! Good catch, thanks, I updated the message to say \"will raise an error in a future major version\""],"created_at":1651682221000,"updated_at":1651833838000,"closed_at":1651833420000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4282","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4282","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4282.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4282.patch","merged_at":1651833420000},"body":"In certain cases, `None` values are replaced by empty lists when casting feature types.\r\n\r\nIt happens every time you cast an array of nested lists like [None, [0, 1, 2, 3]] to a different type (to change the integer precision for example). In this case you'd get [[], [0, 1, 2, 3]] for example. This issue comes from PyArrow, see the discussion in https:\/\/github.com\/huggingface\/datasets\/issues\/3676\r\n\r\nThis issue also happens when no type casting is needed, because casting is supposed to be a no-op in this case. But as https:\/\/github.com\/huggingface\/datasets\/issues\/3676 shown, it's not the case and `None` are replaced by empty lists even if we cast to the exact same type.\r\n\r\nIn this PR I just workaround this bug in the case where no type casting is needed. In particular, I only call `pa.ListArray.from_arrays` only when necessary.\r\n\r\nI also added a warning when some `None` are effectively replaced by empty lists. I wanted to raise an error in this case, but maybe we should wait a major update to do so\r\n\r\nThis PR fixes this particular case, that is occurring in `run_qa.py` in `transformers`:\r\n```python\r\nfrom datasets import Dataset\r\n\r\nds = Dataset.from_dict({\"a\": range(4)})\r\nds = ds.map(lambda x: {\"b\": [[None, [0]]]}, batched=True, batch_size=1, remove_columns=[\"a\"])\r\nprint(ds.to_pandas())\r\n# before:\r\n#              b\r\n# 0  [None, [0]]\r\n# 1    [[], [0]]\r\n# 2    [[], [0]]\r\n# 3    [[], [0]]\r\n#\r\n# now:\r\n#              b\r\n# 0  [None, [0]]\r\n# 1  [None, [0]]\r\n# 2  [None, [0]]\r\n# 3  [None, [0]]\r\n```\r\n\r\ncc @sgugger ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4281","id":1225556939,"node_id":"PR_kwDODunzps43TNBm","number":4281,"title":"Remove a copy-paste sentence in dataset cards","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","The non-passing tests have nothing to do with this PR."],"created_at":1651678915000,"updated_at":1651826283000,"closed_at":1651689196000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4281","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4281","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4281.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4281.patch","merged_at":1651689196000},"body":"Remove the following copy-paste sentence from dataset cards:\r\n```\r\nWe show detailed information for up to 5 configurations of the dataset.\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4280","id":1225446844,"node_id":"PR_kwDODunzps43S2xg","number":4280,"title":"Add missing features to commonsense_qa dataset","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","@albertvillanova it adds question_concept and id which is great. I suppose we'll talk about staying true to the format on another PR. ","Yes, let's merge this PR as it is: it adds missing features.\r\n\r\nA subsequent PR may address the request on changing the dataset feature structure."],"created_at":1651674266000,"updated_at":1651847037000,"closed_at":1651846606000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4280","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4280","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4280.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4280.patch","merged_at":1651846606000},"body":"Fix partially #4275.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4279","id":1225300273,"node_id":"PR_kwDODunzps43SXw5","number":4279,"title":"Update minimal PyArrow version warning","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651667169000,"updated_at":1651740658000,"closed_at":1651740227000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4279","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4279","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4279.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4279.patch","merged_at":1651740227000},"body":"Update the minimal PyArrow version warning (should've been part of #4250). ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4278","id":1225122123,"node_id":"PR_kwDODunzps43RyTs","number":4278,"title":"Add missing features to openbookqa dataset for additional config","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Let's merge this PR as it is: it adds missing features.\r\n\r\nA subsequent PR may address the request on changing the data feature structure."],"created_at":1651656170000,"updated_at":1651842800000,"closed_at":1651842361000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4278","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4278","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4278.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4278.patch","merged_at":1651842361000},"body":"Fix partially #4276.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4277","id":1225002286,"node_id":"PR_kwDODunzps43RZV9","number":4277,"title":"Enable label alignment for token classification datasets","user":{"login":"lewtun","id":26859204,"node_id":"MDQ6VXNlcjI2ODU5MjA0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/26859204?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lewtun","html_url":"https:\/\/github.com\/lewtun","followers_url":"https:\/\/api.github.com\/users\/lewtun\/followers","following_url":"https:\/\/api.github.com\/users\/lewtun\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lewtun\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lewtun\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lewtun\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lewtun\/orgs","repos_url":"https:\/\/api.github.com\/users\/lewtun\/repos","events_url":"https:\/\/api.github.com\/users\/lewtun\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lewtun\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Hmm, not sure why the Windows tests are failing with:\r\n\r\n```\r\nDid not find path entry C:\\tools\\miniconda3\\bin\r\nC:\\tools\\miniconda3\\envs\\py37\\python.exe: No module named pytest\r\n```\r\n\r\nEdit: running the CI again fixed the problem \ud83d\ude43 ","> One last nit and we can merge then\r\n\r\nThanks, done!"],"created_at":1651648516000,"updated_at":1651851735000,"closed_at":1651851391000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4277","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4277","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4277.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4277.patch","merged_at":1651851391000},"body":"This PR extends the `Dataset.align_labels_with_mapping()` method to support alignment of label mappings between datasets and models for token classification (e.g. NER).\r\n\r\nExample of usage:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nner_ds = load_dataset(\"conll2003\", split=\"train\")\r\n# returns [3, 0, 7, 0, 0, 0, 7, 0, 0]\r\nner_ds[0][\"ner_tags\"]\r\n# hypothetical model mapping with O <--> B-LOC\r\nlabel2id = {\r\n    \"B-LOC\": \"0\",\r\n    \"B-MISC\": \"7\",\r\n    \"B-ORG\": \"3\",\r\n    \"B-PER\": \"1\",\r\n    \"I-LOC\": \"6\",\r\n    \"I-MISC\": \"8\",\r\n    \"I-ORG\": \"4\",\r\n    \"I-PER\": \"2\",\r\n    \"O\": \"5\"\r\n  }\r\nner_aligned_ds = ner_ds.align_labels_with_mapping(label2id, \"ner_tags\")\r\n# returns [3, 5, 7, 5, 5, 5, 7, 5, 5]\r\nner_aligned_ds[0][\"ner_tags\"]\r\n```\r\n\r\nContext: we need this in AutoTrain to automatically align datasets \/ models during evaluation. cc @abhishekkrthakur ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4276","id":1224949252,"node_id":"I_kwDODunzps5JAz4E","number":4276,"title":"OpenBookQA has missing and inconsistent field names","user":{"login":"vblagoje","id":458335,"node_id":"MDQ6VXNlcjQ1ODMzNQ==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/458335?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/vblagoje","html_url":"https:\/\/github.com\/vblagoje","followers_url":"https:\/\/api.github.com\/users\/vblagoje\/followers","following_url":"https:\/\/api.github.com\/users\/vblagoje\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/vblagoje\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/vblagoje\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/vblagoje\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/vblagoje\/orgs","repos_url":"https:\/\/api.github.com\/users\/vblagoje\/repos","events_url":"https:\/\/api.github.com\/users\/vblagoje\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/vblagoje\/received_events","type":"User","site_admin":false},"labels":[{"id":2067388877,"node_id":"MDU6TGFiZWwyMDY3Mzg4ODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20bug","name":"dataset bug","color":"2edb81","default":false,"description":"A bug in a dataset script provided in the library"}],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Thanks for reporting, @vblagoje.\r\n\r\nIndeed, I noticed some of these issues while reviewing this PR:\r\n- #4259 \r\n\r\nThis is in my TODO list. ","Ok, awesome @albertvillanova How about #4275 ?","On the other hand, I am not sure if we should always preserve the original nested structure. I think we should also consider other factors as convenience or consistency.\r\n\r\nFor example, other datasets also flatten \"question.stem\" into \"question\":\r\n- ai2_arc:\r\n  ```python\r\n  question = data[\"question\"][\"stem\"]\r\n  choices = data[\"question\"][\"choices\"]\r\n  text_choices = [choice[\"text\"] for choice in choices]\r\n  label_choices = [choice[\"label\"] for choice in choices]\r\n  yield id_, {\r\n      \"id\": id_,\r\n      \"answerKey\": answerkey,\r\n      \"question\": question,\r\n      \"choices\": {\"text\": text_choices, \"label\": label_choices},\r\n  }\r\n  ```\r\n- commonsense_qa:\r\n  ```python\r\n  question = data[\"question\"]\r\n  stem = question[\"stem\"]\r\n  yield id_, {\r\n      \"answerKey\": answerkey,\r\n      \"question\": stem,\r\n      \"choices\": {\"label\": labels, \"text\": texts},\r\n  }\r\n  ```\r\n- cos_e:\r\n  ```python\r\n  \"question\": cqa[\"question\"][\"stem\"],\r\n  ```\r\n- qasc\r\n- quartz\r\n- wiqa\r\n\r\nExceptions:\r\n- exams\r\n\r\nI think we should agree on a CONVENIENT format for QA and use always CONSISTENTLY the same.","@albertvillanova I agree that we should be consistent. In the last month, I have come across tons of code that deals with OpenBookQA and CommonSenseQA and all of that code relies on the original data format structure. We can't expect users to adopt HF Datasets if we arbitrarily change the structure of the format just because we think something makes more sense. I am in that position now (downloading original data rather than using HF Datasets) and undoubtedly it hinders HF Datasets' widespread use and adoption. Missing fields like in the case of #4275 is definitely bad and not even up for a discussion IMHO! cc @lhoestq ","I'm opening a PR that adds the missing fields.\r\n\r\nLet's agree on the feature structure: @lhoestq @mariosasko @polinaeterna ","IMO we should always try to preserve the original structure unless there is a good reason not to (and I don't see one in this case).","I agree with @mariosasko . The transition to the original format could be done in one PR for the next minor release, clearly documenting all dataset changes just as @albertvillanova outlined them above and perhaps even providing a per dataset util method to convert the new valid format to the old for backward compatibility. Users who relied on the old format will update their code with either the util method for a quick fix or slightly more elaborate for the new. ","I don't have a strong opinion on this, besides the fact that whatever decision we agree on, should be applied to all datasets.\r\n\r\nThere is always the tension between:\r\n- preserving each dataset original structure (which has the advantage of not forcing users to learn other structure for the same dataset),\r\n-  and on the other hand performing some king of standardization\/harmonization depending on the task (this has the advantage that once learnt, the same structure applies to all datasets; this has been done for e.g.  POS tagging: all datasets have been adapted to a certain \"standard\" structure).\r\n   - Another advantage: datasets can easily be interchanged (or joined) to be used by the same model\r\n\r\nRecently, in the BigScience BioMedical hackathon, they adopted a different approach:\r\n- they implement a \"source\" config, respecting the original structure as much as possible\r\n- they implement additional config for each task, with a \"standard\" nested structure per task, which is most useful for users.","@albertvillanova, thanks for the detailed answer and the new perspectives. I understand the friction for the best design approach much better now.  Ultimately, it is essential to include all the missing fields and the correct data first. Whatever approach is determined to be optimal is important but not as crucial once all the data is there, and users can create lambda functions to create whatever structure serves them best. "],"created_at":1651643512000,"updated_at":1652013223000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nOpenBookQA implementation is inconsistent with the original dataset.\r\n\r\nWe need to:\r\n\r\n1. The dataset field [question][stem] is flattened into question_stem. Unflatten it to match the original format.\r\n2. Add missing additional fields:\r\n    - 'fact1': row['fact1'],\r\n    - 'humanScore': row['humanScore'],\r\n    - 'clarity': row['clarity'],\r\n    - 'turkIdAnonymized': row['turkIdAnonymized']\r\n3. Ensure the structure and every data item in the original OpenBookQA matches our OpenBookQA version.\r\n\r\n## Expected results\r\nThe structure and every data item in the original OpenBookQA matches our OpenBookQA version.\r\n\r\n## Actual results\r\nTBD\r\n\r\n## Environment info\r\n- `datasets` version: 2.1.0\r\n- Platform: macOS-10.15.7-x86_64-i386-64bit\r\n- Python version: 3.8.13\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4275","id":1224943414,"node_id":"I_kwDODunzps5JAyc2","number":4275,"title":"CommonSenseQA has missing and inconsistent field names","user":{"login":"vblagoje","id":458335,"node_id":"MDQ6VXNlcjQ1ODMzNQ==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/458335?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/vblagoje","html_url":"https:\/\/github.com\/vblagoje","followers_url":"https:\/\/api.github.com\/users\/vblagoje\/followers","following_url":"https:\/\/api.github.com\/users\/vblagoje\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/vblagoje\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/vblagoje\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/vblagoje\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/vblagoje\/orgs","repos_url":"https:\/\/api.github.com\/users\/vblagoje\/repos","events_url":"https:\/\/api.github.com\/users\/vblagoje\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/vblagoje\/received_events","type":"User","site_admin":false},"labels":[{"id":2067388877,"node_id":"MDU6TGFiZWwyMDY3Mzg4ODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20bug","name":"dataset bug","color":"2edb81","default":false,"description":"A bug in a dataset script provided in the library"}],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Thanks for reporting, @vblagoje.\r\n\r\nI'm opening a PR to address this. "],"created_at":1651642739000,"updated_at":1651664478000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nIn short, CommonSenseQA implementation is inconsistent with the original dataset.\r\n\r\nMore precisely, we need to:\r\n\r\n1. Add the dataset matching \"id\" field. The current dataset, instead, regenerates monotonically increasing id.  \r\n2. The [\u201cquestion\u201d][\u201cstem\u201d] field is flattened into \"question\". We should match the original dataset and unflatten it\r\n3. Add the missing \"question_concept\" field in the question tree node\r\n4. Anything else? Go over the data structure of the newly repaired CommonSenseQA and make sure it matches the original\r\n\r\n## Expected results\r\nEvery data item of the CommonSenseQA should structurally and data-wise match the original CommonSenseQA dataset.\r\n\r\n## Actual results\r\nTBD\r\n\r\n## Environment info\r\n- `datasets` version: 2.1.0\r\n- Platform: macOS-10.15.7-x86_64-i386-64bit\r\n- Python version: 3.8.13\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4274","id":1224740303,"node_id":"PR_kwDODunzps43Qm2w","number":4274,"title":"Add API code examples for IterableDataset","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651617857000,"updated_at":1651681772000,"closed_at":1651681324000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4274","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4274","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4274.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4274.patch","merged_at":1651681324000},"body":"This PR adds API code examples for `IterableDataset` and `IterableDatasetDicts`.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4273","id":1224681036,"node_id":"PR_kwDODunzps43QaA6","number":4273,"title":"leadboard info added for TNE","user":{"login":"yanaiela","id":8031035,"node_id":"MDQ6VXNlcjgwMzEwMzU=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8031035?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yanaiela","html_url":"https:\/\/github.com\/yanaiela","followers_url":"https:\/\/api.github.com\/users\/yanaiela\/followers","following_url":"https:\/\/api.github.com\/users\/yanaiela\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yanaiela\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yanaiela\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yanaiela\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yanaiela\/orgs","repos_url":"https:\/\/api.github.com\/users\/yanaiela\/repos","events_url":"https:\/\/api.github.com\/users\/yanaiela\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yanaiela\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651613741000,"updated_at":1651757124000,"closed_at":1651756693000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4273","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4273","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4273.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4273.patch","merged_at":1651756693000},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4272","id":1224635660,"node_id":"PR_kwDODunzps43QQQt","number":4272,"title":"Fix typo in logging docs","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","> This PR fixes #4271.\r\n\r\nThings have not changed when searching \"tqdm\" in the Dataset document. The second result still performs as \"Enable\".","Hi @jiangwy99, the fix will appear on the `main` version of the docs:\r\n\r\n![Screen Shot 2022-05-04 at 8 38 29 AM](https:\/\/user-images.githubusercontent.com\/59462357\/166718225-6848ab91-87d1-4572-9912-40a909af6cb9.png)\r\n","Fixed now, thanks."],"created_at":1651610877000,"updated_at":1651678947000,"closed_at":1651647516000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4272","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4272","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4272.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4272.patch","merged_at":1651647515000},"body":"This PR fixes #4271.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4271","id":1224404403,"node_id":"I_kwDODunzps5I-u2z","number":4271,"title":"A typo in docs of datasets.disable_progress_bar","user":{"login":"jiangwy99","id":39762734,"node_id":"MDQ6VXNlcjM5NzYyNzM0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/39762734?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jiangwy99","html_url":"https:\/\/github.com\/jiangwy99","followers_url":"https:\/\/api.github.com\/users\/jiangwy99\/followers","following_url":"https:\/\/api.github.com\/users\/jiangwy99\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jiangwy99\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jiangwy99\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jiangwy99\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jiangwy99\/orgs","repos_url":"https:\/\/api.github.com\/users\/jiangwy99\/repos","events_url":"https:\/\/api.github.com\/users\/jiangwy99\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jiangwy99\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"assignees":[{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi! Thanks for catching and reporting the typo, a PR has been opened to fix it :)"],"created_at":1651599896000,"updated_at":1651647515000,"closed_at":1651647515000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nin the docs of V2.1.0 datasets.disable_progress_bar, we should replace \"enable\" with \"disable\".","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4270","id":1224244460,"node_id":"PR_kwDODunzps43PC5V","number":4270,"title":"Fix style in openbookqa dataset","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651591294000,"updated_at":1651826286000,"closed_at":1651594852000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4270","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4270","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4270.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4270.patch","merged_at":1651594852000},"body":"CI in PR:\r\n- #4259 \r\n\r\nwas green, but after merging it to master, a code quality error appeared.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4269","id":1223865145,"node_id":"PR_kwDODunzps43Nzwh","number":4269,"title":"Add license and point of contact to big_patent dataset","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651569847000,"updated_at":1651826289000,"closed_at":1651576579000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4269","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4269","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4269.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4269.patch","merged_at":1651576579000},"body":"Update metadata of big_patent dataset with:\r\n- license\r\n- point of contact","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4268","id":1223331964,"node_id":"I_kwDODunzps5I6pB8","number":4268,"title":"error downloading bigscience-catalogue-lm-data\/lm_en_wiktionary_filtered","user":{"login":"i-am-neo","id":102043285,"node_id":"U_kgDOBhUOlQ","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/102043285?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/i-am-neo","html_url":"https:\/\/github.com\/i-am-neo","followers_url":"https:\/\/api.github.com\/users\/i-am-neo\/followers","following_url":"https:\/\/api.github.com\/users\/i-am-neo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/i-am-neo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/i-am-neo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/i-am-neo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/i-am-neo\/orgs","repos_url":"https:\/\/api.github.com\/users\/i-am-neo\/repos","events_url":"https:\/\/api.github.com\/users\/i-am-neo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/i-am-neo\/received_events","type":"User","site_admin":false},"labels":[{"id":2067388877,"node_id":"MDU6TGFiZWwyMDY3Mzg4ODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20bug","name":"dataset bug","color":"2edb81","default":false,"description":"A bug in a dataset script provided in the library"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["It would help a lot to be able to preview the dataset - I'd like to see if the pronunciations are in the dataset, eg. for [\"word\"](https:\/\/en.wiktionary.org\/wiki\/word),\r\n\r\nPronunciation\r\n([Received Pronunciation](https:\/\/en.wikipedia.org\/wiki\/Received_Pronunciation)) [IPA](https:\/\/en.wiktionary.org\/wiki\/Wiktionary:International_Phonetic_Alphabet)([key](https:\/\/en.wiktionary.org\/wiki\/Appendix:English_pronunciation)): \/w\u025c\u02d0d\/\r\n([General American](https:\/\/en.wikipedia.org\/wiki\/General_American)) [enPR](https:\/\/en.wiktionary.org\/wiki\/Appendix:English_pronunciation): w\u00fbrd, [IPA](https:\/\/en.wiktionary.org\/wiki\/Wiktionary:International_Phonetic_Alphabet)([key](https:\/\/en.wiktionary.org\/wiki\/Appendix:English_pronunciation)): \/w\u025dd\/","Hi @i-am-neo, thanks for reporting.\r\n\r\nNormally this dataset should be private and not accessible for public use. @cakiki, @lvwerra, any reason why is it public? I see many other Wikimedia datasets are also public.\r\n\r\nAlso note that last commit \"Add metadata\" (https:\/\/huggingface.co\/datasets\/bigscience-catalogue-lm-data\/lm_en_wiktionary_filtered\/commit\/dc2f458dab50e00f35c94efb3cd4009996858609) introduced buggy data files (`data\/file-01.jsonl.gz.lock`, `data\/file-01.jsonl.gz.lock.lock`). The same bug appears in other datasets as well.\r\n\r\n@i-am-neo, please note that in the near future we are planning to make public all datasets used for the BigScience project (at least all of them whose license allows to do that). Once public, they will be accessible for all the NLP community.","Ah this must be a bug introduced at creation time since the repos were created programmatically; I'll go ahead and make them private; sorry about that!","All datasets are private now. \r\n\r\nRe:that bug I think we're currently avoiding it by avoiding verifications. (i.e. `ignore_verifications=True`)","Thanks a lot, @cakiki.\r\n\r\n@i-am-neo, I'm closing this issue for now because the dataset is not publicly available yet. Just stay tuned, as we will soon release all the BigScience open-license datasets.  ","Thanks for letting me know, @albertvillanova @cakiki.\r\nAny chance of having a subset alpha version in the meantime? \r\nI only need two dicts out of wiktionary: 1) phoneme(as key): word, and 2) word(as key): its phonemes.\r\n\r\nWould like to use it for a mini-poc [Robust ASR](https:\/\/github.com\/huggingface\/transformers\/issues\/13162#issuecomment-1096881290) decoding, cc @patrickvonplaten. \r\n\r\n(Patrick, possible to email you so as not to litter github with comments? I have some observations after experiments training hubert on some YT AMI-like data (11.44% wer).  Also wonder if a robust ASR is on your\/HG's roadmap).  Thanks!","Hey @i-am-neo,\r\n\r\nCool to hear that you're working on Robust ASR! Feel free to drop me a mail :-)","@i-am-neo This particular subset of the dataset was taken from the [CirrusSearch dumps](https:\/\/dumps.wikimedia.org\/other\/cirrussearch\/current\/)\r\nYou're specifically after the [enwiktionary-20220425-cirrussearch-content.json.gz](https:\/\/dumps.wikimedia.org\/other\/cirrussearch\/current\/enwiktionary-20220425-cirrussearch-content.json.gz) file","thanks @cakiki !  <del>I could access the gz file yesterday (but neglected to tuck it away somewhere safe), and today the link is throwing a 404. Can you help? <\/del>  Never mind, got it!","thanks @patrickvonplaten.  will do - getting my observations together."],"created_at":1651523665000,"updated_at":1651852410000,"closed_at":1651577028000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nError generated when attempting to download dataset\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"bigscience-catalogue-lm-data\/lm_en_wiktionary_filtered\")\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\n```\r\nExpectedMoreDownloadedFiles               Traceback (most recent call last)\r\n\r\n[<ipython-input-62-4ac5cf959477>](https:\/\/localhost:8080\/#) in <module>()\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 dataset = load_dataset(\"bigscience-catalogue-lm-data\/lm_en_wiktionary_filtered\")\r\n\r\n3 frames\r\n\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/utils\/info_utils.py](https:\/\/localhost:8080\/#) in verify_checksums(expected_checksums, recorded_checksums, verification_name)\r\n     31         return\r\n     32     if len(set(expected_checksums) - set(recorded_checksums)) > 0:\r\n---> 33         raise ExpectedMoreDownloadedFiles(str(set(expected_checksums) - set(recorded_checksums)))\r\n     34     if len(set(recorded_checksums) - set(expected_checksums)) > 0:\r\n     35         raise UnexpectedDownloadedFile(str(set(recorded_checksums) - set(expected_checksums)))\r\n\r\nExpectedMoreDownloadedFiles: {'\/home\/leandro\/catalogue_data\/datasets\/lm_en_wiktionary_filtered\/data\/file-01.jsonl.gz', '\/home\/leandro\/catalogue_data\/datasets\/lm_en_wiktionary_filtered\/data\/file-01.jsonl.gz.lock'}\r\n```\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.3\r\n- Platform: Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- PyArrow version: 6.0.1\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4267","id":1223214275,"node_id":"PR_kwDODunzps43LzOR","number":4267,"title":"Replace data URL in SAMSum dataset within the same repository","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651516688000,"updated_at":1651826293000,"closed_at":1651518229000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4267","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4267","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4267.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4267.patch","merged_at":1651518229000},"body":"Replace data URL with one in the same repository.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4266","id":1223116436,"node_id":"PR_kwDODunzps43LeXK","number":4266,"title":"Add HF Speech Bench to Librispeech Dataset Card","user":{"login":"sanchit-gandhi","id":93869735,"node_id":"U_kgDOBZhWpw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/93869735?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sanchit-gandhi","html_url":"https:\/\/github.com\/sanchit-gandhi","followers_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/followers","following_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/orgs","repos_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/repos","events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651510771000,"updated_at":1651740440000,"closed_at":1651740009000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4266","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4266","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4266.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4266.patch","merged_at":1651740009000},"body":"Adds the HF Speech Bench to Librispeech Dataset Card in place of the Papers With Code Leaderboard. Should improve usage and visibility of this leaderboard! Wondering whether this can also be done for [Common Voice 7](https:\/\/huggingface.co\/datasets\/mozilla-foundation\/common_voice_7_0) and [8](https:\/\/huggingface.co\/datasets\/mozilla-foundation\/common_voice_8_0) through someone with permissions? \r\n\r\ncc @patrickvonplaten: more leaderboard promotion!","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4263","id":1222723083,"node_id":"PR_kwDODunzps43KLnD","number":4263,"title":"Rename imagenet2012 -> imagenet-1k","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["> Later we can add imagenet-21k as a new dataset if we want.\r\n\r\nisn't it what models refer to as `imagenet` already?","> isn't it what models refer to as imagenet already?\r\n\r\nI wasn't sure, but it looks like it indeed. Therefore having a dataset `imagenet` for ImageNet 21k makes sense actually.\r\n\r\nEDIT: actually not all `imagenet` tag refer to ImageNet 21k - we will need to correct some of them","_The documentation is not available anymore as the PR was closed or merged._","should we remove the repo mirror on the hub side or will you do it?"],"created_at":1651487181000,"updated_at":1651513846000,"closed_at":1651509177000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4263","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4263","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4263.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4263.patch","merged_at":1651509177000},"body":"On the Hugging Face Hub, users refer to imagenet2012 (from #4178 ) as imagenet-1k in their model tags.\r\n\r\nTo correctly link models to imagenet, we should rename this dataset `imagenet-1k`.\r\n\r\nLater we can add `imagenet-21k` as a new dataset if we want.\r\n\r\nOnce this one is merged we can delete the `imagenet2012` dataset repository on the Hub.\r\n\r\nEDIT: to complete the rationale on why we should name it `imagenet-1k`:\r\nIf users specifically added the tag `imagenet-1k` , then it could be for two reasons (not sure which one is predominant), either they\r\n- wanted to make it explicit that it\u2019s not 21k -> the distinction is important for the community\r\n- or they have been following this convention from other models -> the convention implicitly exists already","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263\/reactions","total_count":4,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":1,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4262","id":1222130749,"node_id":"PR_kwDODunzps43IOye","number":4262,"title":"Add YAML tags to Dataset Card rotten tomatoes","user":{"login":"mo6zes","id":10004251,"node_id":"MDQ6VXNlcjEwMDA0MjUx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10004251?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mo6zes","html_url":"https:\/\/github.com\/mo6zes","followers_url":"https:\/\/api.github.com\/users\/mo6zes\/followers","following_url":"https:\/\/api.github.com\/users\/mo6zes\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mo6zes\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mo6zes\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mo6zes\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mo6zes\/orgs","repos_url":"https:\/\/api.github.com\/users\/mo6zes\/repos","events_url":"https:\/\/api.github.com\/users\/mo6zes\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mo6zes\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651406348000,"updated_at":1651588053000,"closed_at":1651587635000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4262","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4262","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4262.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4262.patch","merged_at":1651587635000},"body":"The dataset card for the rotten tomatoes \/ MR movie review dataset had some missing YAML tags. Hopefully, this also improves the visibility of this dataset now that paperswithcode and huggingface link to eachother.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262\/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4261","id":1221883779,"node_id":"I_kwDODunzps5I1HeD","number":4261,"title":"data leakage in `webis\/conclugen` dataset","user":{"login":"xflashxx","id":54585776,"node_id":"MDQ6VXNlcjU0NTg1Nzc2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/54585776?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/xflashxx","html_url":"https:\/\/github.com\/xflashxx","followers_url":"https:\/\/api.github.com\/users\/xflashxx\/followers","following_url":"https:\/\/api.github.com\/users\/xflashxx\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/xflashxx\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/xflashxx\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/xflashxx\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/xflashxx\/orgs","repos_url":"https:\/\/api.github.com\/users\/xflashxx\/repos","events_url":"https:\/\/api.github.com\/users\/xflashxx\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/xflashxx\/received_events","type":"User","site_admin":false},"labels":[{"id":2067388877,"node_id":"MDU6TGFiZWwyMDY3Mzg4ODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20bug","name":"dataset bug","color":"2edb81","default":false,"description":"A bug in a dataset script provided in the library"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi @xflashxx, thanks for reporting.\r\n\r\nPlease note that this dataset was generated and shared by Webis Group: https:\/\/huggingface.co\/webis\r\n\r\nWe are contacting the dataset owners to inform them about the issue you found. We'll keep you updated of their reply.","i'd suggest just pinging the authors here in the issue if possible?","Thanks for reporting this @xflashxx. I'll have a look and get back to you on this.","Hi @xflashxx and @albertvillanova,\r\n\r\nI have updated the files with de-duplicated splits. Apparently the debate portals from which part of the examples were sourced had unique timestamps for some examples (up to 6%; updated counts in the README) without any actual content updated that lead to \"new\" items. The length of `ids_validation` and `ids_testing` is zero.\r\n\r\nRegarding impact on scores:\r\n1. We employed automatic evaluation (on a separate set of 1000 examples) only to justify the exclusion of the smaller models for manual evaluation (due to budget constraints). I am confident the ranking still stands (unsurprisingly, the bigger models doing better than those trained on the smaller splits). We also highlight this in the paper. \r\n\r\n2. The examples used for manual evaluation have no overlap with any splits (also because they do not have any ground truth as we applied the trained models on an unlabeled sample to test its practical usage). I've added these two files to the dataset repository.\r\n\r\nHope this helps!","Thanks @shahbazsyed for your fast fix.\r\n\r\nAs a side note:\r\n- Your email appearing as Point of Contact in the dataset README has a typo: @uni.leipzig.de instead of @uni-leipzig.de\r\n- Your commits on the Hub are not linked to your profile on the Hub: this is because we use the email address to make this link; the email address used in your commit author and the email address set on your Hub account settings."],"created_at":1651340617000,"updated_at":1651557866000,"closed_at":1651557866000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nSome samples (argument-conclusion pairs) in the *training* split of the `webis\/conclugen` dataset are present in both the *validation* and *test* splits, creating data leakage and distorting model results.\r\nFurthermore, all splits contain duplicate samples.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ntraining = load_dataset(\"webis\/conclugen\", \"base\", split=\"train\")\r\nvalidation = load_dataset(\"webis\/conclugen\", \"base\", split=\"validation\")\r\ntesting = load_dataset(\"webis\/conclugen\", \"base\", split=\"test\")\r\n\r\n# collect which sample id's are present in the training split\r\nids_validation = list()\r\nids_testing = list()\r\n\r\nfor train_sample in training:\r\n    train_argument = train_sample[\"argument\"]\r\n    train_conclusion = train_sample[\"conclusion\"]\r\n    train_id = train_sample[\"id\"]\r\n    \r\n    # test if current sample is in validation split\r\n    if train_argument in validation[\"argument\"]:\r\n        for validation_sample in validation:\r\n            validation_argument = validation_sample[\"argument\"]\r\n            validation_conclusion = validation_sample[\"conclusion\"]\r\n            validation_id = validation_sample[\"id\"]\r\n            if train_argument == validation_argument and train_conclusion == validation_conclusion:\r\n                ids_validation.append(validation_id)\r\n    \r\n    # test if current sample is in test split\r\n    if train_argument in testing[\"argument\"]:\r\n        for testing_sample in testing:\r\n            testing_argument = testing_sample[\"argument\"]\r\n            testing_conclusion = testing_sample[\"conclusion\"]\r\n            testing_id = testing_sample[\"id\"]\r\n            if train_argument == testing_argument and train_conclusion == testing_conclusion:\r\n                ids_testing.append(testing_id)\r\n```\r\n\r\n## Expected results\r\nLength of both lists `ids_validation` and `ids_testing` should be zero.\r\n\r\n## Actual results\r\nLength of `ids_validation` = `2556`\r\nLength of `ids_testing` = `287`\r\n\r\nFurthermore, there seems to be duplicate samples in (at least) the *training* split, since:\r\n`print(len(set(ids_validation)))` = `950`\r\n`print(len(set(ids_testing)))` = `101`\r\n\r\nAll in all, around 7% of the samples of each the *validation* and *test* split seems to be present in the *training* split.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.4\r\n- Platform: macOS-12.3.1-arm64-arm-64bit\r\n- Python version: 3.9.10\r\n- PyArrow version: 7.0.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4260","id":1221830292,"node_id":"PR_kwDODunzps43HSfs","number":4260,"title":"Add mr_polarity movie review sentiment classification","user":{"login":"mo6zes","id":10004251,"node_id":"MDQ6VXNlcjEwMDA0MjUx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10004251?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mo6zes","html_url":"https:\/\/github.com\/mo6zes","followers_url":"https:\/\/api.github.com\/users\/mo6zes\/followers","following_url":"https:\/\/api.github.com\/users\/mo6zes\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mo6zes\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mo6zes\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mo6zes\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mo6zes\/orgs","repos_url":"https:\/\/api.github.com\/users\/mo6zes\/repos","events_url":"https:\/\/api.github.com\/users\/mo6zes\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mo6zes\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["whoops just found https:\/\/huggingface.co\/datasets\/rotten_tomatoes"],"created_at":1651324773000,"updated_at":1651328185000,"closed_at":1651328185000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4260","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4260","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4260.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4260.patch","merged_at":null},"body":"Add the MR (Movie Review) dataset. The original dataset contains sentences from Rotten Tomatoes labeled as either \"positive\" or  \"negative\". \r\n\r\nHomepage: [https:\/\/www.cs.cornell.edu\/people\/pabo\/movie-review-data\/](https:\/\/www.cs.cornell.edu\/people\/pabo\/movie-review-data\/)\r\npaperswithcode: [https:\/\/paperswithcode.com\/dataset\/mr](https:\/\/paperswithcode.com\/dataset\/mr)\r\n\r\n- [ ] I was not able to generate dummy data, the original dataset files have \".pos\" and \".neg\" as file extensions so the auto-generator does not work. Is it fine like this or should dummy data be added?\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4259","id":1221768025,"node_id":"PR_kwDODunzps43HHGc","number":4259,"title":"Fix bug in choices labels in openbookqa dataset","user":{"login":"manandey","id":6687858,"node_id":"MDQ6VXNlcjY2ODc4NTg=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/6687858?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/manandey","html_url":"https:\/\/github.com\/manandey","followers_url":"https:\/\/api.github.com\/users\/manandey\/followers","following_url":"https:\/\/api.github.com\/users\/manandey\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/manandey\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/manandey\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/manandey\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/manandey\/orgs","repos_url":"https:\/\/api.github.com\/users\/manandey\/repos","events_url":"https:\/\/api.github.com\/users\/manandey\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/manandey\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651304499000,"updated_at":1651645891000,"closed_at":1651590861000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4259","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4259","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4259.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4259.patch","merged_at":1651590861000},"body":"This PR fixes the Bug in the openbookqa dataset as mentioned in this issue #3550.\r\n\r\nFix #3550.\r\n\r\ncc. @lhoestq @mariosasko ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4258","id":1221637727,"node_id":"PR_kwDODunzps43Gstg","number":4258,"title":"Fix\/start token mask issue and update documentation","user":{"login":"TristanThrush","id":20826878,"node_id":"MDQ6VXNlcjIwODI2ODc4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/20826878?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/TristanThrush","html_url":"https:\/\/github.com\/TristanThrush","followers_url":"https:\/\/api.github.com\/users\/TristanThrush\/followers","following_url":"https:\/\/api.github.com\/users\/TristanThrush\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/TristanThrush\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/TristanThrush\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/TristanThrush\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/TristanThrush\/orgs","repos_url":"https:\/\/api.github.com\/users\/TristanThrush\/repos","events_url":"https:\/\/api.github.com\/users\/TristanThrush\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/TristanThrush\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","> Good catch ! Thanks :)\r\n> \r\n> Next time can you describe your fix in the Pull Request description please ?\r\n\r\nThanks. Also whoops, sorry about not being very descriptive. I updated the pull request description, and will keep this in mind for future PRs."],"created_at":1651272164000,"updated_at":1651509200000,"closed_at":1651508772000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4258","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4258","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4258.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4258.patch","merged_at":1651508772000},"body":"This pr fixes a couple bugs:\r\n\r\n1) the perplexity was calculated with a 0 in the attention mask for the start token, which was causing high perplexity scores that were not correct\r\n2) the documentation was not updated","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4257","id":1221393137,"node_id":"PR_kwDODunzps43GATC","number":4257,"title":"Create metric card for Mahalanobis Distance","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651257447000,"updated_at":1651503018000,"closed_at":1651502604000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4257","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4257","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4257.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4257.patch","merged_at":1651502604000},"body":"proposing a metric card to better explain how Mahalanobis distance works (last one for now :sweat_smile:","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4256","id":1221379625,"node_id":"PR_kwDODunzps43F9Zw","number":4256,"title":"Create metric card for MSE","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651256482000,"updated_at":1651503342000,"closed_at":1651502927000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4256","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4256","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4256.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4256.patch","merged_at":1651502927000},"body":"Proposing a metric card for Mean Squared Error","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4255","id":1221142899,"node_id":"PR_kwDODunzps43FHgR","number":4255,"title":"No google drive URL for pubmed_qa","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","CI is failing because some sections are missing in the dataset card, but this is unrelated to this PR - Merging !"],"created_at":1651247746000,"updated_at":1651249495000,"closed_at":1651249136000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4255","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4255","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4255.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4255.patch","merged_at":1651249136000},"body":"I hosted the data files in https:\/\/huggingface.co\/datasets\/pubmed_qa. This is allowed because the data is under the MIT license.\r\n\r\ncc @stas00 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4254","id":1220204395,"node_id":"PR_kwDODunzps43Bwnj","number":4254,"title":"Replace data URL in SAMSum dataset and support streaming","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651220503000,"updated_at":1651826296000,"closed_at":1651249569000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4254","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4254","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4254.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4254.patch","merged_at":1651249568000},"body":"This PR replaces data URL in SAMSum dataset:\r\n- original host (arxiv.org) does not allow HTTP Range requests\r\n- we have hosted the data on the Hub (license: CC BY-NC-ND 4.0)\r\n\r\nMoreover, it implements support for streaming.\r\n\r\nFix #4146.\r\nRelated to: #4236.\r\n\r\nCC: @severo ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4253","id":1219286408,"node_id":"PR_kwDODunzps42-c8Q","number":4253,"title":"Create metric cards for mean IOU","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651179507000,"updated_at":1651254287000,"closed_at":1651253886000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4253","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4253","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4253.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4253.patch","merged_at":1651253886000},"body":"Proposing a metric card for mIoU :rocket:\r\n\r\nsorry for spamming you with review requests, @albertvillanova ! :hugs: ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4252","id":1219151100,"node_id":"PR_kwDODunzps429--I","number":4252,"title":"Creating metric card for MAE","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651172673000,"updated_at":1651251551000,"closed_at":1651251150000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4252","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4252","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4252.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4252.patch","merged_at":1651251150000},"body":"Initial proposal for MAE metric card","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4251","id":1219116354,"node_id":"PR_kwDODunzps4293dB","number":4251,"title":"Metric card for the XTREME-S dataset","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651170739000,"updated_at":1651250771000,"closed_at":1651250326000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4251","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4251","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4251.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4251.patch","merged_at":1651250326000},"body":"Proposing a metric card for the XTREME-S dataset :hugs:","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4250","id":1219093830,"node_id":"PR_kwDODunzps429yjN","number":4250,"title":"Bump PyArrow Version to 6","user":{"login":"dnaveenr","id":17746528,"node_id":"MDQ6VXNlcjE3NzQ2NTI4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17746528?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/dnaveenr","html_url":"https:\/\/github.com\/dnaveenr","followers_url":"https:\/\/api.github.com\/users\/dnaveenr\/followers","following_url":"https:\/\/api.github.com\/users\/dnaveenr\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/dnaveenr\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/dnaveenr\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/dnaveenr\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/dnaveenr\/orgs","repos_url":"https:\/\/api.github.com\/users\/dnaveenr\/repos","events_url":"https:\/\/api.github.com\/users\/dnaveenr\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/dnaveenr\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Updated meta.yaml as well. Thanks.","I'm OK with bumping PyArrow to version 6 to match the version in Colab, but maybe a better solution would be to stop using extension types in our codebase to avoid similar issues.","> but maybe a better solution would be to stop using extension types in our codebase to avoid similar issues.\r\n\r\nI agree, not much attention has been payed to extension arrays in the latest developments of Arrow anyway.\r\n\r\nLet's not use them more that what we do right now, and try to remove them at one point"],"created_at":1651169450000,"updated_at":1651657012000,"closed_at":1651656586000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4250","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4250","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4250.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4250.patch","merged_at":1651656586000},"body":"Fixes #4152 \r\n\r\nThis PR updates the PyArrow version to 6 in setup.py, CI job files .circleci\/config.yaml and .github\/workflows\/benchmarks.yaml files.\r\nThis will fix ArrayND error which exists in pyarrow 5.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4249","id":1218524424,"node_id":"PR_kwDODunzps42742y","number":4249,"title":"Support streaming XGLUE dataset","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651141643000,"updated_at":1651826301000,"closed_at":1651162083000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4249","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4249","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4249.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4249.patch","merged_at":1651162083000},"body":"Support streaming XGLUE dataset.\r\n\r\nFix #4247.\r\n\r\nCC: @severo ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4248","id":1218460444,"node_id":"I_kwDODunzps5IoDsc","number":4248,"title":"conll2003 dataset loads original data.","user":{"login":"sue991","id":26458611,"node_id":"MDQ6VXNlcjI2NDU4NjEx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/26458611?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sue991","html_url":"https:\/\/github.com\/sue991","followers_url":"https:\/\/api.github.com\/users\/sue991\/followers","following_url":"https:\/\/api.github.com\/users\/sue991\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sue991\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sue991\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sue991\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sue991\/orgs","repos_url":"https:\/\/api.github.com\/users\/sue991\/repos","events_url":"https:\/\/api.github.com\/users\/sue991\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sue991\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Thanks for reporting @sue99.\r\n\r\nUnfortunately. I'm not able to reproduce your problem:\r\n```python\r\nIn [1]: import datasets\r\n   ...: from datasets import load_dataset\r\n   ...: dataset = load_dataset(\"conll2003\")\r\n\r\nIn [2]: dataset\r\nOut[2]: \r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\r\n        num_rows: 14042\r\n    })\r\n    validation: Dataset({\r\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\r\n        num_rows: 3251\r\n    })\r\n    test: Dataset({\r\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\r\n        num_rows: 3454\r\n    })\r\n})\r\n\r\nIn [3]: dataset[\"train\"][0]\r\nOut[3]: \r\n{'id': '0',\r\n 'tokens': ['EU',\r\n  'rejects',\r\n  'German',\r\n  'call',\r\n  'to',\r\n  'boycott',\r\n  'British',\r\n  'lamb',\r\n  '.'],\r\n 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\r\n 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\r\n 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\r\n```\r\n\r\nJust guessing: might be the case that you are calling `load_dataset` from a working directory that contains a local folder named `conll2003` (containing the raw data files)? If that is the case, `datasets` library gives precedence to the local folder over the dataset on the Hub. "],"created_at":1651138411000,"updated_at":1651163473000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nI load `conll2003` dataset to use refined data like [this](https:\/\/huggingface.co\/datasets\/conll2003\/viewer\/conll2003\/train)  preview, but it is original data that contains `'-DOCSTART- -X- -X- O'` text.\r\n\r\nIs this a bug or should I use another dataset_name like `lhoestq\/conll2003` ?\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"conll2003\")\r\n```\r\n\r\n## Expected results\r\n{\r\n    \"chunk_tags\": [11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0],\r\n    \"id\": \"0\",\r\n    \"ner_tags\": [0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\r\n    \"pos_tags\": [12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7],\r\n    \"tokens\": [\"The\", \"European\", \"Commission\", \"said\", \"on\", \"Thursday\", \"it\", \"disagreed\", \"with\", \"German\", \"advice\", \"to\", \"consumers\", \"to\", \"shun\", \"British\", \"lamb\", \"until\", \"scientists\", \"determine\", \"whether\", \"mad\", \"cow\", \"disease\", \"can\", \"be\", \"transmitted\", \"to\", \"sheep\", \".\"]\r\n}\r\n\r\n## Actual results\r\n```python\r\nprint(dataset)\r\n\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['text'],\r\n        num_rows: 219554\r\n    })\r\n    test: Dataset({\r\n        features: ['text'],\r\n        num_rows: 50350\r\n    })\r\n    validation: Dataset({\r\n        features: ['text'],\r\n        num_rows: 55044\r\n    })\r\n})\r\n```\r\n\r\n```python\r\nfor i in range(20):\r\n    print(dataset['train'][i])\r\n\r\n{'text': '-DOCSTART- -X- -X- O'}\r\n{'text': ''}\r\n{'text': 'EU NNP B-NP B-ORG'}\r\n{'text': 'rejects VBZ B-VP O'}\r\n{'text': 'German JJ B-NP B-MISC'}\r\n{'text': 'call NN I-NP O'}\r\n{'text': 'to TO B-VP O'}\r\n{'text': 'boycott VB I-VP O'}\r\n{'text': 'British JJ B-NP B-MISC'}\r\n{'text': 'lamb NN I-NP O'}\r\n{'text': '. . O O'}\r\n{'text': ''}\r\n{'text': 'Peter NNP B-NP B-PER'}\r\n{'text': 'Blackburn NNP I-NP I-PER'}\r\n{'text': ''}\r\n{'text': 'BRUSSELS NNP B-NP B-LOC'}\r\n{'text': '1996-08-22 CD I-NP O'}\r\n{'text': ''}\r\n{'text': 'The DT B-NP O'}\r\n{'text': 'European NNP I-NP B-ORG'}\r\n```\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4247","id":1218320882,"node_id":"I_kwDODunzps5Inhny","number":4247,"title":"The data preview of XGLUE","user":{"login":"czq1999","id":49108847,"node_id":"MDQ6VXNlcjQ5MTA4ODQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/49108847?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/czq1999","html_url":"https:\/\/github.com\/czq1999","followers_url":"https:\/\/api.github.com\/users\/czq1999\/followers","following_url":"https:\/\/api.github.com\/users\/czq1999\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/czq1999\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/czq1999\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/czq1999\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/czq1999\/orgs","repos_url":"https:\/\/api.github.com\/users\/czq1999\/repos","events_url":"https:\/\/api.github.com\/users\/czq1999\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/czq1999\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["![image](https:\/\/user-images.githubusercontent.com\/49108847\/165700611-915b4343-766f-4b81-bdaa-b31950250f06.png)\r\n","Thanks for reporting @czq1999.\r\n\r\nNote that the dataset viewer uses the dataset in streaming mode and that not all datasets support streaming yet.\r\n\r\nThat is the case for XGLUE dataset (as the error message points out): this must be refactored to support streaming. ","Fixed, thanks @albertvillanova !\r\n\r\nhttps:\/\/huggingface.co\/datasets\/xglue\r\n\r\n<img width=\"824\" alt=\"Capture d\u2019e\u0301cran 2022-04-29 a\u0300 10 23 14\" src=\"https:\/\/user-images.githubusercontent.com\/1676121\/165909391-9f98d98a-665a-4e57-822d-8baa2dc9b7c9.png\">\r\n"],"created_at":1651131050000,"updated_at":1651220608000,"closed_at":1651162083000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"It seems that something wrong with the data previvew of XGLUE","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4246","id":1218320293,"node_id":"PR_kwDODunzps427NiD","number":4246,"title":"Support to load dataset with TSV files by passing only dataset name","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651131015000,"updated_at":1651826308000,"closed_at":1651824847000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4246","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4246","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4246.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4246.patch","merged_at":1651824847000},"body":"This PR implements support to load a dataset (w\/o script) containing TSV files by passing only the dataset name (no need to pass `sep='\\t'`):\r\n```python\r\nds = load_dataset(\"dataset\/name\")\r\n```\r\n\r\nThe refactoring allows for future builder kwargs customizations based on file extension.\r\n\r\nRelated to #4238.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4245","id":1217959400,"node_id":"PR_kwDODunzps426AUR","number":4245,"title":"Add code examples for DatasetDict","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651099942000,"updated_at":1651256374000,"closed_at":1651255983000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4245","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4245","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4245.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4245.patch","merged_at":1651255983000},"body":"This PR adds code examples for `DatasetDict` in the API reference :)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4244","id":1217732221,"node_id":"PR_kwDODunzps425Po6","number":4244,"title":"task id update","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Reverted the multi-input-text-classification tag from task_categories and added it as task_ids @lhoestq ","_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651084094000,"updated_at":1651661033000,"closed_at":1651660597000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4244","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4244","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4244.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4244.patch","merged_at":1651660597000},"body":"changed multi input text classification as task id instead of category","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4243","id":1217689909,"node_id":"PR_kwDODunzps425Gkn","number":4243,"title":"WIP: Initial shades loading script and readme","user":{"login":"shayne-longpre","id":69018523,"node_id":"MDQ6VXNlcjY5MDE4NTIz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/69018523?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/shayne-longpre","html_url":"https:\/\/github.com\/shayne-longpre","followers_url":"https:\/\/api.github.com\/users\/shayne-longpre\/followers","following_url":"https:\/\/api.github.com\/users\/shayne-longpre\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/shayne-longpre\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/shayne-longpre\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/shayne-longpre\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/shayne-longpre\/orgs","repos_url":"https:\/\/api.github.com\/users\/shayne-longpre\/repos","events_url":"https:\/\/api.github.com\/users\/shayne-longpre\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/shayne-longpre\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1651081543000,"updated_at":1651081543000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4243","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4243","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4243.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4243.patch","merged_at":null},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4242","id":1217665960,"node_id":"PR_kwDODunzps425BYf","number":4242,"title":"Update auth when mirroring datasets on the hub","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651080151000,"updated_at":1651081024000,"closed_at":1651080642000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4242","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4242","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4242.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4242.patch","merged_at":1651080642000},"body":"We don't need to use extraHeaders anymore for rate limits anymore. Anyway extraHeaders was not working with git LFS because it was passing the wrong auth to S3.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4241","id":1217423686,"node_id":"I_kwDODunzps5IkGlG","number":4241,"title":"NonMatchingChecksumError when attempting to download GLUE","user":{"login":"drussellmrichie","id":9650729,"node_id":"MDQ6VXNlcjk2NTA3Mjk=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/9650729?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/drussellmrichie","html_url":"https:\/\/github.com\/drussellmrichie","followers_url":"https:\/\/api.github.com\/users\/drussellmrichie\/followers","following_url":"https:\/\/api.github.com\/users\/drussellmrichie\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/drussellmrichie\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/drussellmrichie\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/drussellmrichie\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/drussellmrichie\/orgs","repos_url":"https:\/\/api.github.com\/users\/drussellmrichie\/repos","events_url":"https:\/\/api.github.com\/users\/drussellmrichie\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/drussellmrichie\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi :)\r\n\r\nI think your issue may be related to the older `nlp` library. I was able to download `glue` with the latest version of `datasets`. Can you try updating with:\r\n\r\n```py\r\npip install -U datasets\r\n```\r\n\r\nThen you can download:\r\n\r\n```py\r\nfrom datasets import load_dataset\r\nds = load_dataset(\"glue\", \"rte\")\r\n```","This appears to work. Thank you!\n\nOn Wed, Apr 27, 2022, 1:18 PM Steven Liu ***@***.***> wrote:\n\n> Hi :)\n>\n> I think your issue may be related to the older nlp library. I was able to\n> download glue with the latest version of datasets. Can you try updating\n> with:\n>\n> pip install -U datasets\n>\n> Then you can download:\n>\n> from datasets import load_datasetds = load_dataset(\"glue\", \"rte\")\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/huggingface\/datasets\/issues\/4241#issuecomment-1111267650>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/ACJUEKLUP2EL7ES3RRWJRPTVHFZHBANCNFSM5UPJBYXA>\n> .\n> You are receiving this because you authored the thread.Message ID:\n> ***@***.***>\n>\n"],"created_at":1651068861000,"updated_at":1651131927000,"closed_at":1651131927000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nI am trying to download the GLUE dataset from the NLP module but get an error (see below).\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport nlp\r\nnlp.__version__ # '0.2.0'\r\nnlp.load_dataset('glue', name=\"rte\", download_mode=\"force_redownload\")\r\n```\r\n\r\n## Expected results\r\nI expect the dataset to download without an error.\r\n\r\n## Actual results\r\n```\r\nINFO:nlp.load:Checking \/home\/richier\/.cache\/huggingface\/datasets\/5fe6ab0df8a32a3371b2e6a969d31d855a19563724fb0d0f163748c270c0ac60.2ea96febf19981fae5f13f0a43d4e2aa58bc619bc23acf06de66675f425a5538.py for additional imports.\r\nINFO:nlp.load:Found main folder for dataset https:\/\/s3.amazonaws.com\/datasets.huggingface.co\/nlp\/datasets\/glue\/glue.py at \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\r\nINFO:nlp.load:Found specific version folder for dataset https:\/\/s3.amazonaws.com\/datasets.huggingface.co\/nlp\/datasets\/glue\/glue.py at \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4\r\nINFO:nlp.load:Found script file from https:\/\/s3.amazonaws.com\/datasets.huggingface.co\/nlp\/datasets\/glue\/glue.py to \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4\/glue.py\r\nINFO:nlp.load:Found dataset infos file from https:\/\/s3.amazonaws.com\/datasets.huggingface.co\/nlp\/datasets\/glue\/dataset_infos.json to \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4\/dataset_infos.json\r\nINFO:nlp.load:Found metadata file for dataset https:\/\/s3.amazonaws.com\/datasets.huggingface.co\/nlp\/datasets\/glue\/glue.py at \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4\/glue.json\r\nINFO:nlp.info:Loading Dataset Infos from \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4\r\nINFO:nlp.builder:Generating dataset glue (\/home\/richier\/.cache\/huggingface\/datasets\/glue\/rte\/1.0.0)\r\nINFO:nlp.builder:Dataset not on Hf google storage. Downloading and preparing it from source\r\nINFO:nlp.utils.file_utils:Couldn't get ETag version for url https:\/\/firebasestorage.googleapis.com\/v0\/b\/mtl-sentence-representations.appspot.com\/o\/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb\r\nINFO:nlp.utils.file_utils:https:\/\/firebasestorage.googleapis.com\/v0\/b\/mtl-sentence-representations.appspot.com\/o\/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb not found in cache or force_download set to True, downloading to \/home\/richier\/.cache\/huggingface\/datasets\/downloads\/tmpldt3n805\r\nDownloading and preparing dataset glue\/rte (download: 680.81 KiB, generated: 1.83 MiB, total: 2.49 MiB) to \/home\/richier\/.cache\/huggingface\/datasets\/glue\/rte\/1.0.0...\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 73.0\/73.0 [00:00<00:00, 73.9kB\/s]\r\nINFO:nlp.utils.file_utils:storing https:\/\/firebasestorage.googleapis.com\/v0\/b\/mtl-sentence-representations.appspot.com\/o\/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb in cache at \/home\/richier\/.cache\/huggingface\/datasets\/downloads\/e8b62ee44e6f8b6aea761935928579ffe1aa55d161808c482e0725abbdcf9c64\r\nINFO:nlp.utils.file_utils:creating metadata file for \/home\/richier\/.cache\/huggingface\/datasets\/downloads\/e8b62ee44e6f8b6aea761935928579ffe1aa55d161808c482e0725abbdcf9c64\r\n---------------------------------------------------------------------------\r\nNonMatchingChecksumError                  Traceback (most recent call last)\r\n<ipython-input-7-669a8343dcc1> in <module>\r\n----> 1 nlp.load_dataset('glue', name=\"rte\", download_mode=\"force_redownload\")\r\n\r\n~\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    518         download_mode=download_mode,\r\n    519         ignore_verifications=ignore_verifications,\r\n--> 520         save_infos=save_infos,\r\n    521     )\r\n    522 \r\n\r\n~\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\r\n    418                 verify_infos = not save_infos and not ignore_verifications\r\n    419                 self._download_and_prepare(\r\n--> 420                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    421                 )\r\n    422                 # Sync info\r\n\r\n~\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    458         # Checksums verification\r\n    459         if verify_infos:\r\n--> 460             verify_checksums(self.info.download_checksums, dl_manager.get_recorded_sizes_checksums())\r\n    461         for split_generator in split_generators:\r\n    462             if str(split_generator.split_info.name).lower() == \"all\":\r\n\r\n~\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/utils\/info_utils.py in verify_checksums(expected_checksums, recorded_checksums)\r\n     34     bad_urls = [url for url in expected_checksums if expected_checksums[url] != recorded_checksums[url]]\r\n     35     if len(bad_urls) > 0:\r\n---> 36         raise NonMatchingChecksumError(str(bad_urls))\r\n     37     logger.info(\"All the checksums matched successfully.\")\r\n     38 \r\n\r\nNonMatchingChecksumError: ['https:\/\/firebasestorage.googleapis.com\/v0\/b\/mtl-sentence-representations.appspot.com\/o\/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb']\r\n```\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0\r\n- Platform: Linux-4.18.0-348.20.1.el8_5.x86_64-x86_64-with-redhat-8.5-Ootpa\r\n- Python version: 3.6.13\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.1.5\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4240","id":1217287594,"node_id":"PR_kwDODunzps423xRl","number":4240,"title":"Fix yield for crd3","user":{"login":"shanyas10","id":21066979,"node_id":"MDQ6VXNlcjIxMDY2OTc5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/21066979?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/shanyas10","html_url":"https:\/\/github.com\/shanyas10","followers_url":"https:\/\/api.github.com\/users\/shanyas10\/followers","following_url":"https:\/\/api.github.com\/users\/shanyas10\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/shanyas10\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/shanyas10\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/shanyas10\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/shanyas10\/orgs","repos_url":"https:\/\/api.github.com\/users\/shanyas10\/repos","events_url":"https:\/\/api.github.com\/users\/shanyas10\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/shanyas10\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["I don't think you need to generate new dummy data, since they're in the same format as the original data.\r\n\r\nThe CI is failing because of this error:\r\n```python\r\n>                       turn[\"names\"] = turn[\"NAMES\"]\r\nE                       TypeError: tuple indices must be integers or slices, not str\r\n```\r\n\r\nDo you know what could cause this ? If I understand correctly, `turn` is supposed to be a list of dictionaries right ?","> ```  \r\n>   \r\n> Do you know what could cause this ? If I understand correctly, turn is supposed to be a list of dictionaries right ?\r\n> ```\r\n\r\nThis is strange. Let me look into this. As per https:\/\/github.com\/RevanthRameshkumar\/CRD3\/blob\/master\/data\/aligned%20data\/c%3D2\/C1E001_2_0.json TURNS is a list of dictionaries. So when we iterate over `row[\"TURNS]\"` each `turn` is essentially a dictionary. Not sure why it's being considered a tuple here."],"created_at":1651062696000,"updated_at":1651236101000,"closed_at":1651236101000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4240","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4240","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4240.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4240.patch","merged_at":1651236101000},"body":"Modified the `_generate_examples` function to consider all the turns for a chunk id as a single example\r\nModified the features accordingly\r\n\r\n```\r\n\"turns\": [\r\n                        {\r\n                            \"names\": datasets.features.Sequence(datasets.Value(\"string\")),\r\n                            \"utterances\": datasets.features.Sequence(datasets.Value(\"string\")),\r\n                            \"number\": datasets.Value(\"int32\"),\r\n                        }\r\n                    ],\r\n                }\r\n```\r\n\r\nI wasn't able to run `datasets-cli dummy_data datasets` command. Is there a workaround for this? ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4239","id":1217269689,"node_id":"PR_kwDODunzps423tZr","number":4239,"title":"Small fixes in ROC AUC docs","user":{"login":"wschella","id":9478856,"node_id":"MDQ6VXNlcjk0Nzg4NTY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/9478856?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/wschella","html_url":"https:\/\/github.com\/wschella","followers_url":"https:\/\/api.github.com\/users\/wschella\/followers","following_url":"https:\/\/api.github.com\/users\/wschella\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/wschella\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/wschella\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/wschella\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/wschella\/orgs","repos_url":"https:\/\/api.github.com\/users\/wschella\/repos","events_url":"https:\/\/api.github.com\/users\/wschella\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/wschella\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651061750000,"updated_at":1651498137000,"closed_at":1651497723000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4239","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4239","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4239.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4239.patch","merged_at":1651497723000},"body":"The list of use cases did not render on GitHub with the prepended spacing.\r\nAdditionally, some typo's we're fixed.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4238","id":1217168123,"node_id":"I_kwDODunzps5IjIL7","number":4238,"title":"Dataset caching policy","user":{"login":"loretoparisi","id":163333,"node_id":"MDQ6VXNlcjE2MzMzMw==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/163333?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/loretoparisi","html_url":"https:\/\/github.com\/loretoparisi","followers_url":"https:\/\/api.github.com\/users\/loretoparisi\/followers","following_url":"https:\/\/api.github.com\/users\/loretoparisi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/loretoparisi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/loretoparisi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/loretoparisi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/loretoparisi\/orgs","repos_url":"https:\/\/api.github.com\/users\/loretoparisi\/repos","events_url":"https:\/\/api.github.com\/users\/loretoparisi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/loretoparisi\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi @loretoparisi, thanks for reporting.\r\n\r\nThere is an option to force the redownload of the data files (and thus not using previously download and cached data files): `load_dataset(..., download_mode=\"force_redownload\")`.\r\n\r\nPlease, let me know if this fixes your problem.\r\n\r\nI can confirm you that your dataset loads without any problem for me:\r\n```python\r\nIn [2]: ds = load_dataset(\"loretoparisi\/tatoeba-sentences\", data_files={\"train\": \"train.csv\", \"test\": \"test.csv\"}, delimiter=\"\\t\", column_names=['label', 'text'])\r\n\r\nIn [3]: ds\r\nOut[3]: \r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['label', 'text'],\r\n        num_rows: 8256449\r\n    })\r\n    test: Dataset({\r\n        features: ['label', 'text'],\r\n        num_rows: 2061204\r\n    })\r\n})\r\n``` ","@albertvillanova thank you, it seems it still does not work using:\r\n\r\n```python\r\nsentences = load_dataset(\r\n     \"loretoparisi\/tatoeba-sentences\",\r\n     data_files=data_files,\r\n     delimiter='\\t', \r\n     column_names=['label', 'text'],\r\n     download_mode=\"force_redownload\"\r\n)\r\n```\r\n[This](https:\/\/colab.research.google.com\/drive\/1EA6FWo5pHxU8rPHHRn24NlHqRPiOlPTr?usp=sharing) is my notebook!\r\n\r\nThe problem is that the download file's revision for `test.csv` is not correctly parsed\r\n\r\n![Schermata 2022-04-27 alle 18 09 41](https:\/\/user-images.githubusercontent.com\/163333\/165563507-0be53eb6-8f61-49b0-b959-306e59281de3.png)\r\n\r\nIf you download that file `test.csv` from the repo, the line `\\\\N` is not there anymore (it was there at the first file upload).\r\n\r\nMy impression is that the Apache Arrow file is still cached - so server side, despite of enabling a forced download. For what I can see I get those two arrow files, but I cannot grep the bad line (`\\\\N`) since are binary files:\r\n\r\n```\r\n!ls -l \/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-efeff8965c730a2c\/0.0.0\/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\r\n!ls -l \/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-efeff8965c730a2c\/0.0.0\/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\/csv-test.arrow\r\n!head \/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-efeff8965c730a2c\/0.0.0\/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\/dataset_info.json\r\n```\r\n","SOLVED! The problem was the with the file itself, using caching parameter helped indeed.\r\nThanks for helping!"],"created_at":1651056131000,"updated_at":1651076965000,"closed_at":1651076930000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nI cannot clean cache of my datasets files, despite I have updated the `csv` files on the repository [here](https:\/\/huggingface.co\/datasets\/loretoparisi\/tatoeba-sentences). The original file had a line with bad characters, causing the following error\r\n\r\n```\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/features\/features.py](https:\/\/localhost:8080\/#) in str2int(self, values)\r\n    852                 if value not in self._str2int:\r\n    853                     value = str(value).strip()\r\n--> 854                 output.append(self._str2int[str(value)])\r\n    855             else:\r\n    856                 # No names provided, try to integerize\r\n\r\nKeyError: '\\\\N'\r\n```\r\n\r\nThe file now is cleanup up, but I still get the error. This happens even if I inspect the local cached contents, and cleanup the files locally:\r\n\r\n```python\r\nfrom datasets import load_dataset_builder\r\ndataset_builder = load_dataset_builder(\"loretoparisi\/tatoeba-sentences\")\r\nprint(dataset_builder.cache_dir)\r\nprint(dataset_builder.info.features)\r\nprint(dataset_builder.info.splits)\r\n```\r\n\r\n```\r\nUsing custom data configuration loretoparisi--tatoeba-sentences-e59b8ad92f1bb8dd\r\n\/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-e59b8ad92f1bb8dd\/0.0.0\/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\r\nNone\r\nNone\r\n```\r\n\r\nand removing files located at `\/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-*`.\r\n Is there any remote file caching policy in place? If so, is it possibile to programmatically disable it? \r\nCurrently it seems that the file `test.csv` on the repo [here](https:\/\/huggingface.co\/datasets\/loretoparisi\/tatoeba-sentences\/blob\/main\/test.csv) is cached remotely. In fact I download locally the file from raw link, the file is up-to-date; but If I use it within `datasets` as shown above, it gives to me always the first revision of the file, not the last.\r\n\r\nThank you.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset,Features,Value,ClassLabel\r\n\r\nclass_names = [\"cmn\",\"deu\",\"rus\",\"fra\",\"eng\",\"jpn\",\"spa\",\"ita\",\"kor\",\"vie\",\"nld\",\"epo\",\"por\",\"tur\",\"heb\",\"hun\",\"ell\",\"ind\",\"ara\",\"arz\",\"fin\",\"bul\",\"yue\",\"swe\",\"ukr\",\"bel\",\"que\",\"ces\",\"swh\",\"nno\",\"wuu\",\"nob\",\"zsm\",\"est\",\"kat\",\"pol\",\"lat\",\"urd\",\"sqi\",\"isl\",\"fry\",\"afr\",\"ron\",\"fao\",\"san\",\"bre\",\"tat\",\"yid\",\"uig\",\"uzb\",\"srp\",\"qya\",\"dan\",\"pes\",\"slk\",\"eus\",\"cycl\",\"acm\",\"tgl\",\"lvs\",\"kaz\",\"hye\",\"hin\",\"lit\",\"ben\",\"cat\",\"bos\",\"hrv\",\"tha\",\"orv\",\"cha\",\"mon\",\"lzh\",\"scn\",\"gle\",\"mkd\",\"slv\",\"frm\",\"glg\",\"vol\",\"ain\",\"jbo\",\"tok\",\"ina\",\"nds\",\"mal\",\"tlh\",\"roh\",\"ltz\",\"oss\",\"ido\",\"gla\",\"mlt\",\"sco\",\"ast\",\"jav\",\"oci\",\"ile\",\"ota\",\"xal\",\"tel\",\"sjn\",\"nov\",\"khm\",\"tpi\",\"ang\",\"aze\",\"tgk\",\"tuk\",\"chv\",\"hsb\",\"dsb\",\"bod\",\"sme\",\"cym\",\"mri\",\"ksh\",\"kmr\",\"ewe\",\"kab\",\"ber\",\"tpw\",\"udm\",\"lld\",\"pms\",\"lad\",\"grn\",\"mlg\",\"xho\",\"pnb\",\"grc\",\"hat\",\"lao\",\"npi\",\"cor\",\"nah\",\"avk\",\"mar\",\"guj\",\"pan\",\"kir\",\"myv\",\"prg\",\"sux\",\"crs\",\"ckt\",\"bak\",\"zlm\",\"hil\",\"cbk\",\"chr\",\"nav\",\"lkt\",\"enm\",\"arq\",\"lin\",\"abk\",\"pcd\",\"rom\",\"gsw\",\"tam\",\"zul\",\"awa\",\"wln\",\"amh\",\"bar\",\"hbo\",\"mhr\",\"bho\",\"mrj\",\"ckb\",\"osx\",\"pfl\",\"mgm\",\"sna\",\"mah\",\"hau\",\"kan\",\"nog\",\"sin\",\"glv\",\"dng\",\"kal\",\"liv\",\"vro\",\"apc\",\"jdt\",\"fur\",\"che\",\"haw\",\"yor\",\"crh\",\"pdc\",\"ppl\",\"kin\",\"shs\",\"mnw\",\"tet\",\"sah\",\"kum\",\"ngt\",\"nya\",\"pus\",\"hif\",\"mya\",\"moh\",\"wol\",\"tir\",\"ton\",\"lzz\",\"oar\",\"lug\",\"brx\",\"non\",\"mww\",\"hak\",\"nlv\",\"ngu\",\"bua\",\"aym\",\"vec\",\"ibo\",\"tkl\",\"bam\",\"kha\",\"ceb\",\"lou\",\"fuc\",\"smo\",\"gag\",\"lfn\",\"arg\",\"umb\",\"tyv\",\"kjh\",\"oji\",\"cyo\",\"urh\",\"kzj\",\"pam\",\"srd\",\"lmo\",\"swg\",\"mdf\",\"gil\",\"snd\",\"tso\",\"sot\",\"zza\",\"tsn\",\"pau\",\"som\",\"egl\",\"ady\",\"asm\",\"ori\",\"dtp\",\"cho\",\"max\",\"kam\",\"niu\",\"sag\",\"ilo\",\"kaa\",\"fuv\",\"nch\",\"hoc\",\"iba\",\"gbm\",\"sun\",\"war\",\"mvv\",\"pap\",\"ary\",\"kxi\",\"csb\",\"pag\",\"cos\",\"rif\",\"kek\",\"krc\",\"aii\",\"ban\",\"ssw\",\"tvl\",\"mfe\",\"tah\",\"bvy\",\"bcl\",\"hnj\",\"nau\",\"nst\",\"afb\",\"quc\",\"min\",\"tmw\",\"mad\",\"bjn\",\"mai\",\"cjy\",\"got\",\"hsn\",\"gan\",\"tzl\",\"dws\",\"ldn\",\"afh\",\"sgs\",\"krl\",\"vep\",\"rue\",\"tly\",\"mic\",\"ext\",\"izh\",\"sma\",\"jam\",\"cmo\",\"mwl\",\"kpv\",\"koi\",\"bis\",\"ike\",\"run\",\"evn\",\"ryu\",\"mnc\",\"aoz\",\"otk\",\"kas\",\"aln\",\"akl\",\"yua\",\"shy\",\"fkv\",\"gos\",\"fij\",\"thv\",\"zgh\",\"gcf\",\"cay\",\"xmf\",\"tig\",\"div\",\"lij\",\"rap\",\"hrx\",\"cpi\",\"tts\",\"gaa\",\"tmr\",\"iii\",\"ltg\",\"bzt\",\"syc\",\"emx\",\"gom\",\"chg\",\"osp\",\"stq\",\"frr\",\"fro\",\"nys\",\"toi\",\"new\",\"phn\",\"jpa\",\"rel\",\"drt\",\"chn\",\"pli\",\"laa\",\"bal\",\"hdn\",\"hax\",\"mik\",\"ajp\",\"xqa\",\"pal\",\"crk\",\"mni\",\"lut\",\"ayl\",\"ood\",\"sdh\",\"ofs\",\"nus\",\"kiu\",\"diq\",\"qxq\",\"alt\",\"bfz\",\"klj\",\"mus\",\"srn\",\"guc\",\"lim\",\"zea\",\"shi\",\"mnr\",\"bom\",\"sat\",\"szl\"]\r\nfeatures = Features({ 'label': ClassLabel(names=class_names), 'text': Value('string')})\r\nnum_labels = features['label'].num_classes\r\ndata_files = { \"train\": \"train.csv\", \"test\": \"test.csv\" }\r\nsentences = load_dataset(\r\n     \"loretoparisi\/tatoeba-sentences\",\r\n     data_files=data_files,\r\n     delimiter='\\t', \r\n     column_names=['label', 'text'],\r\n)\r\n# You can make this part faster with num_proc=<some int>\r\nsentences = sentences.map(lambda ex: {\"label\" : features[\"label\"].str2int(ex[\"label\"]) if ex[\"label\"] is not None else None}, features=features)\r\nsentences = sentences.shuffle()\r\n```\r\n\r\n## Expected results\r\nProperly tokenize dataset file `test.csv` without issues.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n```\r\nDownloading data files: 100%\r\n2\/2 [00:16<00:00, 7.34s\/it]\r\nDownloading data: 100%\r\n391M\/391M [00:12<00:00, 36.6MB\/s]\r\nDownloading data: 100%\r\n92.4M\/92.4M [00:02<00:00, 40.0MB\/s]\r\nExtracting data files: 100%\r\n2\/2 [00:00<00:00, 47.66it\/s]\r\nDataset csv downloaded and prepared to \/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-efeff8965c730a2c\/0.0.0\/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\r\n100%\r\n2\/2 [00:00<00:00, 25.94it\/s]\r\n11%\r\n942339\/8256449 [01:55<13:11, 9245.85ex\/s]\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n[<ipython-input-3-6a9867fad8d6>](https:\/\/localhost:8080\/#) in <module>()\r\n     12 )\r\n     13 # You can make this part faster with num_proc=<some int>\r\n---> 14 sentences = sentences.map(lambda ex: {\"label\" : features[\"label\"].str2int(ex[\"label\"]) if ex[\"label\"] is not None else None}, features=features)\r\n     15 sentences = sentences.shuffle()\r\n\r\n10 frames\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/features\/features.py](https:\/\/localhost:8080\/#) in str2int(self, values)\r\n    852                 if value not in self._str2int:\r\n    853                     value = str(value).strip()\r\n--> 854                 output.append(self._str2int[str(value)])\r\n    855             else:\r\n    856                 # No names provided, try to integerize\r\n\r\nKeyError: '\\\\N'\r\n```\r\n\r\n## Environment info\r\n```\r\n- `datasets` version: 2.1.0\r\n- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.3.5\r\n- ```\r\n\r\n\r\n```\r\n- `transformers` version: 4.18.0\r\n- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- Huggingface_hub version: 0.5.1\r\n- PyTorch version (GPU?): 1.11.0+cu113 (True)\r\n- Tensorflow version (GPU?): 2.8.0 (True)\r\n- Flax version (CPU?\/GPU?\/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n- ```\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4237","id":1217121044,"node_id":"I_kwDODunzps5Ii8sU","number":4237,"title":"Common Voice 8 doesn't show datasets viewer","user":{"login":"patrickvonplaten","id":23423619,"node_id":"MDQ6VXNlcjIzNDIzNjE5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/23423619?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/patrickvonplaten","html_url":"https:\/\/github.com\/patrickvonplaten","followers_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/followers","following_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/orgs","repos_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/repos","events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/received_events","type":"User","site_admin":false},"labels":[{"id":3470211881,"node_id":"LA_kwDODunzps7O1zsp","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer","name":"dataset-viewer","color":"E5583E","default":false,"description":"Related to the dataset viewer on huggingface.co"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Thanks for reporting. I understand it's an error in the dataset script. To reproduce:\r\n\r\n```python\r\n>>> import datasets as ds\r\n>>> split_names = ds.get_dataset_split_names(\"mozilla-foundation\/common_voice_8_0\", use_auth_token=\"**********\")\r\nDownloading builder script: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.9k\/10.9k [00:00<00:00, 10.9MB\/s]\r\nDownloading extra modules: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.98k\/2.98k [00:00<00:00, 3.36MB\/s]\r\nDownloading extra modules: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 53.1k\/53.1k [00:00<00:00, 650kB\/s]\r\nNo config specified, defaulting to: common_voice\/en\r\nTraceback (most recent call last):\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/libs\/libmodels\/.venv\/lib\/python3.9\/site-packages\/datasets\/inspect.py\", line 280, in get_dataset_config_info\r\n    for split_generator in builder._split_generators(\r\n  File \"\/home\/slesage\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/mozilla-foundation--common_voice_8_0\/720589e6e5ad674019008b719053303a71716db1b27e63c9846df02fdf93f2f3\/common_voice_8_0.py\", line 153, in _split_generators\r\n    self._log_download(self.config.name, bundle_version, hf_auth_token)\r\n  File \"\/home\/slesage\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/mozilla-foundation--common_voice_8_0\/720589e6e5ad674019008b719053303a71716db1b27e63c9846df02fdf93f2f3\/common_voice_8_0.py\", line 139, in _log_download\r\n    email = HfApi().whoami(auth_token)[\"email\"]\r\nKeyError: 'email'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/libs\/libmodels\/.venv\/lib\/python3.9\/site-packages\/datasets\/inspect.py\", line 323, in get_dataset_split_names\r\n    info = get_dataset_config_info(\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/libs\/libmodels\/.venv\/lib\/python3.9\/site-packages\/datasets\/inspect.py\", line 285, in get_dataset_config_info\r\n    raise SplitsNotFoundError(\"The split names could not be parsed from the dataset config.\") from err\r\ndatasets.inspect.SplitsNotFoundError: The split names could not be parsed from the dataset config.\r\n```","Thanks for reporting @patrickvonplaten and thanks for the investigation @severo.\r\n\r\nUnfortunately I'm not able to reproduce the error.\r\n\r\nI think the error has to do with authentication with `huggingface_hub`, because the exception is thrown from these code lines: https:\/\/huggingface.co\/datasets\/mozilla-foundation\/common_voice_8_0\/blob\/main\/common_voice_8_0.py#L137-L139\r\n```python\r\nfrom huggingface_hub import HfApi, HfFolder\r\n\r\nif isinstance(auth_token, bool):\r\n    email = HfApi().whoami(auth_token)\r\nemail = HfApi().whoami(auth_token)[\"email\"]\r\n```\r\n\r\nCould you please verify the previous code with the `auth_token` you pass to `load_dataset(..., use_auth_token=auth_token,...`?","OK, thanks for digging a bit into it. Indeed, the error occurs with the dataset-viewer, but not with a normal user token, because we use an app token, and it does not have a related email!\r\n\r\n```python\r\n>>> from huggingface_hub import HfApi, HfFolder\r\n>>> auth_token = \"hf_app_******\"\r\n>>> t = HfApi().whoami(auth_token)\r\n>>> t\r\n{'type': 'app', 'name': 'dataset-preview-backend'}\r\n>>> t[\"email\"]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nKeyError: 'email'\r\n```\r\n\r\nNote also that the doc (https:\/\/huggingface.co\/docs\/huggingface_hub\/package_reference\/hf_api#huggingface_hub.HfApi.whoami) does not state that `whoami` should return an `email` key.\r\n\r\n@SBrandeis @julien-c: do you think the app token should have an email associated, like the users?","We can workaround this with\r\n```python\r\nemail = HfApi().whoami(auth_token).get(\"email\", \"system@huggingface.co\")\r\n```\r\nin the common voice scripts","Hmmm, does this mean that any person who downloads the common voice dataset will be logged as \"system@huggingface.co\"? If so, it would defeat the purpose of sending the user's email to the commonvoice API, right?","I agree with @severo: we cannot set our system email as default, allowing anybody not authenticated to by-pass the Common Voice usage policy.\r\n\r\nAdditionally, looking at the code, I think we should implement a more robust way to send user email to Common Voice: currently anybody can tweak the script and send somebody else email instead.\r\n\r\nCC: @patrickvonplaten @lhoestq @SBrandeis @julien-c ","Hmm I don't agree here. \r\n\r\nAnybody can always just bypass the system by setting whatever email. As soon as someone has access to the downloading script it's trivial to tweak the code to not send the \"correct\" email but to just whatever and it would work.\r\n\r\nNote that someone only has visibility on the code after having \"signed\" the access-mechanism so I think we can expect the users to have agreed to not do anything malicious. \r\n\r\nI'm fine with both @lhoestq's solution or we find a way that forces the user to be logged in + being able to load the data for the datasets viewer. Wdyt @lhoestq @severo @albertvillanova ?","> Additionally, looking at the code, I think we should implement a more robust way to send user email to Common Voice: currently anybody can tweak the script and send somebody else email instead.\r\n\r\nYes, I agree we can forget about this @patrickvonplaten. After having had a look at Common Voice website, I've seen they only require sending an email (no auth is inplace on their side, contrary to what I had previously thought). Therefore, currently we impose stronger requirements than them: we require the user having logged in and accepted the access mechanism.\r\n\r\nCurrently the script as it is already requires the user being logged in:\r\n```python\r\nHfApi().whoami(auth_token)\r\n```\r\nthrows an exception if None\/invalid auth_token is passed.\r\n\r\nOn the other hand, we should agree on the way to allow the viewer to stream the data.","The preview is back now, thanks !"],"created_at":1651053920000,"updated_at":1652185025000,"closed_at":1652185024000,"author_association":"MEMBER","active_lock_reason":null,"draft":null,"pull_request":null,"body":"https:\/\/huggingface.co\/datasets\/mozilla-foundation\/common_voice_8_0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4236","id":1217115691,"node_id":"PR_kwDODunzps423MOc","number":4236,"title":"Replace data URL in big_patent dataset and support streaming","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","I first uploaded the data files to the Hub: I think it is a good option because we have git lfs to track versions and changes. Moreover people will be able to make PRs to propose updates on the data files.\r\n- I would have preferred to upload it it to the \"data\" org namespace, but it is already taken (although not used): might be possible to take it?\r\n\r\nAs an alternative (and to be consistent with previous datasets), I also uploaded the data files to our AWS bucket.\r\n\r\nWe should decide which to use (now and for future datasets) and set it here before merging. We should remove the data files for the non-chosen option.\r\n\r\nCC: @lhoestq @mariosasko @polinaeterna ","Would it make sense to make the dataset a community one (so, create an organization for it) and store the script and the data in a single repository? Just as it is for most of the datasets. That way we can also access the data using a relative path inside the repo (that's not the point though). The point is that to me it seems a bit more straightforward to store everything in one place. \r\n\r\nI guess the strong argument against this logic is that in this case the canonical version won't work... But maybe there is some redirecting mechanism I don't know about? :)\r\n\r\nAnyway, I'm in favor of hosting data on the Hub instead of AWS :) ","I also think storing everything in one place\/single repository is the best option.\r\n\r\n@polinaeterna Canonical datasets also support data files (see the [`red_caps` repo](https:\/\/huggingface.co\/datasets\/red_caps\/tree\/main) for instance) ","Thanks @polinaeterna and @mariosasko for your comments.\r\n\r\nYes, definitely it is much better to have everything in the same repo. \r\n\r\nI'm transferring their data files to the Hub under \"big_patent\" and deleting them from the other repo and AWS."],"created_at":1651053673000,"updated_at":1651826314000,"closed_at":1651515675000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4236","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4236","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4236.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4236.patch","merged_at":1651515675000},"body":"This PR replaces the Google Drive URL with by our Hub one, once the data owners have approved to host their data on the Hub.\r\n\r\nMoreover, this PR makes the dataset streamable.\r\n\r\nFix #4217.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4235","id":1216952640,"node_id":"I_kwDODunzps5IiTlA","number":4235,"title":"How to load VERY LARGE dataset?","user":{"login":"CaoYiqingT","id":45160643,"node_id":"MDQ6VXNlcjQ1MTYwNjQz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/45160643?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/CaoYiqingT","html_url":"https:\/\/github.com\/CaoYiqingT","followers_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/followers","following_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/orgs","repos_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/repos","events_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The `Trainer` support `IterableDataset`, not just datasets."],"created_at":1651045813000,"updated_at":1651059857000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"### System Info\n\n```shell\nI am using transformer trainer while meeting the issue.\r\nThe trainer requests torch.utils.data.Dataset as input, which loads the whole dataset into the memory at once. Therefore, when the dataset is too large to load, there's nothing I can do except using IterDataset, which loads samples of data seperately, and results in low efficiency. \r\nI wonder if there are any tricks like Sharding in huggingface trainer.\r\nLooking forward to your reply.\n```\n\n\n### Who can help?\n\nTrainer: @sgugger\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nNone\n\n### Expected behavior\n\n```shell\nI wonder if there are any tricks like fairseq Sharding very large datasets https:\/\/fairseq.readthedocs.io\/en\/latest\/getting_started.html.\r\nThanks a lot!\n```\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4234","id":1216818846,"node_id":"PR_kwDODunzps422Mwn","number":4234,"title":"Autoeval config","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Related to: https:\/\/github.com\/huggingface\/autonlp-backend\/issues\/414 and https:\/\/github.com\/huggingface\/autonlp-backend\/issues\/424","The tests are failing due to the changed metadata:\r\n\r\n```\r\ngot an unexpected keyword argument 'train-eval-index'\r\n```\r\n\r\nI think you can fix this by updating the `DatasetMetadata` class and implementing an appropriate `validate_train_eval_index()` function\r\n\r\n@lhoestq we are working with an arbitrary set of tags for `autoeval config`. See https:\/\/github.com\/huggingface\/autonlp-backend\/issues\/414\r\nI need to add a validator function though for the tests to pass. Our set is not well-defined as in the rest https:\/\/github.com\/huggingface\/datasets\/tree\/master\/src\/datasets\/utils\/resources. What's a workaround for this?","On the question of validating the `train-eval-index` metadata, I think the simplest approach would be to validate that the required fields exist and not worry about their values (which are open-ended).\r\n\r\nFor me, the required fields include:\r\n\r\n* `config`\r\n* `task`\r\n* `task_id`\r\n* `splits` (train \/ validation \/ eval)\r\n* `col_mapping`\r\n* `metrics` (checking that each one has `type`, `name`) \r\n\r\nHere I'm using the spec defined in https:\/\/github.com\/huggingface\/autonlp-backend\/issues\/414 as a guide.\r\n\r\nWDYT @lhoestq ?","Makes sense ! Currently the metadata type validator doesn't support subfields - let me open a PR to add it","I ended up improving the metadata validation in this PR x)\r\n\r\nIn particular:\r\n- I added support YAML keys with dashes instead of underscores for `train-eval-index`\r\n- I added `train-eval-index` validation  with `validate_train_eval_index`. It does nothing fancy, it just checks that it is a list if it exists in the YAML, but feel free to improve it if you want\r\n\r\nLet me know if it sounds good to you ! I think we can improve `validate_train_eval_index` in another PR","Come on windows... I didn't do anything advanced...\r\n\r\nAnyway, will try to fix this when I get back home x)","> Come on windows... I didn't do anything advanced...\r\n> \r\n> Anyway, will try to fix this when I get back home x)\r\n\r\nHehe, thanks!","Thanks, @lhoestq this is great! ","Did I just fix it for windows and now it fails on linux ? xD","> Did I just fix it for windows and now it fails on linux ? xD\r\n\r\nLooks like the Heisenberg uncertainty principle is at play here - you cannot simultaneously have unit tests passing in both Linux and Windows \ud83d\ude05 ","The worst is that the tests pass locally both on my windows and my linux x)","Ok fixed it, the issue came from python 3.6 that doesn't return the right `__origin__` for Dict and List types","> Alright thanks for adding the first Autoeval config ! :D\r\n\r\nWoohoo! Thank you so much \ud83e\udd17 ","This is cool!"],"created_at":1651037530000,"updated_at":1651843231000,"closed_at":1651774858000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4234","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4234","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4234.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4234.patch","merged_at":1651774858000},"body":"Added autoeval config to imdb as pilot","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4233","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4233\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4233\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4233\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4233","id":1216665044,"node_id":"PR_kwDODunzps421r-6","number":4233,"title":"Autoeval","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4233). All of your documentation changes will be reflected on that endpoint."],"created_at":1651023129000,"updated_at":1651037370000,"closed_at":1651023143000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4233","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4233","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4233.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4233.patch","merged_at":null},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4233\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4233\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4232","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4232\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4232\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4232\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4232","id":1216659444,"node_id":"PR_kwDODunzps421qz4","number":4232,"title":"adding new tag to tasks.json and modified for existing datasets","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","closing in favor of https:\/\/github.com\/huggingface\/datasets\/pull\/4244"],"created_at":1651022469000,"updated_at":1651587836000,"closed_at":1651587399000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4232","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4232","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4232.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4232.patch","merged_at":null},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4232\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4232\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4231","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4231\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4231\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4231\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4231","id":1216651960,"node_id":"PR_kwDODunzps421pUX","number":4231,"title":"Fix invalid url to CC-Aligned dataset","user":{"login":"juntang-zhuang","id":44451229,"node_id":"MDQ6VXNlcjQ0NDUxMjI5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/44451229?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/juntang-zhuang","html_url":"https:\/\/github.com\/juntang-zhuang","followers_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/followers","following_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/orgs","repos_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/repos","events_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1651021621000,"updated_at":1651021689000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4231","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4231","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4231.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4231.patch","merged_at":null},"body":"The CC-Aligned dataset url has changed to  https:\/\/data.statmt.org\/cc-aligned\/, the old address http:\/\/www.statmt.org\/cc-aligned\/ is no longer valid","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4231\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4231\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4230","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4230\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4230\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4230\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4230","id":1216643661,"node_id":"I_kwDODunzps5IhIJN","number":4230,"title":"Why the `conll2003` dataset on huggingface only contains the `en` subset? Where is the  German data?","user":{"login":"beyondguo","id":37113676,"node_id":"MDQ6VXNlcjM3MTEzNjc2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/37113676?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/beyondguo","html_url":"https:\/\/github.com\/beyondguo","followers_url":"https:\/\/api.github.com\/users\/beyondguo\/followers","following_url":"https:\/\/api.github.com\/users\/beyondguo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/beyondguo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/beyondguo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/beyondguo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/beyondguo\/orgs","repos_url":"https:\/\/api.github.com\/users\/beyondguo\/repos","events_url":"https:\/\/api.github.com\/users\/beyondguo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/beyondguo\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Thanks for reporting @beyondguo.\r\n\r\nIndeed, we generate this dataset from this raw data file URL: https:\/\/data.deepai.org\/conll2003.zip\r\nAnd that URL only contains the English version.","The German data requires payment\r\n\r\nThe [original task page](https:\/\/www.clips.uantwerpen.be\/conll2003\/ner\/) states \"The German data is a collection of articles from the Frankfurter Rundschau. The named entities have been annotated by people of the University of Antwerp. Only the annotations are available here. In order to build these data sets you need access to the ECI Multilingual Text Corpus. It can be ordered from the Linguistic Data Consortium (2003 non-member price: US$ 35.00).\"\r\n\r\nInflation since 2003 has also affected LDC's prices, and today the dataset [LDC94T5](https:\/\/catalog.ldc.upenn.edu\/LDC94T5) is available under license for $75 a copy. The [license](https:\/\/catalog.ldc.upenn.edu\/license\/eci-slash-mci-user-agreement.pdf) includes a non-distribution condition, which is probably why the data has not turned up openly.\r\n\r\nThe ACL hold copyright of this data; I'll mail them and anyone I can find at ECI to see if they'll open this up now. After all, it worked with Microsoft 3DMM, why not here too, after 28 years? :)\r\n"],"created_at":1651020832000,"updated_at":1652358222000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"![image](https:\/\/user-images.githubusercontent.com\/37113676\/165416606-96b5db18-b16c-4b6b-928c-de8620fd943e.png)\r\n\r\nBut on huggingface datasets:\r\n![image](https:\/\/user-images.githubusercontent.com\/37113676\/165416649-8fd77980-ca0d-43f0-935e-f398ba8323a4.png)\r\n\r\nWhere is the  German data?","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4230\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4230\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4229","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4229\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4229\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4229\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4229","id":1216638968,"node_id":"PR_kwDODunzps421mjM","number":4229,"title":"new task tag","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1651020428000,"updated_at":1651020508000,"closed_at":1651020497000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4229","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4229","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4229.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4229.patch","merged_at":null},"body":"multi-input-text-classification tag for classification datasets that take more than one input","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4229\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4229\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4228","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4228\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4228\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4228\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4228","id":1216523043,"node_id":"PR_kwDODunzps421NKL","number":4228,"title":"new task tag","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1651010433000,"updated_at":1651020511000,"closed_at":1651020391000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4228","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4228","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4228.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4228.patch","merged_at":null},"body":"multi-input-text-classification tag for classification datasets that take more than one input","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4228\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4228\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4227","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4227\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4227\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4227\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4227","id":1216455316,"node_id":"PR_kwDODunzps420-mc","number":4227,"title":"Add f1 metric card, update docstring in py file","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651005663000,"updated_at":1651582223000,"closed_at":1651581813000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4227","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4227","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4227.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4227.patch","merged_at":1651581813000},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4227\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4227\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4226","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4226\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4226\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4226\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4226","id":1216331073,"node_id":"PR_kwDODunzps420kAv","number":4226,"title":"Add pearsonr mc, update functionality to match the original docs","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","thank you @lhoestq!! :hugs: "],"created_at":1650997846000,"updated_at":1651597764000,"closed_at":1651597348000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4226","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4226","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4226.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4226.patch","merged_at":1651597348000},"body":"- adds pearsonr metric card\r\n- adds ability to return p-value\r\n    - p-value was mentioned in the original docs as a return value, but there was no option to return it. I updated the _compute function slightly to have an option to return the p-value.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4226\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4226\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4225","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4225\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4225\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4225\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4225","id":1216213464,"node_id":"PR_kwDODunzps420LNM","number":4225,"title":"autoeval config","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1650991114000,"updated_at":1651020511000,"closed_at":1651010426000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4225","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4225","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4225.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4225.patch","merged_at":null},"body":"add train eval index for autoeval","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4225\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4225\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4224","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4224\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4224\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4224\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4224","id":1216209667,"node_id":"PR_kwDODunzps420KX2","number":4224,"title":"autoeval config","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1650990919000,"updated_at":1650991005000,"closed_at":1650991005000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4224","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4224","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4224.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4224.patch","merged_at":null},"body":"add train eval index for autoeval","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4224\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4224\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4223","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4223\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4223\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4223\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4223","id":1216107082,"node_id":"PR_kwDODunzps42z0YV","number":4223,"title":"Add Accuracy Metric Card","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650985846000,"updated_at":1651588065000,"closed_at":1651587647000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4223","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4223","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4223.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4223.patch","merged_at":1651587647000},"body":"- adds accuracy metric card\r\n- updates docstring in accuracy.py\r\n- adds .json file with metric card and docstring information","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4223\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4223\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4222","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4222\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4222\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4222\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4222","id":1216056439,"node_id":"PR_kwDODunzps42zpcd","number":4222,"title":"Fix description links in dataset cards","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Non passing tests are due to other pre-existing errors in dataset cards: not related to this PR."],"created_at":1650983785000,"updated_at":1651826318000,"closed_at":1650991949000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4222","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4222","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4222.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4222.patch","merged_at":1650991949000},"body":"I noticed many links were not properly displayed (only text, no link) on the Hub because of wrong syntax, e.g.: https:\/\/huggingface.co\/datasets\/big_patent\r\n\r\nThis PR fixes all description links in dataset cards.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4222\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4222\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4221","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4221\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4221\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4221\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4221","id":1215911182,"node_id":"I_kwDODunzps5IeVUO","number":4221,"title":"Dictionary Feature","user":{"login":"jordiae","id":2944532,"node_id":"MDQ6VXNlcjI5NDQ1MzI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/2944532?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jordiae","html_url":"https:\/\/github.com\/jordiae","followers_url":"https:\/\/api.github.com\/users\/jordiae\/followers","following_url":"https:\/\/api.github.com\/users\/jordiae\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jordiae\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jordiae\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jordiae\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jordiae\/orgs","repos_url":"https:\/\/api.github.com\/users\/jordiae\/repos","events_url":"https:\/\/api.github.com\/users\/jordiae\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jordiae\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892912,"node_id":"MDU6TGFiZWwxOTM1ODkyOTEy","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/question","name":"question","color":"d876e3","default":true,"description":"Further information is requested"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi @jordiae,\r\n\r\nInstead of the `Sequence` feature, you can use just a regular list: put the dict between `[` and `]`:\r\n```python\r\n\"list_of_dict_feature\": [\r\n    {\r\n        \"key1_in_dict\": datasets.Value(\"string\"),\r\n        \"key2_in_dict\": datasets.Value(\"int32\"),\r\n        ...\r\n    }\r\n],\r\n```\r\n\r\nFeel free to re-open this issue if that does not work for your use case.","> Hi @jordiae,\r\n> \r\n> Instead of the `Sequence` feature, you can use just a regular list: put the dict between `[` and `]`:\r\n> \r\n> ```python\r\n> \"list_of_dict_feature\": [\r\n>     {\r\n>         \"key1_in_dict\": datasets.Value(\"string\"),\r\n>         \"key2_in_dict\": datasets.Value(\"int32\"),\r\n>         ...\r\n>     }\r\n> ],\r\n> ```\r\n> \r\n> Feel free to re-open this issue if that does not work for your use case.\r\n\r\nThank you"],"created_at":1650977418000,"updated_at":1651243939000,"closed_at":1651165498000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"Hi, I'm trying to create the loading script for a dataset in which one feature is a list of dictionaries, which afaik doesn't fit very well the values and structures supported by Value and Sequence. Is there any suggested workaround, am I missing something?\r\n\r\nThank you in advance.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4221\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4221\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4220","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4220\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4220\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4220\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4220","id":1215225802,"node_id":"PR_kwDODunzps42w5YO","number":4220,"title":"Altered faiss installation comment","user":{"login":"vishalsrao","id":36671559,"node_id":"MDQ6VXNlcjM2NjcxNTU5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36671559?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/vishalsrao","html_url":"https:\/\/github.com\/vishalsrao","followers_url":"https:\/\/api.github.com\/users\/vishalsrao\/followers","following_url":"https:\/\/api.github.com\/users\/vishalsrao\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/vishalsrao\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/vishalsrao\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/vishalsrao\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/vishalsrao\/orgs","repos_url":"https:\/\/api.github.com\/users\/vishalsrao\/repos","events_url":"https:\/\/api.github.com\/users\/vishalsrao\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/vishalsrao\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Hi ! Can you explain why this change is needed ?","Facebook recommends installing FAISS using conda (https:\/\/github.com\/facebookresearch\/faiss\/blob\/main\/INSTALL.md). pip does not seem to have the latest version of FAISS. The latest version of faiss is 1.7.2 (https:\/\/anaconda.org\/conda-forge\/faiss), but the latest one available through pip is 1.5.3 (https:\/\/pypi.org\/project\/faiss\/). "],"created_at":1650936043000,"updated_at":1652117374000,"closed_at":1652116929000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4220","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4220","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4220.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4220.patch","merged_at":1652116929000},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4220\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4220\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4219","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4219\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4219\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4219\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4219","id":1214934025,"node_id":"PR_kwDODunzps42v6rE","number":4219,"title":"Add F1 Metric Card","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650914096000,"updated_at":1651005858000,"closed_at":1651005466000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4219","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4219","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4219.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4219.patch","merged_at":null},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4219\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4219\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4218","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4218\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4218\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4218\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4218","id":1214748226,"node_id":"PR_kwDODunzps42vTA0","number":4218,"title":"Make code for image downloading from image urls cacheable","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650903479000,"updated_at":1650992424000,"closed_at":1650980306000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4218","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4218","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4218.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4218.patch","merged_at":1650980306000},"body":"Fix #4199 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4218\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4218\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4217","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4217\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4217\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4217\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4217","id":1214688141,"node_id":"I_kwDODunzps5IZquN","number":4217,"title":"Big_Patent dataset broken","user":{"login":"Matthew-Larsen","id":54189843,"node_id":"MDQ6VXNlcjU0MTg5ODQz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/54189843?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Matthew-Larsen","html_url":"https:\/\/github.com\/Matthew-Larsen","followers_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/followers","following_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/orgs","repos_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/repos","events_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/received_events","type":"User","site_admin":false},"labels":[{"id":4069435429,"node_id":"LA_kwDODunzps7yjqgl","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/hosted-on-google-drive","name":"hosted-on-google-drive","color":"8B51EF","default":false,"description":""}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Thanks for reporting. The issue seems not to be directly related to the dataset viewer or the `datasets` library, but instead to it being hosted on Google Drive.\r\n\r\nSee related issues: https:\/\/github.com\/huggingface\/datasets\/issues?q=is%3Aissue+is%3Aopen+drive.google.com\r\n\r\nTo quote [@lhoestq](https:\/\/github.com\/huggingface\/datasets\/issues\/4075#issuecomment-1087362551):\r\n\r\n> PS: if possible, please try to not use Google Drive links in your dataset script, since Google Drive has download quotas and is not always reliable.\r\n\r\n","We should find out if the dataset license allows redistribution and contact the data owners to propose them to host their data on our Hub.","The data owners have agreed on hosting their data on the Hub."],"created_at":1650900705000,"updated_at":1651515675000,"closed_at":1651515675000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Dataset viewer issue for '*big_patent*'\r\n\r\n**Link:** *[link to the dataset viewer page](https:\/\/huggingface.co\/datasets\/big_patent\/viewer\/all\/train)*\r\n\r\n*Unable to view because it says FileNotFound, also cannot download it through the python API*\r\n\r\nAm I the one who added this dataset ? No\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4217\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4217\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4216","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4216\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4216\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4216\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4216","id":1214614029,"node_id":"PR_kwDODunzps42u1_w","number":4216,"title":"Avoid recursion error in map if example is returned as dict value","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650897632000,"updated_at":1651684806000,"closed_at":1651684372000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4216","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4216","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4216.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4216.patch","merged_at":1651684372000},"body":"I noticed this bug while answering [this question](https:\/\/discuss.huggingface.co\/t\/correct-way-to-create-a-dataset-from-a-csv-file\/15686\/11?u=mariosasko). \r\n\r\nThis code replicates the bug:\r\n```python\r\nfrom datasets import Dataset\r\ndset = Dataset.from_dict({\"en\": [\"aa\", \"bb\"], \"fr\": [\"cc\", \"dd\"]})\r\ndset.map(lambda ex: {\"translation\": ex})\r\n```\r\nand this is the fix for it (before this PR):\r\n```python\r\nfrom datasets import Dataset\r\ndset = Dataset.from_dict({\"en\": [\"aa\", \"bb\"], \"fr\": [\"cc\", \"dd\"]})\r\ndset.map(lambda ex: {\"translation\": dict(ex)})\r\n```\r\n\r\nInternally, this can be fixed by merging two dicts via dict unpacking (instead of `dict.update) `in `Dataset.map`, which avoids creating recursive dictionaries.\r\n\r\nP.S. `{**a, **b}` is slightly more performant than `a.update(b)` in my bencmarks.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4216\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4216\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4215","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4215\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4215\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4215\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4215","id":1214579162,"node_id":"PR_kwDODunzps42uuhY","number":4215,"title":"Add `drop_last_batch` to `IterableDataset.map`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650896119000,"updated_at":1651593367000,"closed_at":1651592934000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4215","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4215","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4215.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4215.patch","merged_at":1651592934000},"body":"Addresses this comment: https:\/\/github.com\/huggingface\/datasets\/pull\/3801#pullrequestreview-901736921","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4215\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4215\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4214","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4214\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4214\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4214\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4214","id":1214572430,"node_id":"PR_kwDODunzps42utC5","number":4214,"title":"Skip checksum computation in Imagefolder by default","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650895841000,"updated_at":1651591712000,"closed_at":1651591289000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4214","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4214","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4214.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4214.patch","merged_at":1651591289000},"body":"Avoids having to set `ignore_verifications=True` in `load_dataset(\"imagefolder\", ...)` to skip checksum verification and speed up loading.\r\n\r\nThe user can still pass `DownloadConfig(record_checksums=True)` to not skip this part. ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4214\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4214\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4213","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4213\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4213\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4213\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4213","id":1214510010,"node_id":"PR_kwDODunzps42uft_","number":4213,"title":"ETT time series dataset","user":{"login":"kashif","id":8100,"node_id":"MDQ6VXNlcjgxMDA=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8100?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kashif","html_url":"https:\/\/github.com\/kashif","followers_url":"https:\/\/api.github.com\/users\/kashif\/followers","following_url":"https:\/\/api.github.com\/users\/kashif\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kashif\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kashif\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kashif\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kashif\/orgs","repos_url":"https:\/\/api.github.com\/users\/kashif\/repos","events_url":"https:\/\/api.github.com\/users\/kashif\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kashif\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","thank you!\r\n"],"created_at":1650893178000,"updated_at":1651753161000,"closed_at":1651752635000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4213","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4213","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4213.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4213.patch","merged_at":1651752635000},"body":"Ready for review.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4213\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4213\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4212","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4212\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4212\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4212\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4212","id":1214498582,"node_id":"PR_kwDODunzps42udRf","number":4212,"title":"[Common Voice] Make sure bytes are correctly deleted if `path` exists","user":{"login":"patrickvonplaten","id":23423619,"node_id":"MDQ6VXNlcjIzNDIzNjE5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/23423619?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/patrickvonplaten","html_url":"https:\/\/github.com\/patrickvonplaten","followers_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/followers","following_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/orgs","repos_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/repos","events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","cool that you noticed that we store unnecessary bytes again :D "],"created_at":1650892706000,"updated_at":1651013668000,"closed_at":1651013307000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4212","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4212","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4212.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4212.patch","merged_at":1651013307000},"body":"`path` should be set to local path inside audio feature if exist so that bytes can correctly be deleted.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4212\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4212\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4211","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4211\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4211\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4211\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4211","id":1214361837,"node_id":"I_kwDODunzps5IYbDt","number":4211,"title":"DatasetDict containing Datasets with different features when pushed to hub gets remapped features","user":{"login":"pietrolesci","id":61748653,"node_id":"MDQ6VXNlcjYxNzQ4NjUz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/61748653?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/pietrolesci","html_url":"https:\/\/github.com\/pietrolesci","followers_url":"https:\/\/api.github.com\/users\/pietrolesci\/followers","following_url":"https:\/\/api.github.com\/users\/pietrolesci\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/pietrolesci\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/pietrolesci\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/pietrolesci\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/pietrolesci\/orgs","repos_url":"https:\/\/api.github.com\/users\/pietrolesci\/repos","events_url":"https:\/\/api.github.com\/users\/pietrolesci\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/pietrolesci\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi @pietrolesci, thanks for reporting.\r\n\r\nPlease note that this is a design purpose: a `DatasetDict` has the same features for all its datasets. Normally, a `DatasetDict` is composed of several sub-datasets each corresponding to a different **split**.\r\n\r\nTo handle sub-datasets with different features, we use another approach: use different **configurations** instead of **splits**.\r\n\r\nHowever, for the moment `push_to_hub` does not support specifying different configurations. IMHO, we should implement this.","Hi @albertvillanova,\r\n\r\nThanks a lot for your reply! I got it now. The strange thing for me was to have it correctly working (i.e., DatasetDict with different features in some datasets) locally and not on the Hub. It would be great to have configuration supported by `push_to_hub`. Personally, this latter functionality allowed me to iterate rather quickly on dataset curation.\r\n\r\nAgain, thanks for your time @albertvillanova!\r\n\r\nBest,\r\nPietro","Hi! Yes, we should override `DatasetDict.__setitem__` and throw an error if features dictionaries are different. `DatasetDict` is a subclass of `dict`, so `DatasetDict.{update\/setdefault}` need to be overridden as well. We could avoid this by subclassing `UserDict`, but then we would get the name collision - `DatasetDict.data` vs. `UserDict.data`. This makes me think we should rename the `data` attribute of `DatasetDict`\/`Dataset` for easier dict subclassing (would also simplify https:\/\/github.com\/huggingface\/datasets\/pull\/3997) and to follow good Python practices. Another option is to have a custom `UserDict` class in `py_utils`, but it can be hard to keep this class consistent with the built-in `UserDict`. \r\n\r\n@albertvillanova @lhoestq wdyt?","I would keep things simple and keep subclassing dict. Regarding the features check, I guess this can be done only for `push_to_hub` right ? It is the only function right now that requires the underlying datasets to be splits (e.g. train\/test) and have the same features.\r\n\r\nNote that later you will be able to push datasets with different features as different dataset **configurations** (similarly to the [GLUE subsets](https:\/\/huggingface.co\/datasets\/glue) for example). We will work on this soon"],"created_at":1650885774000,"updated_at":1650990732000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"Hi there,\r\n\r\nI am trying to load a dataset to the Hub. This dataset is a `DatasetDict` composed of various splits. Some splits have a different `Feature` mapping. Locally, the DatasetDict preserves the individual features but if I `push_to_hub` and then `load_dataset`, the features are all the same.\r\n\r\nDataset and code to reproduce available [here](https:\/\/huggingface.co\/datasets\/pietrolesci\/robust_nli).\r\n\r\nIn short:\r\n\r\nI have 3 feature mapping\r\n```python\r\nTri_features = Features(\r\n    {\r\n        \"idx\": Value(dtype=\"int64\"),\r\n        \"premise\": Value(dtype=\"string\"),\r\n        \"hypothesis\": Value(dtype=\"string\"),\r\n        \"label\": ClassLabel(num_classes=3, names=[\"entailment\", \"neutral\", \"contradiction\"]),\r\n    }\r\n)\r\n\r\nEnt_features = Features(\r\n    {\r\n        \"idx\": Value(dtype=\"int64\"),\r\n        \"premise\": Value(dtype=\"string\"),\r\n        \"hypothesis\": Value(dtype=\"string\"),\r\n        \"label\": ClassLabel(num_classes=2, names=[\"non-entailment\", \"entailment\"]),\r\n    }\r\n)\r\n\r\nCon_features = Features(\r\n    {\r\n        \"idx\": Value(dtype=\"int64\"),\r\n        \"premise\": Value(dtype=\"string\"),\r\n        \"hypothesis\": Value(dtype=\"string\"),\r\n        \"label\": ClassLabel(num_classes=2, names=[\"non-contradiction\", \"contradiction\"]),\r\n    }\r\n)\r\n```\r\n\r\nThen I create different datasets\r\n\r\n```python\r\ndataset_splits = {}\r\n\r\nfor split in df[\"split\"].unique():\r\n    print(split)\r\n    df_split = df.loc[df[\"split\"] == split].copy()\r\n    \r\n    if split in Tri_dataset:\r\n        df_split[\"label\"] = df_split[\"label\"].map({\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2})\r\n        ds = Dataset.from_pandas(df_split, features=Tri_features)\r\n    \r\n    elif split in Ent_bin_dataset:\r\n        df_split[\"label\"] = df_split[\"label\"].map({\"non-entailment\": 0, \"entailment\": 1})\r\n        ds = Dataset.from_pandas(df_split, features=Ent_features)\r\n    \r\n    elif split in Con_bin_dataset:\r\n        df_split[\"label\"] = df_split[\"label\"].map({\"non-contradiction\": 0, \"contradiction\": 1})\r\n        ds = Dataset.from_pandas(df_split, features=Con_features)\r\n\r\n    else:\r\n        print(\"ERROR:\", split)\r\n    dataset_splits[split] = ds\r\ndatasets = DatasetDict(dataset_splits)\r\n```\r\n\r\nI then push to hub\r\n\r\n```python\r\ndatasets.push_to_hub(\"pietrolesci\/robust_nli\", token=\"<token>\")\r\n```\r\n\r\nFinally, I load it from the hub\r\n\r\n```python\r\ndatasets_loaded_from_hub = load_dataset(\"pietrolesci\/robust_nli\")\r\n```\r\n\r\nAnd I get that\r\n\r\n```python\r\ndatasets[\"LI_TS\"].features != datasets_loaded_from_hub[\"LI_TS\"].features\r\n```\r\n\r\nsince \r\n\r\n```python\r\n\"label\": ClassLabel(num_classes=2, names=[\"non-contradiction\", \"contradiction\"])\r\n```\r\n\r\ngets remapped to \r\n\r\n```python\r\n \"label\": ClassLabel(num_classes=3, names=[\"entailment\", \"neutral\", \"contradiction\"])\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4211\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4211\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4210","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4210\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4210\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4210\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4210","id":1214089130,"node_id":"I_kwDODunzps5IXYeq","number":4210,"title":"TypeError: Cannot cast array data from dtype('O') to dtype('int64') according to the rule 'safe'","user":{"login":"loretoparisi","id":163333,"node_id":"MDQ6VXNlcjE2MzMzMw==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/163333?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/loretoparisi","html_url":"https:\/\/github.com\/loretoparisi","followers_url":"https:\/\/api.github.com\/users\/loretoparisi\/followers","following_url":"https:\/\/api.github.com\/users\/loretoparisi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/loretoparisi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/loretoparisi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/loretoparisi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/loretoparisi\/orgs","repos_url":"https:\/\/api.github.com\/users\/loretoparisi\/repos","events_url":"https:\/\/api.github.com\/users\/loretoparisi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/loretoparisi\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi! Casting class labels from strings is currently not supported in the CSV loader, but you can get the same result with an additional map as follows:\r\n```python\r\nfrom datasets import load_dataset,Features,Value,ClassLabel\r\nclass_names = [\"cmn\",\"deu\",\"rus\",\"fra\",\"eng\",\"jpn\",\"spa\",\"ita\",\"kor\",\"vie\",\"nld\",\"epo\",\"por\",\"tur\",\"heb\",\"hun\",\"ell\",\"ind\",\"ara\",\"arz\",\"fin\",\"bul\",\"yue\",\"swe\",\"ukr\",\"bel\",\"que\",\"ces\",\"swh\",\"nno\",\"wuu\",\"nob\",\"zsm\",\"est\",\"kat\",\"pol\",\"lat\",\"urd\",\"sqi\",\"isl\",\"fry\",\"afr\",\"ron\",\"fao\",\"san\",\"bre\",\"tat\",\"yid\",\"uig\",\"uzb\",\"srp\",\"qya\",\"dan\",\"pes\",\"slk\",\"eus\",\"cycl\",\"acm\",\"tgl\",\"lvs\",\"kaz\",\"hye\",\"hin\",\"lit\",\"ben\",\"cat\",\"bos\",\"hrv\",\"tha\",\"orv\",\"cha\",\"mon\",\"lzh\",\"scn\",\"gle\",\"mkd\",\"slv\",\"frm\",\"glg\",\"vol\",\"ain\",\"jbo\",\"tok\",\"ina\",\"nds\",\"mal\",\"tlh\",\"roh\",\"ltz\",\"oss\",\"ido\",\"gla\",\"mlt\",\"sco\",\"ast\",\"jav\",\"oci\",\"ile\",\"ota\",\"xal\",\"tel\",\"sjn\",\"nov\",\"khm\",\"tpi\",\"ang\",\"aze\",\"tgk\",\"tuk\",\"chv\",\"hsb\",\"dsb\",\"bod\",\"sme\",\"cym\",\"mri\",\"ksh\",\"kmr\",\"ewe\",\"kab\",\"ber\",\"tpw\",\"udm\",\"lld\",\"pms\",\"lad\",\"grn\",\"mlg\",\"xho\",\"pnb\",\"grc\",\"hat\",\"lao\",\"npi\",\"cor\",\"nah\",\"avk\",\"mar\",\"guj\",\"pan\",\"kir\",\"myv\",\"prg\",\"sux\",\"crs\",\"ckt\",\"bak\",\"zlm\",\"hil\",\"cbk\",\"chr\",\"nav\",\"lkt\",\"enm\",\"arq\",\"lin\",\"abk\",\"pcd\",\"rom\",\"gsw\",\"tam\",\"zul\",\"awa\",\"wln\",\"amh\",\"bar\",\"hbo\",\"mhr\",\"bho\",\"mrj\",\"ckb\",\"osx\",\"pfl\",\"mgm\",\"sna\",\"mah\",\"hau\",\"kan\",\"nog\",\"sin\",\"glv\",\"dng\",\"kal\",\"liv\",\"vro\",\"apc\",\"jdt\",\"fur\",\"che\",\"haw\",\"yor\",\"crh\",\"pdc\",\"ppl\",\"kin\",\"shs\",\"mnw\",\"tet\",\"sah\",\"kum\",\"ngt\",\"nya\",\"pus\",\"hif\",\"mya\",\"moh\",\"wol\",\"tir\",\"ton\",\"lzz\",\"oar\",\"lug\",\"brx\",\"non\",\"mww\",\"hak\",\"nlv\",\"ngu\",\"bua\",\"aym\",\"vec\",\"ibo\",\"tkl\",\"bam\",\"kha\",\"ceb\",\"lou\",\"fuc\",\"smo\",\"gag\",\"lfn\",\"arg\",\"umb\",\"tyv\",\"kjh\",\"oji\",\"cyo\",\"urh\",\"kzj\",\"pam\",\"srd\",\"lmo\",\"swg\",\"mdf\",\"gil\",\"snd\",\"tso\",\"sot\",\"zza\",\"tsn\",\"pau\",\"som\",\"egl\",\"ady\",\"asm\",\"ori\",\"dtp\",\"cho\",\"max\",\"kam\",\"niu\",\"sag\",\"ilo\",\"kaa\",\"fuv\",\"nch\",\"hoc\",\"iba\",\"gbm\",\"sun\",\"war\",\"mvv\",\"pap\",\"ary\",\"kxi\",\"csb\",\"pag\",\"cos\",\"rif\",\"kek\",\"krc\",\"aii\",\"ban\",\"ssw\",\"tvl\",\"mfe\",\"tah\",\"bvy\",\"bcl\",\"hnj\",\"nau\",\"nst\",\"afb\",\"quc\",\"min\",\"tmw\",\"mad\",\"bjn\",\"mai\",\"cjy\",\"got\",\"hsn\",\"gan\",\"tzl\",\"dws\",\"ldn\",\"afh\",\"sgs\",\"krl\",\"vep\",\"rue\",\"tly\",\"mic\",\"ext\",\"izh\",\"sma\",\"jam\",\"cmo\",\"mwl\",\"kpv\",\"koi\",\"bis\",\"ike\",\"run\",\"evn\",\"ryu\",\"mnc\",\"aoz\",\"otk\",\"kas\",\"aln\",\"akl\",\"yua\",\"shy\",\"fkv\",\"gos\",\"fij\",\"thv\",\"zgh\",\"gcf\",\"cay\",\"xmf\",\"tig\",\"div\",\"lij\",\"rap\",\"hrx\",\"cpi\",\"tts\",\"gaa\",\"tmr\",\"iii\",\"ltg\",\"bzt\",\"syc\",\"emx\",\"gom\",\"chg\",\"osp\",\"stq\",\"frr\",\"fro\",\"nys\",\"toi\",\"new\",\"phn\",\"jpa\",\"rel\",\"drt\",\"chn\",\"pli\",\"laa\",\"bal\",\"hdn\",\"hax\",\"mik\",\"ajp\",\"xqa\",\"pal\",\"crk\",\"mni\",\"lut\",\"ayl\",\"ood\",\"sdh\",\"ofs\",\"nus\",\"kiu\",\"diq\",\"qxq\",\"alt\",\"bfz\",\"klj\",\"mus\",\"srn\",\"guc\",\"lim\",\"zea\",\"shi\",\"mnr\",\"bom\",\"sat\",\"szl\"]\r\nfeatures = Features({ 'label': ClassLabel(names=class_names), 'text': Value('string')})\r\nnum_labels = features['label'].num_classes\r\ndata_files = { \"train\": \"train.csv\", \"test\": \"test.csv\" }\r\nsentences = load_dataset(\r\n     \"loretoparisi\/tatoeba-sentences\",\r\n     data_files=data_files,\r\n     delimiter='\\t', \r\n     column_names=['label', 'text'],\r\n)\r\n# You can make this part faster with num_proc=<some int>\r\nsentences = sentences.map(lambda ex: features[\"label\"].str2int(ex[\"label\"]) if ex[\"label\"] is not None else None, features=features)\r\n```\r\n\r\n@lhoestq IIRC, I suggested adding `cast_to_storage` to `ClassLabel`  + `table_cast` to the packaged loaders if the `ClassLabel`\/`Image`\/`Audio` type is present in `features` to avoid this kind of error, but your concern was speed. IMO shouldn't be a problem if we do `table_cast` only when these features are present.","I agree packaged loaders should support `ClassLabel` feature without throwing an error.","@albertvillanova @mariosasko thank you, with that change now I get\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n[<ipython-input-9-eeb68eeb9bec>](https:\/\/localhost:8080\/#) in <module>()\r\n     11 )\r\n     12 # You can make this part faster with num_proc=<some int>\r\n---> 13 sentences = sentences.map(lambda ex: features[\"label\"].str2int(ex[\"label\"]) if ex[\"label\"] is not None else None, features=features)\r\n     14 sentences = sentences.shuffle()\r\n\r\n8 frames\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in validate_function_output(processed_inputs, indices)\r\n   2193             if processed_inputs is not None and not isinstance(processed_inputs, (Mapping, pa.Table)):\r\n   2194                 raise TypeError(\r\n-> 2195                     f\"Provided `function` which is applied to all elements of table returns a variable of type {type(processed_inputs)}. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects.\"\r\n   2196                 )\r\n   2197             elif isinstance(indices, list) and isinstance(processed_inputs, Mapping):\r\n\r\nTypeError: Provided `function` which is applied to all elements of table returns a variable of type <class 'int'>. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects.\r\n```\r\n\r\nthe error is raised by [this](https:\/\/github.com\/huggingface\/datasets\/blob\/master\/src\/datasets\/arrow_dataset.py#L2221)\r\n\r\n```\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in validate_function_output(processed_inputs, indices)\r\n```","@mariosasko changed it like\r\n\r\n```python\r\nsentences = sentences.map(lambda ex: {\"label\" : features[\"label\"].str2int(ex[\"label\"]) if ex[\"label\"] is not None else None}, features=features)\r\n```\r\n\r\nto avoid the above errorr.","Any update on this? Is this correct ?\r\n> @mariosasko changed it like\r\n> \r\n> ```python\r\n> sentences = sentences.map(lambda ex: {\"label\" : features[\"label\"].str2int(ex[\"label\"]) if ex[\"label\"] is not None else None}, features=features)\r\n> ```\r\n> \r\n> to avoid the above errorr.\r\n\r\n"],"created_at":1650871722000,"updated_at":1651247976000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"### System Info\r\n\r\n```shell\r\n- `transformers` version: 4.18.0\r\n- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- Huggingface_hub version: 0.5.1\r\n- PyTorch version (GPU?): 1.10.0+cu111 (True)\r\n- Tensorflow version (GPU?): 2.8.0 (True)\r\n- Flax version (CPU?\/GPU?\/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n```\r\n\r\n\r\n### Who can help?\r\n\r\n@LysandreJik \r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [X] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n```python\r\nfrom datasets import load_dataset,Features,Value,ClassLabel\r\n\r\nclass_names = [\"cmn\",\"deu\",\"rus\",\"fra\",\"eng\",\"jpn\",\"spa\",\"ita\",\"kor\",\"vie\",\"nld\",\"epo\",\"por\",\"tur\",\"heb\",\"hun\",\"ell\",\"ind\",\"ara\",\"arz\",\"fin\",\"bul\",\"yue\",\"swe\",\"ukr\",\"bel\",\"que\",\"ces\",\"swh\",\"nno\",\"wuu\",\"nob\",\"zsm\",\"est\",\"kat\",\"pol\",\"lat\",\"urd\",\"sqi\",\"isl\",\"fry\",\"afr\",\"ron\",\"fao\",\"san\",\"bre\",\"tat\",\"yid\",\"uig\",\"uzb\",\"srp\",\"qya\",\"dan\",\"pes\",\"slk\",\"eus\",\"cycl\",\"acm\",\"tgl\",\"lvs\",\"kaz\",\"hye\",\"hin\",\"lit\",\"ben\",\"cat\",\"bos\",\"hrv\",\"tha\",\"orv\",\"cha\",\"mon\",\"lzh\",\"scn\",\"gle\",\"mkd\",\"slv\",\"frm\",\"glg\",\"vol\",\"ain\",\"jbo\",\"tok\",\"ina\",\"nds\",\"mal\",\"tlh\",\"roh\",\"ltz\",\"oss\",\"ido\",\"gla\",\"mlt\",\"sco\",\"ast\",\"jav\",\"oci\",\"ile\",\"ota\",\"xal\",\"tel\",\"sjn\",\"nov\",\"khm\",\"tpi\",\"ang\",\"aze\",\"tgk\",\"tuk\",\"chv\",\"hsb\",\"dsb\",\"bod\",\"sme\",\"cym\",\"mri\",\"ksh\",\"kmr\",\"ewe\",\"kab\",\"ber\",\"tpw\",\"udm\",\"lld\",\"pms\",\"lad\",\"grn\",\"mlg\",\"xho\",\"pnb\",\"grc\",\"hat\",\"lao\",\"npi\",\"cor\",\"nah\",\"avk\",\"mar\",\"guj\",\"pan\",\"kir\",\"myv\",\"prg\",\"sux\",\"crs\",\"ckt\",\"bak\",\"zlm\",\"hil\",\"cbk\",\"chr\",\"nav\",\"lkt\",\"enm\",\"arq\",\"lin\",\"abk\",\"pcd\",\"rom\",\"gsw\",\"tam\",\"zul\",\"awa\",\"wln\",\"amh\",\"bar\",\"hbo\",\"mhr\",\"bho\",\"mrj\",\"ckb\",\"osx\",\"pfl\",\"mgm\",\"sna\",\"mah\",\"hau\",\"kan\",\"nog\",\"sin\",\"glv\",\"dng\",\"kal\",\"liv\",\"vro\",\"apc\",\"jdt\",\"fur\",\"che\",\"haw\",\"yor\",\"crh\",\"pdc\",\"ppl\",\"kin\",\"shs\",\"mnw\",\"tet\",\"sah\",\"kum\",\"ngt\",\"nya\",\"pus\",\"hif\",\"mya\",\"moh\",\"wol\",\"tir\",\"ton\",\"lzz\",\"oar\",\"lug\",\"brx\",\"non\",\"mww\",\"hak\",\"nlv\",\"ngu\",\"bua\",\"aym\",\"vec\",\"ibo\",\"tkl\",\"bam\",\"kha\",\"ceb\",\"lou\",\"fuc\",\"smo\",\"gag\",\"lfn\",\"arg\",\"umb\",\"tyv\",\"kjh\",\"oji\",\"cyo\",\"urh\",\"kzj\",\"pam\",\"srd\",\"lmo\",\"swg\",\"mdf\",\"gil\",\"snd\",\"tso\",\"sot\",\"zza\",\"tsn\",\"pau\",\"som\",\"egl\",\"ady\",\"asm\",\"ori\",\"dtp\",\"cho\",\"max\",\"kam\",\"niu\",\"sag\",\"ilo\",\"kaa\",\"fuv\",\"nch\",\"hoc\",\"iba\",\"gbm\",\"sun\",\"war\",\"mvv\",\"pap\",\"ary\",\"kxi\",\"csb\",\"pag\",\"cos\",\"rif\",\"kek\",\"krc\",\"aii\",\"ban\",\"ssw\",\"tvl\",\"mfe\",\"tah\",\"bvy\",\"bcl\",\"hnj\",\"nau\",\"nst\",\"afb\",\"quc\",\"min\",\"tmw\",\"mad\",\"bjn\",\"mai\",\"cjy\",\"got\",\"hsn\",\"gan\",\"tzl\",\"dws\",\"ldn\",\"afh\",\"sgs\",\"krl\",\"vep\",\"rue\",\"tly\",\"mic\",\"ext\",\"izh\",\"sma\",\"jam\",\"cmo\",\"mwl\",\"kpv\",\"koi\",\"bis\",\"ike\",\"run\",\"evn\",\"ryu\",\"mnc\",\"aoz\",\"otk\",\"kas\",\"aln\",\"akl\",\"yua\",\"shy\",\"fkv\",\"gos\",\"fij\",\"thv\",\"zgh\",\"gcf\",\"cay\",\"xmf\",\"tig\",\"div\",\"lij\",\"rap\",\"hrx\",\"cpi\",\"tts\",\"gaa\",\"tmr\",\"iii\",\"ltg\",\"bzt\",\"syc\",\"emx\",\"gom\",\"chg\",\"osp\",\"stq\",\"frr\",\"fro\",\"nys\",\"toi\",\"new\",\"phn\",\"jpa\",\"rel\",\"drt\",\"chn\",\"pli\",\"laa\",\"bal\",\"hdn\",\"hax\",\"mik\",\"ajp\",\"xqa\",\"pal\",\"crk\",\"mni\",\"lut\",\"ayl\",\"ood\",\"sdh\",\"ofs\",\"nus\",\"kiu\",\"diq\",\"qxq\",\"alt\",\"bfz\",\"klj\",\"mus\",\"srn\",\"guc\",\"lim\",\"zea\",\"shi\",\"mnr\",\"bom\",\"sat\",\"szl\"]\r\nfeatures = Features({ 'label': ClassLabel(names=class_names), 'text': Value('string')})\r\nnum_labels = features['label'].num_classes\r\ndata_files = { \"train\": \"train.csv\", \"test\": \"test.csv\" }\r\nsentences = load_dataset(\"loretoparisi\/tatoeba-sentences\",\r\n                             data_files=data_files,\r\n                             delimiter='\\t', \r\n                             column_names=['label', 'text'],\r\n                             features = features\r\n```\r\n\r\nERROR:\r\n```\r\nClassLabel(num_classes=403, names=['cmn', 'deu', 'rus', 'fra', 'eng', 'jpn', 'spa', 'ita', 'kor', 'vie', 'nld', 'epo', 'por', 'tur', 'heb', 'hun', 'ell', 'ind', 'ara', 'arz', 'fin', 'bul', 'yue', 'swe', 'ukr', 'bel', 'que', 'ces', 'swh', 'nno', 'wuu', 'nob', 'zsm', 'est', 'kat', 'pol', 'lat', 'urd', 'sqi', 'isl', 'fry', 'afr', 'ron', 'fao', 'san', 'bre', 'tat', 'yid', 'uig', 'uzb', 'srp', 'qya', 'dan', 'pes', 'slk', 'eus', 'cycl', 'acm', 'tgl', 'lvs', 'kaz', 'hye', 'hin', 'lit', 'ben', 'cat', 'bos', 'hrv', 'tha', 'orv', 'cha', 'mon', 'lzh', 'scn', 'gle', 'mkd', 'slv', 'frm', 'glg', 'vol', 'ain', 'jbo', 'tok', 'ina', 'nds', 'mal', 'tlh', 'roh', 'ltz', 'oss', 'ido', 'gla', 'mlt', 'sco', 'ast', 'jav', 'oci', 'ile', 'ota', 'xal', 'tel', 'sjn', 'nov', 'khm', 'tpi', 'ang', 'aze', 'tgk', 'tuk', 'chv', 'hsb', 'dsb', 'bod', 'sme', 'cym', 'mri', 'ksh', 'kmr', 'ewe', 'kab', 'ber', 'tpw', 'udm', 'lld', 'pms', 'lad', 'grn', 'mlg', 'xho', 'pnb', 'grc', 'hat', 'lao', 'npi', 'cor', 'nah', 'avk', 'mar', 'guj', 'pan', 'kir', 'myv', 'prg', 'sux', 'crs', 'ckt', 'bak', 'zlm', 'hil', 'cbk', 'chr', 'nav', 'lkt', 'enm', 'arq', 'lin', 'abk', 'pcd', 'rom', 'gsw', 'tam', 'zul', 'awa', 'wln', 'amh', 'bar', 'hbo', 'mhr', 'bho', 'mrj', 'ckb', 'osx', 'pfl', 'mgm', 'sna', 'mah', 'hau', 'kan', 'nog', 'sin', 'glv', 'dng', 'kal', 'liv', 'vro', 'apc', 'jdt', 'fur', 'che', 'haw', 'yor', 'crh', 'pdc', 'ppl', 'kin', 'shs', 'mnw', 'tet', 'sah', 'kum', 'ngt', 'nya', 'pus', 'hif', 'mya', 'moh', 'wol', 'tir', 'ton', 'lzz', 'oar', 'lug', 'brx', 'non', 'mww', 'hak', 'nlv', 'ngu', 'bua', 'aym', 'vec', 'ibo', 'tkl', 'bam', 'kha', 'ceb', 'lou', 'fuc', 'smo', 'gag', 'lfn', 'arg', 'umb', 'tyv', 'kjh', 'oji', 'cyo', 'urh', 'kzj', 'pam', 'srd', 'lmo', 'swg', 'mdf', 'gil', 'snd', 'tso', 'sot', 'zza', 'tsn', 'pau', 'som', 'egl', 'ady', 'asm', 'ori', 'dtp', 'cho', 'max', 'kam', 'niu', 'sag', 'ilo', 'kaa', 'fuv', 'nch', 'hoc', 'iba', 'gbm', 'sun', 'war', 'mvv', 'pap', 'ary', 'kxi', 'csb', 'pag', 'cos', 'rif', 'kek', 'krc', 'aii', 'ban', 'ssw', 'tvl', 'mfe', 'tah', 'bvy', 'bcl', 'hnj', 'nau', 'nst', 'afb', 'quc', 'min', 'tmw', 'mad', 'bjn', 'mai', 'cjy', 'got', 'hsn', 'gan', 'tzl', 'dws', 'ldn', 'afh', 'sgs', 'krl', 'vep', 'rue', 'tly', 'mic', 'ext', 'izh', 'sma', 'jam', 'cmo', 'mwl', 'kpv', 'koi', 'bis', 'ike', 'run', 'evn', 'ryu', 'mnc', 'aoz', 'otk', 'kas', 'aln', 'akl', 'yua', 'shy', 'fkv', 'gos', 'fij', 'thv', 'zgh', 'gcf', 'cay', 'xmf', 'tig', 'div', 'lij', 'rap', 'hrx', 'cpi', 'tts', 'gaa', 'tmr', 'iii', 'ltg', 'bzt', 'syc', 'emx', 'gom', 'chg', 'osp', 'stq', 'frr', 'fro', 'nys', 'toi', 'new', 'phn', 'jpa', 'rel', 'drt', 'chn', 'pli', 'laa', 'bal', 'hdn', 'hax', 'mik', 'ajp', 'xqa', 'pal', 'crk', 'mni', 'lut', 'ayl', 'ood', 'sdh', 'ofs', 'nus', 'kiu', 'diq', 'qxq', 'alt', 'bfz', 'klj', 'mus', 'srn', 'guc', 'lim', 'zea', 'shi', 'mnr', 'bom', 'sat', 'szl'], id=None)\r\nValue(dtype='string', id=None)\r\nUsing custom data configuration loretoparisi--tatoeba-sentences-7b2c5e991f398f39\r\nDownloading and preparing dataset csv\/loretoparisi--tatoeba-sentences to \/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-7b2c5e991f398f39\/0.0.0\/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\r\nDownloading data files: 100%\r\n2\/2 [00:18<00:00, 8.06s\/it]\r\nDownloading data: 100%\r\n391M\/391M [00:13<00:00, 35.3MB\/s]\r\nDownloading data: 100%\r\n92.4M\/92.4M [00:02<00:00, 36.5MB\/s]\r\nFailed to read file '\/root\/.cache\/huggingface\/datasets\/downloads\/933132df9905194ea9faeb30cabca8c49318795612f6495fcb941a290191dd5d' with error <class 'ValueError'>: invalid literal for int() with base 10: 'cmn'\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader._convert_tokens()\r\n\r\nTypeError: Cannot cast array data from dtype('O') to dtype('int64') according to the rule 'safe'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n15 frames\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader._convert_tokens()\r\n\r\nValueError: invalid literal for int() with base 10: 'cmn'\r\n```\r\n\r\nwhile loading without `features` it loads without errors\r\n\r\n```\r\nsentences = load_dataset(\"loretoparisi\/tatoeba-sentences\",\r\n                             data_files=data_files,\r\n                             delimiter='\\t', \r\n                             column_names=['label', 'text']\r\n                         )\r\n```\r\n\r\nbut the `label` col seems to be wrong (without the `ClassLabel` object):\r\n\r\n```\r\nsentences['train'].features\r\n{'label': Value(dtype='string', id=None),\r\n 'text': Value(dtype='string', id=None)}\r\n```\r\n\r\nThe dataset was https:\/\/huggingface.co\/datasets\/loretoparisi\/tatoeba-sentences\r\n\r\n\r\nDataset format is:\r\n\r\n```\r\nces\tNechci v\u011bd\u011bt, co je tam uvnit\u0159.\r\nces\tKdo o tom chce sly\u0161et?\r\ndeu\tTom sagte, er f\u00fchle sich nicht wohl.\r\nber\tMel-iyi-d anida-t tura ?\r\nhun\tGondom lesz r\u00e1 r\u00f6gt\u00f6n.\r\nber\tMel-iyi-d anida-tt tura ?\r\ndeu\tIch will dich nicht reden h\u00f6ren.\r\n```\r\n\r\n### Expected behavior\r\n\r\n```shell\r\ncorrectly load train and test files.\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4210\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4210\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4208","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4208\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4208\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4208\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4208","id":1213716426,"node_id":"PR_kwDODunzps42r7bW","number":4208,"title":"Add CMU MoCap Dataset","user":{"login":"dnaveenr","id":17746528,"node_id":"MDQ6VXNlcjE3NzQ2NTI4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17746528?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/dnaveenr","html_url":"https:\/\/github.com\/dnaveenr","followers_url":"https:\/\/api.github.com\/users\/dnaveenr\/followers","following_url":"https:\/\/api.github.com\/users\/dnaveenr\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/dnaveenr\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/dnaveenr\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/dnaveenr\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/dnaveenr\/orgs","repos_url":"https:\/\/api.github.com\/users\/dnaveenr\/repos","events_url":"https:\/\/api.github.com\/users\/dnaveenr\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/dnaveenr\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4208). All of your documentation changes will be reflected on that endpoint.","- Updated the readme.\r\n- Added dummy_data.zip and ran the all the tests.\r\n\r\nThe dataset works for \"asf\/amc\" and \"avi\" formats which have a single download link for the complete dataset. But \"c3d\" and \"mpg\" have multiple download links, can we combine and host these links on the Hub since the dataset is free to use ?","\"c3d\" and \"mpg\" have multiple download links (part archives) and dl_manager.download_and_extract() extracts the files to multiple paths, is there a way to extract these multiple archives into one folder ? Any other way to go about this ?\r\nCan we combine and host these links on the Hub since the dataset is free to use ?"],"created_at":1650821468000,"updated_at":1652203875000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4208","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4208","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4208.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4208.patch","merged_at":null},"body":"Resolves #3457 \r\n\r\nDataset Request : Add CMU Graphics Lab Motion Capture dataset [#3457](https:\/\/github.com\/huggingface\/datasets\/issues\/3457)\r\nThis PR adds the CMU MoCap Dataset.\r\n\r\nThe authors didn't respond even after multiple follow ups, so I ended up crawling the website to get categories, subcategories and description information. Some of the subjects do not have category\/subcategory\/description as well. I am using a subject to categories, subcategories and description map (metadata file).\r\n\r\nCurrently the loading of the dataset works for \"asf\/amc\" and \"avi\" formats since they have a single download link. But \"c3d\" and \"mpg\" have multiple download links (part archives) and dl_manager.download_and_extract() extracts the files to multiple paths, is there a way to extract these multiple archives into one folder ? Any other way to go about this ?\r\nAny suggestions\/inputs on this would be helpful. Thank you.\r\n\r\n\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4208\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4208\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4207","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4207\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4207\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4207\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4207","id":1213604615,"node_id":"PR_kwDODunzps42rmbK","number":4207,"title":"[Minor edit] Fix typo in class name","user":{"login":"cakiki","id":3664563,"node_id":"MDQ6VXNlcjM2NjQ1NjM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3664563?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/cakiki","html_url":"https:\/\/github.com\/cakiki","followers_url":"https:\/\/api.github.com\/users\/cakiki\/followers","following_url":"https:\/\/api.github.com\/users\/cakiki\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/cakiki\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/cakiki\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/cakiki\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/cakiki\/orgs","repos_url":"https:\/\/api.github.com\/users\/cakiki\/repos","events_url":"https:\/\/api.github.com\/users\/cakiki\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/cakiki\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1650793777000,"updated_at":1651756667000,"closed_at":1651756667000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4207","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4207","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4207.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4207.patch","merged_at":1651756667000},"body":"Typo: `datasets.DatsetDict` -> `datasets.DatasetDict`","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4207\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4207\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4206","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4206\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4206\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4206\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4206","id":1212715581,"node_id":"PR_kwDODunzps42pJQW","number":4206,"title":"Add Nerval Metric","user":{"login":"mdadda","id":49372461,"node_id":"MDQ6VXNlcjQ5MzcyNDYx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/49372461?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mdadda","html_url":"https:\/\/github.com\/mdadda","followers_url":"https:\/\/api.github.com\/users\/mdadda\/followers","following_url":"https:\/\/api.github.com\/users\/mdadda\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mdadda\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mdadda\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mdadda\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mdadda\/orgs","repos_url":"https:\/\/api.github.com\/users\/mdadda\/repos","events_url":"https:\/\/api.github.com\/users\/mdadda\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mdadda\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1650656700000,"updated_at":1651404219000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4206","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4206","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4206.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4206.patch","merged_at":null},"body":"This PR adds readme.md and ner_val.py to metrics.\r\nNerval is a python package that helps evaluate NER models. It creates classification report and confusion matrix at entity level.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4206\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4206\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4205","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4205\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4205\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4205\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4205","id":1212466138,"node_id":"PR_kwDODunzps42oVFE","number":4205,"title":"Fix `convert_file_size_to_int` for kilobits and megabits","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650639381000,"updated_at":1651591722000,"closed_at":1651591308000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4205","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4205","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4205.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4205.patch","merged_at":1651591308000},"body":"Minor change to fully align this function with the recent change in Transformers (https:\/\/github.com\/huggingface\/transformers\/pull\/16891) ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4205\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4205\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4204","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4204\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4204\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4204\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4204","id":1212431764,"node_id":"PR_kwDODunzps42oN0j","number":4204,"title":"Add Recall Metric Card","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","This looks good to me! "],"created_at":1650637466000,"updated_at":1651584203000,"closed_at":1651583784000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4204","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4204","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4204.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4204.patch","merged_at":1651583784000},"body":"What this PR mainly does:\r\n- add metric card for recall metric\r\n- update docs in recall python file\r\n\r\nNote: I've also included a .json file with all of the metric card information. I've started compiling the relevant information in this type of .json files, and then using a script I wrote to generate the formatted metric card, as well as the docs to go in the .py file. I figured I'd upload the .json because it could be useful, especially if I also make a PR with the script I'm using (let me know if that's something you think would be beneficial!)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4204\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4204\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4203","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4203\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4203\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4203\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4203","id":1212431067,"node_id":"PR_kwDODunzps42oNrS","number":4203,"title":"Add Precision Metric Card","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650637428000,"updated_at":1651587820000,"closed_at":1651587406000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4203","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4203","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4203.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4203.patch","merged_at":1651587405000},"body":"What this PR mainly does:\r\n- add metric card for precision metric\r\n- update docs in precision python file\r\n\r\nNote: I've also included a .json file with all of the metric card information. I've started compiling the relevant information in this type of .json files, and then using a script I wrote to generate the formatted metric card, as well as the docs to go in the .py file. I figured I'd upload the .json because it could be useful, especially if I also make a PR with the script I'm using (let me know if that's something you think would be beneficial!)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4203\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4203\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4202","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4202\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4202\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4202\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4202","id":1212326288,"node_id":"PR_kwDODunzps42n278","number":4202,"title":"Fix some type annotation in doc","user":{"login":"thomasw21","id":24695242,"node_id":"MDQ6VXNlcjI0Njk1MjQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/24695242?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/thomasw21","html_url":"https:\/\/github.com\/thomasw21","followers_url":"https:\/\/api.github.com\/users\/thomasw21\/followers","following_url":"https:\/\/api.github.com\/users\/thomasw21\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/thomasw21\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/thomasw21\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/thomasw21\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/thomasw21\/orgs","repos_url":"https:\/\/api.github.com\/users\/thomasw21\/repos","events_url":"https:\/\/api.github.com\/users\/thomasw21\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/thomasw21\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650632011000,"updated_at":1650639780000,"closed_at":1650639403000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4202","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4202","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4202.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4202.patch","merged_at":1650639403000},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4202\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4202\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4201","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4201\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4201\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4201\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4201","id":1212086420,"node_id":"PR_kwDODunzps42nIRm","number":4201,"title":"Update GH template for dataset viewer issues","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","You can see rendering at: https:\/\/github.com\/huggingface\/datasets\/blob\/6b48fedbdafe12a42c7b6edcecc32820af1a4822\/.github\/ISSUE_TEMPLATE\/dataset-viewer.yml"],"created_at":1650620084000,"updated_at":1651826323000,"closed_at":1650962755000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4201","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4201","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4201.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4201.patch","merged_at":1650962755000},"body":"Update template to use new issue forms instead.\r\n\r\nWith this PR we can check if this new feature is useful for us.\r\n\r\nOnce validated, we can update the other templates.\r\n\r\nCC: @severo ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4201\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4201\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4200","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4200\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4200\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4200\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4200","id":1211980110,"node_id":"PR_kwDODunzps42mz0w","number":4200,"title":"Add to docs how to load from local script","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650614905000,"updated_at":1651826365000,"closed_at":1650692845000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4200","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4200","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4200.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4200.patch","merged_at":1650692844000},"body":"This option was missing from the docs guide (it was only explained in the docstring of `load_dataset`). Although this is an infrequent use case, there might be some users interested in it.\r\n\r\nRelated to #4192\r\n\r\nCC: @stevhliu ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4200\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4200\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4199","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4199\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4199\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4199\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4199","id":1211953308,"node_id":"I_kwDODunzps5IPPCc","number":4199,"title":"Cache miss during reload for datasets using image fetch utilities through map ","user":{"login":"apsdehal","id":3616806,"node_id":"MDQ6VXNlcjM2MTY4MDY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3616806?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/apsdehal","html_url":"https:\/\/github.com\/apsdehal","followers_url":"https:\/\/api.github.com\/users\/apsdehal\/followers","following_url":"https:\/\/api.github.com\/users\/apsdehal\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/apsdehal\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/apsdehal\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/apsdehal\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/apsdehal\/orgs","repos_url":"https:\/\/api.github.com\/users\/apsdehal\/repos","events_url":"https:\/\/api.github.com\/users\/apsdehal\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/apsdehal\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"assignees":[{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi ! Maybe one of the objects in the function is not deterministic across sessions ? You can read more about it and how to investigate here: https:\/\/huggingface.co\/docs\/datasets\/about_cache","Hi @apsdehal! Can you verify that replacing\r\n```python\r\ndef fetch_single_image(image_url, timeout=None, retries=0):\r\n    for _ in range(retries + 1):\r\n        try:\r\n            request = urllib.request.Request(\r\n                image_url,\r\n                data=None,\r\n                headers={\"user-agent\": get_datasets_user_agent()},\r\n            )\r\n            with urllib.request.urlopen(request, timeout=timeout) as req:\r\n                image = PIL.Image.open(io.BytesIO(req.read()))\r\n            break\r\n        except Exception:\r\n            image = None\r\n    return image\r\n```\r\nwith \r\n```python\r\nUSER_AGENT = get_datasets_user_agent()\r\n\r\ndef fetch_single_image(image_url, timeout=None, retries=0):\r\n    for _ in range(retries + 1):\r\n        try:\r\n            request = urllib.request.Request(\r\n                image_url,\r\n                data=None,\r\n                headers={\"user-agent\": USER_AGENT},\r\n            )\r\n            with urllib.request.urlopen(request, timeout=timeout) as req:\r\n                image = PIL.Image.open(io.BytesIO(req.read()))\r\n            break\r\n        except Exception:\r\n            image = None\r\n    return image\r\n```\r\nfixes the issue?","Thanks @mariosasko. That does fix the issue. In general, I think these image downloading utilities since they are being used by a lot of image dataset should be provided as a part of `datasets` library right to keep the logic consistent and READMEs smaller? If they already exists, that is also great, please point me to those. I saw that `http_get` does exist.","You can find my rationale (and a proposed solution) for why these utilities are not a part of `datasets` here: https:\/\/github.com\/huggingface\/datasets\/pull\/4100#issuecomment-1097994003.","Makes sense. But, I think as the number of image datasets as grow, more people are copying pasting original code from docs to work as it is while we make fixes to them later. I think we do need a central place for these to avoid that confusion as well as more easier access to image datasets. Should we restart that discussion, possible on slack?"],"created_at":1650613628000,"updated_at":1650992432000,"closed_at":1650980306000,"author_association":"MEMBER","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\n\r\nIt looks like that result of `.map` operation dataset are missing the cache when you reload the script and always run from scratch. In same interpretor session, they are able to find the cache and reload it. But, when you exit the interpretor and reload it, the downloading starts from scratch.\r\n\r\n## Steps to reproduce the bug\r\n\r\nUsing the example provided in `red_caps` dataset.\r\n```python\r\nfrom concurrent.futures import ThreadPoolExecutor\r\nfrom functools import partial\r\nimport io\r\nimport urllib\r\n\r\nimport PIL.Image\r\n\r\nimport datasets\r\nfrom datasets import load_dataset\r\nfrom datasets.utils.file_utils import get_datasets_user_agent\r\n\r\n\r\ndef fetch_single_image(image_url, timeout=None, retries=0):\r\n    for _ in range(retries + 1):\r\n        try:\r\n            request = urllib.request.Request(\r\n                image_url,\r\n                data=None,\r\n                headers={\"user-agent\": get_datasets_user_agent()},\r\n            )\r\n            with urllib.request.urlopen(request, timeout=timeout) as req:\r\n                image = PIL.Image.open(io.BytesIO(req.read()))\r\n            break\r\n        except Exception:\r\n            image = None\r\n    return image\r\n\r\n\r\ndef fetch_images(batch, num_threads, timeout=None, retries=0):\r\n    fetch_single_image_with_args = partial(fetch_single_image, timeout=timeout, retries=retries)\r\n    with ThreadPoolExecutor(max_workers=num_threads) as executor:\r\n        batch[\"image\"] = list(executor.map(lambda image_urls: [fetch_single_image_with_args(image_url) for image_url in image_urls], batch[\"image_url\"]))\r\n    return batch\r\n\r\n\r\ndef process_image_urls(batch):\r\n    processed_batch_image_urls = []\r\n    for image_url in batch[\"image_url\"]:\r\n        processed_example_image_urls = []\r\n        image_url_splits = re.findall(r\"http\\S+\", image_url)\r\n        for image_url_split in image_url_splits:\r\n            if \"imgur\" in image_url_split and \",\" in image_url_split:\r\n                for image_url_part in image_url_split.split(\",\"):\r\n                    if not image_url_part:\r\n                        continue\r\n                    image_url_part = image_url_part.strip()\r\n                    root, ext = os.path.splitext(image_url_part)\r\n                    if not root.startswith(\"http\"):\r\n                      root = \"http:\/\/i.imgur.com\/\" + root\r\n                    root = root.split(\"#\")[0]\r\n                    if not ext:\r\n                      ext = \".jpg\"\r\n                    ext = re.split(r\"[?%]\", ext)[0]\r\n                    image_url_part = root + ext\r\n                    processed_example_image_urls.append(image_url_part)\r\n            else:\r\n                processed_example_image_urls.append(image_url_split)\r\n        processed_batch_image_urls.append(processed_example_image_urls)\r\n    batch[\"image_url\"] = processed_batch_image_urls\r\n    return batch\r\n\r\n\r\ndset = load_dataset(\"red_caps\", \"jellyfish\")\r\ndset = dset.map(process_image_urls, batched=True, num_proc=4)\r\nfeatures = dset[\"train\"].features.copy()\r\nfeatures[\"image\"] = datasets.Sequence(datasets.Image())\r\nnum_threads = 5\r\ndset = dset.map(fetch_images, batched=True, batch_size=50, features=features, fn_kwargs={\"num_threads\": num_threads})\r\n```\r\n\r\nRun this in an interpretor or as a script twice and see that the cache is missed the second time.\r\n\r\n## Expected results\r\nAt reload there should not be any cache miss\r\n\r\n## Actual results\r\nEvery time script is run, cache is missed and dataset is built from scratch.\r\n\r\n## Environment info\r\n- `datasets` version: 2.1.1.dev0\r\n- Platform: Linux-4.19.0-20-cloud-amd64-x86_64-with-glibc2.10\r\n- Python version: 3.8.13\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.1\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4199\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4199\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4198","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4198\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4198\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4198\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4198","id":1211456559,"node_id":"I_kwDODunzps5INVwv","number":4198,"title":"There is no dataset","user":{"login":"wilfoderek","id":1625647,"node_id":"MDQ6VXNlcjE2MjU2NDc=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1625647?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/wilfoderek","html_url":"https:\/\/github.com\/wilfoderek","followers_url":"https:\/\/api.github.com\/users\/wilfoderek\/followers","following_url":"https:\/\/api.github.com\/users\/wilfoderek\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/wilfoderek\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/wilfoderek\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/wilfoderek\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/wilfoderek\/orgs","repos_url":"https:\/\/api.github.com\/users\/wilfoderek\/repos","events_url":"https:\/\/api.github.com\/users\/wilfoderek\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/wilfoderek\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1650568766000,"updated_at":1651577345000,"closed_at":1650607945000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Dataset viewer issue for '*name of the dataset*'\r\n\r\n**Link:** *link to the dataset viewer page*\r\n\r\n*short description of the issue*\r\n\r\nAm I the one who added this dataset ? Yes-No\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4198\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4198\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4197","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4197\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4197\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4197\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4197","id":1211342558,"node_id":"PR_kwDODunzps42kyXD","number":4197,"title":"Add remove_columns=True","user":{"login":"thomasw21","id":24695242,"node_id":"MDQ6VXNlcjI0Njk1MjQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/24695242?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/thomasw21","html_url":"https:\/\/github.com\/thomasw21","followers_url":"https:\/\/api.github.com\/users\/thomasw21\/followers","following_url":"https:\/\/api.github.com\/users\/thomasw21\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/thomasw21\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/thomasw21\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/thomasw21\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/thomasw21\/orgs","repos_url":"https:\/\/api.github.com\/users\/thomasw21\/repos","events_url":"https:\/\/api.github.com\/users\/thomasw21\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/thomasw21\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Any reason why we can't just do `[inputs.copy()]` in this line for in-place operations to not have effects anymore:\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/bf432011ff9155a5bc16c03956bc63e514baf80d\/src\/datasets\/arrow_dataset.py#L2232.\r\n\r\n(in the `batched` case, we can also copy the inputs' values (list objects) to ignore in-place modifications to the inputs' columns)\r\n\r\nI think `remove_columns=True` has no meaning, so I'm not a fan of this change.","@mariosasko copy does have a cost associated with it ... and plus you'll have to consider `deepcopy` Imagine columnds that are list of list of list of list .... Though I have to agree that `remove_columns=True` doesn't make sense (but, IMO, neither does it in its current use-case as it should refer to `input_columns`) ","Okay closing this PR for the following reasons:\r\n - `remove_columns=True` was expected to keep the `.update`-like operator for `.map`. I initially thought it would be a good way to ignore function side effects and only keep output of that function (cf. PR description).\r\n - expected `remove_columns=True` is a bad API according to @mariosasko and introduces unecessary changes for little gain (strictly equivalent to `remove_columns=dset.column_names`)"],"created_at":1650562093000,"updated_at":1650639101000,"closed_at":1650638730000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4197","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4197","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4197.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4197.patch","merged_at":null},"body":"This should fix all the issue we have with in place operations in mapping functions. This is crucial as where we do some weird things like:\r\n```\r\ndef apply(batch):\r\n    batch_size = len(batch[\"id\"])\r\n    batch[\"text\"] = [\"potato\" for _ range(batch_size)]\r\n    return {}\r\n\r\n# Columns are: {\"id\": int}\r\ndset.map(apply, batched=True, remove_columns=\"text\") # crashes because `text` is not in the original columns\r\ndset.map(apply, batched=True) # mapped datasets has `text` column\r\n```\r\n\r\nIn this PR we suggest to have `remove_columns=True` so that we ignore the input completely, and just use the output to generate mapped dataset. This means that inplace operations won't have any effects anymore.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4197\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4197\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4196","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4196\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4196\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4196\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4196","id":1211271261,"node_id":"I_kwDODunzps5IMohd","number":4196,"title":"Embed image and audio files in `save_to_disk`","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1650558318000,"updated_at":1650558318000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":null,"pull_request":null,"body":"Following https:\/\/github.com\/huggingface\/datasets\/pull\/4184, currently a dataset saved using `save_to_disk` doesn't actually contain the bytes of the image or audio files. Instead it stores the path to your local files. \r\n\r\nAdding `embed_external_files` and set it to True by default to save_to_disk would be kind of a breaking change since some users will get bigger Arrow files when updating the lib, but the advantages are nice:\r\n\r\n- the resulting dataset is self contained, in case you want to delete your cache for example or share it with someone else\r\n- users also upload these Arrow files to cloud storage via the fs parameter, and in this case they would expect to upload a self-contained dataset\r\n- consistency with push_to_hub\r\n\r\nThis can be implemented at the same time as sharding for `save_to_disk` for efficiency, and reuse the helpers from `push_to_hub` to embed the external files.\r\n\r\ncc @mariosasko ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4196\/reactions","total_count":5,"+1":5,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4196\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4194","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4194\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4194\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4194\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4194","id":1210958602,"node_id":"PR_kwDODunzps42jjD3","number":4194,"title":"Support lists of multi-dimensional numpy arrays","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650543746000,"updated_at":1652368594000,"closed_at":1652368120000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4194","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4194","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4194.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4194.patch","merged_at":1652368120000},"body":"Fix #4191.\r\n\r\nCC: @SaulLu ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4194\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4194\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4193","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4193\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4193\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4193\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4193","id":1210734701,"node_id":"PR_kwDODunzps42izQG","number":4193,"title":"Document save_to_disk and push_to_hub on images and audio files","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Good catch, I updated the docstrings"],"created_at":1650531876000,"updated_at":1650621355000,"closed_at":1650620971000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4193","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4193","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4193.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4193.patch","merged_at":1650620971000},"body":"Following https:\/\/github.com\/huggingface\/datasets\/pull\/4187, I explained in the documentation of `save_to_disk` and `push_to_hub` how they handle image and audio data.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4193\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4193\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4192","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4192\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4192\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4192\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4192","id":1210692554,"node_id":"I_kwDODunzps5IKbPK","number":4192,"title":"load_dataset can't load local dataset,Unable to find ...","user":{"login":"ahf876828330","id":33253979,"node_id":"MDQ6VXNlcjMzMjUzOTc5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/33253979?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ahf876828330","html_url":"https:\/\/github.com\/ahf876828330","followers_url":"https:\/\/api.github.com\/users\/ahf876828330\/followers","following_url":"https:\/\/api.github.com\/users\/ahf876828330\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ahf876828330\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ahf876828330\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ahf876828330\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ahf876828330\/orgs","repos_url":"https:\/\/api.github.com\/users\/ahf876828330\/repos","events_url":"https:\/\/api.github.com\/users\/ahf876828330\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ahf876828330\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi! :)\r\n\r\nI believe that should work unless `dataset_infos.json` isn't actually a dataset. For Hugging Face datasets, there is usually a file named `dataset_infos.json` which contains metadata about the dataset (eg. the dataset citation, license, description, etc). Can you double-check that `dataset_infos.json` isn't just metadata please?","Hi @ahf876828330, \r\n\r\nAs @stevhliu pointed out, the proper way to load a dataset is not trying to load its metadata file.\r\n\r\nIn your case, as the dataset script is local, you should better point to your local loading script:\r\n```python\r\ndataset = load_dataset(\"dataset\/opus_books.py\")\r\n```\r\n\r\nPlease, feel free to re-open this issue if the previous code snippet does not work for you.","> Hi! :)\r\n> \r\n> I believe that should work unless `dataset_infos.json` isn't actually a dataset. For Hugging Face datasets, there is usually a file named `dataset_infos.json` which contains metadata about the dataset (eg. the dataset citation, license, description, etc). Can you double-check that `dataset_infos.json` isn't just metadata please?\r\n\r\nYes\uff0cyou are right!So if I have a metadata dataset local,How can I turn it to a dataset that can be used by the load_dataset() function\uff1fAre there some examples?","The metadata file isn't a dataset so you can't turn it into one. You should try @albertvillanova's code snippet above (now merged in the docs [here](https:\/\/huggingface.co\/docs\/datasets\/master\/en\/loading#local-loading-script)), which uses your local loading script `opus_books.py` to:\r\n\r\n1. Download the actual dataset. \r\n2. Once the dataset is downloaded, `load_dataset` will load it for you."],"created_at":1650529738000,"updated_at":1650905517000,"closed_at":1650613193000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"\r\nTraceback (most recent call last):\r\n  File \"\/home\/gs603\/ahf\/pretrained\/model.py\", line 48, in <module>\r\n    dataset = load_dataset(\"json\",data_files=\"dataset\/dataset_infos.json\")\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/load.py\", line 1675, in load_dataset\r\n    **config_kwargs,\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/load.py\", line 1496, in load_dataset_builder\r\n    data_files=data_files,\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/load.py\", line 1155, in dataset_module_factory\r\n    download_mode=download_mode,\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/load.py\", line 800, in get_module\r\n    data_files = DataFilesDict.from_local_or_remote(patterns, use_auth_token=self.downnload_config.use_auth_token)\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/data_files.py\", line 582, in from_local_or_remote\r\n    if not isinstance(patterns_for_key, DataFilesList)\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/data_files.py\", line 544, in from_local_or_remote\r\n    data_files = resolve_patterns_locally_or_by_urls(base_path, patterns, allowed_extensions)\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/data_files.py\", line 194, in resolve_patterns_locally_or_by_urls\r\n    for path in _resolve_single_pattern_locally(base_path, pattern, allowed_extensions):\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/data_files.py\", line 144, in _resolve_single_pattern_locally\r\n    raise FileNotFoundError(error_msg)\r\nFileNotFoundError: Unable to find '\/home\/gs603\/ahf\/pretrained\/dataset\/dataset_infos.json' at \/home\/gs603\/ahf\/pretrained\r\n\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/33253979\/164413285-84ea65ac-9126-408f-9cd2-ce4751a5dd73.png)\r\n![image](https:\/\/user-images.githubusercontent.com\/33253979\/164413338-4735142f-408b-41d9-ab87-8484de2be54f.png)\r\n\r\nthe code is in the model.py,why I can't use the load_dataset function to load my local dataset?","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4192\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4192\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4191","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4191\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4191\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4191\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4191","id":1210028090,"node_id":"I_kwDODunzps5IH5A6","number":4191,"title":"feat: create an `Array3D` column from a list of arrays of dimension 2","user":{"login":"SaulLu","id":55560583,"node_id":"MDQ6VXNlcjU1NTYwNTgz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/55560583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/SaulLu","html_url":"https:\/\/github.com\/SaulLu","followers_url":"https:\/\/api.github.com\/users\/SaulLu\/followers","following_url":"https:\/\/api.github.com\/users\/SaulLu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/SaulLu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/SaulLu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/SaulLu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/SaulLu\/orgs","repos_url":"https:\/\/api.github.com\/users\/SaulLu\/repos","events_url":"https:\/\/api.github.com\/users\/SaulLu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/SaulLu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi @SaulLu, thanks for your proposal.\r\n\r\nJust I got a bit confused about the dimensions...\r\n- For the 2D case, you mention it is possible to create an `Array2D` from a list of arrays of dimension 1\r\n- However, you give an example of creating an `Array2D` from arrays of dimension 2:\r\n  - the values of `data_map` are arrays of dimension 2\r\n  - the outer list in `prepare_dataset_2D` should not be taken into account in the dimension counting, as it is used because in `map` you pass `batched=True`\r\n\r\nNote that for the 3D alternatives you mention:\r\n- In `prepare_dataset_3D_ter`, you create an `Array3D` from arrays of dimension 3:\r\n  - the array `data_map[index][np.newaxis, :, :]` has dimension 3\r\n  - the outer list in `prepare_dataset_3D_ter` is the one used by `batched=True`\r\n- In `prepare_dataset_3D_bis`, you create an `Array3D` from a list of list of lists:\r\n  - the value of `data_map[index].tolist()` is a list of lists\r\n  - it is enclosed by another list `[data_map[index].tolist()]`, thus giving a list of list of lists\r\n  - the outer list is the one used by `batched=True`\r\n\r\nTherefore, if I understand correctly, your request would be to be able to create an `Array3D` from a list of an array of dimension 2:\r\n- In `prepare_dataset_3D`, `data_map[index]` is an array of dimension 2\r\n- it is enclosed by a list `[data_map[index]]`, thus giving a list of an array of dimension 2\r\n- the outer list is the one used by `batched=True`\r\n\r\nPlease, feel free to tell me if I did not understand you correctly.","Hi @albertvillanova ,\r\n\r\nIndeed my message was confusing and you guessed right :smile: : I think would be interesting to be able to create an Array3D from a list of an array of dimension 2. \r\n\r\nFor the 2D case I should have given as a \"similar\" example:\r\n```python\r\n\r\ndata_map_1D = {\r\n    1: np.array([0.2, 0.4]),\r\n    2: np.array([0.1, 0.4]),\r\n}\r\n\r\ndef prepare_dataset_2D(batch):\r\n    batch[\"pixel_values\"] = [[data_map_1D[index]] for index in batch[\"id\"]]\r\n    return batch\r\n    \r\nds_2D = ds.map(\r\n    prepare_dataset_2D, \r\n    batched=True, \r\n    remove_columns=ds.column_names, \r\n    features=features.Features({\"pixel_values\": features.Array2D(shape=(1, 2), dtype=\"float32\")})\r\n)\r\n```"],"created_at":1650477872000,"updated_at":1652368120000,"closed_at":1652368120000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"**Is your feature request related to a problem? Please describe.**\r\n\r\nIt is possible to create an `Array2D` column from a list of arrays of dimension 1. Similarly, I think it might be nice to be able to create a `Array3D` column from a list of lists of arrays of dimension 1.\r\n\r\nTo illustrate my proposal, let's take the following toy dataset t:\r\n```python\r\nimport numpy as np\r\nfrom datasets import Dataset, features\r\n\r\ndata_map = {\r\n    1: np.array([[0.2, 0,4],[0.19, 0,3]]),\r\n    2: np.array([[0.1, 0,4],[0.19, 0,3]]),\r\n}\r\n\r\ndef create_toy_ds():\r\n    my_dict = {\"id\":[1, 2]}\r\n    return Dataset.from_dict(my_dict)\r\n\r\nds = create_toy_ds()\r\n```\r\n\r\nThe following 2D processing works without any errors raised:\r\n```python\r\ndef prepare_dataset_2D(batch):\r\n    batch[\"pixel_values\"] = [data_map[index] for index in batch[\"id\"]]\r\n    return batch\r\n    \r\nds_2D = ds.map(\r\n    prepare_dataset_2D, \r\n    batched=True, \r\n    remove_columns=ds.column_names, \r\n    features=features.Features({\"pixel_values\": features.Array2D(shape=(2, 3), dtype=\"float32\")})\r\n)\r\n```\r\n\r\nThe following 3D processing doesn't work:\r\n```python\r\ndef prepare_dataset_3D(batch):\r\n    batch[\"pixel_values\"] = [[data_map[index]] for index in batch[\"id\"]]\r\n    return batch\r\n    \r\nds_3D = ds.map(\r\n    prepare_dataset_3D, \r\n    batched=True, \r\n    remove_columns=ds.column_names, \r\n    features=features.Features({\"pixel_values\": features.Array3D(shape=(1,  2, 3, dtype=\"float32\")})\r\n)\r\n```\r\nThe error raised is:\r\n```\r\n---------------------------------------------------------------------------\r\nArrowInvalid                              Traceback (most recent call last)\r\n[<ipython-input-6-676547e4cd41>](https:\/\/localhost:8080\/#) in <module>()\r\n      3     batched=True,\r\n      4     remove_columns=ds.column_names,\r\n----> 5     features=features.Features({\"pixel_values\": features.Array3D(shape=(1, 2, 3), dtype=\"float32\")})\r\n      6 )\r\n\r\n12 frames\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\r\n   1971                 new_fingerprint=new_fingerprint,\r\n   1972                 disable_tqdm=disable_tqdm,\r\n-> 1973                 desc=desc,\r\n   1974             )\r\n   1975         else:\r\n\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in wrapper(*args, **kwargs)\r\n    518             self: \"Dataset\" = kwargs.pop(\"self\")\r\n    519         # apply actual function\r\n--> 520         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    521         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    522         for dataset in datasets:\r\n\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in wrapper(*args, **kwargs)\r\n    485         }\r\n    486         # apply actual function\r\n--> 487         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    488         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    489         # re-apply format to the output\r\n\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/fingerprint.py](https:\/\/localhost:8080\/#) in wrapper(*args, **kwargs)\r\n    456             # Call actual function\r\n    457 \r\n--> 458             out = func(self, *args, **kwargs)\r\n    459 \r\n    460             # Update fingerprint of in-place transforms + update in-place history of transforms\r\n\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in _map_single(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\r\n   2354                                 writer.write_table(batch)\r\n   2355                             else:\r\n-> 2356                                 writer.write_batch(batch)\r\n   2357                 if update_data and writer is not None:\r\n   2358                     writer.finalize()  # close_stream=bool(buf_writer is None))  # We only close if we are writing in a file\r\n\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_writer.py](https:\/\/localhost:8080\/#) in write_batch(self, batch_examples, writer_batch_size)\r\n    505             col_try_type = try_features[col] if try_features is not None and col in try_features else None\r\n    506             typed_sequence = OptimizedTypedSequence(batch_examples[col], type=col_type, try_type=col_try_type, col=col)\r\n--> 507             arrays.append(pa.array(typed_sequence))\r\n    508             inferred_features[col] = typed_sequence.get_inferred_type()\r\n    509         schema = inferred_features.arrow_schema if self.pa_writer is None else self.schema\r\n\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pyarrow\/array.pxi in pyarrow.lib.array()\r\n\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pyarrow\/array.pxi in pyarrow.lib._handle_arrow_array_protocol()\r\n\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_writer.py](https:\/\/localhost:8080\/#) in __arrow_array__(self, type)\r\n    175                     storage = list_of_np_array_to_pyarrow_listarray(data, type=pa_type.value_type)\r\n    176                 else:\r\n--> 177                     storage = pa.array(data, pa_type.storage_dtype)\r\n    178                 return pa.ExtensionArray.from_storage(pa_type, storage)\r\n    179 \r\n\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pyarrow\/array.pxi in pyarrow.lib.array()\r\n\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pyarrow\/array.pxi in pyarrow.lib._sequence_to_array()\r\n\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pyarrow\/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pyarrow\/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: Can only convert 1-dimensional array values\r\n```\r\n\r\n**Describe the solution you'd like**\r\nNo error in the second scenario and an identical result to the following snippets.\r\n\r\n**Describe alternatives you've considered**\r\nThere are other alternatives that work such as:\r\n```python\r\n\r\ndef prepare_dataset_3D_bis(batch):\r\n    batch[\"pixel_values\"] = [[data_map[index].tolist()] for index in batch[\"id\"]]\r\n    return batch\r\n\r\nds_3D_bis = ds.map(\r\n    prepare_dataset_3D_bis, \r\n    batched=True, \r\n    remove_columns=ds.column_names, \r\n    features=features.Features({\"pixel_values\": features.Array3D(shape=(1, 2, 3), dtype=\"float32\")})\r\n)\r\n```\r\nor\r\n```python\r\ndef prepare_dataset_3D_ter(batch):\r\n    batch[\"pixel_values\"] = [data_map[index][np.newaxis, :, :] for index in batch[\"id\"]]\r\n    return batch\r\n\r\nds_3D_ter = ds.map(\r\n    prepare_dataset_3D_ter, \r\n    batched=True, \r\n    remove_columns=ds.column_names, \r\n    features=features.Features({\"pixel_values\": features.Array3D(shape=(1,  2, 3), dtype=\"float32\")})\r\n)\r\n```\r\nBut both solutions require the user to be aware that `data_map[index]` is an `np.array` type.\r\n\r\ncc @lhoestq as we discuss this offline :smile: ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4191\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4191\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4190","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4190\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4190\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4190\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4190","id":1209901677,"node_id":"PR_kwDODunzps42gK3y","number":4190,"title":"Deprecate `shard_size` in `push_to_hub` in favor of `max_shard_size`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650470881000,"updated_at":1650635905000,"closed_at":1650635520000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4190","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4190","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4190.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4190.patch","merged_at":1650635520000},"body":"This PR adds a `max_shard_size` param to `push_to_hub` and deprecates `shard_size` in favor of this new param to have a more descriptive name (a shard has at most the `shard_size` bytes in `push_to_hub`) for the param and to align the API with [Transformers](https:\/\/github.com\/huggingface\/transformers\/blob\/ff06b177917384137af2d9585697d2d76c40cdfc\/src\/transformers\/modeling_utils.py#L1350).\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4190\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4190\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4189","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4189\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4189\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4189\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4189","id":1209881351,"node_id":"PR_kwDODunzps42gGv5","number":4189,"title":"Document how to use FAISS index for special operations","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650469916000,"updated_at":1651826590000,"closed_at":1651826152000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4189","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4189","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4189.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4189.patch","merged_at":1651826152000},"body":"Document how to use FAISS index for special operations, by accessing the index itself.\r\n\r\nClose #4029.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4189\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4189\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4188","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4188\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4188\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4188\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4188","id":1209740957,"node_id":"PR_kwDODunzps42fpMv","number":4188,"title":"Support streaming cnn_dailymail dataset","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Did you run the `datasets-cli` command before merging to make sure you generate all the examples ?"],"created_at":1650463476000,"updated_at":1652276346000,"closed_at":1650469969000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4188","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4188","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4188.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4188.patch","merged_at":1650469969000},"body":"Support streaming cnn_dailymail dataset.\r\n\r\nFix #3969.\r\n\r\nCC: @severo ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4188\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4188\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4187","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4187\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4187\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4187\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4187","id":1209721532,"node_id":"PR_kwDODunzps42flGp","number":4187,"title":"Don't duplicate data when encoding audio or image","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","I'm not familiar with the concept of streaming vs non-streaming in HF datasets. I just wonder that you have the distinction here. Why doesn't it work to always make use of `bytes`? \"using a local file - which is often required for audio\" - why would that be?\r\n\r\nThe `path` would always point to some location in the `cache_dir`? I think this can be problematic. I would have expected that after I did `dataset.save_to_disk(...)` that I can remove the cache dir. But maybe just because I'm not familiar with HF. Or maybe the docs can be improved to clarify this.\r\n","We could always load every data file into `bytes` and save it this way the audio as bytes in `arrow` format, but the problem then would be that it makes the `file` column useless, *i.e.* people cannot inspect the audio file locally anymore or else they would need to first save bytes as a file which is not evident. This either breaks backwards compatibility or forces the user to stored 2x the required size locally. There was a longer discussion here: https:\/\/github.com\/huggingface\/datasets\/issues\/3663\r\n\r\nIt's a good argument though that `dataset.save_to_disk(...)` should save everything that is needed to the disk and should be independent of other folders, but I do think the arguments of #3663 to not break backwards compatibility and to allow people to inspect the downloaded audio files locally are a bit more important here. \r\n\r\nBut maybe, we could add a flag, `save_files_as_bytes` or `make_independent`, `make_self_contained` or a better name to `save_to_disk(...)` and `push_to_hub(...)` that would allow to make the resulting folder completely independent. ","What do you think @mariosasko @lhoestq @polinaeterna @anton-l ?\r\n","For context: you can either store the path to local images or audio files, or the bytes of those files.\r\n\r\nIf your images and audio files are local files, then the arrow file from `save_to_disk` will store paths to these files.\r\nIf you want to include the bytes or your images or audio files instead, you must `read()` those files first.\r\nThis can be done by storing the \"bytes\" instead of the \"path\" of the images or audio files.\r\n\r\nOn the other hand, the resulting Parquet files from `push_to_hub` are self-contained, so that anyone can reload the dataset from the Hub. If your dataset contains image or audio data, the Parquet files will store the bytes of your images or audio files.\r\n\r\nFor now I just updated the documentation: https:\/\/github.com\/huggingface\/datasets\/pull\/4193. Maybe we can also embed the image and audio bytes in `save_to_disk` when we implement sharding, so that is can be done as efficiently as `push_to_hub`.\r\n\r\nAnyway, merging this one :)"],"created_at":1650462637000,"updated_at":1650532620000,"closed_at":1650532247000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4187","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4187","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4187.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4187.patch","merged_at":1650532247000},"body":"Right now if you pass both the `bytes` and a local `path` for audio or image data, then the `bytes` are unnecessarily written in the Arrow file, while we could just keep the local `path`.\r\n\r\nThis PR discards the `bytes` when the audio or image file exists locally.\r\n\r\nIn particular it's common for audio datasets builders to provide both the bytes and the local path in order to work for both streaming (using the bytes) and non-streaming mode (using a local file - which is often required for audio).\r\n\r\ncc @patrickvonplaten ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4187\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4187\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4186","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4186\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4186\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4186\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4186","id":1209463599,"node_id":"PR_kwDODunzps42evF5","number":4186,"title":"Fix outdated docstring about default dataset config","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650449091000,"updated_at":1650632084000,"closed_at":1650631711000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4186","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4186","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4186.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4186.patch","merged_at":1650631711000},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4186\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4186\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4185","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4185\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4185\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4185\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4185","id":1209429743,"node_id":"I_kwDODunzps5IFm7v","number":4185,"title":"Librispeech documentation, clarification on format","user":{"login":"albertz","id":59132,"node_id":"MDQ6VXNlcjU5MTMy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59132?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertz","html_url":"https:\/\/github.com\/albertz","followers_url":"https:\/\/api.github.com\/users\/albertz\/followers","following_url":"https:\/\/api.github.com\/users\/albertz\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertz\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertz\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertz\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertz\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertz\/repos","events_url":"https:\/\/api.github.com\/users\/albertz\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertz\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["(@patrickvonplaten )","Also cc @lhoestq here","The documentation in the code is definitely outdated - thanks for letting me know, I'll remove it in https:\/\/github.com\/huggingface\/datasets\/pull\/4184 .\r\n\r\nYou're exactly right `audio` `array` already decodes the audio file to the correct waveform. This is done on the fly, which is also why one should **not** do `ds[\"audio\"][\"array\"][0]` as this will decode all dataset samples, but instead `ds[0][\"audio\"][\"array\"]` see: https:\/\/huggingface.co\/docs\/datasets\/audio_process#audio-datasets\r\n\r\n","So, again to clarify: On disk, only the raw flac file content is stored? Is this also the case after `save_to_disk`?\r\n\r\nAnd is it simple to also store it re-encoded as ogg or mp3 instead?\r\n","Hey, \r\n\r\nSorry yeah I was just about to look into this! We actually had an outdated version of Librispeech ASR that didn't save any files, but instead converted the audio files to a byte string, then was then decoded on-the-fly. This however is not very user-friendly so we recently decided to instead show the full path of the audio files with the `path` parameter.\r\n\r\nI'm currently changing this for Librispeech here: https:\/\/github.com\/huggingface\/datasets\/pull\/4184 .\r\nYou should be able to see the audio file in the original `flac` format under `path` then. I don't think it's a good idea to convert to MP3 out-of-the-box, but we could maybe think about some kind of convert function for audio datasets cc @lhoestq ? ","> I don't think it's a good idea to convert to MP3 out-of-the-box, but we could maybe think about some kind of convert function for audio datasets cc @lhoestq ?\r\n\r\nSure, I would expect that `load_dataset(\"librispeech_asr\")` would give you the original (not re-encoded) data (flac or already decoded). So such re-encoding logic would be some separate generic function. So I could do sth like `dataset.reencode_as_ogg(**ogg_encode_opts).save_to_disk(...)` or so.\r\n","A follow-up question: I wonder whether a Parquet dataset is maybe more what we actually want to have? (Following also my comment here: https:\/\/github.com\/huggingface\/datasets\/pull\/4184#issuecomment-1105045491.) Because I think we actually would prefer to embed the data content in the dataset.\r\n\r\nSo, instead of `save_to_disk`\/`load_from_disk`, we would use `to_parquet`,`from_parquet`? Is there any downside? Are arrow files more efficient?\r\n\r\nRelated is also the doc update in #4193.\r\n","`save_to_disk` saves the dataset as an Arrow file, which is the format we use to load a dataset using memory mapping. This way the dataset does not fill your RAM, but is read from your disk instead.\r\n\r\nTherefore you can directly reload a dataset saved with `save_to_disk` using `load_from_disk`.\r\n\r\nParquet files are used for cold storage: to use memory mapping on a Parquet dataset, you first have to convert it to Arrow. We use Parquet to reduce the I\/O when pushing\/downloading data from the Hugging face Hub. When you load a Parquet file from the Hub, it is converted to Arrow on the fly during the download."],"created_at":1650447355000,"updated_at":1650538853000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"https:\/\/github.com\/huggingface\/datasets\/blob\/cd3ce34ab1604118351e1978d26402de57188901\/datasets\/librispeech_asr\/librispeech_asr.py#L53\r\n\r\n> Note that in order to limit the required storage for preparing this dataset, the audio\r\n> is stored in the .flac format and is not converted to a float32 array. To convert, the audio\r\n> file to a float32 array, please make use of the `.map()` function as follows:\r\n> \r\n> ```python\r\n> import soundfile as sf\r\n> def map_to_array(batch):\r\n>     speech_array, _ = sf.read(batch[\"file\"])\r\n>     batch[\"speech\"] = speech_array\r\n>     return batch\r\n> dataset = dataset.map(map_to_array, remove_columns=[\"file\"])\r\n> ```\r\n\r\nIs this still true?\r\n\r\nIn my case, `ds[\"train.100\"]` returns:\r\n```\r\nDataset({\r\n    features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\r\n    num_rows: 28539\r\n})\r\n```\r\nand taking the first instance yields:\r\n```\r\n{'file': '374-180298-0000.flac',\r\n 'audio': {'path': '374-180298-0000.flac',\r\n  'array': array([ 7.01904297e-04,  7.32421875e-04,  7.32421875e-04, ...,\r\n         -2.74658203e-04, -1.83105469e-04, -3.05175781e-05]),\r\n  'sampling_rate': 16000},\r\n 'text': 'CHAPTER SIXTEEN I MIGHT HAVE TOLD YOU OF THE BEGINNING OF THIS LIAISON IN A FEW LINES BUT I WANTED YOU TO SEE EVERY STEP BY WHICH WE CAME I TO AGREE TO WHATEVER MARGUERITE WISHED',\r\n 'speaker_id': 374,\r\n 'chapter_id': 180298,\r\n 'id': '374-180298-0000'}\r\n```\r\n\r\nThe `audio` `array` seems to be already decoded. So such convert\/decode code as mentioned in the doc is wrong?\r\n\r\nBut I wonder, is it actually stored as flac on disk, and the decoding is done on-the-fly? Or was it decoded already during the preparation and is stored as raw samples on disk?\r\n\r\nNote that I also used `datasets.load_dataset(\"librispeech_asr\", \"clean\").save_to_disk(...)` and then `datasets.load_from_disk(...)` in this example. Does this change anything on how it is stored on disk?\r\n\r\nA small related question: Actually I would prefer to even store it as mp3 or ogg on disk. Is this easy to convert?","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4185\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4185\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4184","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4184\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4184\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4184\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4184","id":1208592669,"node_id":"PR_kwDODunzps42cB2j","number":4184,"title":"[Librispeech] Add 'all' config","user":{"login":"patrickvonplaten","id":23423619,"node_id":"MDQ6VXNlcjIzNDIzNjE5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/23423619?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/patrickvonplaten","html_url":"https:\/\/github.com\/patrickvonplaten","followers_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/followers","following_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/orgs","repos_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/repos","events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Fix https:\/\/github.com\/huggingface\/datasets\/issues\/4179","_The documentation is not available anymore as the PR was closed or merged._","Just that I understand: With this change, simply doing `load_dataset(\"librispeech_asr\")` is possible and returns the whole dataset?\r\n\r\nAnd to get the subsets, I do sth like:\r\n```python\r\nds = load_dataset(\"librispeech_asr\")\r\ntrain_ds = ds[\"train\"]\r\ndev_clean_ds = ds[\"dev-clean\"]\r\ndev_other_ds = ds[\"dev-other\"]\r\ntest_clean_ds = ds[\"test-clean\"]\r\ntest_other_ds = ds[\"test-other\"]\r\n```\r\n?\r\n","> Just that I understand: With this change, simply doing `load_dataset(\"librispeech_asr\")` is possible and returns the whole dataset?\r\n> \r\n> And to get the subsets, I do sth like:\r\n> \r\n> ```python\r\n> ds = load_dataset(\"librispeech_asr\")\r\n> train_ds = ds[\"train\"]\r\n> dev_clean_ds = ds[\"dev-clean\"]\r\n> dev_other_ds = ds[\"dev-other\"]\r\n> test_clean_ds = ds[\"test-clean\"]\r\n> test_other_ds = ds[\"test-other\"]\r\n> ```\r\n> \r\n> ?\r\n\r\nYou could do:\r\n\r\n\r\n```python\r\nds = load_dataset(\"librispeech_asr\", \"all\")  # <- note that we have to pass a config\r\ntrain_ds = ds[\"train\"]\r\ndev_clean_ds = ds[\"dev-clean\"]\r\ndev_other_ds = ds[\"dev-other\"]\r\ntest_clean_ds = ds[\"test-clean\"]\r\ntest_other_ds = ds[\"test-other\"]\r\n```","So, `load_dataset(\"librispeech_asr\")` is not possible, it must be `load_dataset(\"librispeech_asr\", \"all\")`?\r\n\r\nWhy is that?\r\n\r\nThe docs say:\r\n```\r\nname: `str` name, optional configuration for the dataset that affects the data generated on disk. Different\r\n    `builder_config`s will have their own subdirectories and versions.\r\n    If not provided, uses the first configuration in self.BUILDER_CONFIGS\r\n```\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/cd3ce34ab1604118351e1978d26402de57188901\/src\/datasets\/builder.py#L228\r\n\r\nOr maybe you could just define `DEFAULT_CONFIG_NAME`?\r\n","> If not provided, uses the first configuration in self.BUILDER_CONFIGS\r\n\r\nOh crap this is outdated documentation. No it doesn't take the first config by default.\r\n\r\nEDIT: opened a PR to fix this: https:\/\/github.com\/huggingface\/datasets\/pull\/4186","> No it doesn't take the first config by default.\r\n\r\nBut defining `DEFAULT_CONFIG_NAME` would work?\r\n\r\nSo should we define `DEFAULT_CONFIG_NAME = \"all\"` here as well? I think this is a reasonable default config.\r\n\r\nDon't most datasets have some default config?\r\n","> But defining DEFAULT_CONFIG_NAME would work?\r\n>\r\n> So should we define DEFAULT_CONFIG_NAME = \"all\" here as well? I think this is a reasonable default config.\r\n\r\nYes that would work, and I also find it reasonable to do it :)\r\n\r\n> Don't most datasets have some default config?\r\n\r\nMost datasets only have one configuration, so the single configuration is the default one. Then other datasets gave several configurations, and whether they have a default one is decided case-by-case.\r\n\r\ne.g. `glue` is a benchmark and doesn't have a default task, one must choose which task of `glue` they want to use explicitely.","Thanks a lot for the feedback! \r\n\r\nUsing `\"all\"` now as the default config. I changed the layout a bit so that there is not a single \"train\", but instead we have multiple \"train.clean.100\", \"train.clean.360\", \"train.other.500\". This way we don't even need to do filtering and it's also cleaner IMO.\r\n\r\n@albertz - you should now be able to do the following:\r\n\r\n```python\r\nload_dataset(\"librispeech_asr\")  # <- run this once to download, prepare dataset and cache everything\r\n\r\n# The following operations will be very fast since all the downloading and processing is already cached\r\ntrain_1 = load_dataset(\"librispeech_asr\", split=\"train.clean.100\")\r\nprint(train_1)\r\ntrain_2 = load_dataset(\"librispeech_asr\", split=\"train.clean.100+train.clean.360\")\r\nprint(train_2)\r\ntrain_full = load_dataset(\"librispeech_asr\", split=\"train.clean.100+train.clean.360+train.other.500\")\r\nprint(train_full)\r\ndev_clean_ds = load_dataset(\"librispeech_asr\", split=\"validation.clean\")\r\nprint(dev_clean_ds)\r\ndev_other_ds = load_dataset(\"librispeech_asr\", split=\"validation.other\")\r\nprint(dev_other_ds)\r\ntest_clean_ds = load_dataset(\"librispeech_asr\", split=\"test.clean\")\r\nprint(test_clean_ds)\r\ntest_other_ds = load_dataset(\"librispeech_asr\", split=\"test.other\")\r\nprint(test_other_ds)\r\n```\r\n\r\n\r\n","Think this way we have the best of both worlds. Also @lhoestq, I think we could highlight better in the docs that it's possible to combine different splits. We do this actually quite a lot for speech. For Common Voice many people include \"validation\" in the training if the data is too small, e.g.: https:\/\/github.com\/huggingface\/transformers\/blob\/ff06b177917384137af2d9585697d2d76c40cdfc\/examples\/pytorch\/speech-recognition\/run_speech_recognition_ctc.py#L147\r\n\r\nShould we maybe add a short section to the loading tutorial here: https:\/\/huggingface.co\/docs\/datasets\/v2.1.0\/en\/loading#hugging-face-hub ? (Happy to do it)","Is there any advantage or difference in calling `load_dataset` multiple times for each split? Or why not just call `load_dataset` once and then access each split?\r\n\r\nNote in our case, we cannot really use the caching mechanism because we have a recipe pipeline used by multiple users (and I think a common cache dir for all users might end up in problems) and we basically would use `load_dataset(\"librispeech_asr\").save_to_disk(...)` and then later `load_from_disk(...)`. (See here: https:\/\/github.com\/rwth-i6\/i6_core\/pull\/253)\r\n\r\nSo with `load_from_disk`, we cannot really provide the split this way, so we anyway would do sth like:\r\n```python\r\nds = datasets.load_from_disk(...)\r\ntrain = ds[\"train\"]\r\n```\r\nOr with your latest proposal, it would look like:\r\n```python\r\nds = datasets.load_from_disk(...)\r\ntrain_ds = datasets.concatenate_datasets(\r\n  [ds[\"train.clean.100\"], ds[\"train.clean.360\"], ds[\"train.other.500\"]])\r\n```\r\nright?\r\n","> Is there any advantage or difference in calling `load_dataset` multiple times for each split? Or why not just call `load_dataset` once and then access each split?\r\n> \r\n> Note in our case, we cannot really use the caching mechanism because we have a recipe pipeline used by multiple users (and I think a common cache dir for all users might end up in problems) and we basically would use `load_dataset(\"librispeech_asr\").save_to_disk(...)` and then later `load_from_disk(...)`. (See here: [rwth-i6\/i6_core#253](https:\/\/github.com\/rwth-i6\/i6_core\/pull\/253))\r\n> \r\n> So with `load_from_disk`, we cannot really provide the split this way, so we anyway would do sth like:\r\n> \r\n> ```python\r\n> ds = datasets.load_from_disk(...)\r\n> train = ds[\"train\"]\r\n> ```\r\n> \r\n> Or with your latest proposal, it would look like:\r\n> \r\n> ```python\r\n> ds = datasets.load_from_disk(...)\r\n> train_ds = datasets.concatenate_datasets(\r\n>   [ds[\"train.clean.100\"], ds[\"train.clean.360\"], ds[\"train.other.500\"]])\r\n> ```\r\n> \r\n> right?\r\n\r\nI see the use case! The only advantage by calling `datasets` multiple times is that one can easily \"merge\" splits with `\"+\"`, but yeah you can do the exact same with `concatenate`.\r\n\r\n@lhoestq what do you think is the best approach with `load_from_disk`? \r\n\r\n@albertz, you could also define the `cache_dir` when doing `load_dataset(...)` which will then put all the relevant `arrow` files int the cache dir that you defined, e.g.:\r\n\r\n```python\r\nload_dataset(\"librispeech_asr\", cache_dir=\"\/easy\/to\/access\/directory\")\r\n```","@albertz, I took a read through https:\/\/github.com\/rwth-i6\/i6_core\/pull\/253 . \r\n\r\nI think the best would be the following:\r\n\r\n1. Do `ds = load_dataset(..., cache_dir=\"\/dir\/that\/is\/easy\/to\/access\")` <- having merged this PR, this will save all the original `.flac` files in the `cache_dir`\r\n2. Do `ds.save_to_disk(\"local\/path\")` this should then only save the `arrow.format` with a `path` string to the audio files which are located in `cache_dir` <- this won't require a lot of memory after https:\/\/github.com\/huggingface\/datasets\/pull\/4184#discussion_r854132740 is fixed and can be done for each person individually.\r\n3. `ds = datasets.load_from_disk(\"local\/path\")` can the be used. An object of `ds` will then have a `path` variable that links to the original audio files in the `cache_dir`. You can change these audio files then easily to `.mp3. You could do this with the `.map(...)` function, e.g. define a function that maps through all audio files, load them and then save them on disk afterward.","@lhoestq - I think this one is good to go","> @albertz, I took a read through [rwth-i6\/i6_core#253](https:\/\/github.com\/rwth-i6\/i6_core\/pull\/253) .\r\n> \r\n> I think the best would be the following:\r\n> \r\n> 1. Do `ds = load_dataset(..., cache_dir=\"\/dir\/that\/is\/easy\/to\/access\")` <- having merged this PR, this will save all the original `.flac` files in the `cache_dir`\r\n> 2. Do `ds.save_to_disk(\"local\/path\")` this should then only save the `arrow.format` with a `path` string to the audio files which are located in `cache_dir` <- this won't require a lot of memory after [[Librispeech] Add 'all' config\u00a0#4184 (comment)](https:\/\/github.com\/huggingface\/datasets\/pull\/4184#discussion_r854132740) is fixed and can be done for each person individually.\r\n> 3. `ds = datasets.load_from_disk(\"local\/path\")` can the be used. An object of `ds` will then have a `path` variable that links to the original audio files in the `cache_dir`. You can change these audio files then easily to `.mp3. You could do this with the `.map(...)` function, e.g. define a function that maps through all audio files, load them and then save them on disk afterward.\r\n\r\nOh, so you say that our current implementation in https:\/\/github.com\/rwth-i6\/i6_core\/pull\/253 is broken? Because our cache dir is just some temp directory which will be removed afterwards, and we just store what we get out of `save_to_disk`. I think it would be good to clarify that in the doc of `save_to_disk`, that this is not enough and can depend on files from the cache dir. (@dthulke)\r\n\r\nSo, you say we anyway need to share the cache dir among users? But we would want to make sure that after the initial download and preparation of the data, this is set to readonly, because we want to make sure that other people will not modify the data in any way. Right?\r\n\r\nBut then, we don't really need the `save_to_disk` and `load_from_disk` at all, right?\r\n","@albertz \r\n\r\n> Oh, so you say that our current implementation in https:\/\/github.com\/rwth-i6\/i6_core\/pull\/253 is broken? Because our cache dir is just some temp directory which will be removed afterwards, and we just store what we get out of save_to_disk. I think it would be good to clarify that in the doc of save_to_disk, that this is not enough and can depend on files from the cache dir. (@dthulke)\r\n\r\nOh, I wasn't aware that audio files are handled this way. Then we should have the cache directory as an additional job output, so that we keep the audio files. \r\n\r\n> So, you say we anyway need to share the cache dir among users?\r\n\r\nNo, the cache dir can still be a directory in the job output folder. Then the audio paths in the corresponding dataset column correspond to the flac files in that directory. This way the \"output\" of the job is contained into the job directory and we don't write files to a global cache directory that is independent of the sisyphus graph.\r\n\r\nIf we want to share the audio data between different users, we can just link to a central instance of the job (similar to how we do it with the `DownloadLibriSpeechCorpusJob`).","@dthulke - that's a good point actually! So you can do both things:\r\n\r\n1. Convert all audio files to bytes. Bytes can be saved by `arrow` so in this case you can do `save_to_disk(...)`, but then you cannot really inspect the audio files locally as they'll just be saved within a large arrow file (this actually used to be the default case but we're changing this now). The problem of this is summarized here a bit:  https:\/\/github.com\/huggingface\/datasets\/issues\/3663 . You can still do this if you'd like, e.g. you could do:\r\n\r\n```python\r\nds = load_dataset(\"librispeech_asr\")\r\n\r\ndef read_file(batch):\r\n    with open(batch[\"file\"], \"r\") as f:\r\n        batch[\"bytes\"] = f.read()   \r\n    return batch\r\n\r\nds = ds.map(read_file)\r\nds.save_to_disk(\"\/path\") <- the saved arrow object will now contain everything you need\r\n```\r\n\r\nhowever this is not recommend - it's should be much easier to just save the path to the downloaded audio files.\r\n\r\n2. Not convert audio files to bytes, but just leave them in their original file format. Then only the path to the original files will be save in arrow. This will be the default case. This means that when you do `load_dataset(...)` both the orginal audio data and the arrow file will be saved in the `cache_dir` (which can be saved locally for every user or in a shared cache - we actually use a shared cache quite a bit at Hugging Face). When do you do `save_to_disk(...)` now only the `path` will be saved in `arrow` format (after this PR is merged, you'll see that the `arrow files should be very light weight` meaning that `save_to_disk(...)` can be done for every user, but has a dependency on the `cache_dir` (because the audio files live there).\r\n\r\n=> Now what you could do as well would be to simply move all the audio files to the folder you want (the `save_to_disk(...)` folder) and then change the path of every sample to this folder (maybe with `map(...)`) and then this folder would be self contained. I do however think it's better to just specific a `cache_dir` and re-use `load_dataset(...)` every time instead of `load_from_disk` or `save_to_disk(...)`. Note that you can even pass the relevant cache files to `load_dataset(...)` here: https:\/\/huggingface.co\/docs\/datasets\/v2.1.0\/en\/package_reference\/loading_methods#datasets.load_dataset.data_files in which case you can be 100% sure that nothing is redownloaded. \r\n\r\nWe discussed storing audio files quite a bit, e.g. see: https:\/\/github.com\/huggingface\/datasets\/issues\/3663 and had (too many) changes around this topic recently, but we've come to the conclusion that the best is to leave the audio format in the format it was originally (`.flac` for Librispeech) so that the user can easily inspect it \/ understand the data. Arrow cannot save data is `.flac` so we'll just save a path to the original data. Curious to hear your guys opinion on this as well.","So what I would suggest here is to do the following:\r\n\r\n1. Do `load_dataset(..., cache_dir=\/a\/read-only\/folder)`\r\n2. \r\n- Either just re-use `load_dataset(..., cache_dir=...)` which should always re-use the data in the `cache_dir` since the hash of the url matches - so there should never be any duplicated downloading \r\n\r\nor \r\n\r\n- If you want to store the files in MP3 locally, first convert the files to MP3 in the read-only folder, then take do `ds.save_to_disk(\/some\/path)` which will save the correct path to the read-only folder to MP3 and then you can easily re-use the small arrow dataset that is saved in `\/some\/path`","> So what I would suggest here is to do the following:\r\n> \r\n> 1. Do `load_dataset(..., cache_dir=\/a\/read-only\/folder)`\r\n> \r\n> * Either just re-use `load_dataset(..., cache_dir=...)` which should always re-use the data in the `cache_dir` since the hash of the url matches - so there should never be any duplicated downloading\r\n> \r\n> or\r\n> \r\n> * If you want to store the files in MP3 locally, first convert the files to MP3 in the read-only folder, then take do `ds.save_to_disk(\/some\/path)` which will save the correct path to the read-only folder to MP3 and then you can easily re-use the small arrow dataset that is saved in `\/some\/path`\r\n\r\nAlso relevant here: https:\/\/github.com\/huggingface\/datasets\/issues\/3663","I also added some documentation about how `save_to_disk` handles audio files here: https:\/\/github.com\/huggingface\/datasets\/pull\/4193","> > So, you say we anyway need to share the cache dir among users?\r\n> \r\n> No, the cache dir can still be a directory in the job output folder.\r\n\r\n@dthulke But this is what I mean. When we share the job output folder, it means we share the cache dir among users.\r\n\r\nI wonder if `load_dataset(..., cache_dir=job_output_cache_dir)` is always save to do then, that it really would not modify the `job_output_cache_dir`.\r\n\r\nWe could enforce that by making the `job_output_cache_dir` read-only afterwards. We currently don't do this.\r\n\r\n@patrickvonplaten @dthulke But in any case, we actually prefer the data content to be inside the dataset (the arrow files). Lots of small files would be very problematic for our cache manager. We have one main copy of the data on NFS, but accessing the NFS directly by all computing nodes is not feasible, so the cache manager will have copies of the files on the nodes. So it means, whenever we access some file, we query the cache manager DB whether the file is already cached somewhere (some other computing node) and if so, it copies it from the other computing node and not from NFS. This works very well when there are not too many files (but the files can be big). So, we want to have only a few but big files. Even for NFS access this is much better.\r\n\r\nI also commented in #3663.\r\n","Hey @albertz @dthulke,\r\n\r\nThanks a lot for your input! \r\n\r\nWe've discussed quite a bit with @lhoestq and we think the best approach is the following:\r\n\r\n\r\na)\r\n`load_dataset(...)` will not store both bytes and the files because this would mean that 3x the size of the dataset would often be needed (1. the compressed `tar.gz` file, 2. the extracted file b, 3. the raw bytes in arrow format). \r\n\r\nFor canonical datasets like librispeech and common voice I think we want to keep the dataset filenames because of i) no breaking changes and ii) reasons explained in #3663\r\n\r\nHowever it's also trivial to write your own datasetset downloading script of librispeech and just not extract the folder e.g. this line: https:\/\/huggingface.co\/datasets\/common_voice\/blob\/main\/common_voice.py#L671\r\n\r\nAnd then it'll be allowed to save the bytes and the dataset will be self-contained out-of-the-box when using `load_dataset(...)`\r\n\r\nb) Now, one major problem that you guys uncovered is that `save_to_disk(...)` is currently not necessarily saving a dataset to be self-contained. We will change that asap. This means that after we've corrected this when you do download the canonical librispeech dataset the following will work:\r\n\r\n```python\r\nds = load_dataset(\"....\")  # <- here we have a dependency on the filepathes\r\nds[0][\"audio\"][\"bytes\"] # <- will not work\r\n\r\nds.save_to_disk(\"\/local\/path\")  # <- now we want to have a self-contained dataset in arrow format, so we load the files into bytes and save it in arrow format\r\n\r\n# now you can delete everything besides \"\/local\/path\"\r\n\r\nds = load_from_disk(\"\/local\/path\")  # <- this will work\r\n```\r\n\r\nSo either option a) where you define your own librispeech data downloading script (you guys could just sign up here: https:\/\/huggingface.co\/join) and upload a dataset loading script in private mode so that no one can see it and you would always store the audio as bytes or b) where you first load then save to disk then delete cache would work. \r\n\r\nHope that fits in your vision :-)\r\n\r\ncc @lhoestq @mariosasko ","@patrickvonplaten sounds like a good approach to me. For b) this could even be configurable with a parameter like `embed_external_files` as you have for `push_to_hub` (if people prefer to keep separate audio files).\r\n","> However it's also trivial to write your own datasetset downloading script of librispeech and just not extract the folder\r\n\r\nI don't exactly understand. In all cases, we need to extract it to prepare the dataset, or not? No matter if we want to store the raw bytes inside the dataset or leaving them as local files. Just in the first case, we can safely delete the extracted files after the dataset preparation.\r\n\r\n> `save_to_disk(...)` is currently not necessarily saving a dataset to be self-contained. We will change that asap.\r\n\r\nFor us, this sounds exactly like what we want.\r\n\r\nBut regarding not introducing breaking changes, wouldn't this maybe also break some setups for users who don't expect this new behavior?\r\n","@albertz I would suggest to move the discussion on implementation details on our side to the following issue: rwth-i6\/i6_core\/issues\/257","I like the idea of adding `embed_external_files` and set it to True by default to `save_to_disk`.\r\nIt's indeed a kind of breaking change since some users will get bigger Arrow files when updating the lib, but the advantages are nice:\r\n1. I like the idea of having it self contained, in case you want to delete your cache\r\n2. users also upload these Arrow files to cloud storage via the `fs` parameter, and in this case they would expect to upload a self-contained dataset\r\n3. consistency with `push_to_hub`\r\n\r\nIf it sounds good to you I'll open an issue to discuss this and track the advancements"],"created_at":1650385676000,"updated_at":1650621099000,"closed_at":1650620717000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4184","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4184","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4184.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4184.patch","merged_at":1650620717000},"body":"Add `\"all\"` config to Librispeech","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4184\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4184\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4183","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4183\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4183\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4183\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4183","id":1208449335,"node_id":"PR_kwDODunzps42bjXn","number":4183,"title":"Document librispeech configs","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["I think the main purpose of #4179 was how to be able to load both configs into one, so should we maybe add this part of the code: https:\/\/github.com\/huggingface\/datasets\/issues\/4179#issuecomment-1102383717 \r\n\r\nto the doc? \r\n\r\nActually @lhoestq would this work given that they have different split names: https:\/\/huggingface.co\/datasets\/librispeech_asr#data-splits ? ","This doc extension does not explain why I can't simply load the whole dataset. Or what workaround I need to get the whole dataset, which is what people usually want for Librispeech.","_The documentation is not available anymore as the PR was closed or merged._","@lhoestq, I can add a `\"all\"` config to Librispeech have the datasets already cached somewhere ","I'm closing this PR then, feel free to continue the discussion in https:\/\/github.com\/huggingface\/datasets\/issues\/4179\r\n"],"created_at":1650378419000,"updated_at":1650381696000,"closed_at":1650381320000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4183","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4183","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4183.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4183.patch","merged_at":null},"body":"Added an example of how to load one config or the other","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4183\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4183\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4182","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4182\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4182\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4182\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4182","id":1208285235,"node_id":"I_kwDODunzps5IBPgz","number":4182,"title":"Zenodo.org download is not responding","user":{"login":"dkajtoch","id":32985207,"node_id":"MDQ6VXNlcjMyOTg1MjA3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/32985207?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/dkajtoch","html_url":"https:\/\/github.com\/dkajtoch","followers_url":"https:\/\/api.github.com\/users\/dkajtoch\/followers","following_url":"https:\/\/api.github.com\/users\/dkajtoch\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/dkajtoch\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/dkajtoch\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/dkajtoch\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/dkajtoch\/orgs","repos_url":"https:\/\/api.github.com\/users\/dkajtoch\/repos","events_url":"https:\/\/api.github.com\/users\/dkajtoch\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/dkajtoch\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["[Off topic but related: Is the uptime of S3 provably better than Zenodo's?]","Hi @dkajtoch, please note that at HuggingFace we are not hosting this dataset: we are just using a script to download their data file and create a dataset from it.\r\n\r\nIt was the dataset owners decision to host their data at Zenodo. You can see this on their website: https:\/\/marcobaroni.org\/composes\/sick.html\r\n\r\nAnd yes, you are right: Zenodo is currently having some incidents and people are reporting problems from it.\r\n\r\nOn the other hand, we could contact the data owners and propose them to host their data at our Hugging Face Hub.\r\n\r\n@julien-c I guess so.\r\n","Thanks @albertvillanova. I know that the problem lies in the source data. I just wanted to point out that these kind of problems are unavoidable without having one place where data sources are cached. Websites may go down or data sources may move. Having a copy in Hugging Face Hub would be a great solution. ","Definitely, @dkajtoch! But we have to ask permission to the data owners. And many dataset licenses directly forbid data redistribution: in those cases we are not allowed to host their data on our Hub.","Ahhh good point! License is the problem :("],"created_at":1650371217000,"updated_at":1650438665000,"closed_at":1650438665000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nSource download_url from zenodo.org does not respond. \r\n`_DOWNLOAD_URL = \"https:\/\/zenodo.org\/record\/2787612\/files\/SICK.zip?download=1\"`\r\nOther datasets also use zenodo.org to store data and they cannot be downloaded as well.\r\n\r\nIt would be better to actually use more reliable way to store original data like s3 bucket.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nload_dataset(\"sick\")\r\n```\r\n\r\n## Expected results\r\nDataset should be downloaded.\r\n\r\n## Actual results\r\nConnectionError: Couldn't reach https:\/\/zenodo.org\/record\/2787612\/files\/SICK.zip?download=1 (ReadTimeout(ReadTimeoutError(\"HTTPSConnectionPool(host='zenodo.org', port=443): Read timed out. (read timeout=100)\")))\r\n\r\n## Environment info\r\n- `datasets` version: 2.1.0\r\n- Platform: Darwin-21.4.0-x86_64-i386-64bit\r\n- Python version: 3.7.11\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.3.5\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4182\/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4182\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4181","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4181\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4181\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4181\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4181","id":1208194805,"node_id":"I_kwDODunzps5IA5b1","number":4181,"title":"FLEURS","user":{"login":"patrickvonplaten","id":23423619,"node_id":"MDQ6VXNlcjIzNDIzNjE5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/23423619?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/patrickvonplaten","html_url":"https:\/\/github.com\/patrickvonplaten","followers_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/followers","following_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/orgs","repos_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/repos","events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/received_events","type":"User","site_admin":false},"labels":[{"id":2067388877,"node_id":"MDU6TGFiZWwyMDY3Mzg4ODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20bug","name":"dataset bug","color":"2edb81","default":false,"description":"A bug in a dataset script provided in the library"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Yes, you just have to use `dl_manager.iter_archive` instead of `dl_manager.download_and_extract`.\r\n\r\nThat's because `download_and_extract` doesn't support TAR archives in streaming mode.","Tried to make it streamable, but I don't think it's really possible. @lhoestq @polinaeterna maybe you guys can check: \r\nhttps:\/\/huggingface.co\/datasets\/google\/fleurs\/commit\/dcf80160cd77977490a8d32b370c027107f2407b \r\n\r\nreal quick. \r\n\r\nI think the problem is that we cannot ensure that the metadata file is found before the audio. Or is this possible somehow @lhoestq ? ","@patrickvonplaten I think the metadata file should be found first because the audio files are contained in a folder next to the metadata files (just as in common voice), so the metadata files should be \"on top of the list\"   as they are closer to the root in the directories hierarchy ","@patrickvonplaten but apparently it doesn't... I don't really know why.","Yeah! Any ideas what could be the reason here? cc @lhoestq ?","The order of the files is determined when the TAR archive is created, depending on the commands the creator ran.\r\nIf the metadata file is not at the beginning of the file, that makes streaming completely inefficient. In this case the TAR archive needs to be recreated in an appropriate order.","Actually we could maybe just host the metadata file ourselves and then stream the audio data only. Don't think that this would be a problem for the FLEURS authors (I can ask them :-)) "],"created_at":1650366596000,"updated_at":1651094950000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Dataset viewer issue for '*name of the dataset*'\r\n\r\nhttps:\/\/huggingface.co\/datasets\/google\/fleurs\r\n\r\n```\r\nStatus code:   400\r\nException:     NotImplementedError\r\nMessage:       Extraction protocol for TAR archives like 'https:\/\/storage.googleapis.com\/xtreme_translations\/FLEURS\/af_za.tar.gz' is not implemented in streaming mode. Please use `dl_manager.iter_archive` instead.\r\n```\r\n\r\nAm I the one who added this dataset ? Yes\r\n\r\nCan I fix this somehow in the script? @lhoestq @severo \r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4181\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4181\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4180","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4180\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4180\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4180\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4180","id":1208042320,"node_id":"I_kwDODunzps5IAUNQ","number":4180,"title":"Add some iteration method on a dataset column (specific for inference)","user":{"login":"Narsil","id":204321,"node_id":"MDQ6VXNlcjIwNDMyMQ==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/204321?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Narsil","html_url":"https:\/\/github.com\/Narsil","followers_url":"https:\/\/api.github.com\/users\/Narsil\/followers","following_url":"https:\/\/api.github.com\/users\/Narsil\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Narsil\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Narsil\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Narsil\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Narsil\/orgs","repos_url":"https:\/\/api.github.com\/users\/Narsil\/repos","events_url":"https:\/\/api.github.com\/users\/Narsil\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Narsil\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Thanks for the suggestion ! I agree it would be nice to have something directly in `datasets` to do something as simple as that\r\n\r\ncc @albertvillanova @mariosasko @polinaeterna What do you think if we have something similar to pandas `Series` that wouldn't bring everything in memory when doing `dataset[\"audio\"]` ? Currently it returns a list with all the decoded audio data in memory.\r\n\r\nIt would be a breaking change though, since `isinstance(dataset[\"audio\"], list)` wouldn't work anymore, but we could implement a `Sequence` so that `dataset[\"audio\"][0]` still works and only loads one item in memory.\r\n\r\nYour alternative suggestion with `iterate` is also sensible, though maybe less satisfactory in terms of experience IMO","I agree that current behavior (decoding all audio file sin the dataset when accessing `dataset[\"audio\"]`) is not useful, IMHO. Indeed in our docs, we are constantly warning our collaborators not to do that.\r\n\r\nTherefore I upvote for a \"useful\" behavior of `dataset[\"audio\"]`. I don't think the breaking change is important in this case, as I guess no many people use it with its current behavior. Therefore, for me it seems reasonable to return a generator (instead of an in-memeory list) for \"special\" features, like Audio\/Image.\r\n\r\n@lhoestq on the other hand I don't understand your proposal about Pandas-like... ","I recall I had the same idea while working on the `Image` feature, so I agree implementing something similar to `pd.Series` that lazily brings elements in memory would be beneficial.","@lhoestq @mariosasko Could you please give a link to that new feature of `pandas.Series`? As far as I remember since I worked with pandas for more than 6 years, there was no lazy in-memory feature; it was everything in-memory; that was the reason why other frameworks were created, like Vaex or Dask, e.g. ","Yea pandas doesn't do lazy loading. I was referring to pandas.Series to say that they have a dedicated class to represent a column ;)"],"created_at":1650359745000,"updated_at":1650537058000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":null,"pull_request":null,"body":"**Is your feature request related to a problem? Please describe.**\r\nA clear and concise description of what the problem is.\r\n\r\nCurrently, `dataset[\"audio\"]` will load EVERY element in the dataset in RAM, which can be quite big for an audio dataset.\r\nHaving an iterator (or sequence) type of object, would make inference with `transformers` 's `pipeline` easier to use and not so memory hungry.\r\n\r\n**Describe the solution you'd like**\r\nA clear and concise description of what you want to happen.\r\n\r\nFor a non breaking change:\r\n\r\n```python\r\nfor audio in dataset.iterate(\"audio\"):\r\n    # {\"array\": np.array(...), \"sampling_rate\":...}\r\n```\r\n\r\nFor a  breaking change solution (not necessary), changing the type of `dataset[\"audio\"]` to a sequence type so that\r\n\r\n```python\r\npipe = pipeline(model=\"...\")\r\nfor out in pipe(dataset[\"audio\"]):\r\n    # {\"text\":....}\r\n```\r\ncould work\r\n\r\n**Describe alternatives you've considered**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\n```python\r\ndef iterate(dataset, key):\r\n    for item in dataset:\r\n        yield dataset[key]\r\n\r\nfor out in pipeline(iterate(dataset, \"audio\")):\r\n    # {\"array\": ...}\r\n```\r\n\r\nThis works but requires the helper function which feels slightly clunky.\r\n\r\n**Additional context**\r\nAdd any other context about the feature request here.\r\n\r\nThe context is actually to showcase better integration between  `pipeline` and `datasets` in the Quicktour demo: https:\/\/github.com\/huggingface\/transformers\/pull\/16723\/files\r\n\r\n@lhoestq \r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4180\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4180\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4179","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4179\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4179\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4179\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4179","id":1208001118,"node_id":"I_kwDODunzps5IAKJe","number":4179,"title":"Dataset librispeech_asr fails to load","user":{"login":"albertz","id":59132,"node_id":"MDQ6VXNlcjU5MTMy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59132?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertz","html_url":"https:\/\/github.com\/albertz","followers_url":"https:\/\/api.github.com\/users\/albertz\/followers","following_url":"https:\/\/api.github.com\/users\/albertz\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertz\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertz\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertz\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertz\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertz\/repos","events_url":"https:\/\/api.github.com\/users\/albertz\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertz\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["@patrickvonplaten Hi! I saw that you prepared this? :)","Another thing, but maybe this should be a separate issue: As I see from the code, it would try to use up to 16 simultaneous downloads? This is problematic for Librispeech or anything on OpenSLR. On [the homepage](https:\/\/www.openslr.org\/), it says:\r\n\r\n> If you want to download things from this site, please download them one at a time, and please don't use any fancy software-- just download things from your browser or use 'wget'. We have a firewall rule to drop connections from hosts with more than 5 simultaneous connections, and certain types of download software may activate this rule.\r\n\r\nRelated: https:\/\/github.com\/tensorflow\/datasets\/issues\/3885","Hey @albertz,\r\n\r\nNice to see you here! It's been a while ;-) ","Sorry maybe the docs haven't been super clear here. By `split` we mean one of `train.500`, `train.360`, `train.100`, `validation`, `test`. For Librispeech, you'll have to specific a config (either `other` or `clean`) though:\r\n\r\n```py\r\ndatasets.load_dataset(\"librispeech_asr\", \"clean\")\r\n```\r\n\r\nshould work and give you all splits (being \"train\", \"test\", ...) for the clean config of the dataset.\r\n","If you need both `\"clean\"` and `\"other\"` I think you'll have to do concatenate them as follows: \r\n\r\n```py\r\nfrom datasets import concatenate_datasets, load_dataset\r\n\r\nother = load_dataset(\"librispeech_asr\", \"other\")\r\nclean = load_dataset(\"librispeech_asr\", \"clean\")\r\n\r\nlibrispeech = concatenate_datasets([other, clean])\r\n```\r\n\r\nSee https:\/\/huggingface.co\/docs\/datasets\/v2.1.0\/en\/process#concatenate","Downloading one split would be:\r\n\r\n```py\r\nfrom datasets import load_dataset\r\n\r\nother = load_dataset(\"librispeech_asr\", \"other\", split=\"train.500\")\r\n```\r\n\r\n\r\n","cc @lhoestq FYI maybe the docs can be improved here","Ah thanks. But wouldn't it be easier\/nicer (and more canonical) to just make it in a way that simply `load_dataset(\"librispeech_asr\")` works?","Pinging @lhoestq here, think this could make sense! Not sure however how the dictionary would then look like","Would it make sense to have `clean` as the default config ?\r\n\r\nAlso I think `load_dataset(\"librispeech_asr\")` should have raised you an error that says that you need to specify a config\r\n\r\nI also opened a PR to improve the doc: https:\/\/github.com\/huggingface\/datasets\/pull\/4183","> Would it make sense to have `clean` as the default config ?\r\n\r\nI think a user would expect that the default would give you the full dataset.\r\n\r\n> Also I think `load_dataset(\"librispeech_asr\")` should have raised you an error that says that you need to specify a config\r\n\r\nIt does raise an error, but this error confused me because I did not understand why I needed a config, or why I could not simply download the whole dataset, which is what people usually do with Librispeech.\r\n","+1 for @albertz. Also think lots of people download the whole dataset (`\"clean\"` + `\"other\"`) for Librispeech.\r\n\r\nThink there are also some people though who:\r\n- a) Don't have the memory to store the whole dataset\r\n- b) Just want to evaluate on one of the two configs","Ok ! Adding the \"all\" configuration would do the job then, thanks ! In the \"all\" configuration we can merge all the train.xxx splits into one \"train\" split, or keep them separate depending on what's the most practical to use (probably put everything in \"train\" no ?)","I'm not too familiar with how to work with HuggingFace datasets, but people often do some curriculum learning scheme, where they start with train.100, later go over to train.100 + train.360, and then later use the whole train (960h). It would be good if this is easily possible.\r\n","Hey @albertz, \r\n\r\nopened a PR here. Think by adding the \"subdataset\" class to each split \"train\", \"dev\", \"other\" as shown here: https:\/\/github.com\/huggingface\/datasets\/pull\/4184\/files#r853272727 it should be easily possible (e.g. with the filter function https:\/\/huggingface.co\/docs\/datasets\/v2.1.0\/en\/package_reference\/main_classes#datasets.Dataset.filter )","But also since everything is cached one could also just do:\r\n\r\n```python\r\nload_dataset(\"librispeech\", \"clean\", \"train.100\")\r\nload_dataset(\"librispeech\", \"clean\", \"train.100+train.360\")\r\nload_dataset(\"librispeech\" \"all\", \"train\") \r\n```"],"created_at":1650357948000,"updated_at":1650386186000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nThe dataset librispeech_asr (standard Librispeech) fails to load.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndatasets.load_dataset(\"librispeech_asr\")\r\n```\r\n\r\n## Expected results\r\nIt should download and prepare the whole dataset (all subsets).\r\n\r\nIn [the doc](https:\/\/huggingface.co\/datasets\/librispeech_asr), it says it has two configurations (clean and other).\r\nHowever, the dataset doc says that not specifying `split` should just load the whole dataset, which is what I want.\r\n\r\nAlso, in case of this specific dataset, this is also the standard what the community uses. When you look at any publications with results on Librispeech, they always use the whole train dataset for training.\r\n\r\n## Actual results\r\n```\r\n...\r\n  File \"\/home\/az\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/librispeech_asr\/1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c\/librispeech_asr.py\", line 119, in LibrispeechASR._split_generators\r\n    line: archive_path = dl_manager.download(_DL_URLS[self.config.name])\r\n    locals:\r\n      archive_path = <not found>\r\n      dl_manager = <local> <datasets.utils.download_manager.DownloadManager object at 0x7fc07b426160>\r\n      dl_manager.download = <local> <bound method DownloadManager.download of <datasets.utils.download_manager.DownloadManager object at 0x7fc07b426160>>\r\n      _DL_URLS = <global> {'clean': {'dev': 'http:\/\/www.openslr.org\/resources\/12\/dev-clean.tar.gz', 'test': 'http:\/\/www.openslr.org\/resources\/12\/test-clean.tar.gz', 'train.100': 'http:\/\/www.openslr.org\/resources\/12\/train-clean-100.tar.gz', 'train.360': 'http:\/\/www.openslr.org\/resources\/12\/train-clean-360.tar.gz'}, 'other'...\r\n      self = <local> <datasets_modules.datasets.librispeech_asr.1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c.librispeech_asr.LibrispeechASR object at 0x7fc12a633310>\r\n      self.config = <local> BuilderConfig(name='default', version=0.0.0, data_dir='\/home\/az\/i6\/setups\/2022-03-20--sis\/work\/i6_core\/datasets\/huggingface\/DownloadAndPrepareHuggingFaceDatasetJob.TV6Nwm6dFReF\/output\/data_dir', data_files=None, description=None)\r\n      self.config.name = <local> 'default', len = 7\r\nKeyError: 'default'\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.1.0\r\n- Platform: Linux-5.4.0-107-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.9\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.4.2\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4179\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4179\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4178","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4178\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4178\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4178\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4178","id":1207787073,"node_id":"PR_kwDODunzps42ZfFN","number":4178,"title":"[feat] Add ImageNet dataset","user":{"login":"apsdehal","id":3616806,"node_id":"MDQ6VXNlcjM2MTY4MDY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3616806?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/apsdehal","html_url":"https:\/\/github.com\/apsdehal","followers_url":"https:\/\/api.github.com\/users\/apsdehal\/followers","following_url":"https:\/\/api.github.com\/users\/apsdehal\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/apsdehal\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/apsdehal\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/apsdehal\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/apsdehal\/orgs","repos_url":"https:\/\/api.github.com\/users\/apsdehal\/repos","events_url":"https:\/\/api.github.com\/users\/apsdehal\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/apsdehal\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Thanks for the comments. I believe I have addressed all of them and also decreased the size of the dummy data file, so it should be ready for a re-review. I also made a change to allow adding synset mapping and valprep script in config in case we add ImageNet 21k some time later. ","@lhoestq I have updated the PR to address all of the review comments."],"created_at":1650348095000,"updated_at":1651268639000,"closed_at":1651268228000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4178","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4178","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4178.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4178.patch","merged_at":1651268228000},"body":"To use the dataset download the tar file\r\n[imagenet_object_localization_patched2019.tar.gz](https:\/\/www.kaggle.com\/competitions\/imagenet-object-localization-challenge\/data?select=imagenet_object_localization_patched2019.tar.gz) from Kaggle and then point the datasets library to it by using:\r\n\r\n```py\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"imagenet\",\r\ndata_dir=\"\/path\/to\/imagenet_object_localization_patched2019.tar.gz\")\r\n```\r\n\r\nCurrently train and validation splits are supported.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4178\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4178\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4177","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4177\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4177\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4177\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4177","id":1207535920,"node_id":"PR_kwDODunzps42Yxca","number":4177,"title":"Adding missing subsets to the `SemEval-2018 Task 1` dataset","user":{"login":"micahcarroll","id":11460267,"node_id":"MDQ6VXNlcjExNDYwMjY3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/11460267?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/micahcarroll","html_url":"https:\/\/github.com\/micahcarroll","followers_url":"https:\/\/api.github.com\/users\/micahcarroll\/followers","following_url":"https:\/\/api.github.com\/users\/micahcarroll\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/micahcarroll\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/micahcarroll\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/micahcarroll\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/micahcarroll\/orgs","repos_url":"https:\/\/api.github.com\/users\/micahcarroll\/repos","events_url":"https:\/\/api.github.com\/users\/micahcarroll\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/micahcarroll\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1650322770000,"updated_at":1651095857000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4177","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4177","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4177.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4177.patch","merged_at":null},"body":"This dataset for the [1st task of SemEval-2018](https:\/\/competitions.codalab.org\/competitions\/17751) competition was missing all subtasks except for subtask 5. I added another two subtasks (subtask 1 and 2), which are each comprised of 12 additional data subsets: for each language in En, Es, Ar, there are 4 datasets, broken down by emotions (anger, fear, joy, sadness).\r\n\r\n## Remaining questions\r\n\r\nI wasn't able to find any documentation about how one should make PRs to modify datasets. Because of that, I just did my best to integrate the new data into the code, and tested locally that this worked. I'm sorry if I'm not respecting your contributing guidelines \u2013 if they are documented somewhere, I'd appreciate if you could send a pointer!\r\n\r\nNot sure how `dataset_infos.json` and `dummy` should be updated. My understanding is that they were automatically generated at the time of the original dataset creation?","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4177\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4177\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4176","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4176\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4176\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4176\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4176","id":1206515563,"node_id":"I_kwDODunzps5H6fdr","number":4176,"title":"Very slow between two operations","user":{"login":"yananchen1989","id":26405281,"node_id":"MDQ6VXNlcjI2NDA1Mjgx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/26405281?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yananchen1989","html_url":"https:\/\/github.com\/yananchen1989","followers_url":"https:\/\/api.github.com\/users\/yananchen1989\/followers","following_url":"https:\/\/api.github.com\/users\/yananchen1989\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yananchen1989\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yananchen1989\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yananchen1989\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yananchen1989\/orgs","repos_url":"https:\/\/api.github.com\/users\/yananchen1989\/repos","events_url":"https:\/\/api.github.com\/users\/yananchen1989\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yananchen1989\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1650239549000,"updated_at":1650240180000,"closed_at":1650240180000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"Hello, in the processing stage, I use two operations. The first one : map + filter, is very fast and it uses the full cores, while the socond step is very slow and did not use full cores. \r\n\r\nAlso, there is a significant lag between them.  Am I missing something ?\r\n\r\n\r\n\r\n ```\r\nraw_datasets = raw_datasets.map(split_func, \r\n                batched=False,\r\n                num_proc=args.preprocessing_num_workers,\r\n                load_from_cache_file=not args.overwrite_cache, \r\n                desc = \"running split para ==>\")\\\r\n                .filter(lambda example: example['text1']!='' and example['text2']!='', \r\n                    num_proc=args.preprocessing_num_workers, desc=\"filtering ==>\")\r\n\r\n\r\n    processed_datasets = raw_datasets.map(\r\n        preprocess_function,\r\n        batched=True, \r\n        num_proc=args.preprocessing_num_workers,\r\n        remove_columns=column_names,\r\n        load_from_cache_file=not args.overwrite_cache,\r\n        desc=\"Running tokenizer on dataset===>\",\r\n    )\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4176\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4176\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4175","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4175\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4175\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4175\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4175","id":1205589842,"node_id":"PR_kwDODunzps42SqF-","number":4175,"title":"Add WIT Dataset","user":{"login":"thomasw21","id":24695242,"node_id":"MDQ6VXNlcjI0Njk1MjQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/24695242?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/thomasw21","html_url":"https:\/\/github.com\/thomasw21","followers_url":"https:\/\/api.github.com\/users\/thomasw21\/followers","following_url":"https:\/\/api.github.com\/users\/thomasw21\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/thomasw21\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/thomasw21\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/thomasw21\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/thomasw21\/orgs","repos_url":"https:\/\/api.github.com\/users\/thomasw21\/repos","events_url":"https:\/\/api.github.com\/users\/thomasw21\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/thomasw21\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Hi! Coming in late with some context.\r\n\r\nThere are two versions of the WIT dataset:\r\n1. The original source dataset managed by Wikimedia. It has more information, raw image representations, and each row corresponds to an image linked to all of its captions wherever it happens in Wikipedia (in multiple languages)\r\n2. The Google version, corresponding to the data script in this PR, which duplicates image instances and requires the user to download the images themselves from the provided URL (note that a basic implementation will have them download the same picture several time. @thomasw21 using our download manager instead of `urllib` could help with that, but it wouldn't be required if people had access to the first version)\r\n\r\nThe Wikimedia folks were really interested in us hosting a ready-to-go streaming version of this dataset where users don't have to download the version themselves, which is why we have the pre-processed versions on an HF bucket, with the raw images and a pre-computed embedding (don't remember the model, we can keep it ). That's the data script currently in https:\/\/github.com\/huggingface\/datasets\/pull\/2981 . It's nearly ready to go, the one thing we should do is move the raw data from our HF google Cloud bucket to the Hub.\r\n\r\nHow do you want to move forward? IMO the best way would be to have a WIT dataset under the Wikimedia org with both configurations, but it depends on everyone's timelines","Okay after offline discussion. We'll improve this versions and push it to the hub under `google` namespace. \r\n\r\n> which duplicates image instances and requires the user to download the images themselves from the provided URL (note that a basic implementation will have them download the same picture several time. @thomasw21 using our download manager instead of urllib could help with that, but it wouldn't be required if people had access to the first version)\r\n\r\nAh interesting wasn't aware of this duplication issue, concretely it'll just mean that our dataset in bigger than expected ... I think this should be handled after this loading script (though I have to figure our how to spawn a dl_manager).\r\n\r\n> The Wikimedia folks were really interested in us hosting a ready-to-go streaming version of this dataset where users don't have to download the version themselves, which is why we have the pre-processed versions on an HF bucket, with the raw images and a pre-computed embedding (don't remember the model, we can keep it ). That's the data script currently in https:\/\/github.com\/huggingface\/datasets\/pull\/2981 . It's nearly ready to go, the one thing we should do is move the raw data from our HF google Cloud bucket to the Hub.\r\n\r\nSimilarly a script will be written and pushed to `wikimedia` organisation.","@mariosasko can you make one last review concerning the text description changes? Then I'll handle putting it under `google` namespace and close this PR.","Looks all good now. Great job! ","Closing as this has been migrated to the hub under `google` namespace: https:\/\/huggingface.co\/datasets\/google\/wit"],"created_at":1650030152000,"updated_at":1651502041000,"closed_at":1651501601000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4175","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4175","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4175.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4175.patch","merged_at":null},"body":"closes #2981 #2810\r\n\r\n@nateraw @hassiahk I've listed you guys as co-author as you've contributed previously to this dataset","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4175\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4175\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4174","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4174\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4174\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4174\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4174","id":1205575941,"node_id":"PR_kwDODunzps42SnJS","number":4174,"title":"Fix when map function modifies input in-place","user":{"login":"thomasw21","id":24695242,"node_id":"MDQ6VXNlcjI0Njk1MjQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/24695242?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/thomasw21","html_url":"https:\/\/github.com\/thomasw21","followers_url":"https:\/\/api.github.com\/users\/thomasw21\/followers","following_url":"https:\/\/api.github.com\/users\/thomasw21\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/thomasw21\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/thomasw21\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/thomasw21\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/thomasw21\/orgs","repos_url":"https:\/\/api.github.com\/users\/thomasw21\/repos","events_url":"https:\/\/api.github.com\/users\/thomasw21\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/thomasw21\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650028995000,"updated_at":1650034327000,"closed_at":1650033958000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4174","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4174","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4174.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4174.patch","merged_at":1650033958000},"body":"When `function` modifies input in-place, the guarantee that columns in `remove_columns` are contained in `input` doesn't hold true anymore. Therefore we need to relax way we pop elements by checking if that column exists.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4174\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4174\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4173","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4173\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4173\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4173\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4173","id":1204657114,"node_id":"PR_kwDODunzps42Ppnd","number":4173,"title":"Stream private zipped images","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","oops looks like some tests are failing sorry, will fix them tomorrow\r\n\r\nEDIT: not today but asap hopefully","cc @mariosasko this is ready for review, let me know what you think !"],"created_at":1649949307000,"updated_at":1651759554000,"closed_at":1651759115000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4173","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4173","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4173.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4173.patch","merged_at":1651759115000},"body":"As mentioned in https:\/\/github.com\/huggingface\/datasets\/issues\/4139 it's currently not possible to stream private\/gated zipped images from the Hub.\r\n\r\nThis is because `Image.decode_example` does not handle authentication. Indeed decoding requires to access and download the file from the private repository.\r\n\r\nIn this PR I added authentication to `Image.decode_example` via a `token_per_repo_id` optional argument. I first wanted to just pass `use_auth_token` but a single `Image` instance can be responsible of decoding images from a combination of several datasets together (from `interleave_datasets` for example). Therefore I just used a dictionary `repo_id` -> `token` instead.\r\n\r\nI'm getting the `repo_id` from the dataset builder (I replaced the `namepace` attribute with `repo_id`)\r\n\r\nI did the same for `Audio.decode_example`\r\n\r\ncc @SBrandeis @severo ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4173\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4173\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4172","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4172\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4172\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4172\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4172","id":1204433160,"node_id":"PR_kwDODunzps42O7LW","number":4172,"title":"Update assin2 dataset_infos.json","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649937186000,"updated_at":1650034062000,"closed_at":1650033682000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4172","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4172","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4172.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4172.patch","merged_at":1650033682000},"body":"Following comments in https:\/\/github.com\/huggingface\/datasets\/issues\/4003 we found that it was outdated and casing an error when loading the dataset","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4172\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4172\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4170","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4170\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4170\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4170\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4170","id":1204413620,"node_id":"PR_kwDODunzps42O2-L","number":4170,"title":"to_tf_dataset rewrite","user":{"login":"Rocketknight1","id":12866554,"node_id":"MDQ6VXNlcjEyODY2NTU0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12866554?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Rocketknight1","html_url":"https:\/\/github.com\/Rocketknight1","followers_url":"https:\/\/api.github.com\/users\/Rocketknight1\/followers","following_url":"https:\/\/api.github.com\/users\/Rocketknight1\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Rocketknight1\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Rocketknight1\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Rocketknight1\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Rocketknight1\/orgs","repos_url":"https:\/\/api.github.com\/users\/Rocketknight1\/repos","events_url":"https:\/\/api.github.com\/users\/Rocketknight1\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Rocketknight1\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4170). All of your documentation changes will be reflected on that endpoint.","[Magic is now banned](https:\/\/www.youtube.com\/watch?v=WIn58XoY728#t=36s) by decree of @sgugger. This is honestly much cleaner, and the functionality will make much more sense in `transformers` anyway!","@gante I renamed the default collator to `minimal_tf_collate_fn`!","@lhoestq @sgugger @gante \r\n\r\nI think this should now be ready, it looks good in testing! I'll try a few more notebooks today and tomorrow to be sure before I merge. Key changes are:\r\n\r\n- No column autodetection magic (will make a separate PR to add this as a `transformers` function)\r\n- Drops non-numerical features automatically (this is more of a 'DataLoader' method, we'll have a separate method to expose 'raw' datasets to `tf.data`)\r\n- Better autodetection of numerical features.\r\n- Shouldn't randomly crash mid-function :skull: \r\n\r\nWe definitely have some questions still to resolve about how to handle making a 'DataLoader' dataset versus a 'raw' dataset - see [the Notion doc](https:\/\/www.notion.so\/huggingface2\/Splitting-to_tf_dataset-c2e0773c4bec484384064b30ed634383) if you're interested. Still, since this PR is just fixes\/improvements to an existing method which never supported non-numerical features anyway, we can merge it before we've resolved those issues, and then think about how to name and split things afterwards.","P.S. I'll take out the region comments at the end before I merge, I promise! They're just helpful while I'm editing it","+1 for the tests\r\n\r\n> Drops non-numerical features automatically\r\n\r\nCan you give more details on how this work and the rationale as well ? This is not explained in the docs\r\n\r\nAlso why are you adding `error_on_missing` and `auto_fix_label_names ` ? The rationale is not clear to me. In particular I think it is sensible enough to expect users to not ask columns that don't exist, and to rename a label column when required.","@lhoestq I rewrote those parts - they were causing some other issues too! `error_on_missing` and `auto_fix_label_names` have been removed. The new logic is to simply drop (before batch collation) all columns the user doesn't ask for, but not to raise errors if the user asked for columns not in the dataset, as they may be added by the collator. Hopefully this cleans it up and matches the documentation better!"],"created_at":1649935858000,"updated_at":1652190267000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4170","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4170","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4170.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4170.patch","merged_at":null},"body":"This PR rewrites almost all of `to_tf_dataset()`, which makes it kind of hard to list all the changes, but the most critical ones are:\r\n\r\n- Much better stability and no more dropping unexpected column names (Sorry @NielsRogge)\r\n- Doesn't clobber custom transforms on the data (Sorry @NielsRogge again)\r\n- Much better handling of the situation when the `collate_fn` adds columns that aren't in the dataset.\r\n- Better inference of shapes and data types\r\n- Lots of hacky special-casing code removed\r\n- Can return string columns (as `tf.String`)\r\n- Most arguments have default values, calling the method should be much simpler\r\n- ~~Can accept a `model` argument and only return columns that are valid inputs to that model~~\r\n- Drops the `dummy_labels` argument - this was a workaround for Keras issues that have been resolved by changes in `transformers`. Also remove it from tests and the Overview notebook.\r\n\r\nI still have a couple of TODOs remaining and some testing to do, so don't merge yet, but it should be mostly ready for review at this point!","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4170\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4170\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4169","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4169\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4169\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4169\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4169","id":1203995869,"node_id":"I_kwDODunzps5Hw4Td","number":4169,"title":"Timit_asr dataset cannot be previewed recently","user":{"login":"YingLi001","id":75192317,"node_id":"MDQ6VXNlcjc1MTkyMzE3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/75192317?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/YingLi001","html_url":"https:\/\/github.com\/YingLi001","followers_url":"https:\/\/api.github.com\/users\/YingLi001\/followers","following_url":"https:\/\/api.github.com\/users\/YingLi001\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/YingLi001\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/YingLi001\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/YingLi001\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/YingLi001\/orgs","repos_url":"https:\/\/api.github.com\/users\/YingLi001\/repos","events_url":"https:\/\/api.github.com\/users\/YingLi001\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/YingLi001\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Thanks for reporting. The bug has already been detected, and we hope to fix it soon.","TIMIT is now a dataset that requires manual download, see #4145 \r\n\r\nTherefore it might take a bit more time to fix it","> TIMIT is now a dataset that requires manual download, see #4145\r\n> \r\n> Therefore it might take a bit more time to fix it\r\n\r\nThank you for your quickly response. Exactly, I also found the manual download issue in the morning. But when I used *list_datasets()* to check the available datasets, *'timit_asr'* is still in the list. So I am a little bit confused. If *'timit_asr'* need to be manually downloaded, does that mean we can **not** automatically download it **any more** in the future?","Yes exactly. If you try to load the dataset it will ask you to download it manually first, and to pass the downloaded and extracted data like `load_dataset(\"timir_asr\", data_dir=\"path\/to\/extracted\/data\")`\r\n\r\nThe URL we were using was coming from a host that doesn't have the permission to redistribute the data, and the dataset owners (LDC) notified us about it."],"created_at":1649906911000,"updated_at":1651853211000,"closed_at":1651853211000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Dataset viewer issue for '*timit_asr*'\r\n\r\n**Link:** *https:\/\/huggingface.co\/datasets\/timit_asr*\r\n\r\nIssue: The timit-asr dataset cannot be previewed recently.\r\n\r\nAm I the one who added this dataset ? Yes-No\r\nNo","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4169\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4169\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4168","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4168\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4168\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4168\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4168","id":1203867540,"node_id":"PR_kwDODunzps42NL6F","number":4168,"title":"Add code examples to API docs","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","> Do you think it is clearer to make every code example fully reproducible so when users copy the code they can actually run it and get an output? This seems quite repetitive - maybe even unnecessary - but it is definitely clearer.\r\n\r\nI think it's ok to be repetitive to get more clarity. Many users come from `transformers` and may have little experience with some processing methods (especially torch users).\r\n\r\n> Should we showcase a function with more than one parameter to highlight different use-cases (it's pretty basic right now, but I'd be happy to add more)?\r\n\r\nMaybe let's do it case by case, depending on whether there are parameters that are likely to be used often ?\r\n\r\n> For the class_encode_column function, let me know if there is a simpler dataset with fewer columns (currently using winograd_wsc) so it is easier for users to see what changed.\r\n\r\nYou can try with `boolq`, it has a boolean column that can be converted to labels\r\n\r\n> Where possible, I try to show the input before and the output after using a function like flatten for example. Do you think this is too much and just showing the usage (ie, >>> ds.flatten()) will be sufficient?\r\n\r\nNo I don't think it's too much, it's nice this way thanks :)","Updated each code example so they are fully reproducible (where applicable)! The next step will be to identify some functions where we can show off some parameters that are useful or commonly used. Some useful parameters can be:\r\n\r\n- use `map(batched=True)` to process batches of examples.\r\n- set a seed in `shuffle`.\r\n- set `shuffle` and `seed` in `train_test_split`.\r\n\r\nLet me know if you think of anything else related to the functions in `arrow_dataset.py`!","Cool thanks ! I think you can also do `num_proc` for `map`"],"created_at":1649891018000,"updated_at":1651085617000,"closed_at":1651085314000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4168","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4168","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4168.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4168.patch","merged_at":1651085314000},"body":"This PR adds code examples for functions related to the base Datasets class to highlight usage. Most of the examples use the `rotten_tomatoes` dataset since it is nice and small. Several things I would appreciate feedback on:\r\n\r\n- Do you think it is clearer to make every code example fully reproducible so when users copy the code they can actually run it and get an output? This seems quite repetitive - maybe even unnecessary - but it is definitely clearer. Personally, I think we might be able to get away with not including this since users probably want to try the function on their own dataset. For example:\r\n\r\n   ```py\r\n   >>> from datasets import load_dataset\r\n   >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\r\n   >>> code example goes here\r\n   ```\r\n\r\n- Should we showcase a function with more than one parameter to highlight different use-cases (it's pretty basic right now, but I'd be happy to add more)?\r\n- For the `class_encode_column` function, let me know if there is a simpler dataset with fewer columns (currently using `winograd_wsc`) so it is easier for users to see what changed.\r\n- Where possible, I try to show the input before and the output after using a function like `flatten` for example. Do you think this is too much and just showing the usage (ie, `>>> ds.flatten()`) will be sufficient?\r\n\r\nThanks :)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4168\/reactions","total_count":2,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":2,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4168\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4167","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4167\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4167\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4167\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4167","id":1203761614,"node_id":"PR_kwDODunzps42M1O5","number":4167,"title":"Avoid rate limit in update hub repositories","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["I also set GIT_LFS_SKIP_SMUDGE=1 to speed up git clones","_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649881937000,"updated_at":1649883401000,"closed_at":1649883032000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4167","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4167","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4167.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4167.patch","merged_at":1649883032000},"body":"use http.extraHeader to avoid rate limit","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4167\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4167\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4166","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4166\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4166\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4166\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4166","id":1203758004,"node_id":"PR_kwDODunzps42M0dS","number":4166,"title":"Fix exact match","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649881686000,"updated_at":1651580611000,"closed_at":1651580187000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4166","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4166","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4166.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4166.patch","merged_at":1651580187000},"body":"Clarify docs and add clarifying example to the exact_match metric","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4166\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4166\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4165","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4165\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4165\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4165\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4165","id":1203730187,"node_id":"PR_kwDODunzps42MubF","number":4165,"title":"Fix google bleu typos, examples","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649879994000,"updated_at":1651580632000,"closed_at":1651580204000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4165","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4165","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4165.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4165.patch","merged_at":1651580204000},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4165\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4165\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4164","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4164\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4164\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4164\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4164","id":1203661346,"node_id":"PR_kwDODunzps42MfxX","number":4164,"title":"Fix duplicate key in multi_news","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649875704000,"updated_at":1649883856000,"closed_at":1649883482000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4164","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4164","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4164.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4164.patch","merged_at":1649883482000},"body":"To merge after this job succeeded: https:\/\/github.com\/huggingface\/datasets\/runs\/6012207928","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4164\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4164\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4163","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4163\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4163\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4163\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4163","id":1203539268,"node_id":"I_kwDODunzps5HvI1E","number":4163,"title":"Optional Content Warning for Datasets","user":{"login":"TristanThrush","id":20826878,"node_id":"MDQ6VXNlcjIwODI2ODc4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/20826878?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/TristanThrush","html_url":"https:\/\/github.com\/TristanThrush","followers_url":"https:\/\/api.github.com\/users\/TristanThrush\/followers","following_url":"https:\/\/api.github.com\/users\/TristanThrush\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/TristanThrush\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/TristanThrush\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/TristanThrush\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/TristanThrush\/orgs","repos_url":"https:\/\/api.github.com\/users\/TristanThrush\/repos","events_url":"https:\/\/api.github.com\/users\/TristanThrush\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/TristanThrush\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi! You can use the `extra_gated_prompt` YAML field in a dataset card for displaying custom messages\/warnings that the user must accept before gaining access to the actual dataset. This option also keeps the viewer hidden until the user agrees to terms. ","Hi @mariosasko, thanks for explaining how to add this feature. \r\n\r\nIf the current dataset yaml is:\r\n```\r\n---\r\nannotations_creators:\r\n- expert\r\nlanguage_creators:\r\n- expert-generated\r\nlanguages:\r\n- en\r\nlicense:\r\n- cc-by-4.0\r\nmultilinguality:\r\n- monolingual\r\npretty_name: HatemojiBuild\r\nsize_categories:\r\n- 1K<n<10K\r\nsource_datasets:\r\n- original\r\ntask_categories:\r\n- text-classification\r\ntask_ids:\r\n- hate-speech-detection\r\n---\r\n```\r\n\r\nCan you provide a minimal working example of how to added the gated prompt?\r\n\r\nThanks!","```\r\n---\r\nannotations_creators:\r\n- expert\r\nlanguage_creators:\r\n- expert-generated\r\nlanguages:\r\n- en\r\nlicense:\r\n- cc-by-4.0\r\nmultilinguality:\r\n- monolingual\r\npretty_name: HatemojiBuild\r\nsize_categories:\r\n- 1K<n<10K\r\nsource_datasets:\r\n- original\r\ntask_categories:\r\n- text-classification\r\ntask_ids:\r\n- hate-speech-detection\r\nextra_gated_prompt: \"This repository contains harmful content.\"\r\n---\r\n```\r\n\\+ enable `User Access requests` under the Settings pane.\r\n\r\nThere's a brief guide here https:\/\/discuss.huggingface.co\/t\/how-to-customize-the-user-access-requests-message\/13953 , and you can see the field in action here, https:\/\/huggingface.co\/datasets\/mozilla-foundation\/common_voice_7_0\/blob\/main\/README.md (you need to agree the terms in the Dataset Card pane to be able to access the files pane, so this comes up 403 at first).\r\n\r\nAnd a working example here! https:\/\/huggingface.co\/datasets\/DDSC\/dkhate :) Great to be able to mitigate harms in text.","-- is there a way to gate content anonymously, i.e. without registering which users access it?"],"created_at":1649867881000,"updated_at":1652176554000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":null,"pull_request":null,"body":"**Is your feature request related to a problem? Please describe.**\r\nA clear and concise description of what the problem is.\r\n\r\nWe now have hate speech datasets on the hub, like this one: https:\/\/huggingface.co\/datasets\/HannahRoseKirk\/HatemojiBuild\r\n\r\nI'm wondering if there is an option to select a content warning message that appears before the dataset preview? Otherwise, people immediately see hate speech when clicking on this dataset.\r\n\r\n**Describe the solution you'd like**\r\nA clear and concise description of what you want to happen.\r\n\r\nImplementation of a content warning message that separates users from the dataset preview until they click out of the warning.\r\n\r\n**Describe alternatives you've considered**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\nPossibly just a way to remove the dataset preview completely? I think I like the content warning option better, though.\r\n\r\n**Additional context**\r\nAdd any other context about the feature request here.\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4163\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4163\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4162","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4162\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4162\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4162\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4162","id":1203421909,"node_id":"PR_kwDODunzps42LtGO","number":4162,"title":"Add Conceptual 12M","user":{"login":"thomasw21","id":24695242,"node_id":"MDQ6VXNlcjI0Njk1MjQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/24695242?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/thomasw21","html_url":"https:\/\/github.com\/thomasw21","followers_url":"https:\/\/api.github.com\/users\/thomasw21\/followers","following_url":"https:\/\/api.github.com\/users\/thomasw21\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/thomasw21\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/thomasw21\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/thomasw21\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/thomasw21\/orgs","repos_url":"https:\/\/api.github.com\/users\/thomasw21\/repos","events_url":"https:\/\/api.github.com\/users\/thomasw21\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/thomasw21\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Looks like your dummy_data.zip file is not in the right location ;)\r\ndatasets\/datasets\/conceptual_12m\/dummy\/default\/0.0.0\/dummy_data.zip\r\n->\r\ndatasets\/conceptual_12m\/dummy\/default\/0.0.0\/dummy_data.zip"],"created_at":1649861843000,"updated_at":1650010381000,"closed_at":1650009985000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4162","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4162","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4162.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4162.patch","merged_at":1650009985000},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4162\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4162\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4161","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4161\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4161\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4161\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4161","id":1203230485,"node_id":"PR_kwDODunzps42LEhi","number":4161,"title":"Add Visual Genome","user":{"login":"thomasw21","id":24695242,"node_id":"MDQ6VXNlcjI0Njk1MjQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/24695242?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/thomasw21","html_url":"https:\/\/github.com\/thomasw21","followers_url":"https:\/\/api.github.com\/users\/thomasw21\/followers","following_url":"https:\/\/api.github.com\/users\/thomasw21\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/thomasw21\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/thomasw21\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/thomasw21\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/thomasw21\/orgs","repos_url":"https:\/\/api.github.com\/users\/thomasw21\/repos","events_url":"https:\/\/api.github.com\/users\/thomasw21\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/thomasw21\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Hum there seems to be some issues with tasks in test:\r\n - some tasks don't fit anything in `tasks.json`. Do I remove them in `task_categories`?\r\n - some tasks should exist, typically `visual-question-answering` (https:\/\/github.com\/huggingface\/datasets\/blame\/9f2ff14673cac1f1ad56d80221a793f5938b68c7\/src\/datasets\/utils\/resources\/tasks.json#L195) yet the exception is failing on me. I'm guessing it's because my `master` is not up-to-date. However this means that the testing only tests my branch instead of the one merged with master?\r\n \r\n cc @mariosasko @lhoestq ","> some tasks don't fit anything in tasks.json. Do I remove them in task_categories?\r\n\r\nYou can keep them, but add `other-` as a prefix to those tasks to make the CI ignore it\r\n\r\n> some tasks should exist, typically visual-question-answering (https:\/\/github.com\/huggingface\/datasets\/blame\/9f2ff14673cac1f1ad56d80221a793f5938b68c7\/src\/datasets\/utils\/resources\/tasks.json#L195) yet the exception is failing on me. I'm guessing it's because my master is not up-to-date. However this means that the testing only tests my branch instead of the one merged with master?\r\n\r\nFeel free to merge upstream\/master into your branch ;)\r\n\r\nEDIT: actually I just noticed you've already done this, thanks !","After offline discussions: will keep that image essentially it's necessary as I have a mapping that creates a mapping between url and local path (images are downloaded via a zip file) and dummy data needs to store that dummy image. The issue is when I read an annotation, I get a url, compute the local path, and basically I assume the local path exists since I've extracted all the images ... This isn't true if dummy data doesn't have all the images, so instead I've added a script that \"fixes\" the dummy data after using the CLI, it essentially adds the dummy image in the zip corresponding to the url."],"created_at":1649852724000,"updated_at":1650555769000,"closed_at":1650546532000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4161","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4161","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4161.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4161.patch","merged_at":1650546532000},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4161\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4161\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4160","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4160\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4160\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4160\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4160","id":1202845874,"node_id":"I_kwDODunzps5Hsfiy","number":4160,"title":"RGBA images not showing","user":{"login":"cceyda","id":15624271,"node_id":"MDQ6VXNlcjE1NjI0Mjcx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/15624271?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/cceyda","html_url":"https:\/\/github.com\/cceyda","followers_url":"https:\/\/api.github.com\/users\/cceyda\/followers","following_url":"https:\/\/api.github.com\/users\/cceyda\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/cceyda\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/cceyda\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/cceyda\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/cceyda\/orgs","repos_url":"https:\/\/api.github.com\/users\/cceyda\/repos","events_url":"https:\/\/api.github.com\/users\/cceyda\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/cceyda\/received_events","type":"User","site_admin":false},"labels":[{"id":3470211881,"node_id":"LA_kwDODunzps7O1zsp","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer","name":"dataset-viewer","color":"E5583E","default":false,"description":"Related to the dataset viewer on huggingface.co"},{"id":4030246674,"node_id":"LA_kwDODunzps7wOK8S","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer-rgba-images","name":"dataset-viewer-rgba-images","color":"6C5FC0","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},"assignees":[{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Thanks for reporting. It's a known issue, and we hope to fix it soon."],"created_at":1649833163000,"updated_at":1649833660000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Dataset viewer issue for ceyda\/smithsonian_butterflies_transparent\r\n\r\n[**Link:** *link to the dataset viewer page*](https:\/\/huggingface.co\/datasets\/ceyda\/smithsonian_butterflies_transparent)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/15624271\/163117683-e91edb28-41bf-43d9-b371-5c62e14f40c9.png)\r\n\r\nAm I the one who added this dataset ? Yes\r\n\r\n\ud83d\udc49  More of a general issue of 'RGBA' png images not being supported \r\n(the dataset itself is just for the huggan sprint and not that important, consider it just an example)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4160\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4160\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4159","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4159\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4159\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4159\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4159","id":1202522153,"node_id":"PR_kwDODunzps42Izmd","number":4159,"title":"Add `TruthfulQA` dataset","user":{"login":"jon-tow","id":41410219,"node_id":"MDQ6VXNlcjQxNDEwMjE5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/41410219?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jon-tow","html_url":"https:\/\/github.com\/jon-tow","followers_url":"https:\/\/api.github.com\/users\/jon-tow\/followers","following_url":"https:\/\/api.github.com\/users\/jon-tow\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jon-tow\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jon-tow\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jon-tow\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jon-tow\/orgs","repos_url":"https:\/\/api.github.com\/users\/jon-tow\/repos","events_url":"https:\/\/api.github.com\/users\/jon-tow\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jon-tow\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4159). All of your documentation changes will be reflected on that endpoint."],"created_at":1649805544000,"updated_at":1649806237000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4159","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4159","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4159.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4159.patch","merged_at":null},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4159\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4159\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4158","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4158\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4158\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4158\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4158","id":1202376843,"node_id":"PR_kwDODunzps42ITg3","number":4158,"title":"Add AUC ROC Metric","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649796808000,"updated_at":1651002110000,"closed_at":1651001722000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4158","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4158","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4158.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4158.patch","merged_at":1651001722000},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4158\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4158\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4157","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4157\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4157\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4157\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4157","id":1202239622,"node_id":"PR_kwDODunzps42H2Wf","number":4157,"title":"Fix formatting in BLEU metric card","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649788191000,"updated_at":1649860225000,"closed_at":1649859394000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4157","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4157","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4157.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4157.patch","merged_at":1649859394000},"body":"Fix #4148 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4157\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4157\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4156","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4156\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4156\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4156\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4156","id":1202220531,"node_id":"PR_kwDODunzps42HySw","number":4156,"title":"Adding STSb-TR dataset","user":{"login":"figenfikri","id":12762065,"node_id":"MDQ6VXNlcjEyNzYyMDY1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12762065?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/figenfikri","html_url":"https:\/\/github.com\/figenfikri","followers_url":"https:\/\/api.github.com\/users\/figenfikri\/followers","following_url":"https:\/\/api.github.com\/users\/figenfikri\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/figenfikri\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/figenfikri\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/figenfikri\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/figenfikri\/orgs","repos_url":"https:\/\/api.github.com\/users\/figenfikri\/repos","events_url":"https:\/\/api.github.com\/users\/figenfikri\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/figenfikri\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1649787005000,"updated_at":1649787155000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4156","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4156","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4156.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4156.patch","merged_at":null},"body":"Semantic Textual Similarity benchmark Turkish (STSb-TR) dataset introduced in our paper [Semantic Similarity Based Evaluation for Abstractive News Summarization](https:\/\/aclanthology.org\/2021.gem-1.3.pdf) added.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4156\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4156\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4155","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4155\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4155\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4155\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4155","id":1202183608,"node_id":"PR_kwDODunzps42Hqam","number":4155,"title":"Make HANS dataset streamable","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649784853000,"updated_at":1649851426000,"closed_at":1649851055000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4155","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4155","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4155.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4155.patch","merged_at":1649851054000},"body":"Fix #4133 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4155\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4155\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4154","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4154\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4154\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4154\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4154","id":1202145721,"node_id":"PR_kwDODunzps42Hh14","number":4154,"title":"Generate tasks.json taxonomy from `huggingface_hub`","user":{"login":"julien-c","id":326577,"node_id":"MDQ6VXNlcjMyNjU3Nw==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/326577?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/julien-c","html_url":"https:\/\/github.com\/julien-c","followers_url":"https:\/\/api.github.com\/users\/julien-c\/followers","following_url":"https:\/\/api.github.com\/users\/julien-c\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/julien-c\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/julien-c\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/julien-c\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/julien-c\/orgs","repos_url":"https:\/\/api.github.com\/users\/julien-c\/repos","events_url":"https:\/\/api.github.com\/users\/julien-c\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/julien-c\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Ok recomputed the json file, this should be ready to review now! @lhoestq ","Note: the generated JSON from `hf\/hub-docs` can be found in the output of a GitHub Action run on that repo, for instance in https:\/\/github.com\/huggingface\/hub-docs\/runs\/6006686983?check_suite_focus=true\r\n\r\n(click on \"Run export-tasks script\")","Should we not add the tasks with hideInDatasets?","yes, probably true \u2013 i'll change that in a PR in `hub-docs`","Yes that's good :) feel free to merge","thanks to the both of you!"],"created_at":1649783566000,"updated_at":1649932352000,"closed_at":1649931973000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4154","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4154","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4154.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4154.patch","merged_at":1649931973000},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4154\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4154\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4153","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4153\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4153\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4153\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4153","id":1202040506,"node_id":"PR_kwDODunzps42HLA8","number":4153,"title":"Adding Text-based NP Enrichment (TNE) dataset","user":{"login":"yanaiela","id":8031035,"node_id":"MDQ6VXNlcjgwMzEwMzU=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8031035?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yanaiela","html_url":"https:\/\/github.com\/yanaiela","followers_url":"https:\/\/api.github.com\/users\/yanaiela\/followers","following_url":"https:\/\/api.github.com\/users\/yanaiela\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yanaiela\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yanaiela\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yanaiela\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yanaiela\/orgs","repos_url":"https:\/\/api.github.com\/users\/yanaiela\/repos","events_url":"https:\/\/api.github.com\/users\/yanaiela\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yanaiela\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hey @lhoestq, can you please have a look? \ud83d\ude4f","Great, thanks again @lhoestq! I think we're good to go now","Done"],"created_at":1649778423000,"updated_at":1651586748000,"closed_at":1651586748000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4153","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4153","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4153.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4153.patch","merged_at":1651586748000},"body":"Added the [TNE](https:\/\/github.com\/yanaiela\/TNE) dataset to the library","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4153\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4153\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4152","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4152\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4152\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4152\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4152","id":1202034115,"node_id":"I_kwDODunzps5HpZXD","number":4152,"title":"ArrayND error in pyarrow 5","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Where do we bump the required pyarrow version? Any inputs on how I fix this issue? ","We need to bump it in `setup.py` as well as update some CI job to use pyarrow 6 instead of 5 in `.circleci\/config.yaml` and `.github\/workflows\/benchmarks.yaml`"],"created_at":1649778100000,"updated_at":1651656586000,"closed_at":1651656586000,"author_association":"MEMBER","active_lock_reason":null,"draft":null,"pull_request":null,"body":"As found in https:\/\/github.com\/huggingface\/datasets\/pull\/3903, The ArrayND features fail on pyarrow 5:\r\n```python\r\nimport pyarrow as pa\r\nfrom datasets import Array2D\r\nfrom datasets.table import cast_array_to_feature\r\n\r\narr = pa.array([[[0]]])\r\nfeature_type = Array2D(shape=(1, 1), dtype=\"int64\")\r\ncast_array_to_feature(arr, feature_type)\r\n```\r\nraises\r\n```python\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-8-04610f9fa78c> in <module>\r\n----> 1 cast_array_to_feature(pa.array([[[0]]]), Array2D(shape=(1, 1), dtype=\"int32\"))\r\n\r\n~\/Desktop\/hf\/datasets\/src\/datasets\/table.py in wrapper(array, *args, **kwargs)\r\n   1672             return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n   1673         else:\r\n-> 1674             return func(array, *args, **kwargs)\r\n   1675 \r\n   1676     return wrapper\r\n\r\n~\/Desktop\/hf\/datasets\/src\/datasets\/table.py in cast_array_to_feature(array, feature, allow_number_to_str)\r\n   1806         return array_cast(array, get_nested_type(feature), allow_number_to_str=allow_number_to_str)\r\n   1807     elif not isinstance(feature, (Sequence, dict, list, tuple)):\r\n-> 1808         return array_cast(array, feature(), allow_number_to_str=allow_number_to_str)\r\n   1809     raise TypeError(f\"Couldn't cast array of type\\n{array.type}\\nto\\n{feature}\")\r\n   1810 \r\n\r\n~\/Desktop\/hf\/datasets\/src\/datasets\/table.py in wrapper(array, *args, **kwargs)\r\n   1672             return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n   1673         else:\r\n-> 1674             return func(array, *args, **kwargs)\r\n   1675 \r\n   1676     return wrapper\r\n\r\n~\/Desktop\/hf\/datasets\/src\/datasets\/table.py in array_cast(array, pa_type, allow_number_to_str)\r\n   1705         array = array.storage\r\n   1706     if isinstance(pa_type, pa.ExtensionType):\r\n-> 1707         return pa_type.wrap_array(array)\r\n   1708     elif pa.types.is_struct(array.type):\r\n   1709         if pa.types.is_struct(pa_type) and (\r\n\r\nAttributeError: 'Array2DExtensionType' object has no attribute 'wrap_array'\r\n```\r\n\r\nThe thing is that `cast_array_to_feature` is called when writing an Arrow file, so creating an Arrow dataset using any ArrayND type currently fails.\r\n\r\n`wrap_array` has been added in pyarrow 6, so we can either bump the required pyarrow version or fix this for pyarrow 5","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4152\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4152\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4151","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4151\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4151\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4151\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4151","id":1201837999,"node_id":"PR_kwDODunzps42GgLu","number":4151,"title":"Add missing label for emotion description","user":{"login":"lijiazheng99","id":44396506,"node_id":"MDQ6VXNlcjQ0Mzk2NTA2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/44396506?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lijiazheng99","html_url":"https:\/\/github.com\/lijiazheng99","followers_url":"https:\/\/api.github.com\/users\/lijiazheng99\/followers","following_url":"https:\/\/api.github.com\/users\/lijiazheng99\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lijiazheng99\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lijiazheng99\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lijiazheng99\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lijiazheng99\/orgs","repos_url":"https:\/\/api.github.com\/users\/lijiazheng99\/repos","events_url":"https:\/\/api.github.com\/users\/lijiazheng99\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lijiazheng99\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1649769457000,"updated_at":1649771930000,"closed_at":1649771930000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4151","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4151","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4151.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4151.patch","merged_at":1649771930000},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4151\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4151\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4150","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4150\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4150\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4150\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4150","id":1201689730,"node_id":"I_kwDODunzps5HoFSC","number":4150,"title":"Inconsistent splits generation for datasets without loading script (packaged dataset puts everything into a single split)","user":{"login":"polinaeterna","id":16348744,"node_id":"MDQ6VXNlcjE2MzQ4NzQ0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/16348744?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/polinaeterna","html_url":"https:\/\/github.com\/polinaeterna","followers_url":"https:\/\/api.github.com\/users\/polinaeterna\/followers","following_url":"https:\/\/api.github.com\/users\/polinaeterna\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/polinaeterna\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/polinaeterna\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/polinaeterna\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/polinaeterna\/orgs","repos_url":"https:\/\/api.github.com\/users\/polinaeterna\/repos","events_url":"https:\/\/api.github.com\/users\/polinaeterna\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/polinaeterna\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1649762155000,"updated_at":1651179764000,"closed_at":1651179764000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nSplits for dataset loaders without scripts are prepared inconsistently. I think it might be confusing for users.\r\n\r\n## Steps to reproduce the bug\r\n* If you load a packaged datasets from Hub, it infers splits from directory structure \/ filenames (check out the data [here](https:\/\/huggingface.co\/datasets\/nateraw\/test-imagefolder-dataset)):\r\n```python\r\nds = load_dataset(\"nateraw\/test-imagefolder-dataset\")\r\nprint(ds)\r\n### Output:\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['image', 'label'],\r\n        num_rows: 6\r\n    })\r\n    test: Dataset({\r\n        features: ['image', 'label'],\r\n        num_rows: 4\r\n    })\r\n})\r\n```\r\n* If you do the same from locally stored data specifying only directory path you'll get the same:\r\n```python\r\nds = load_dataset(\"\/path\/to\/local\/data\/test-imagefolder-dataset\")\r\nprint(ds)\r\n### Output:\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['image', 'label'],\r\n        num_rows: 6\r\n    })\r\n    test: Dataset({\r\n        features: ['image', 'label'],\r\n        num_rows: 4\r\n    })\r\n})\r\n```\r\n* However, if you explicitely specify package name (like `imagefolder`, `csv`, `json`), all the data is put into a single split:\r\n```python\r\nds = load_dataset(\"imagefolder\", data_dir=\"\/path\/to\/local\/data\/test-imagefolder-dataset\")\r\nprint(ds)\r\n### Output:\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['image', 'label'],\r\n        num_rows: 10\r\n    })\r\n})\r\n```\r\n\r\n## Expected results\r\nFor `load_dataset(\"imagefolder\", data_dir=\"\/path\/to\/local\/data\/test-imagefolder-dataset\")` I expect the same output as of the two first options.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4150\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4150\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4149","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4149\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4149\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4149\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4149","id":1201389221,"node_id":"I_kwDODunzps5Hm76l","number":4149,"title":"load_dataset for winoground returning decoding error","user":{"login":"odellus","id":4686956,"node_id":"MDQ6VXNlcjQ2ODY5NTY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/4686956?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/odellus","html_url":"https:\/\/github.com\/odellus","followers_url":"https:\/\/api.github.com\/users\/odellus\/followers","following_url":"https:\/\/api.github.com\/users\/odellus\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/odellus\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/odellus\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/odellus\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/odellus\/orgs","repos_url":"https:\/\/api.github.com\/users\/odellus\/repos","events_url":"https:\/\/api.github.com\/users\/odellus\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/odellus\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["I thought I had fixed it with this after some helpful hints from @severo\r\n```python\r\nimport datasets \r\ntoken = 'hf_XXXXX'\r\ndataset = datasets.load_dataset(\r\n    'facebook\/winoground', \r\n    name='facebook--winoground', \r\n    split='train', \r\n    streaming=True,\r\n    use_auth_token=token,\r\n)\r\n```\r\nbut I found out that wasn't the case\r\n```python\r\n[x for x in dataset]\r\n...\r\nClientResponseError: 401, message='Unauthorized', url=URL('https:\/\/huggingface.co\/datasets\/facebook\/winoground\/resolve\/a86a60456fbbd242e9a744199071a6bd3e7fd9de\/examples.jsonl')\r\n```","Hi ! This dataset structure (image + labels in a JSON file) is not supported yet, though we're adding support for this in  in #4069 \r\n\r\nThe following structure will be supported soon:\r\n```\r\nmetadata.json\r\nimages\/\r\n     image0.png\r\n     image1.png\r\n    ...\r\n```\r\nWhere `metadata.json` is a JSON Lines file with labels or other metadata, and each line must have a \"file_name\" field with the name of the image file.\r\n\r\nFor the moment are only supported:\r\n- JSON files only\r\n- image files only\r\n\r\nSince this dataset is a mix of the two, at the moment it fails trying to read the images as JSON.\r\n\r\nTherefore to be able to load this dataset we need to wait for the new structure to be supported (very soon ^^), or add a dataset script in the repository that reads both the JSON and the images cc @TristanThrush \r\n","We'll also investigate the issue with the streaming download manager in https:\/\/github.com\/huggingface\/datasets\/issues\/4139 ;) thanks for reporting","Are there any updates on this?","In the meantime, anyone can always download the images.zip and examples.jsonl files directly from huggingface.co - let me know if anyone has issues with that.","I mirrored the files at https:\/\/huggingface.co\/datasets\/facebook\/winoground in a folder on my local machine `winground`\r\nand when I tried\r\n```python\r\nimport datasets\r\nds = datasets.load_from_disk('.\/winoground')\r\n```\r\nI get the following error\r\n```python\r\n--------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\nInput In [2], in <cell line: 1>()\r\n----> 1 ds = datasets.load_from_disk('.\/winoground')\r\n\r\nFile ~\/.local\/lib\/python3.8\/site-packages\/datasets\/load.py:1759, in load_from_disk(dataset_path, fs, keep_in_memory)\r\n   1757     return DatasetDict.load_from_disk(dataset_path, fs, keep_in_memory=keep_in_memory)\r\n   1758 else:\r\n-> 1759     raise FileNotFoundError(\r\n   1760         f\"Directory {dataset_path} is neither a dataset directory nor a dataset dict directory.\"\r\n   1761     )\r\n\r\nFileNotFoundError: Directory .\/winoground is neither a dataset directory nor a dataset dict directory.\r\n```\r\nso still some work to be done on the backend imo.","Note that `load_from_disk` is the function that reloads an Arrow dataset saved with `my_dataset.save_to_disk`.\r\n\r\nOnce we do support images with metadata you'll be able to use `load_dataset(\"facebook\/winoground\")` directly (or `load_dataset(\".\/winoground\")` of you've cloned the winoground repository locally).","Apologies for the delay. I added a custom dataset loading script for winoground. It should work now, with an auth token:\r\n\r\n`examples = load_dataset('facebook\/winoground', use_auth_token=<your auth token>)`\r\n\r\nLet me know if there are any issues","Adding the dataset loading script definitely didn't take as long as I thought it would \ud83d\ude05","killer"],"created_at":1649751376000,"updated_at":1651707638000,"closed_at":1651707638000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nI am trying to use datasets to load winoground and I'm getting a JSON decoding error.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ntoken = 'hf_XXXXX' # my HF access token\r\ndatasets = load_dataset('facebook\/winoground', use_auth_token=token)\r\n```\r\n\r\n## Expected results\r\nI downloaded images.zip and examples.jsonl manually. I was expecting to have some trouble decoding json so I didn't use jsonlines but instead was able to get a complete set of 400 examples by doing\r\n```python\r\nimport json\r\n\r\nwith open('examples.jsonl', 'r') as f:\r\n    examples = f.read().split('\\n')\r\n\r\n# Thinking this would error if the JSON is not utf-8 encoded\r\njson_data = [json.loads(x) for x in examples]\r\nprint(json_data[-1])\r\n```\r\nand I see\r\n```python\r\n{'caption_0': 'someone is overdoing it',\r\n 'caption_1': 'someone is doing it over',\r\n 'collapsed_tag': 'Relation',\r\n 'id': 399,\r\n 'image_0': 'ex_399_img_0',\r\n 'image_1': 'ex_399_img_1',\r\n 'num_main_preds': 1,\r\n 'secondary_tag': 'Morpheme-Level',\r\n 'tag': 'Scope, Preposition'}\r\n\r\n```\r\nso I'm not sure what's going on here honestly. The file `examples.jsonl` doesn't have non-UTF-8 encoded text.\r\n\r\n## Actual results\r\nDuring the split operation after downloading, datasets encounters an error in the JSON ([trace](https:\/\/gist.github.com\/odellus\/e55d390ca203386bf551f38e0c63a46b) abbreviated for brevity).\r\n```\r\ndatasets\/packaged_modules\/json\/json.py:144 in Json._generate_tables(self, files)\r\n...\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\r\n```\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.4\r\n- Platform: Linux-5.13.0-39-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 7.0.0\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4149\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4149\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4148","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4148\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4148\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4148\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4148","id":1201169242,"node_id":"I_kwDODunzps5HmGNa","number":4148,"title":"fix confusing bleu metric example","user":{"login":"aizawa-naoki","id":6253193,"node_id":"MDQ6VXNlcjYyNTMxOTM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/6253193?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/aizawa-naoki","html_url":"https:\/\/github.com\/aizawa-naoki","followers_url":"https:\/\/api.github.com\/users\/aizawa-naoki\/followers","following_url":"https:\/\/api.github.com\/users\/aizawa-naoki\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/aizawa-naoki\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/aizawa-naoki\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/aizawa-naoki\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/aizawa-naoki\/orgs","repos_url":"https:\/\/api.github.com\/users\/aizawa-naoki\/repos","events_url":"https:\/\/api.github.com\/users\/aizawa-naoki\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/aizawa-naoki\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1649744306000,"updated_at":1649859394000,"closed_at":1649859394000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"**Is your feature request related to a problem? Please describe.**\r\nI would like to see the example in \"Metric Card for BLEU\" changed.\r\nThe 0th element in the predictions list is not closed in square brackets, and the 1st list is missing a comma.\r\nThe BLEU score are calculated correctly, but it is difficult to understand, so it would be helpful if you could correct this.\r\n```\r\n>> predictions = [\r\n...     [\"hello\", \"there\", \"general\", \"kenobi\",       # <- no closing square bracket.\r\n...     [\"foo\", \"bar\" \"foobar\"]                              # <- no comma between \"bar\" and \"foobar\"\r\n... ]\r\n>>> references = [\r\n...     [[\"hello\", \"there\", \"general\", \"kenobi\"]],\r\n...     [[\"foo\", \"bar\", \"foobar\"]]\r\n... ]\r\n>>> bleu = datasets.load_metric(\"bleu\")\r\n>>> results = bleu.compute(predictions=predictions, references=references)\r\n>>> print(results)\r\n{'bleu': 0.6370964381207871, ...\r\n```\r\n\r\n**Describe the solution you'd like**\r\n```\r\n>> predictions = [\r\n...     [\"hello\", \"there\", \"general\", \"kenobi\",       # <- no closing square bracket.\r\n...     [\"foo\", \"bar\" \"foobar\"]                              # <- no comma between \"bar\" and \"foobar\"\r\n... ]\r\n# and\r\n>>> print(results)\r\n{'bleu':1.0, ...\r\n```\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4148\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4148\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4147","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4147\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4147\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4147\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4147","id":1200756008,"node_id":"PR_kwDODunzps42CtPl","number":4147,"title":"Adjust path to datasets tutorial in How-To","user":{"login":"NimaBoscarino","id":6765188,"node_id":"MDQ6VXNlcjY3NjUxODg=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/6765188?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/NimaBoscarino","html_url":"https:\/\/github.com\/NimaBoscarino","followers_url":"https:\/\/api.github.com\/users\/NimaBoscarino\/followers","following_url":"https:\/\/api.github.com\/users\/NimaBoscarino\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/NimaBoscarino\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/NimaBoscarino\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/NimaBoscarino\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/NimaBoscarino\/orgs","repos_url":"https:\/\/api.github.com\/users\/NimaBoscarino\/repos","events_url":"https:\/\/api.github.com\/users\/NimaBoscarino\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/NimaBoscarino\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649726434000,"updated_at":1649752344000,"closed_at":1649751962000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4147","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4147","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4147.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4147.patch","merged_at":1649751962000},"body":"The link in the How-To overview page to the Datasets tutorials is currently broken. This is just a small adjustment to make it match the format used in https:\/\/github.com\/huggingface\/datasets\/blob\/master\/docs\/source\/tutorial.md.\r\n\r\n(Edit to add: The link in the PR deployment (https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4147\/en\/how_to) is also broken since it's actually hardcoded to `master` and not dynamic to the branch name, but other links seem to behave similarly.)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4147\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4147\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4146","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4146\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4146\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4146\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4146","id":1200215789,"node_id":"I_kwDODunzps5Hidbt","number":4146,"title":"SAMSum dataset viewer not working","user":{"login":"aakashnegi10","id":39906333,"node_id":"MDQ6VXNlcjM5OTA2MzMz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/39906333?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/aakashnegi10","html_url":"https:\/\/github.com\/aakashnegi10","followers_url":"https:\/\/api.github.com\/users\/aakashnegi10\/followers","following_url":"https:\/\/api.github.com\/users\/aakashnegi10\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/aakashnegi10\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/aakashnegi10\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/aakashnegi10\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/aakashnegi10\/orgs","repos_url":"https:\/\/api.github.com\/users\/aakashnegi10\/repos","events_url":"https:\/\/api.github.com\/users\/aakashnegi10\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/aakashnegi10\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["https:\/\/huggingface.co\/datasets\/samsum\r\n\r\n```\r\nStatus code:   400\r\nException:     ValueError\r\nMessage:       Cannot seek streaming HTTP file\r\n```","Currently, only the datasets that can be streamed support the dataset viewer. Maybe @lhoestq @albertvillanova or @mariosasko could give more details about why the dataset cannot be streamed.","It looks like the host (https:\/\/arxiv.org) doesn't allow HTTP Range requests, which is what we use to stream data.\r\n\r\nThis can be fix if we host the data ourselves, which is ok since the dataset is under CC BY-NC-ND 4.0"],"created_at":1649694177000,"updated_at":1651249569000,"closed_at":1651249569000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Dataset viewer issue for '*name of the dataset*'\r\n\r\n**Link:** *link to the dataset viewer page*\r\n\r\n*short description of the issue*\r\n\r\nAm I the one who added this dataset ? Yes-No\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4146\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4146\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4145","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4145\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4145\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4145\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4145","id":1200209781,"node_id":"PR_kwDODunzps42A6Rt","number":4145,"title":"Redirect TIMIT download from LDC","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["CI is failing because some tags are outdated, but they're fixed in #4067 ","_The documentation is not available anymore as the PR was closed or merged._","We may do a release pretty soon (today ?), let me know if it's fine to include it in the new release","Fine to include this change!"],"created_at":1649693875000,"updated_at":1649864371000,"closed_at":1649863984000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4145","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4145","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4145.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4145.patch","merged_at":1649863983000},"body":"LDC data is protected under US copyright laws and under various legal agreements between the Linguistic Data Consortium\/the University of Pennsylvania and data providers which prohibit redistribution of that data by anyone other than LDC. Similarly, LDC's membership agreements, non-member user agreement and various corpus-specific license agreements specifically state that users cannot publish, retransmit, disclose, copy, reproduce or redistribute LDC databases to others outside their organizations.\r\n\r\nLDC explicitly asked us to remove the download script for the TIMIT dataset. In this PR I remove all means to download the dataset, and redirect users to download the data from https:\/\/catalog.ldc.upenn.edu\/LDC93S1 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4145\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4145\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4144","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4144\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4144\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4144\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4144","id":1200016983,"node_id":"PR_kwDODunzps42ARmu","number":4144,"title":"Fix splits in local packaged modules, local datasets without script and hub datasets without script","user":{"login":"polinaeterna","id":16348744,"node_id":"MDQ6VXNlcjE2MzQ4NzQ0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/16348744?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/polinaeterna","html_url":"https:\/\/github.com\/polinaeterna","followers_url":"https:\/\/api.github.com\/users\/polinaeterna\/followers","following_url":"https:\/\/api.github.com\/users\/polinaeterna\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/polinaeterna\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/polinaeterna\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/polinaeterna\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/polinaeterna\/orgs","repos_url":"https:\/\/api.github.com\/users\/polinaeterna\/repos","events_url":"https:\/\/api.github.com\/users\/polinaeterna\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/polinaeterna\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Thanks !\r\nI'm in favor of this change, even though it's a breaking change:\r\n\r\nif you had a dataset\r\n```\r\ndata\/\r\n  train.csv\r\n  test.csv\r\n```\r\n\r\nthen running this code would now return both train and test splits:\r\n```python\r\nload_dataset(\"csv\", data_dir=\"data\/\")\r\n```\r\nwhereas right now it returns only a train split with the data from both CSV files.\r\n\r\nIn my opinion it's ok do do this breaking change because:\r\n- it makes this behavior consistent with `load_dataset(\"path\/to\/data\")` that also returns both splits: data_files resolution must be the same\r\n- I don't expect too many affected users (unless people really wanted to group train and test images in the train split on purpose ?) compared to the many new users to come (especially with #4069 )\r\n- this usage will become more and more common as we add packaged builder and imagefolder\/audiofolder usage grows, so it may be better to do this change early\r\n\r\nLet me know if you think this is acceptable @mariosasko @albertvillanova or not, and if you think we need to first have a warning for some time before switching to this new behavior","Also, if people really want to put train and test, say, images in a single train split they could do \r\n`load_dataset(\"imagefolder\", data_files={\"train\": \"\/path\/to\/data\/**})`. Probably (arguably :)), if this is a more counterintuitive case, then it should require manual files specification, not a default one (in which we expect that users do want to infer splits from filenames \/ dir structure but currently they have to pass smth like `{\"train\": \"\/path\/to\/data\/train*\", \"test\": \"\/path\/to\/data\/test*\"}` explicitly as `data_files`)  ","I also like this change, and  I don't think we even need a warning during the transition period, considering I've been asked several times since the release of `imagefolder` why splits are not correctly inferred if the directory structure is as follows:\r\n```\r\ndata_dir\r\n    train\r\n        label_a\r\n            0.jpg\r\n            ...\r\n        label_b \r\n            0.jpg\r\n            ...\r\n    test\r\n        label_a\r\n            0.jpg\r\n            ...\r\n        label_b \r\n            0.jpg\r\n            ...\r\n```","Cool ! Feel free to add a test (maybe something similar to `test_PackagedDatasetModuleFactory_with_data_dir` but with a data_dir that contains several splits) and mark this PR as ready for review then @polinaeterna :)","@lhoestq @mariosasko do you think it's a good idea to do the same with `HubDatasetModuleFactoryWithoutScript` and `LocalDatasetModuleFactoryWithoutScript` (see the latest change). If we agree on the current change, doing \r\n```python\r\nds = load_dataset(\"polinaeterna\/jsonl_test\", data_dir=\"data\/\")\r\n```\r\non dataset with the following structure:\r\n```\r\ntrain.jsonl\r\ntest.jsonl\r\ndata\/\r\n   train.jsonl\r\n   test.jsonl\r\n```\r\nwill result in having two splits from files under `data\/` dir in specified repo, while master version returns a single train split. \r\nThe same would be for local dataset without script if doing smth like:\r\n```python\r\nds = load_dataset(\"\/home\/polina\/workspace\/repos\/jsonl_test\", data_dir=\"\/home\/polina\/workspace\/repos\/jsonl_test\/data\")\r\n```\r\n(though I'm not sure I understand this use case :D)\r\nLet me know if you think we should preserve the same logic for all factories or if I should roll back this change.","@lhoestq to test passing subdirectory (`base_path`) to data_files functions and methods, I extended the temporary test directory with data so that it contains subdirectory. Because of that the number of files in this directory increased, so I had to change some numbers and patterns to account for this change - [907ddf0](https:\/\/github.com\/huggingface\/datasets\/pull\/4144\/commits\/907ddf09d3afece5afbae18675c859d6e453f2bf)\r\n\r\nDo you think it's ok? Another option is to create another tmp dir and do all the checks inside it. "],"created_at":1649685453000,"updated_at":1651223534000,"closed_at":1651179765000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4144","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4144","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4144.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4144.patch","merged_at":1651179764000},"body":"fixes #4150\r\n\r\nI suggest to infer splits structure from files when `data_dir` is passed with `get_patterns_locally`, analogous to what's done in `LocalDatasetModuleFactoryWithoutScript` with `self.path`, instead of generating files with `data_dir\/**` patterns and putting them all into a single default (train) split.\r\n\r\nI would also suggest to align `HubDatasetModuleFactoryWithoutScript` and `LocalDatasetModuleFactoryWithoutScript` with this logic (remove `data_files = os.path.join(data_dir, \"**\")`). It's not reflected in the current code now as I'd like to discuss it cause I might be unaware of some use cases. @lhoestq @mariosasko @albertvillanova  WDYT?","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4144\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4144\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4143","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4143\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4143\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4143\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4143","id":1199937961,"node_id":"I_kwDODunzps5HhZmp","number":4143,"title":"Unable to download `Wikepedia` 20220301.en version","user":{"login":"beyondguo","id":37113676,"node_id":"MDQ6VXNlcjM3MTEzNjc2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/37113676?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/beyondguo","html_url":"https:\/\/github.com\/beyondguo","followers_url":"https:\/\/api.github.com\/users\/beyondguo\/followers","following_url":"https:\/\/api.github.com\/users\/beyondguo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/beyondguo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/beyondguo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/beyondguo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/beyondguo\/orgs","repos_url":"https:\/\/api.github.com\/users\/beyondguo\/repos","events_url":"https:\/\/api.github.com\/users\/beyondguo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/beyondguo\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi! We've recently updated the Wikipedia script, so these changes are only available on master and can be fetched as follows:\r\n```python\r\ndataset_wikipedia = load_dataset(\"wikipedia\", \"20220301.en\", revision=\"master\")\r\n```","Hi, how can I load the previous \"20200501.en\" version of wikipedia which had been downloaded to the default path? Thanks!"],"created_at":1649682014000,"updated_at":1650967453000,"closed_at":1650560654000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\n\r\nUnable to download `Wikepedia` dataset, 20220301.en version\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n!pip install apache_beam mwparserfromhell\r\ndataset_wikipedia = load_dataset(\"wikipedia\", \"20220301.en\")\r\n```\r\n\r\n## Actual results\r\n```\r\nValueError: BuilderConfig 20220301.en not found. \r\nAvailable: ['20200501.aa', '20200501.ab', '20200501.ace', '20200501.ady', '20200501.af', '20200501.ak', '20200501.als', '20200501.am', '20200501.an', '20200501.ang', '20200501.ar', '20200501.arc', '20200501.arz', '20200501.as', '20200501.ast', '20200501.atj', '20200501.av', '20200501.ay', '20200501.az', '20200501.azb', '20200501.ba', '20200501.bar', '20200501.bat-smg', '20200501.bcl', '20200501.be', '20200501.be-x-old', '20200501.bg', '20200501.bh', '20200501.bi', '20200501.bjn', '20200501.bm', '20200501.bn', '20200501.bo', '20200501.bpy', '20200501.br', '20200501.bs', '20200501.bug', '20200501.bxr', '20200501.ca', '20200501.cbk-zam', '20200501.cdo', '20200501.ce', '20200501.ceb', '20200501.ch', '20200501.cho', '20200501.chr', '20200501.chy', '20200501.ckb', '20200501.co', '20200501.cr', '20200501.crh', '20200501.cs', '20200501.csb', '20200501.cu', '20200501.cv', '20200501.cy', '20200501.da', '20200501.de', '20200501.din', '20200501.diq', '20200501.dsb', '20200501.dty', '20200501.dv', '20200501.dz', '20200501.ee', '20200501.el', '20200501.eml', '20200501.en', '20200501.eo', '20200501.es', '20200501.et', '20200501.eu', '20200501.ext', '20200501.fa', '20200501.ff', '20200501.fi', '20200501.fiu-vro', '20200501.fj', '20200501.fo', '20200501.fr', '20200501.frp', '20200501.frr', '20200501.fur', '20200501.fy', '20200501.ga', '20200501.gag', '20200501.gan', '20200501.gd', '20200501.gl', '20200501.glk', '20200501.gn', '20200501.gom', '20200501.gor', '20200501.got', '20200501.gu', '20200501.gv', '20200501.ha', '20200501.hak', '20200501.haw', '20200501.he', '20200501.hi', '20200501.hif', '20200501.ho', '20200501.hr', '20200501.hsb', '20200501.ht', '20200501.hu', '20200501.hy', '20200501.ia', '20200501.id', '20200501.ie', '20200501.ig', '20200501.ii', '20200501.ik', '20200501.ilo', '20200501.inh', '20200501.io', '20200501.is', '20200501.it', '20200501.iu', '20200501.ja', '20200501.jam', '20200501.jbo', '20200501.jv', '20200501.ka', '20200501.kaa', '20200501.kab', '20200501.kbd', '20200501.kbp', '20200501.kg', '20200501.ki', '20200501.kj', '20200501.kk', '20200501.kl', '20200501.km', '20200501.kn', '20200501.ko', '20200501.koi', '20200501.krc', '20200501.ks', '20200501.ksh', '20200501.ku', '20200501.kv', '20200501.kw', '20200501.ky', '20200501.la', '20200501.lad', '20200501.lb', '20200501.lbe', '20200501.lez', '20200501.lfn', '20200501.lg', '20200501.li', '20200501.lij', '20200501.lmo', '20200501.ln', '20200501.lo', '20200501.lrc', '20200501.lt', '20200501.ltg', '20200501.lv', '20200501.mai', '20200501.map-bms', '20200501.mdf', '20200501.mg', '20200501.mh', '20200501.mhr', '20200501.mi', '20200501.min', '20200501.mk', '20200501.ml', '20200501.mn', '20200501.mr', '20200501.mrj', '20200501.ms', '20200501.mt', '20200501.mus', '20200501.mwl', '20200501.my', '20200501.myv', '20200501.mzn', '20200501.na', '20200501.nah', '20200501.nap', '20200501.nds', '20200501.nds-nl', '20200501.ne', '20200501.new', '20200501.ng', '20200501.nl', '20200501.nn', '20200501.no', '20200501.nov', '20200501.nrm', '20200501.nso', '20200501.nv', '20200501.ny', '20200501.oc', '20200501.olo', '20200501.om', '20200501.or', '20200501.os', '20200501.pa', '20200501.pag', '20200501.pam', '20200501.pap', '20200501.pcd', '20200501.pdc', '20200501.pfl', '20200501.pi', '20200501.pih', '20200501.pl', '20200501.pms', '20200501.pnb', '20200501.pnt', '20200501.ps', '20200501.pt', '20200501.qu', '20200501.rm', '20200501.rmy', '20200501.rn', '20200501.ro', '20200501.roa-rup', '20200501.roa-tara', '20200501.ru', '20200501.rue', '20200501.rw', '20200501.sa', '20200501.sah', '20200501.sat', '20200501.sc', '20200501.scn', '20200501.sco', '20200501.sd', '20200501.se', '20200501.sg', '20200501.sh', '20200501.si', '20200501.simple', '20200501.sk', '20200501.sl', '20200501.sm', '20200501.sn', '20200501.so', '20200501.sq', '20200501.sr', '20200501.srn', '20200501.ss', '20200501.st', '20200501.stq', '20200501.su', '20200501.sv', '20200501.sw', '20200501.szl', '20200501.ta', '20200501.tcy', '20200501.te', '20200501.tet', '20200501.tg', '20200501.th', '20200501.ti', '20200501.tk', '20200501.tl', '20200501.tn', '20200501.to', '20200501.tpi', '20200501.tr', '20200501.ts', '20200501.tt', '20200501.tum', '20200501.tw', '20200501.ty', '20200501.tyv', '20200501.udm', '20200501.ug', '20200501.uk', '20200501.ur', '20200501.uz', '20200501.ve', '20200501.vec', '20200501.vep', '20200501.vi', '20200501.vls', '20200501.vo', '20200501.wa', '20200501.war', '20200501.wo', '20200501.wuu', '20200501.xal', '20200501.xh', '20200501.xmf', '20200501.yi', '20200501.yo', '20200501.za', '20200501.zea', '20200501.zh', '20200501.zh-classical', '20200501.zh-min-nan', '20200501.zh-yue', '20200501.zu']\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0\r\n- Platform: Ubuntu\r\n- Python version: 3.6\r\n- PyArrow version: 6.0.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4143\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4143\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4142","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4142\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4142\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4142\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4142","id":1199794750,"node_id":"I_kwDODunzps5Hg2o-","number":4142,"title":"Add ObjectFolder 2.0 dataset","user":{"login":"osanseviero","id":7246357,"node_id":"MDQ6VXNlcjcyNDYzNTc=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/7246357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/osanseviero","html_url":"https:\/\/github.com\/osanseviero","followers_url":"https:\/\/api.github.com\/users\/osanseviero\/followers","following_url":"https:\/\/api.github.com\/users\/osanseviero\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/osanseviero\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/osanseviero\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/osanseviero\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/osanseviero\/orgs","repos_url":"https:\/\/api.github.com\/users\/osanseviero\/repos","events_url":"https:\/\/api.github.com\/users\/osanseviero\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/osanseviero\/received_events","type":"User","site_admin":false},"labels":[{"id":2067376369,"node_id":"MDU6TGFiZWwyMDY3Mzc2MzY5","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20request","name":"dataset request","color":"e99695","default":false,"description":"Requesting to add a new dataset"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1649674671000,"updated_at":1649674671000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Adding a Dataset\r\n- **Name:** ObjectFolder 2.0\r\n- **Description:** ObjectFolder 2.0 is a dataset of 1,000 objects in the form of implicit representations. It contains 1,000 Object Files each containing the complete multisensory profile for an object instance.\r\n- **Paper:** [*link to the dataset paper if available*](https:\/\/arxiv.org\/abs\/2204.02389)\r\n- **Data:** https:\/\/github.com\/rhgao\/ObjectFolder\r\n\r\nInstructions to add a new dataset can be found [here](https:\/\/github.com\/huggingface\/datasets\/blob\/master\/ADD_NEW_DATASET.md).\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4142\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4142\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4141","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4141\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4141\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4141\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4141","id":1199610885,"node_id":"I_kwDODunzps5HgJwF","number":4141,"title":"Why is the dataset not visible under the dataset preview section?","user":{"login":"Nid989","id":75028682,"node_id":"MDQ6VXNlcjc1MDI4Njgy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/75028682?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Nid989","html_url":"https:\/\/github.com\/Nid989","followers_url":"https:\/\/api.github.com\/users\/Nid989\/followers","following_url":"https:\/\/api.github.com\/users\/Nid989\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Nid989\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Nid989\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Nid989\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Nid989\/orgs","repos_url":"https:\/\/api.github.com\/users\/Nid989\/repos","events_url":"https:\/\/api.github.com\/users\/Nid989\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Nid989\/received_events","type":"User","site_admin":false},"labels":[{"id":3470211881,"node_id":"LA_kwDODunzps7O1zsp","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer","name":"dataset-viewer","color":"E5583E","default":false,"description":"Related to the dataset viewer on huggingface.co"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1649666202000,"updated_at":1649703332000,"closed_at":1649696989000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Dataset viewer issue for '*name of the dataset*'\r\n\r\n**Link:** *link to the dataset viewer page*\r\n\r\n*short description of the issue*\r\n\r\nAm I the one who added this dataset ? Yes-No\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4141\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4141\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4140","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4140\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4140\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4140\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4140","id":1199492356,"node_id":"I_kwDODunzps5Hfs0E","number":4140,"title":"Error loading arxiv data set","user":{"login":"yjqiu","id":5383918,"node_id":"MDQ6VXNlcjUzODM5MTg=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/5383918?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yjqiu","html_url":"https:\/\/github.com\/yjqiu","followers_url":"https:\/\/api.github.com\/users\/yjqiu\/followers","following_url":"https:\/\/api.github.com\/users\/yjqiu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yjqiu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yjqiu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yjqiu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yjqiu\/orgs","repos_url":"https:\/\/api.github.com\/users\/yjqiu\/repos","events_url":"https:\/\/api.github.com\/users\/yjqiu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yjqiu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi! I think this error may be related to using an older version of the library. I was able to load the dataset without any issues using the latest version of `datasets`.  Can you upgrade to the latest version of `datasets` and try again? :)","Hi! As @stevhliu suggested, to fix the issue, update the lib to the newest version with:\r\n```\r\npip install -U datasets\r\n```\r\nand download the dataset as follows:\r\n```python\r\nfrom datasets import load_dataset\r\ndset =  load_dataset('scientific_papers', 'arxiv', download_mode=\"force_redownload\")\r\n```","Thanks for the quick response! It works now. The problem is that I used nlp. load_dataset instead of datasets. load_dataset."],"created_at":1649660794000,"updated_at":1649780648000,"closed_at":1649780648000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Describe the bug\r\nA clear and concise description of what the bug is.\r\n\r\nI met the error below when loading arxiv dataset via `nlp.load_dataset('scientific_papers', 'arxiv',)`. \r\n```\r\nTraceback (most recent call last):\r\n  File \"scripts\/summarization.py\", line 354, in <module>\r\n    main(args)\r\n  File \"scripts\/summarization.py\", line 306, in main\r\n    model.hf_datasets = nlp.load_dataset('scientific_papers', 'arxiv')\r\n  File \"\/opt\/conda\/envs\/longformer\/lib\/python3.7\/site-packages\/nlp\/load.py\", line 549, in load_dataset\r\n    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,\r\n  File \"\/opt\/conda\/envs\/longformer\/lib\/python3.7\/site-packages\/nlp\/builder.py\", line 463, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"\/opt\/conda\/envs\/longformer\/lib\/python3.7\/site-packages\/nlp\/builder.py\", line 522, in _download_and_prepare\r\n    self.info.download_checksums, dl_manager.get_recorded_sizes_checksums(), \"dataset source files\"\r\n  File \"\/opt\/conda\/envs\/longformer\/lib\/python3.7\/site-packages\/nlp\/utils\/info_utils.py\", line 38, in verify_checksums\r\n    raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\nnlp.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https:\/\/drive.google.com\/uc?id=1b3rmCSIoh6VhD4HKWjI4HOW-cSwcwbeC&export=download', 'https:\/\/drive.google.com\/uc?id=1lvsqvsFi3W-pE1SqNZI0s8NR9rC1tsja&export=download']\r\n```\r\n\r\nI then tried to ignore verification steps by `ignore_verifications=True` and there is another error.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/opt\/conda\/envs\/longformer\/lib\/python3.7\/site-packages\/nlp\/builder.py\", line 537, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"\/opt\/conda\/envs\/longformer\/lib\/python3.7\/site-packages\/nlp\/builder.py\", line 810, in _prepare_split\r\n    for key, record in utils.tqdm(generator, unit=\" examples\", total=split_info.num_examples, leave=False):\r\n  File \"\/opt\/conda\/envs\/longformer\/lib\/python3.7\/site-packages\/tqdm\/std.py\", line 1195, in __iter__\r\n    for obj in iterable:\r\n  File \"\/opt\/conda\/envs\/longformer\/lib\/python3.7\/site-packages\/nlp\/datasets\/scientific_papers\/9e4f2cfe3d8494e9f34a84ce49c3214605b4b52a3d8eb199104430d04c52cc12\/scientific_papers.py\", line 108, in _generate_examples\r\n    with open(path, encoding=\"utf-8\") as f:\r\nNotADirectoryError: [Errno 20] Not a directory: '\/home\/username\/.cache\/huggingface\/datasets\/downloads\/c0deae7af7d9c87f25dfadf621f7126f708d7dcac6d353c7564883084a000076\/arxiv-dataset\/train.txt'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"scripts\/summarization.py\", line 354, in <module>\r\n    main(args)\r\n  File \"scripts\/summarization.py\", line 306, in main\r\n    model.hf_datasets = nlp.load_dataset('scientific_papers', 'arxiv', ignore_verifications=True)\r\n  File \"\/opt\/conda\/envs\/longformer\/lib\/python3.7\/site-packages\/nlp\/load.py\", line 549, in load_dataset\r\n    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,\r\n  File \"\/opt\/conda\/envs\/longformer\/lib\/python3.7\/site-packages\/nlp\/builder.py\", line 463, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"\/opt\/conda\/envs\/longformer\/lib\/python3.7\/site-packages\/nlp\/builder.py\", line 539, in _download_and_prepare\r\n    raise OSError(\"Cannot find data file. \" + (self.manual_download_instructions or \"\"))\r\nOSError: Cannot find data file.\r\n```\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:\r\n- Platform:\r\n- Python version:\r\n- PyArrow version:\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4140\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4140\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4139","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4139\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4139\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4139\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4139","id":1199443822,"node_id":"I_kwDODunzps5Hfg9u","number":4139,"title":"Dataset viewer issue for Winoground","user":{"login":"alcinos","id":7438704,"node_id":"MDQ6VXNlcjc0Mzg3MDQ=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/7438704?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alcinos","html_url":"https:\/\/github.com\/alcinos","followers_url":"https:\/\/api.github.com\/users\/alcinos\/followers","following_url":"https:\/\/api.github.com\/users\/alcinos\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alcinos\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alcinos\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alcinos\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alcinos\/orgs","repos_url":"https:\/\/api.github.com\/users\/alcinos\/repos","events_url":"https:\/\/api.github.com\/users\/alcinos\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alcinos\/received_events","type":"User","site_admin":false},"labels":[{"id":3470211881,"node_id":"LA_kwDODunzps7O1zsp","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer","name":"dataset-viewer","color":"E5583E","default":false,"description":"Related to the dataset viewer on huggingface.co"},{"id":4030248571,"node_id":"LA_kwDODunzps7wOLZ7","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer-gated","name":"dataset-viewer-gated","color":"51F745","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},"assignees":[{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},{"login":"SBrandeis","id":33657802,"node_id":"MDQ6VXNlcjMzNjU3ODAy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/33657802?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/SBrandeis","html_url":"https:\/\/github.com\/SBrandeis","followers_url":"https:\/\/api.github.com\/users\/SBrandeis\/followers","following_url":"https:\/\/api.github.com\/users\/SBrandeis\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/SBrandeis\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/SBrandeis\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/SBrandeis\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/SBrandeis\/orgs","repos_url":"https:\/\/api.github.com\/users\/SBrandeis\/repos","events_url":"https:\/\/api.github.com\/users\/SBrandeis\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/SBrandeis\/received_events","type":"User","site_admin":false},{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["related (same dataset): https:\/\/github.com\/huggingface\/datasets\/issues\/4149. But the issue is different. Looking at it","I thought this issue was related to the error I was seeing, but upon consideration I'd think the dataset viewer would return a 500 (unable to create the split like me) or a 404 (unable to load split b\/c it was never created) error if it was having the issue I was seeing in #4149. 401 message makes it look like dataset viewer isn't passing through the identity of the user who has signed the licensing agreement when making the request to GET [examples.jsonl](https:\/\/huggingface.co\/datasets\/facebook\/winoground\/resolve\/a86a60456fbbd242e9a744199071a6bd3e7fd9de\/examples.jsonl).","Pinging @SBrandeis, as it seems related to gated datasets and access tokens.","To replicate:\r\n\r\n```python\r\n>>> import datasets\r\n>>> dataset= datasets.load_dataset('facebook\/winoground', name='facebook--winoground', split='train', use_auth_token=\"hf_app_...\", streaming=True)\r\n>>> next(iter(dataset))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/iterable_dataset.py\", line 497, in __iter__\r\n    for key, example in self._iter():\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/iterable_dataset.py\", line 494, in _iter\r\n    yield from ex_iterable\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/iterable_dataset.py\", line 87, in __iter__\r\n    yield from self.generate_examples_fn(**self.kwargs)\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/iterable_dataset.py\", line 439, in wrapper\r\n    for key, table in generate_tables_fn(**kwargs):\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/packaged_modules\/json\/json.py\", line 85, in _generate_tables\r\n    for file_idx, file in enumerate(files):\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/utils\/streaming_download_manager.py\", line 679, in __iter__\r\n    yield from self.generator(*self.args, **self.kwargs)\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/utils\/streaming_download_manager.py\", line 731, in _iter_from_urlpaths\r\n    for dirpath, _, filenames in xwalk(urlpath, use_auth_token=use_auth_token):\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/utils\/streaming_download_manager.py\", line 623, in xwalk\r\n    for dirpath, dirnames, filenames in fs.walk(main_hop):\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/fsspec\/spec.py\", line 372, in walk\r\n    listing = self.ls(path, detail=True, **kwargs)\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/fsspec\/asyn.py\", line 85, in wrapper\r\n    return sync(self.loop, func, *args, **kwargs)\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/fsspec\/asyn.py\", line 65, in sync\r\n    raise return_result\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/fsspec\/asyn.py\", line 25, in _runner\r\n    result[0] = await coro\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/fsspec\/implementations\/http.py\", line 196, in _ls\r\n    out = await self._ls_real(url, detail=detail, **kwargs)\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/fsspec\/implementations\/http.py\", line 150, in _ls_real\r\n    self._raise_not_found_for_status(r, url)\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/fsspec\/implementations\/http.py\", line 208, in _raise_not_found_for_status\r\n    response.raise_for_status()\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/aiohttp\/client_reqrep.py\", line 1004, in raise_for_status\r\n    raise ClientResponseError(\r\naiohttp.client_exceptions.ClientResponseError: 401, message='Unauthorized', url=URL('https:\/\/huggingface.co\/datasets\/facebook\/winoground\/resolve\/a86a60456fbbd242e9a744199071a6bd3e7fd9de\/examples.jsonl')\r\n```\r\n\r\n*edited to fix `use_token` -> `use_auth_token`, thx @odellus*","~~Using your command to replicate and changing `use_token` to `use_auth_token` fixes the problem I was seeing in #4149.~~\r\nNevermind it gave me an iterator to a method returning the same 401s. Changing `use_token` to `use_auth_token` does not fix the issue.","After investigation with @severo , we found a potential culprit: https:\/\/github.com\/huggingface\/datasets\/blob\/3cd0a009a43f9f174056d70bfa2ca32216181926\/src\/datasets\/utils\/streaming_download_manager.py#L610-L624\r\n\r\nThe streaming manager does not seem to pass `use_auth_token` to `fsspec` when streaming and not iterating content of a zip archive\r\n\r\ncc @albertvillanova @lhoestq ","I was able to reproduce it on a private dataset, let me work on a fix","Hey @lhoestq, Thanks for working on a fix! Any plans to merge #4173 into master? ","Thanks for the heads up, I still need to fix some tests that are failing in the CI before merging ;)","The fix has been merged, we'll do a new release soon, and update the dataset viewer"],"created_at":1649657501000,"updated_at":1651830191000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Dataset viewer issue for 'Winoground'\r\n\r\n**Link:** [*link to the dataset viewer page*](https:\/\/huggingface.co\/datasets\/facebook\/winoground\/viewer\/facebook--winoground\/train)\r\n\r\n*short description of the issue*\r\nGetting 401, message='Unauthorized'\r\nThe dataset is subject to authorization, but I can access the files from the interface, so I assume I'm granted to access it. I'd assume the permission somehow doesn't propagate to the dataset viewer tool.\r\n\r\nAm I the one who added this dataset ? No\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4139\/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4139\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4138","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4138\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4138\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4138\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4138","id":1199291730,"node_id":"I_kwDODunzps5He71S","number":4138,"title":"Incorrect Russian filenames encoding after extraction by datasets.DownloadManager.download_and_extract()","user":{"login":"MalakhovIlyaPavlovich","id":55381086,"node_id":"MDQ6VXNlcjU1MzgxMDg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/55381086?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/MalakhovIlyaPavlovich","html_url":"https:\/\/github.com\/MalakhovIlyaPavlovich","followers_url":"https:\/\/api.github.com\/users\/MalakhovIlyaPavlovich\/followers","following_url":"https:\/\/api.github.com\/users\/MalakhovIlyaPavlovich\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/MalakhovIlyaPavlovich\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/MalakhovIlyaPavlovich\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/MalakhovIlyaPavlovich\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/MalakhovIlyaPavlovich\/orgs","repos_url":"https:\/\/api.github.com\/users\/MalakhovIlyaPavlovich\/repos","events_url":"https:\/\/api.github.com\/users\/MalakhovIlyaPavlovich\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/MalakhovIlyaPavlovich\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["To reproduce:\r\n\r\n```python\r\n>>> import datasets\r\n>>> datasets.get_dataset_split_names('MalakhovIlya\/RuREBus', config_name='raw_txt')\r\nTraceback (most recent call last):\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/inspect.py\", line 280, in get_dataset_config_info\r\n    for split_generator in builder._split_generators(\r\n  File \"\/home\/slesage\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/MalakhovIlya--RuREBus\/21046f5f1a0cf91187d68c30918d78d934ec7113ec435e146776d4f28f12c4ed\/RuREBus.py\", line 101, in _split_generators\r\n    decode_file_names(folder)\r\n  File \"\/home\/slesage\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/MalakhovIlya--RuREBus\/21046f5f1a0cf91187d68c30918d78d934ec7113ec435e146776d4f28f12c4ed\/RuREBus.py\", line 26, in decode_file_names\r\n    for root, dirs, files in os.walk(folder, topdown=False):\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/streaming.py\", line 66, in wrapper\r\n    return function(*args, use_auth_token=use_auth_token, **kwargs)\r\nTypeError: xwalk() got an unexpected keyword argument 'topdown'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/inspect.py\", line 323, in get_dataset_split_names\r\n    info = get_dataset_config_info(\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/inspect.py\", line 285, in get_dataset_config_info\r\n    raise SplitsNotFoundError(\"The split names could not be parsed from the dataset config.\") from err\r\ndatasets.inspect.SplitsNotFoundError: The split names could not be parsed from the dataset config.\r\n```\r\n\r\nIt's not related to the dataset viewer. Maybe @albertvillanova or @lhoestq could help more on this issue.","Hi! This issue stems from the fact that `xwalk`, which is a streamable version of `os.walk`, doesn't support the `topdown` param due to `fsspec`'s `walk` also not supporting it, so fixing this issue could be tricky.  \r\n\r\n@MalakhovIlyaPavlovich You can avoid the error by tweaking your data processing and not using this param. (and `Path.rename`, which also cannot be streamed) ","@mariosasko thank you for your reply. I couldn't reproduce error showed by @severo either on Ubuntu 20.04.3 LTS, Windows 10 and Google Colab environments. But trying to avoid using os.walk(topdown=False) and Path.rename(), In _split_generators I replaced\r\n```\r\ndef decode_file_names(folder):\r\n    for root, dirs, files in os.walk(folder, topdown=False):\r\n        root = Path(root)\r\n        for file in files:\r\n            old_name = root \/ Path(file)\r\n            new_name = root \/ Path(\r\n                file.encode('cp437').decode('cp866'))\r\n            old_name.rename(new_name)\r\n        for dir in dirs:\r\n            old_name = root \/ Path(dir)\r\n            new_name = root \/ Path(dir.encode('cp437').decode('cp866'))\r\n            old_name.rename(new_name)\r\n\r\nfolder = dl_manager.download_and_extract(self._RAW_TXT_URLS)['raw_txt']\r\ndecode_file_names(folder)\r\n```\r\nby\r\n```\r\ndef extract(zip_file_path):\r\n    p = Path(zip_file_path)\r\n    dest_dir = str(p.parent \/ 'extracted' \/ p.stem)\r\n    os.makedirs(dest_dir, exist_ok=True)\r\n    with zipfile.ZipFile(zip_file_path) as archive:\r\n        for file_info in tqdm(archive.infolist(), desc='Extracting'):\r\n            filename = file_info.filename.encode('cp437').decode('cp866')\r\n            target = os.path.join(dest_dir, *filename.split('\/'))\r\n            os.makedirs(os.path.dirname(target), exist_ok=True)\r\n            if not file_info.is_dir():\r\n                with archive.open(file_info) as source, open(target, 'wb') as dest:\r\n                    shutil.copyfileobj(source, dest)\r\n    return dest_dir\r\n\r\nzip_file = dl_manager.download(self._RAW_TXT_URLS)['raw_txt']\r\nif not is_url(zip_file):\r\n    folder = extract(zip_file)\r\nelse:\r\n    folder = None\r\n```\r\nand now everything works well except data viewer for \"raw_txt\" subset: dataset preview on hub shows \"No data.\". As far as I understand dl_manager.download returns original URL when we are calling datasets.get_dataset_split_names and my suspicions are that dataset viewer can do smth similar. I couldn't find information about how it works. I would be very grateful, if you could tell me how to fix this)","This is what I get when I try to stream the `raw_txt` subset:\r\n```python\r\n>>> dset = load_dataset(\"MalakhovIlya\/RuREBus\", \"raw_txt\", split=\"raw_txt\", streaming=True)\r\n>>> next(iter(dset))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nStopIteration\r\n```\r\nSo there is a bug in your script.","streaming=True helped me to find solution. I fixed\r\n```\r\ndef extract(zip_file_path):\r\n    p = Path(zip_file_path)\r\n    dest_dir = str(p.parent \/ 'extracted' \/ p.stem)\r\n    os.makedirs(dest_dir, exist_ok=True)\r\n    with zipfile.ZipFile(zip_file_path) as archive:\r\n        for file_info in tqdm(archive.infolist(), desc='Extracting'):\r\n            filename = file_info.filename.encode('cp437').decode('cp866')\r\n            target = os.path.join(dest_dir, *filename.split('\/'))\r\n            os.makedirs(os.path.dirname(target), exist_ok=True)\r\n            if not file_info.is_dir():\r\n                with archive.open(file_info) as source, open(target, 'wb') as dest:\r\n                    shutil.copyfileobj(source, dest)\r\n    return dest_dir\r\n\r\nzip_file = dl_manager.download(self._RAW_TXT_URLS)['raw_txt']\r\nfolder = extract(zip_file)\r\n```\r\nby \r\n```\r\nfolder = dl_manager.download_and_extract(self._RAW_TXT_URLS)['raw_txt']\r\npath = os.path.join(folder, 'MED_txt\/unparsed_txt')\r\nfor root, dirs, files in os.walk(path):\r\n    decoded_root_name = Path(root).name.encode('cp437').decode('cp866')\r\n```\r\n@mariosasko thank you for your help :)"],"created_at":1649642833000,"updated_at":1650338146000,"closed_at":1650123989000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Dataset viewer issue for 'MalakhovIlya\/RuREBus'\r\n\r\n**Link:** https:\/\/huggingface.co\/datasets\/MalakhovIlya\/RuREBus\r\n\r\n**Description**\r\nUsing os.walk(topdown=False) in DatasetBuilder causes following error:\r\nStatus code:   400\r\nException:     TypeError\r\nMessage:       xwalk() got an unexpected keyword argument 'topdown'\r\nCouldn't find where \"xwalk\" come from. How can I fix this?\r\n\r\nAm I the one who added this dataset ? Yes\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4138\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4138\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4137","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4137\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4137\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4137\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4137","id":1199000453,"node_id":"PR_kwDODunzps419D6A","number":4137,"title":"Add single dataset citations for TweetEval","user":{"login":"gchhablani","id":29076344,"node_id":"MDQ6VXNlcjI5MDc2MzQ0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/29076344?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/gchhablani","html_url":"https:\/\/github.com\/gchhablani","followers_url":"https:\/\/api.github.com\/users\/gchhablani\/followers","following_url":"https:\/\/api.github.com\/users\/gchhablani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/gchhablani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/gchhablani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/gchhablani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/gchhablani\/orgs","repos_url":"https:\/\/api.github.com\/users\/gchhablani\/repos","events_url":"https:\/\/api.github.com\/users\/gchhablani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/gchhablani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","The `test_dataset_cards` method is failing with the error:\r\n\r\n```\r\nif error_messages:\r\n>           raise ValueError(\"\\n\".join(error_messages))\r\nE           ValueError: The following issues have been found in the dataset cards:\r\nE           YAML tags:\r\nE           The following typing errors are found: {'annotations_creators': \"(Expected `typing.List` with length > 0. Found value of type: `<class 'list'>`, with length: 0.\\n)\\nOR\\n(Expected `typing.Dict` with length > 0. Found value of type: `<class 'list'>`, with length: 0.\\n)\"}\r\n```\r\n\r\nAdding `found` as annotation creators."],"created_at":1649591514000,"updated_at":1649750242000,"closed_at":1649749875000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4137","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4137","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4137.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4137.patch","merged_at":1649749875000},"body":"This PR adds single data citations as per request of the original creators of the TweetEval dataset.\r\n\r\nThis is a recent email from the creator:\r\n\r\n> Could I ask you a favor? Would you be able to add at the end of the README the citations of the single datasets as well? You can just copy our readme maybe? https:\/\/github.com\/cardiffnlp\/tweeteval#citing-tweeteval\r\n(just to be sure that the creator of the single datasets also get credits when tweeteval is used)\r\n\r\nPlease let me know if this looks okay or if any changes are needed.\r\n\r\nThanks,\r\nGunjan\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4137\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4137\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4135","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4135\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4135\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4135\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4135","id":1198307610,"node_id":"PR_kwDODunzps416-Rn","number":4135,"title":"Support streaming xtreme dataset for PAN-X config","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649485188000,"updated_at":1651826380000,"closed_at":1649660054000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4135","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4135","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4135.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4135.patch","merged_at":1649660054000},"body":"Support streaming xtreme dataset for PAN-X config.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4135\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4135\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4134","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4134\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4134\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4134\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4134","id":1197937146,"node_id":"I_kwDODunzps5HZxH6","number":4134,"title":"ELI5 supporting documents","user":{"login":"Slayer-007","id":69015896,"node_id":"MDQ6VXNlcjY5MDE1ODk2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/69015896?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Slayer-007","html_url":"https:\/\/github.com\/Slayer-007","followers_url":"https:\/\/api.github.com\/users\/Slayer-007\/followers","following_url":"https:\/\/api.github.com\/users\/Slayer-007\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Slayer-007\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Slayer-007\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Slayer-007\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Slayer-007\/orgs","repos_url":"https:\/\/api.github.com\/users\/Slayer-007\/repos","events_url":"https:\/\/api.github.com\/users\/Slayer-007\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Slayer-007\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892912,"node_id":"MDU6TGFiZWwxOTM1ODkyOTEy","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/question","name":"question","color":"d876e3","default":true,"description":"Further information is requested"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi ! Please post your question on the [forum](https:\/\/discuss.huggingface.co\/), more people will be able to help you there ;)"],"created_at":1649460987000,"updated_at":1649857966000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"if i am using dense search to create supporting documents for eli5 how much time it will take bcz i read somewhere that it takes about 18 hrs??","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4134\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4134\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4133","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4133\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4133\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4133\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4133","id":1197830623,"node_id":"I_kwDODunzps5HZXHf","number":4133,"title":"HANS dataset preview broken","user":{"login":"pietrolesci","id":61748653,"node_id":"MDQ6VXNlcjYxNzQ4NjUz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/61748653?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/pietrolesci","html_url":"https:\/\/github.com\/pietrolesci","followers_url":"https:\/\/api.github.com\/users\/pietrolesci\/followers","following_url":"https:\/\/api.github.com\/users\/pietrolesci\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/pietrolesci\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/pietrolesci\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/pietrolesci\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/pietrolesci\/orgs","repos_url":"https:\/\/api.github.com\/users\/pietrolesci\/repos","events_url":"https:\/\/api.github.com\/users\/pietrolesci\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/pietrolesci\/received_events","type":"User","site_admin":false},"labels":[{"id":3287858981,"node_id":"MDU6TGFiZWwzMjg3ODU4OTgx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/streaming","name":"streaming","color":"fef2c0","default":false,"description":""}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The dataset cannot be loaded, be it in normal or streaming mode.\r\n\r\n```python\r\n>>> import datasets\r\n>>> dataset=datasets.load_dataset(\"hans\", split=\"train\", streaming=True)\r\n>>> next(iter(dataset))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/iterable_dataset.py\", line 497, in __iter__\r\n    for key, example in self._iter():\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/iterable_dataset.py\", line 494, in _iter\r\n    yield from ex_iterable\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/iterable_dataset.py\", line 87, in __iter__\r\n    yield from self.generate_examples_fn(**self.kwargs)\r\n  File \"\/home\/slesage\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/hans\/1bbcb735c482acd54f2e118074b59cfd2bf5f7a5a285d4d540d1e632216672ac\/hans.py\", line 121, in _generate_examples\r\n    for idx, line in enumerate(open(filepath, \"rb\")):\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/fsspec\/spec.py\", line 1595, in __next__\r\n    out = self.readline()\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/fsspec\/spec.py\", line 1592, in readline\r\n    return self.readuntil(b\"\\n\")\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/fsspec\/spec.py\", line 1581, in readuntil\r\n    self.seek(start + found + len(char))\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/fsspec\/implementations\/http.py\", line 676, in seek\r\n    raise ValueError(\"Cannot seek streaming HTTP file\")\r\nValueError: Cannot seek streaming HTTP file\r\n>>> dataset=datasets.load_dataset(\"hans\", split=\"train\", streaming=False)\r\nDownloading and preparing dataset hans\/plain_text (download: 29.51 MiB, generated: 30.34 MiB, post-processed: Unknown size, total: 59.85 MiB) to \/home\/slesage\/.cache\/huggingface\/datasets\/hans\/plain_text\/1.0.0\/1bbcb735c482acd54f2e118074b59cfd2bf5f7a5a285d4d540d1e632216672ac...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/load.py\", line 1687, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/builder.py\", line 605, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/builder.py\", line 1104, in _download_and_prepare\r\n    super()._download_and_prepare(dl_manager, verify_infos, check_duplicate_keys=verify_infos)\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/builder.py\", line 694, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/builder.py\", line 1087, in _prepare_split\r\n    for key, record in logging.tqdm(\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/tqdm\/std.py\", line 1180, in __iter__\r\n    for obj in iterable:\r\n  File \"\/home\/slesage\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/hans\/1bbcb735c482acd54f2e118074b59cfd2bf5f7a5a285d4d540d1e632216672ac\/hans.py\", line 121, in _generate_examples\r\n    for idx, line in enumerate(open(filepath, \"rb\")):\r\nValueError: readline of closed file\r\n```\r\n\r\n","Hi! I've opened a PR that should make this dataset stremable. You can test it as follows:\r\n```python\r\nfrom datasets import load_dataset\r\ndset = load_dataset(\"hans\", split=\"train\", streaming=True, revision=\"49decd29839c792ecc24ac88f861cbdec30c1c40\")\r\n```\r\n\r\n@severo The current script doesn't throw an error in normal mode (only in streaming mode) on my local machine or in Colab. Can you update your installation of `datasets` and see if that fixes the issue?","Thanks for this. It works well, thanks! The dataset viewer is using https:\/\/github.com\/huggingface\/datasets\/releases\/tag\/2.0.0, I'm eager to upgrade to 2.0.1 \ud83d\ude09"],"created_at":1649451975000,"updated_at":1649851054000,"closed_at":1649851054000,"author_association":"NONE","active_lock_reason":null,"draft":null,"pull_request":null,"body":"## Dataset viewer issue for '*hans*'\r\n\r\n**Link:** [https:\/\/huggingface.co\/datasets\/hans](https:\/\/huggingface.co\/datasets\/hans)\r\n\r\nHANS dataset preview is broken with error 400\r\n\r\nAm I the one who added this dataset ? No\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4133\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4133\/timeline","performed_via_github_app":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4132","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4132\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4132\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4132\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4132","id":1197661720,"node_id":"PR_kwDODunzps41460R","number":4132,"title":"Support streaming xtreme dataset for PAWS-X config","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649442332000,"updated_at":1651826382000,"closed_at":1649451764000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4132","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4132","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4132.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4132.patch","merged_at":1649451764000},"body":"Support streaming xtreme dataset for PAWS-X config.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4132\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4132\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4131","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4131\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4131\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4131\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4131","id":1197472249,"node_id":"PR_kwDODunzps414Zt1","number":4131,"title":"Support streaming xtreme dataset for udpos config","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649431849000,"updated_at":1651826386000,"closed_at":1649435287000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4131","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4131","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4131.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4131.patch","merged_at":1649435287000},"body":"Support streaming xtreme dataset for udpos config.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4131\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4131\/timeline","performed_via_github_app":null,"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4130","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4130\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4130\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4130\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4130","id":1197456857,"node_id":"PR_kwDODunzps414Wqx","number":4130,"title":"Add SBU Captions Photo Dataset","user":{"login":"thomasw21","id":24695242,"node_id":"MDQ6VXNlcjI0Njk1MjQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/24695242?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/thomasw21","html_url":"https:\/\/github.com\/thomasw21","followers_url":"https:\/\/api.github.com\/users\/thomasw21\/followers","following_url":"https:\/\/api.github.com\/users\/thomasw21\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/thomasw21\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/thomasw21\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/thomasw21\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/thomasw21\/orgs","repos_url":"https:\/\/api.github.com\/users\/thomasw21\/repos","events_url":"https:\/\/api.github.com\/users\/thomasw21\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/thomasw21\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649431059000,"updated_at":1649760451000,"closed_at":1649760089000,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4130","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4130","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4130.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4130.patch","merged_at":1649760089000},"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4130\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4130\/timeline","performed_via_github_app":null,"is_pull_request":true}
