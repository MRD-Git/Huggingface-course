{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4291","id":1227777500,"node_id":"I_kwDODunzps5JLmXc","number":4291,"title":"Dataset Viewer issue for strombergnlp\/ipm_nel : preview is empty, no error message","user":{"login":"leondz","id":121934,"node_id":"MDQ6VXNlcjEyMTkzNA==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/121934?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/leondz","html_url":"https:\/\/github.com\/leondz","followers_url":"https:\/\/api.github.com\/users\/leondz\/followers","following_url":"https:\/\/api.github.com\/users\/leondz\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/leondz\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/leondz\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/leondz\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/leondz\/orgs","repos_url":"https:\/\/api.github.com\/users\/leondz\/repos","events_url":"https:\/\/api.github.com\/users\/leondz\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/leondz\/received_events","type":"User","site_admin":false},"labels":[{"id":3470211881,"node_id":"LA_kwDODunzps7O1zsp","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer","name":"dataset-viewer","color":"E5583E","default":false,"description":"Related to the dataset viewer on huggingface.co"}],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi @leondz, thanks for reporting.\r\n\r\nIndeed, the dataset viewer relies on the dataset being streamable (passing `streaming=True` to `load_dataset`). Whereas most of the datastes are streamable out of the box (thanks to our implementation of streaming), there are still some exceptions.\r\n\r\nIn particular, in your case, that is due to the data file being TAR. This format is not streamable out of the box (it does not allow random access to the archived files), but we use a trick to allow streaming: using `dl_manager.iter_archive`.\r\n\r\nLet me know if you need some help: I could push a commit to your repo with the fix."],"created_at":1651838607000,"updated_at":1651848811000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Link\n\nhttps:\/\/huggingface.co\/datasets\/strombergnlp\/ipm_nel\/viewer\/ipm_nel\/train\n\n### Description\n\nThe viewer is blank. I tried my best to emulate a dataset with a working viewer, but this one just doesn't seem to want to come up. What did I miss?\n\n### Owner\n\nYes","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4290","id":1227592826,"node_id":"PR_kwDODunzps43Zr08","number":4290,"title":"Update README.md","user":{"login":"monk1337","id":17107749,"node_id":"MDQ6VXNlcjE3MTA3NzQ5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17107749?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/monk1337","html_url":"https:\/\/github.com\/monk1337","followers_url":"https:\/\/api.github.com\/users\/monk1337\/followers","following_url":"https:\/\/api.github.com\/users\/monk1337\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/monk1337\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/monk1337\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/monk1337\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/monk1337\/orgs","repos_url":"https:\/\/api.github.com\/users\/monk1337\/repos","events_url":"https:\/\/api.github.com\/users\/monk1337\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/monk1337\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4290). All of your documentation changes will be reflected on that endpoint."],"created_at":1651827171000,"updated_at":1651827848000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Updating readme in medmcqa dataset.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4290","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4290","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4290.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4290.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4288","id":1226821732,"node_id":"PR_kwDODunzps43XLKi","number":4288,"title":"Add missing `faiss` import to fix https:\/\/github.com\/huggingface\/datasets\/issues\/4287","user":{"login":"alvarobartt","id":36760800,"node_id":"MDQ6VXNlcjM2NzYwODAw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36760800?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alvarobartt","html_url":"https:\/\/github.com\/alvarobartt","followers_url":"https:\/\/api.github.com\/users\/alvarobartt\/followers","following_url":"https:\/\/api.github.com\/users\/alvarobartt\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alvarobartt\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alvarobartt\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alvarobartt\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alvarobartt\/orgs","repos_url":"https:\/\/api.github.com\/users\/alvarobartt\/repos","events_url":"https:\/\/api.github.com\/users\/alvarobartt\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alvarobartt\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1651764109000,"updated_at":1651764109000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"This PR fixes the issue recently mentioned in https:\/\/github.com\/huggingface\/datasets\/issues\/4287 \ud83e\udd17 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4288","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4288","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4288.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4288.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4287","id":1226806652,"node_id":"I_kwDODunzps5JH5V8","number":4287,"title":"\"NameError: name 'faiss' is not defined\" on `.add_faiss_index` when `device` is not None","user":{"login":"alvarobartt","id":36760800,"node_id":"MDQ6VXNlcjM2NzYwODAw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36760800?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alvarobartt","html_url":"https:\/\/github.com\/alvarobartt","followers_url":"https:\/\/api.github.com\/users\/alvarobartt\/followers","following_url":"https:\/\/api.github.com\/users\/alvarobartt\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alvarobartt\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alvarobartt\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alvarobartt\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alvarobartt\/orgs","repos_url":"https:\/\/api.github.com\/users\/alvarobartt\/repos","events_url":"https:\/\/api.github.com\/users\/alvarobartt\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alvarobartt\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["So I managed to solve this by adding a missing `import faiss` in the `@staticmethod` defined in https:\/\/github.com\/huggingface\/datasets\/blob\/f51b6994db27ea69261ef919fb7775928f9ec10b\/src\/datasets\/search.py#L305, triggered from https:\/\/github.com\/huggingface\/datasets\/blob\/f51b6994db27ea69261ef919fb7775928f9ec10b\/src\/datasets\/search.py#L249 when trying to `ds_with_embeddings.add_faiss_index(column='embeddings', device=0)` with the code above.\r\n\r\nAs it seems that the `@staticmethod` doesn't recognize the `import faiss` defined in https:\/\/github.com\/huggingface\/datasets\/blob\/f51b6994db27ea69261ef919fb7775928f9ec10b\/src\/datasets\/search.py#L261, so whenever the value of `device` is not None in https:\/\/github.com\/huggingface\/datasets\/blob\/71f76e0bdeaddadedc4f9c8d15cfff5a36d62f66\/src\/datasets\/search.py#L438, that exception is triggered.\r\n\r\nSo on, adding `import faiss` inside https:\/\/github.com\/huggingface\/datasets\/blob\/71f76e0bdeaddadedc4f9c8d15cfff5a36d62f66\/src\/datasets\/search.py#L305 right after the check of `device`'s value, solves the issue and lets you calculate the indices in GPU.\r\n\r\nI'll add the code in a PR linked to this issue in case you want to merge it!","Adding here the complete error traceback!\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/home\/alvarobartt\/lol.py\", line 12, in <module>\r\n    ds_with_embeddings.add_faiss_index(column='embeddings', device=0) # default `device=None`\r\n  File \"\/home\/alvarobartt\/.local\/lib\/python3.9\/site-packages\/datasets\/arrow_dataset.py\", line 3656, in add_faiss_index\r\n    super().add_faiss_index(\r\n  File \"\/home\/alvarobartt\/.local\/lib\/python3.9\/site-packages\/datasets\/search.py\", line 478, in add_faiss_index\r\n    faiss_index.add_vectors(self, column=column, train_size=train_size, faiss_verbose=True)\r\n  File \"\/home\/alvarobartt\/.local\/lib\/python3.9\/site-packages\/datasets\/search.py\", line 281, in add_vectors\r\n    self.faiss_index = self._faiss_index_to_device(index, self.device)\r\n  File \"\/home\/alvarobartt\/.local\/lib\/python3.9\/site-packages\/datasets\/search.py\", line 327, in _faiss_index_to_device\r\n    faiss_res = faiss.StandardGpuResources()\r\nNameError: name 'faiss' is not defined\r\n```"],"created_at":1651763385000,"updated_at":1651837063000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\n\r\nWhen using `datasets` to calculate the FAISS indices of a dataset, the exception `NameError: name 'faiss' is not defined` is triggered when trying to calculate those on a device (GPU), so `.add_faiss_index(..., device=0)` fails with that exception.\r\n\r\nAll that assuming that `datasets` is properly installed and `faiss-gpu` too, as well as all the CUDA drivers required.\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\n# Sample code to reproduce the bug\r\nfrom transformers import DPRContextEncoder, DPRContextEncoderTokenizer\r\nimport torch\r\ntorch.set_grad_enabled(False)\r\nctx_encoder = DPRContextEncoder.from_pretrained(\"facebook\/dpr-ctx_encoder-single-nq-base\")\r\nctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook\/dpr-ctx_encoder-single-nq-base\")\r\n\r\nfrom datasets import load_dataset\r\nds = load_dataset('crime_and_punish', split='train[:100]')\r\nds_with_embeddings = ds.map(lambda example: {'embeddings': ctx_encoder(**ctx_tokenizer(example[\"line\"], return_tensors=\"pt\"))[0][0].numpy()})\r\n\r\nds_with_embeddings.add_faiss_index(column='embeddings', device=0) # default `device=None`\r\n```\r\n\r\n## Expected results\r\n\r\nA new column named `embeddings` in the dataset that we're adding the index to.\r\n\r\n## Actual results\r\n\r\nAn exception is triggered with the following message `NameError: name 'faiss' is not defined`.\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 2.1.0\r\n- Platform: Linux-5.13.0-1022-azure-x86_64-with-glibc2.31\r\n- Python version: 3.9.12\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.2\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4286","id":1226758621,"node_id":"PR_kwDODunzps43W-DI","number":4286,"title":"Add Lahnda language tag","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4286). All of your documentation changes will be reflected on that endpoint."],"created_at":1651761260000,"updated_at":1651762010000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"This language is present in [Wikimedia's WIT](https:\/\/huggingface.co\/datasets\/wikimedia\/wit_base) dataset.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4286","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4286","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4286.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4286.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4285","id":1226374831,"node_id":"PR_kwDODunzps43VtEa","number":4285,"title":"Update LexGLUE README.md","user":{"login":"iliaschalkidis","id":1626984,"node_id":"MDQ6VXNlcjE2MjY5ODQ=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1626984?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/iliaschalkidis","html_url":"https:\/\/github.com\/iliaschalkidis","followers_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/followers","following_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/orgs","repos_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/repos","events_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651739810000,"updated_at":1651757944000,"closed_at":1651757615000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Update the leaderboard based on the latest results presented in the ACL 2022 version of the article.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4285","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4285","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4285.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4285.patch","merged_at":1651757615000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4284","id":1226200727,"node_id":"I_kwDODunzps5JFlaX","number":4284,"title":"Issues in processing very large datasets","user":{"login":"sajastu","id":10419055,"node_id":"MDQ6VXNlcjEwNDE5MDU1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10419055?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sajastu","html_url":"https:\/\/github.com\/sajastu","followers_url":"https:\/\/api.github.com\/users\/sajastu\/followers","following_url":"https:\/\/api.github.com\/users\/sajastu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sajastu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sajastu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sajastu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sajastu\/orgs","repos_url":"https:\/\/api.github.com\/users\/sajastu\/repos","events_url":"https:\/\/api.github.com\/users\/sajastu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sajastu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1651726869000,"updated_at":1651726869000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\nI'm trying to add a feature called \"subgraph\" to CNN\/DM dataset (modifications on run_summarization.py of Huggingface Transformers script) --- I'm not quite sure if I'm doing it the right way, though--- but the main problem appears when the training starts where the error ` [OSError: [Errno 12] Cannot allocate memory]`  appears. I suppose this problem roots in RAM issues and how the dataset is loaded during training, but I have no clue of what I can do to fix it.  Observing the dataset's cache directory, I see that it takes ~600GB of memory and that's why I believe special care is needed when loading it into the memory. \r\n\r\n\r\nHere are my modifications to `run_summarization.py` code. \r\n\r\n\r\n```\r\n# loading pre-computed dictionary where keys are 'id' of article and values are corresponding subgraph\r\ngraph_data_train = get_graph_data('train') \r\ngraph_data_validation = get_graph_data('val')\r\n...\r\n...\r\n\r\n\r\nwith training_args.main_process_first(desc=\"train dataset map pre-processing\"):\r\n    train_dataset = train_dataset.map(\r\n        preprocess_function_train,\r\n        batched=True,\r\n        num_proc=data_args.preprocessing_num_workers,\r\n        remove_columns=column_names,\r\n        load_from_cache_file=not data_args.overwrite_cache,\r\n        desc=\"Running tokenizer on train dataset\",\r\n    )\r\n\r\n```\r\n\r\n\r\nAnd here is the modified preprocessed function:\r\n\r\n```\r\ndef preprocess_function_train(examples):\r\n        inputs, targets, sub_graphs, ids = [], [], [], []\r\n        for i in range(len(examples[text_column])):\r\n            if examples[text_column][i] is not None and examples[summary_column][i] is not None:\r\n                # if examples['doc_id'][i] in graph_data.keys():\r\n                inputs.append(examples[text_column][i])\r\n                targets.append(examples[summary_column][i])\r\n                sub_graphs.append(graph_data_train[examples['id'][i]])\r\n                ids.append(examples['id'][i])\r\n\r\n        inputs = [prefix + inp for inp in inputs]\r\n        model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True,\r\n                                 sub_graphs=sub_graphs, ids=ids)\r\n\r\n            # Setup the tokenizer for targets\r\n        with tokenizer.as_target_tokenizer():\r\n            labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\r\n\r\n        # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\r\n        # padding in the loss.\r\n        if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\r\n            labels[\"input_ids\"] = [\r\n                [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\r\n            ]\r\n\r\n        model_inputs[\"labels\"] = labels[\"input_ids\"]\r\n        return model_inputs\r\n```\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:  2.1.0\r\n- Platform: Linux Ubuntu\r\n- Python version: 3.6\r\n- PyArrow version: 6.0.1\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4283","id":1225686988,"node_id":"PR_kwDODunzps43Tnxo","number":4283,"title":"Fix filesystem docstring","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651686162000,"updated_at":1651854722000,"closed_at":1651818137000,"author_association":"MEMBER","active_lock_reason":null,"body":"This PR untangles the `S3FileSystem` docstring so the [parameters](https:\/\/huggingface.co\/docs\/datasets\/master\/en\/package_reference\/main_classes#parameters) are properly displayed.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4283","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4283","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4283.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4283.patch","merged_at":1651818137000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4282","id":1225616545,"node_id":"PR_kwDODunzps43TZYL","number":4282,"title":"Don't do unnecessary list type casting to avoid replacing None values by empty lists","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Quick question about the message in the warning. You say \"will be fixed in a future major version\" but don't you mean \"will raise an error in a future major version\"?","Right ! Good catch, thanks, I updated the message to say \"will raise an error in a future major version\""],"created_at":1651682221000,"updated_at":1651833838000,"closed_at":1651833420000,"author_association":"MEMBER","active_lock_reason":null,"body":"In certain cases, `None` values are replaced by empty lists when casting feature types.\r\n\r\nIt happens every time you cast an array of nested lists like [None, [0, 1, 2, 3]] to a different type (to change the integer precision for example). In this case you'd get [[], [0, 1, 2, 3]] for example. This issue comes from PyArrow, see the discussion in https:\/\/github.com\/huggingface\/datasets\/issues\/3676\r\n\r\nThis issue also happens when no type casting is needed, because casting is supposed to be a no-op in this case. But as https:\/\/github.com\/huggingface\/datasets\/issues\/3676 shown, it's not the case and `None` are replaced by empty lists even if we cast to the exact same type.\r\n\r\nIn this PR I just workaround this bug in the case where no type casting is needed. In particular, I only call `pa.ListArray.from_arrays` only when necessary.\r\n\r\nI also added a warning when some `None` are effectively replaced by empty lists. I wanted to raise an error in this case, but maybe we should wait a major update to do so\r\n\r\nThis PR fixes this particular case, that is occurring in `run_qa.py` in `transformers`:\r\n```python\r\nfrom datasets import Dataset\r\n\r\nds = Dataset.from_dict({\"a\": range(4)})\r\nds = ds.map(lambda x: {\"b\": [[None, [0]]]}, batched=True, batch_size=1, remove_columns=[\"a\"])\r\nprint(ds.to_pandas())\r\n# before:\r\n#              b\r\n# 0  [None, [0]]\r\n# 1    [[], [0]]\r\n# 2    [[], [0]]\r\n# 3    [[], [0]]\r\n#\r\n# now:\r\n#              b\r\n# 0  [None, [0]]\r\n# 1  [None, [0]]\r\n# 2  [None, [0]]\r\n# 3  [None, [0]]\r\n```\r\n\r\ncc @sgugger ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4282","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4282","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4282.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4282.patch","merged_at":1651833420000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4281","id":1225556939,"node_id":"PR_kwDODunzps43TNBm","number":4281,"title":"Remove a copy-paste sentence in dataset cards","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","The non-passing tests have nothing to do with this PR."],"created_at":1651678915000,"updated_at":1651826283000,"closed_at":1651689196000,"author_association":"MEMBER","active_lock_reason":null,"body":"Remove the following copy-paste sentence from dataset cards:\r\n```\r\nWe show detailed information for up to 5 configurations of the dataset.\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4281","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4281","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4281.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4281.patch","merged_at":1651689196000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4280","id":1225446844,"node_id":"PR_kwDODunzps43S2xg","number":4280,"title":"Add missing features to commonsense_qa dataset","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","@albertvillanova it adds question_concept and id which is great. I suppose we'll talk about staying true to the format on another PR. ","Yes, let's merge this PR as it is: it adds missing features.\r\n\r\nA subsequent PR may address the request on changing the dataset feature structure."],"created_at":1651674266000,"updated_at":1651847037000,"closed_at":1651846606000,"author_association":"MEMBER","active_lock_reason":null,"body":"Fix partially #4275.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4280","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4280","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4280.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4280.patch","merged_at":1651846606000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4279","id":1225300273,"node_id":"PR_kwDODunzps43SXw5","number":4279,"title":"Update minimal PyArrow version warning","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651667169000,"updated_at":1651740658000,"closed_at":1651740227000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Update the minimal PyArrow version warning (should've been part of #4250). ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4279","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4279","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4279.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4279.patch","merged_at":1651740227000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4278","id":1225122123,"node_id":"PR_kwDODunzps43RyTs","number":4278,"title":"Add missing features to openbookqa dataset for additional config","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Let's merge this PR as it is: it adds missing features.\r\n\r\nA subsequent PR may address the request on changing the data feature structure."],"created_at":1651656170000,"updated_at":1651842800000,"closed_at":1651842361000,"author_association":"MEMBER","active_lock_reason":null,"body":"Fix partially #4276.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4278","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4278","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4278.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4278.patch","merged_at":1651842361000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4277","id":1225002286,"node_id":"PR_kwDODunzps43RZV9","number":4277,"title":"Enable label alignment for token classification datasets","user":{"login":"lewtun","id":26859204,"node_id":"MDQ6VXNlcjI2ODU5MjA0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/26859204?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lewtun","html_url":"https:\/\/github.com\/lewtun","followers_url":"https:\/\/api.github.com\/users\/lewtun\/followers","following_url":"https:\/\/api.github.com\/users\/lewtun\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lewtun\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lewtun\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lewtun\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lewtun\/orgs","repos_url":"https:\/\/api.github.com\/users\/lewtun\/repos","events_url":"https:\/\/api.github.com\/users\/lewtun\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lewtun\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Hmm, not sure why the Windows tests are failing with:\r\n\r\n```\r\nDid not find path entry C:\\tools\\miniconda3\\bin\r\nC:\\tools\\miniconda3\\envs\\py37\\python.exe: No module named pytest\r\n```\r\n\r\nEdit: running the CI again fixed the problem \ud83d\ude43 ","> One last nit and we can merge then\r\n\r\nThanks, done!"],"created_at":1651648516000,"updated_at":1651851735000,"closed_at":1651851391000,"author_association":"MEMBER","active_lock_reason":null,"body":"This PR extends the `Dataset.align_labels_with_mapping()` method to support alignment of label mappings between datasets and models for token classification (e.g. NER).\r\n\r\nExample of usage:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nner_ds = load_dataset(\"conll2003\", split=\"train\")\r\n# returns [3, 0, 7, 0, 0, 0, 7, 0, 0]\r\nner_ds[0][\"ner_tags\"]\r\n# hypothetical model mapping with O <--> B-LOC\r\nlabel2id = {\r\n    \"B-LOC\": \"0\",\r\n    \"B-MISC\": \"7\",\r\n    \"B-ORG\": \"3\",\r\n    \"B-PER\": \"1\",\r\n    \"I-LOC\": \"6\",\r\n    \"I-MISC\": \"8\",\r\n    \"I-ORG\": \"4\",\r\n    \"I-PER\": \"2\",\r\n    \"O\": \"5\"\r\n  }\r\nner_aligned_ds = ner_ds.align_labels_with_mapping(label2id, \"ner_tags\")\r\n# returns [3, 5, 7, 5, 5, 5, 7, 5, 5]\r\nner_aligned_ds[0][\"ner_tags\"]\r\n```\r\n\r\nContext: we need this in AutoTrain to automatically align datasets \/ models during evaluation. cc @abhishekkrthakur ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4277","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4277","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4277.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4277.patch","merged_at":1651851391000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4276","id":1224949252,"node_id":"I_kwDODunzps5JAz4E","number":4276,"title":"OpenBookQA has missing and inconsistent field names","user":{"login":"vblagoje","id":458335,"node_id":"MDQ6VXNlcjQ1ODMzNQ==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/458335?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/vblagoje","html_url":"https:\/\/github.com\/vblagoje","followers_url":"https:\/\/api.github.com\/users\/vblagoje\/followers","following_url":"https:\/\/api.github.com\/users\/vblagoje\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/vblagoje\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/vblagoje\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/vblagoje\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/vblagoje\/orgs","repos_url":"https:\/\/api.github.com\/users\/vblagoje\/repos","events_url":"https:\/\/api.github.com\/users\/vblagoje\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/vblagoje\/received_events","type":"User","site_admin":false},"labels":[{"id":2067388877,"node_id":"MDU6TGFiZWwyMDY3Mzg4ODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20bug","name":"dataset bug","color":"2edb81","default":false,"description":"A bug in a dataset script provided in the library"}],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Thanks for reporting, @vblagoje.\r\n\r\nIndeed, I noticed some of these issues while reviewing this PR:\r\n- #4259 \r\n\r\nThis is in my TODO list. ","Ok, awesome @albertvillanova How about #4275 ?","On the other hand, I am not sure if we should always preserve the original nested structure. I think we should also consider other factors as convenience or consistency.\r\n\r\nFor example, other datasets also flatten \"question.stem\" into \"question\":\r\n- ai2_arc:\r\n  ```python\r\n  question = data[\"question\"][\"stem\"]\r\n  choices = data[\"question\"][\"choices\"]\r\n  text_choices = [choice[\"text\"] for choice in choices]\r\n  label_choices = [choice[\"label\"] for choice in choices]\r\n  yield id_, {\r\n      \"id\": id_,\r\n      \"answerKey\": answerkey,\r\n      \"question\": question,\r\n      \"choices\": {\"text\": text_choices, \"label\": label_choices},\r\n  }\r\n  ```\r\n- commonsense_qa:\r\n  ```python\r\n  question = data[\"question\"]\r\n  stem = question[\"stem\"]\r\n  yield id_, {\r\n      \"answerKey\": answerkey,\r\n      \"question\": stem,\r\n      \"choices\": {\"label\": labels, \"text\": texts},\r\n  }\r\n  ```\r\n- cos_e:\r\n  ```python\r\n  \"question\": cqa[\"question\"][\"stem\"],\r\n  ```\r\n- qasc\r\n- quartz\r\n- wiqa\r\n\r\nExceptions:\r\n- exams\r\n\r\nI think we should agree on a CONVENIENT format for QA and use always CONSISTENTLY the same.","@albertvillanova I agree that we should be consistent. In the last month, I have come across tons of code that deals with OpenBookQA and CommonSenseQA and all of that code relies on the original data format structure. We can't expect users to adopt HF Datasets if we arbitrarily change the structure of the format just because we think something makes more sense. I am in that position now (downloading original data rather than using HF Datasets) and undoubtedly it hinders HF Datasets' widespread use and adoption. Missing fields like in the case of #4275 is definitely bad and not even up for a discussion IMHO! cc @lhoestq ","I'm opening a PR that adds the missing fields.\r\n\r\nLet's agree on the feature structure: @lhoestq @mariosasko @polinaeterna ","IMO we should always try to preserve the original structure unless there is a good reason not to (and I don't see one in this case).","I agree with @mariosasko . The transition to the original format could be done in one PR for the next minor release, clearly documenting all dataset changes just as @albertvillanova outlined them above and perhaps even providing a per dataset util method to convert the new valid format to the old for backward compatibility. Users who relied on the old format will update their code with either the util method for a quick fix or slightly more elaborate for the new. ","I don't have a strong opinion on this, besides the fact that whatever decision we agree on, should be applied to all datasets.\r\n\r\nThere is always the tension between:\r\n- preserving each dataset original structure (which has the advantage of not forcing users to learn other structure for the same dataset),\r\n-  and on the other hand performing some king of standardization\/harmonization depending on the task (this has the advantage that once learnt, the same structure applies to all datasets; this has been done for e.g.  POS tagging: all datasets have been adapted to a certain \"standard\" structure).\r\n   - Another advantage: datasets can easily be interchanged (or joined) to be used by the same model\r\n\r\nRecently, in the BigScience BioMedical hackathon, they adopted a different approach:\r\n- they implement a \"source\" config, respecting the original structure as much as possible\r\n- they implement additional config for each task, with a \"standard\" nested structure per task, which is most useful for users."],"created_at":1651643512000,"updated_at":1651842081000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"## Describe the bug\r\nOpenBookQA implementation is inconsistent with the original dataset.\r\n\r\nWe need to:\r\n\r\n1. The dataset field [question][stem] is flattened into question_stem. Unflatten it to match the original format.\r\n2. Add missing additional fields:\r\n    - 'fact1': row['fact1'],\r\n    - 'humanScore': row['humanScore'],\r\n    - 'clarity': row['clarity'],\r\n    - 'turkIdAnonymized': row['turkIdAnonymized']\r\n3. Ensure the structure and every data item in the original OpenBookQA matches our OpenBookQA version.\r\n\r\n## Expected results\r\nThe structure and every data item in the original OpenBookQA matches our OpenBookQA version.\r\n\r\n## Actual results\r\nTBD\r\n\r\n## Environment info\r\n- `datasets` version: 2.1.0\r\n- Platform: macOS-10.15.7-x86_64-i386-64bit\r\n- Python version: 3.8.13\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4275","id":1224943414,"node_id":"I_kwDODunzps5JAyc2","number":4275,"title":"CommonSenseQA has missing and inconsistent field names","user":{"login":"vblagoje","id":458335,"node_id":"MDQ6VXNlcjQ1ODMzNQ==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/458335?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/vblagoje","html_url":"https:\/\/github.com\/vblagoje","followers_url":"https:\/\/api.github.com\/users\/vblagoje\/followers","following_url":"https:\/\/api.github.com\/users\/vblagoje\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/vblagoje\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/vblagoje\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/vblagoje\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/vblagoje\/orgs","repos_url":"https:\/\/api.github.com\/users\/vblagoje\/repos","events_url":"https:\/\/api.github.com\/users\/vblagoje\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/vblagoje\/received_events","type":"User","site_admin":false},"labels":[{"id":2067388877,"node_id":"MDU6TGFiZWwyMDY3Mzg4ODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20bug","name":"dataset bug","color":"2edb81","default":false,"description":"A bug in a dataset script provided in the library"}],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Thanks for reporting, @vblagoje.\r\n\r\nI'm opening a PR to address this. "],"created_at":1651642739000,"updated_at":1651664478000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"## Describe the bug\r\nIn short, CommonSenseQA implementation is inconsistent with the original dataset.\r\n\r\nMore precisely, we need to:\r\n\r\n1. Add the dataset matching \"id\" field. The current dataset, instead, regenerates monotonically increasing id.  \r\n2. The [\u201cquestion\u201d][\u201cstem\u201d] field is flattened into \"question\". We should match the original dataset and unflatten it\r\n3. Add the missing \"question_concept\" field in the question tree node\r\n4. Anything else? Go over the data structure of the newly repaired CommonSenseQA and make sure it matches the original\r\n\r\n## Expected results\r\nEvery data item of the CommonSenseQA should structurally and data-wise match the original CommonSenseQA dataset.\r\n\r\n## Actual results\r\nTBD\r\n\r\n## Environment info\r\n- `datasets` version: 2.1.0\r\n- Platform: macOS-10.15.7-x86_64-i386-64bit\r\n- Python version: 3.8.13\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4274","id":1224740303,"node_id":"PR_kwDODunzps43Qm2w","number":4274,"title":"Add API code examples for IterableDataset","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651617857000,"updated_at":1651681772000,"closed_at":1651681324000,"author_association":"MEMBER","active_lock_reason":null,"body":"This PR adds API code examples for `IterableDataset` and `IterableDatasetDicts`.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4274","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4274","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4274.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4274.patch","merged_at":1651681324000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4273","id":1224681036,"node_id":"PR_kwDODunzps43QaA6","number":4273,"title":"leadboard info added for TNE","user":{"login":"yanaiela","id":8031035,"node_id":"MDQ6VXNlcjgwMzEwMzU=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8031035?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yanaiela","html_url":"https:\/\/github.com\/yanaiela","followers_url":"https:\/\/api.github.com\/users\/yanaiela\/followers","following_url":"https:\/\/api.github.com\/users\/yanaiela\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yanaiela\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yanaiela\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yanaiela\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yanaiela\/orgs","repos_url":"https:\/\/api.github.com\/users\/yanaiela\/repos","events_url":"https:\/\/api.github.com\/users\/yanaiela\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yanaiela\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651613741000,"updated_at":1651757124000,"closed_at":1651756693000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4273","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4273","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4273.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4273.patch","merged_at":1651756693000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4272","id":1224635660,"node_id":"PR_kwDODunzps43QQQt","number":4272,"title":"Fix typo in logging docs","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","> This PR fixes #4271.\r\n\r\nThings have not changed when searching \"tqdm\" in the Dataset document. The second result still performs as \"Enable\".","Hi @jiangwy99, the fix will appear on the `main` version of the docs:\r\n\r\n![Screen Shot 2022-05-04 at 8 38 29 AM](https:\/\/user-images.githubusercontent.com\/59462357\/166718225-6848ab91-87d1-4572-9912-40a909af6cb9.png)\r\n","Fixed now, thanks."],"created_at":1651610877000,"updated_at":1651678947000,"closed_at":1651647516000,"author_association":"MEMBER","active_lock_reason":null,"body":"This PR fixes #4271.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4272","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4272","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4272.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4272.patch","merged_at":1651647515000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4271","id":1224404403,"node_id":"I_kwDODunzps5I-u2z","number":4271,"title":"A typo in docs of datasets.disable_progress_bar","user":{"login":"jiangwy99","id":39762734,"node_id":"MDQ6VXNlcjM5NzYyNzM0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/39762734?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jiangwy99","html_url":"https:\/\/github.com\/jiangwy99","followers_url":"https:\/\/api.github.com\/users\/jiangwy99\/followers","following_url":"https:\/\/api.github.com\/users\/jiangwy99\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jiangwy99\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jiangwy99\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jiangwy99\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jiangwy99\/orgs","repos_url":"https:\/\/api.github.com\/users\/jiangwy99\/repos","events_url":"https:\/\/api.github.com\/users\/jiangwy99\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jiangwy99\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"assignees":[{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi! Thanks for catching and reporting the typo, a PR has been opened to fix it :)"],"created_at":1651599896000,"updated_at":1651647515000,"closed_at":1651647515000,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\nin the docs of V2.1.0 datasets.disable_progress_bar, we should replace \"enable\" with \"disable\".","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4270","id":1224244460,"node_id":"PR_kwDODunzps43PC5V","number":4270,"title":"Fix style in openbookqa dataset","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651591294000,"updated_at":1651826286000,"closed_at":1651594852000,"author_association":"MEMBER","active_lock_reason":null,"body":"CI in PR:\r\n- #4259 \r\n\r\nwas green, but after merging it to master, a code quality error appeared.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4270","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4270","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4270.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4270.patch","merged_at":1651594852000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4269","id":1223865145,"node_id":"PR_kwDODunzps43Nzwh","number":4269,"title":"Add license and point of contact to big_patent dataset","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651569847000,"updated_at":1651826289000,"closed_at":1651576579000,"author_association":"MEMBER","active_lock_reason":null,"body":"Update metadata of big_patent dataset with:\r\n- license\r\n- point of contact","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4269","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4269","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4269.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4269.patch","merged_at":1651576579000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4268","id":1223331964,"node_id":"I_kwDODunzps5I6pB8","number":4268,"title":"error downloading bigscience-catalogue-lm-data\/lm_en_wiktionary_filtered","user":{"login":"i-am-neo","id":102043285,"node_id":"U_kgDOBhUOlQ","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/102043285?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/i-am-neo","html_url":"https:\/\/github.com\/i-am-neo","followers_url":"https:\/\/api.github.com\/users\/i-am-neo\/followers","following_url":"https:\/\/api.github.com\/users\/i-am-neo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/i-am-neo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/i-am-neo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/i-am-neo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/i-am-neo\/orgs","repos_url":"https:\/\/api.github.com\/users\/i-am-neo\/repos","events_url":"https:\/\/api.github.com\/users\/i-am-neo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/i-am-neo\/received_events","type":"User","site_admin":false},"labels":[{"id":2067388877,"node_id":"MDU6TGFiZWwyMDY3Mzg4ODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20bug","name":"dataset bug","color":"2edb81","default":false,"description":"A bug in a dataset script provided in the library"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["It would help a lot to be able to preview the dataset - I'd like to see if the pronunciations are in the dataset, eg. for [\"word\"](https:\/\/en.wiktionary.org\/wiki\/word),\r\n\r\nPronunciation\r\n([Received Pronunciation](https:\/\/en.wikipedia.org\/wiki\/Received_Pronunciation)) [IPA](https:\/\/en.wiktionary.org\/wiki\/Wiktionary:International_Phonetic_Alphabet)([key](https:\/\/en.wiktionary.org\/wiki\/Appendix:English_pronunciation)): \/w\u025c\u02d0d\/\r\n([General American](https:\/\/en.wikipedia.org\/wiki\/General_American)) [enPR](https:\/\/en.wiktionary.org\/wiki\/Appendix:English_pronunciation): w\u00fbrd, [IPA](https:\/\/en.wiktionary.org\/wiki\/Wiktionary:International_Phonetic_Alphabet)([key](https:\/\/en.wiktionary.org\/wiki\/Appendix:English_pronunciation)): \/w\u025dd\/","Hi @i-am-neo, thanks for reporting.\r\n\r\nNormally this dataset should be private and not accessible for public use. @cakiki, @lvwerra, any reason why is it public? I see many other Wikimedia datasets are also public.\r\n\r\nAlso note that last commit \"Add metadata\" (https:\/\/huggingface.co\/datasets\/bigscience-catalogue-lm-data\/lm_en_wiktionary_filtered\/commit\/dc2f458dab50e00f35c94efb3cd4009996858609) introduced buggy data files (`data\/file-01.jsonl.gz.lock`, `data\/file-01.jsonl.gz.lock.lock`). The same bug appears in other datasets as well.\r\n\r\n@i-am-neo, please note that in the near future we are planning to make public all datasets used for the BigScience project (at least all of them whose license allows to do that). Once public, they will be accessible for all the NLP community.","Ah this must be a bug introduced at creation time since the repos were created programmatically; I'll go ahead and make them private; sorry about that!","All datasets are private now. \r\n\r\nRe:that bug I think we're currently avoiding it by avoiding verifications. (i.e. `ignore_verifications=True`)","Thanks a lot, @cakiki.\r\n\r\n@i-am-neo, I'm closing this issue for now because the dataset is not publicly available yet. Just stay tuned, as we will soon release all the BigScience open-license datasets.  ","Thanks for letting me know, @albertvillanova @cakiki.\r\nAny chance of having a subset alpha version in the meantime? \r\nI only need two dicts out of wiktionary: 1) phoneme(as key): word, and 2) word(as key): its phonemes.\r\n\r\nWould like to use it for a mini-poc [Robust ASR](https:\/\/github.com\/huggingface\/transformers\/issues\/13162#issuecomment-1096881290) decoding, cc @patrickvonplaten. \r\n\r\n(Patrick, possible to email you so as not to litter github with comments? I have some observations after experiments training hubert on some YT AMI-like data (11.44% wer).  Also wonder if a robust ASR is on your\/HG's roadmap).  Thanks!","Hey @i-am-neo,\r\n\r\nCool to hear that you're working on Robust ASR! Feel free to drop me a mail :-)","@i-am-neo This particular subset of the dataset was taken from the [CirrusSearch dumps](https:\/\/dumps.wikimedia.org\/other\/cirrussearch\/current\/)\r\nYou're specifically after the [enwiktionary-20220425-cirrussearch-content.json.gz](https:\/\/dumps.wikimedia.org\/other\/cirrussearch\/current\/enwiktionary-20220425-cirrussearch-content.json.gz) file","thanks @cakiki !  <del>I could access the gz file yesterday (but neglected to tuck it away somewhere safe), and today the link is throwing a 404. Can you help? <\/del>  Never mind, got it!","thanks @patrickvonplaten.  will do - getting my observations together."],"created_at":1651523665000,"updated_at":1651852410000,"closed_at":1651577028000,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\nError generated when attempting to download dataset\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"bigscience-catalogue-lm-data\/lm_en_wiktionary_filtered\")\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\n```\r\nExpectedMoreDownloadedFiles               Traceback (most recent call last)\r\n\r\n[<ipython-input-62-4ac5cf959477>](https:\/\/localhost:8080\/#) in <module>()\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 dataset = load_dataset(\"bigscience-catalogue-lm-data\/lm_en_wiktionary_filtered\")\r\n\r\n3 frames\r\n\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/utils\/info_utils.py](https:\/\/localhost:8080\/#) in verify_checksums(expected_checksums, recorded_checksums, verification_name)\r\n     31         return\r\n     32     if len(set(expected_checksums) - set(recorded_checksums)) > 0:\r\n---> 33         raise ExpectedMoreDownloadedFiles(str(set(expected_checksums) - set(recorded_checksums)))\r\n     34     if len(set(recorded_checksums) - set(expected_checksums)) > 0:\r\n     35         raise UnexpectedDownloadedFile(str(set(recorded_checksums) - set(expected_checksums)))\r\n\r\nExpectedMoreDownloadedFiles: {'\/home\/leandro\/catalogue_data\/datasets\/lm_en_wiktionary_filtered\/data\/file-01.jsonl.gz', '\/home\/leandro\/catalogue_data\/datasets\/lm_en_wiktionary_filtered\/data\/file-01.jsonl.gz.lock'}\r\n```\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.3\r\n- Platform: Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- PyArrow version: 6.0.1\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4267","id":1223214275,"node_id":"PR_kwDODunzps43LzOR","number":4267,"title":"Replace data URL in SAMSum dataset within the same repository","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651516688000,"updated_at":1651826293000,"closed_at":1651518229000,"author_association":"MEMBER","active_lock_reason":null,"body":"Replace data URL with one in the same repository.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4267","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4267","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4267.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4267.patch","merged_at":1651518229000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4266","id":1223116436,"node_id":"PR_kwDODunzps43LeXK","number":4266,"title":"Add HF Speech Bench to Librispeech Dataset Card","user":{"login":"sanchit-gandhi","id":93869735,"node_id":"U_kgDOBZhWpw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/93869735?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sanchit-gandhi","html_url":"https:\/\/github.com\/sanchit-gandhi","followers_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/followers","following_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/orgs","repos_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/repos","events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651510771000,"updated_at":1651740440000,"closed_at":1651740009000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Adds the HF Speech Bench to Librispeech Dataset Card in place of the Papers With Code Leaderboard. Should improve usage and visibility of this leaderboard! Wondering whether this can also be done for [Common Voice 7](https:\/\/huggingface.co\/datasets\/mozilla-foundation\/common_voice_7_0) and [8](https:\/\/huggingface.co\/datasets\/mozilla-foundation\/common_voice_8_0) through someone with permissions? \r\n\r\ncc @patrickvonplaten: more leaderboard promotion!","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4266","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4266","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4266.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4266.patch","merged_at":1651740009000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4263","id":1222723083,"node_id":"PR_kwDODunzps43KLnD","number":4263,"title":"Rename imagenet2012 -> imagenet-1k","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["> Later we can add imagenet-21k as a new dataset if we want.\r\n\r\nisn't it what models refer to as `imagenet` already?","> isn't it what models refer to as imagenet already?\r\n\r\nI wasn't sure, but it looks like it indeed. Therefore having a dataset `imagenet` for ImageNet 21k makes sense actually.\r\n\r\nEDIT: actually not all `imagenet` tag refer to ImageNet 21k - we will need to correct some of them","_The documentation is not available anymore as the PR was closed or merged._","should we remove the repo mirror on the hub side or will you do it?"],"created_at":1651487181000,"updated_at":1651513846000,"closed_at":1651509177000,"author_association":"MEMBER","active_lock_reason":null,"body":"On the Hugging Face Hub, users refer to imagenet2012 (from #4178 ) as imagenet-1k in their model tags.\r\n\r\nTo correctly link models to imagenet, we should rename this dataset `imagenet-1k`.\r\n\r\nLater we can add `imagenet-21k` as a new dataset if we want.\r\n\r\nOnce this one is merged we can delete the `imagenet2012` dataset repository on the Hub.\r\n\r\nEDIT: to complete the rationale on why we should name it `imagenet-1k`:\r\nIf users specifically added the tag `imagenet-1k` , then it could be for two reasons (not sure which one is predominant), either they\r\n- wanted to make it explicit that it\u2019s not 21k -> the distinction is important for the community\r\n- or they have been following this convention from other models -> the convention implicitly exists already","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263\/reactions","total_count":3,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":1,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4263","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4263","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4263.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4263.patch","merged_at":1651509177000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4262","id":1222130749,"node_id":"PR_kwDODunzps43IOye","number":4262,"title":"Add YAML tags to Dataset Card rotten tomatoes","user":{"login":"mo6zes","id":10004251,"node_id":"MDQ6VXNlcjEwMDA0MjUx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10004251?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mo6zes","html_url":"https:\/\/github.com\/mo6zes","followers_url":"https:\/\/api.github.com\/users\/mo6zes\/followers","following_url":"https:\/\/api.github.com\/users\/mo6zes\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mo6zes\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mo6zes\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mo6zes\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mo6zes\/orgs","repos_url":"https:\/\/api.github.com\/users\/mo6zes\/repos","events_url":"https:\/\/api.github.com\/users\/mo6zes\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mo6zes\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651406348000,"updated_at":1651588053000,"closed_at":1651587635000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"The dataset card for the rotten tomatoes \/ MR movie review dataset had some missing YAML tags. Hopefully, this also improves the visibility of this dataset now that paperswithcode and huggingface link to eachother.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262\/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4262","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4262","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4262.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4262.patch","merged_at":1651587635000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4261","id":1221883779,"node_id":"I_kwDODunzps5I1HeD","number":4261,"title":"data leakage in `webis\/conclugen` dataset","user":{"login":"xflashxx","id":54585776,"node_id":"MDQ6VXNlcjU0NTg1Nzc2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/54585776?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/xflashxx","html_url":"https:\/\/github.com\/xflashxx","followers_url":"https:\/\/api.github.com\/users\/xflashxx\/followers","following_url":"https:\/\/api.github.com\/users\/xflashxx\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/xflashxx\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/xflashxx\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/xflashxx\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/xflashxx\/orgs","repos_url":"https:\/\/api.github.com\/users\/xflashxx\/repos","events_url":"https:\/\/api.github.com\/users\/xflashxx\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/xflashxx\/received_events","type":"User","site_admin":false},"labels":[{"id":2067388877,"node_id":"MDU6TGFiZWwyMDY3Mzg4ODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20bug","name":"dataset bug","color":"2edb81","default":false,"description":"A bug in a dataset script provided in the library"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi @xflashxx, thanks for reporting.\r\n\r\nPlease note that this dataset was generated and shared by Webis Group: https:\/\/huggingface.co\/webis\r\n\r\nWe are contacting the dataset owners to inform them about the issue you found. We'll keep you updated of their reply.","i'd suggest just pinging the authors here in the issue if possible?","Thanks for reporting this @xflashxx. I'll have a look and get back to you on this.","Hi @xflashxx and @albertvillanova,\r\n\r\nI have updated the files with de-duplicated splits. Apparently the debate portals from which part of the examples were sourced had unique timestamps for some examples (up to 6%; updated counts in the README) without any actual content updated that lead to \"new\" items. The length of `ids_validation` and `ids_testing` is zero.\r\n\r\nRegarding impact on scores:\r\n1. We employed automatic evaluation (on a separate set of 1000 examples) only to justify the exclusion of the smaller models for manual evaluation (due to budget constraints). I am confident the ranking still stands (unsurprisingly, the bigger models doing better than those trained on the smaller splits). We also highlight this in the paper. \r\n\r\n2. The examples used for manual evaluation have no overlap with any splits (also because they do not have any ground truth as we applied the trained models on an unlabeled sample to test its practical usage). I've added these two files to the dataset repository.\r\n\r\nHope this helps!","Thanks @shahbazsyed for your fast fix.\r\n\r\nAs a side note:\r\n- Your email appearing as Point of Contact in the dataset README has a typo: @uni.leipzig.de instead of @uni-leipzig.de\r\n- Your commits on the Hub are not linked to your profile on the Hub: this is because we use the email address to make this link; the email address used in your commit author and the email address set on your Hub account settings."],"created_at":1651340617000,"updated_at":1651557866000,"closed_at":1651557866000,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\nSome samples (argument-conclusion pairs) in the *training* split of the `webis\/conclugen` dataset are present in both the *validation* and *test* splits, creating data leakage and distorting model results.\r\nFurthermore, all splits contain duplicate samples.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ntraining = load_dataset(\"webis\/conclugen\", \"base\", split=\"train\")\r\nvalidation = load_dataset(\"webis\/conclugen\", \"base\", split=\"validation\")\r\ntesting = load_dataset(\"webis\/conclugen\", \"base\", split=\"test\")\r\n\r\n# collect which sample id's are present in the training split\r\nids_validation = list()\r\nids_testing = list()\r\n\r\nfor train_sample in training:\r\n    train_argument = train_sample[\"argument\"]\r\n    train_conclusion = train_sample[\"conclusion\"]\r\n    train_id = train_sample[\"id\"]\r\n    \r\n    # test if current sample is in validation split\r\n    if train_argument in validation[\"argument\"]:\r\n        for validation_sample in validation:\r\n            validation_argument = validation_sample[\"argument\"]\r\n            validation_conclusion = validation_sample[\"conclusion\"]\r\n            validation_id = validation_sample[\"id\"]\r\n            if train_argument == validation_argument and train_conclusion == validation_conclusion:\r\n                ids_validation.append(validation_id)\r\n    \r\n    # test if current sample is in test split\r\n    if train_argument in testing[\"argument\"]:\r\n        for testing_sample in testing:\r\n            testing_argument = testing_sample[\"argument\"]\r\n            testing_conclusion = testing_sample[\"conclusion\"]\r\n            testing_id = testing_sample[\"id\"]\r\n            if train_argument == testing_argument and train_conclusion == testing_conclusion:\r\n                ids_testing.append(testing_id)\r\n```\r\n\r\n## Expected results\r\nLength of both lists `ids_validation` and `ids_testing` should be zero.\r\n\r\n## Actual results\r\nLength of `ids_validation` = `2556`\r\nLength of `ids_testing` = `287`\r\n\r\nFurthermore, there seems to be duplicate samples in (at least) the *training* split, since:\r\n`print(len(set(ids_validation)))` = `950`\r\n`print(len(set(ids_testing)))` = `101`\r\n\r\nAll in all, around 7% of the samples of each the *validation* and *test* split seems to be present in the *training* split.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.4\r\n- Platform: macOS-12.3.1-arm64-arm-64bit\r\n- Python version: 3.9.10\r\n- PyArrow version: 7.0.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4260","id":1221830292,"node_id":"PR_kwDODunzps43HSfs","number":4260,"title":"Add mr_polarity movie review sentiment classification","user":{"login":"mo6zes","id":10004251,"node_id":"MDQ6VXNlcjEwMDA0MjUx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10004251?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mo6zes","html_url":"https:\/\/github.com\/mo6zes","followers_url":"https:\/\/api.github.com\/users\/mo6zes\/followers","following_url":"https:\/\/api.github.com\/users\/mo6zes\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mo6zes\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mo6zes\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mo6zes\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mo6zes\/orgs","repos_url":"https:\/\/api.github.com\/users\/mo6zes\/repos","events_url":"https:\/\/api.github.com\/users\/mo6zes\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mo6zes\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["whoops just found https:\/\/huggingface.co\/datasets\/rotten_tomatoes"],"created_at":1651324773000,"updated_at":1651328185000,"closed_at":1651328185000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Add the MR (Movie Review) dataset. The original dataset contains sentences from Rotten Tomatoes labeled as either \"positive\" or  \"negative\". \r\n\r\nHomepage: [https:\/\/www.cs.cornell.edu\/people\/pabo\/movie-review-data\/](https:\/\/www.cs.cornell.edu\/people\/pabo\/movie-review-data\/)\r\npaperswithcode: [https:\/\/paperswithcode.com\/dataset\/mr](https:\/\/paperswithcode.com\/dataset\/mr)\r\n\r\n- [ ] I was not able to generate dummy data, the original dataset files have \".pos\" and \".neg\" as file extensions so the auto-generator does not work. Is it fine like this or should dummy data be added?\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4260","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4260","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4260.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4260.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4259","id":1221768025,"node_id":"PR_kwDODunzps43HHGc","number":4259,"title":"Fix bug in choices labels in openbookqa dataset","user":{"login":"manandey","id":6687858,"node_id":"MDQ6VXNlcjY2ODc4NTg=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/6687858?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/manandey","html_url":"https:\/\/github.com\/manandey","followers_url":"https:\/\/api.github.com\/users\/manandey\/followers","following_url":"https:\/\/api.github.com\/users\/manandey\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/manandey\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/manandey\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/manandey\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/manandey\/orgs","repos_url":"https:\/\/api.github.com\/users\/manandey\/repos","events_url":"https:\/\/api.github.com\/users\/manandey\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/manandey\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651304499000,"updated_at":1651645891000,"closed_at":1651590861000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"This PR fixes the Bug in the openbookqa dataset as mentioned in this issue #3550.\r\n\r\nFix #3550.\r\n\r\ncc. @lhoestq @mariosasko ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4259","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4259","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4259.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4259.patch","merged_at":1651590861000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4258","id":1221637727,"node_id":"PR_kwDODunzps43Gstg","number":4258,"title":"Fix\/start token mask issue and update documentation","user":{"login":"TristanThrush","id":20826878,"node_id":"MDQ6VXNlcjIwODI2ODc4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/20826878?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/TristanThrush","html_url":"https:\/\/github.com\/TristanThrush","followers_url":"https:\/\/api.github.com\/users\/TristanThrush\/followers","following_url":"https:\/\/api.github.com\/users\/TristanThrush\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/TristanThrush\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/TristanThrush\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/TristanThrush\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/TristanThrush\/orgs","repos_url":"https:\/\/api.github.com\/users\/TristanThrush\/repos","events_url":"https:\/\/api.github.com\/users\/TristanThrush\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/TristanThrush\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","> Good catch ! Thanks :)\r\n> \r\n> Next time can you describe your fix in the Pull Request description please ?\r\n\r\nThanks. Also whoops, sorry about not being very descriptive. I updated the pull request description, and will keep this in mind for future PRs."],"created_at":1651272164000,"updated_at":1651509200000,"closed_at":1651508772000,"author_association":"MEMBER","active_lock_reason":null,"body":"This pr fixes a couple bugs:\r\n\r\n1) the perplexity was calculated with a 0 in the attention mask for the start token, which was causing high perplexity scores that were not correct\r\n2) the documentation was not updated","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4258","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4258","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4258.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4258.patch","merged_at":1651508772000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4257","id":1221393137,"node_id":"PR_kwDODunzps43GATC","number":4257,"title":"Create metric card for Mahalanobis Distance","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651257447000,"updated_at":1651503018000,"closed_at":1651502604000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"proposing a metric card to better explain how Mahalanobis distance works (last one for now :sweat_smile:","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4257","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4257","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4257.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4257.patch","merged_at":1651502604000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4256","id":1221379625,"node_id":"PR_kwDODunzps43F9Zw","number":4256,"title":"Create metric card for MSE","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651256482000,"updated_at":1651503342000,"closed_at":1651502927000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Proposing a metric card for Mean Squared Error","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4256","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4256","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4256.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4256.patch","merged_at":1651502927000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4255","id":1221142899,"node_id":"PR_kwDODunzps43FHgR","number":4255,"title":"No google drive URL for pubmed_qa","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","CI is failing because some sections are missing in the dataset card, but this is unrelated to this PR - Merging !"],"created_at":1651247746000,"updated_at":1651249495000,"closed_at":1651249136000,"author_association":"MEMBER","active_lock_reason":null,"body":"I hosted the data files in https:\/\/huggingface.co\/datasets\/pubmed_qa. This is allowed because the data is under the MIT license.\r\n\r\ncc @stas00 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4255","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4255","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4255.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4255.patch","merged_at":1651249136000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4254","id":1220204395,"node_id":"PR_kwDODunzps43Bwnj","number":4254,"title":"Replace data URL in SAMSum dataset and support streaming","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651220503000,"updated_at":1651826296000,"closed_at":1651249569000,"author_association":"MEMBER","active_lock_reason":null,"body":"This PR replaces data URL in SAMSum dataset:\r\n- original host (arxiv.org) does not allow HTTP Range requests\r\n- we have hosted the data on the Hub (license: CC BY-NC-ND 4.0)\r\n\r\nMoreover, it implements support for streaming.\r\n\r\nFix #4146.\r\nRelated to: #4236.\r\n\r\nCC: @severo ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4254","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4254","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4254.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4254.patch","merged_at":1651249568000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4253","id":1219286408,"node_id":"PR_kwDODunzps42-c8Q","number":4253,"title":"Create metric cards for mean IOU","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651179507000,"updated_at":1651254287000,"closed_at":1651253886000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Proposing a metric card for mIoU :rocket:\r\n\r\nsorry for spamming you with review requests, @albertvillanova ! :hugs: ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4253","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4253","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4253.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4253.patch","merged_at":1651253886000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4252","id":1219151100,"node_id":"PR_kwDODunzps429--I","number":4252,"title":"Creating metric card for MAE","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651172673000,"updated_at":1651251551000,"closed_at":1651251150000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Initial proposal for MAE metric card","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4252","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4252","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4252.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4252.patch","merged_at":1651251150000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4251","id":1219116354,"node_id":"PR_kwDODunzps4293dB","number":4251,"title":"Metric card for the XTREME-S dataset","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651170739000,"updated_at":1651250771000,"closed_at":1651250326000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Proposing a metric card for the XTREME-S dataset :hugs:","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4251","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4251","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4251.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4251.patch","merged_at":1651250326000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4250","id":1219093830,"node_id":"PR_kwDODunzps429yjN","number":4250,"title":"Bump PyArrow Version to 6","user":{"login":"dnaveenr","id":17746528,"node_id":"MDQ6VXNlcjE3NzQ2NTI4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17746528?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/dnaveenr","html_url":"https:\/\/github.com\/dnaveenr","followers_url":"https:\/\/api.github.com\/users\/dnaveenr\/followers","following_url":"https:\/\/api.github.com\/users\/dnaveenr\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/dnaveenr\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/dnaveenr\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/dnaveenr\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/dnaveenr\/orgs","repos_url":"https:\/\/api.github.com\/users\/dnaveenr\/repos","events_url":"https:\/\/api.github.com\/users\/dnaveenr\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/dnaveenr\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Updated meta.yaml as well. Thanks.","I'm OK with bumping PyArrow to version 6 to match the version in Colab, but maybe a better solution would be to stop using extension types in our codebase to avoid similar issues.","> but maybe a better solution would be to stop using extension types in our codebase to avoid similar issues.\r\n\r\nI agree, not much attention has been payed to extension arrays in the latest developments of Arrow anyway.\r\n\r\nLet's not use them more that what we do right now, and try to remove them at one point"],"created_at":1651169450000,"updated_at":1651657012000,"closed_at":1651656586000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fixes #4152 \r\n\r\nThis PR updates the PyArrow version to 6 in setup.py, CI job files .circleci\/config.yaml and .github\/workflows\/benchmarks.yaml files.\r\nThis will fix ArrayND error which exists in pyarrow 5.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4250","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4250","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4250.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4250.patch","merged_at":1651656586000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4249","id":1218524424,"node_id":"PR_kwDODunzps42742y","number":4249,"title":"Support streaming XGLUE dataset","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651141643000,"updated_at":1651826301000,"closed_at":1651162083000,"author_association":"MEMBER","active_lock_reason":null,"body":"Support streaming XGLUE dataset.\r\n\r\nFix #4247.\r\n\r\nCC: @severo ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4249","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4249","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4249.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4249.patch","merged_at":1651162083000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4248","id":1218460444,"node_id":"I_kwDODunzps5IoDsc","number":4248,"title":"conll2003 dataset loads original data.","user":{"login":"sue991","id":26458611,"node_id":"MDQ6VXNlcjI2NDU4NjEx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/26458611?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sue991","html_url":"https:\/\/github.com\/sue991","followers_url":"https:\/\/api.github.com\/users\/sue991\/followers","following_url":"https:\/\/api.github.com\/users\/sue991\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sue991\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sue991\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sue991\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sue991\/orgs","repos_url":"https:\/\/api.github.com\/users\/sue991\/repos","events_url":"https:\/\/api.github.com\/users\/sue991\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sue991\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Thanks for reporting @sue99.\r\n\r\nUnfortunately. I'm not able to reproduce your problem:\r\n```python\r\nIn [1]: import datasets\r\n   ...: from datasets import load_dataset\r\n   ...: dataset = load_dataset(\"conll2003\")\r\n\r\nIn [2]: dataset\r\nOut[2]: \r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\r\n        num_rows: 14042\r\n    })\r\n    validation: Dataset({\r\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\r\n        num_rows: 3251\r\n    })\r\n    test: Dataset({\r\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\r\n        num_rows: 3454\r\n    })\r\n})\r\n\r\nIn [3]: dataset[\"train\"][0]\r\nOut[3]: \r\n{'id': '0',\r\n 'tokens': ['EU',\r\n  'rejects',\r\n  'German',\r\n  'call',\r\n  'to',\r\n  'boycott',\r\n  'British',\r\n  'lamb',\r\n  '.'],\r\n 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\r\n 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\r\n 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\r\n```\r\n\r\nJust guessing: might be the case that you are calling `load_dataset` from a working directory that contains a local folder named `conll2003` (containing the raw data files)? If that is the case, `datasets` library gives precedence to the local folder over the dataset on the Hub. "],"created_at":1651138411000,"updated_at":1651163473000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\nI load `conll2003` dataset to use refined data like [this](https:\/\/huggingface.co\/datasets\/conll2003\/viewer\/conll2003\/train)  preview, but it is original data that contains `'-DOCSTART- -X- -X- O'` text.\r\n\r\nIs this a bug or should I use another dataset_name like `lhoestq\/conll2003` ?\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"conll2003\")\r\n```\r\n\r\n## Expected results\r\n{\r\n    \"chunk_tags\": [11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0],\r\n    \"id\": \"0\",\r\n    \"ner_tags\": [0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\r\n    \"pos_tags\": [12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7],\r\n    \"tokens\": [\"The\", \"European\", \"Commission\", \"said\", \"on\", \"Thursday\", \"it\", \"disagreed\", \"with\", \"German\", \"advice\", \"to\", \"consumers\", \"to\", \"shun\", \"British\", \"lamb\", \"until\", \"scientists\", \"determine\", \"whether\", \"mad\", \"cow\", \"disease\", \"can\", \"be\", \"transmitted\", \"to\", \"sheep\", \".\"]\r\n}\r\n\r\n## Actual results\r\n```python\r\nprint(dataset)\r\n\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['text'],\r\n        num_rows: 219554\r\n    })\r\n    test: Dataset({\r\n        features: ['text'],\r\n        num_rows: 50350\r\n    })\r\n    validation: Dataset({\r\n        features: ['text'],\r\n        num_rows: 55044\r\n    })\r\n})\r\n```\r\n\r\n```python\r\nfor i in range(20):\r\n    print(dataset['train'][i])\r\n\r\n{'text': '-DOCSTART- -X- -X- O'}\r\n{'text': ''}\r\n{'text': 'EU NNP B-NP B-ORG'}\r\n{'text': 'rejects VBZ B-VP O'}\r\n{'text': 'German JJ B-NP B-MISC'}\r\n{'text': 'call NN I-NP O'}\r\n{'text': 'to TO B-VP O'}\r\n{'text': 'boycott VB I-VP O'}\r\n{'text': 'British JJ B-NP B-MISC'}\r\n{'text': 'lamb NN I-NP O'}\r\n{'text': '. . O O'}\r\n{'text': ''}\r\n{'text': 'Peter NNP B-NP B-PER'}\r\n{'text': 'Blackburn NNP I-NP I-PER'}\r\n{'text': ''}\r\n{'text': 'BRUSSELS NNP B-NP B-LOC'}\r\n{'text': '1996-08-22 CD I-NP O'}\r\n{'text': ''}\r\n{'text': 'The DT B-NP O'}\r\n{'text': 'European NNP I-NP B-ORG'}\r\n```\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4247","id":1218320882,"node_id":"I_kwDODunzps5Inhny","number":4247,"title":"The data preview of XGLUE","user":{"login":"czq1999","id":49108847,"node_id":"MDQ6VXNlcjQ5MTA4ODQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/49108847?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/czq1999","html_url":"https:\/\/github.com\/czq1999","followers_url":"https:\/\/api.github.com\/users\/czq1999\/followers","following_url":"https:\/\/api.github.com\/users\/czq1999\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/czq1999\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/czq1999\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/czq1999\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/czq1999\/orgs","repos_url":"https:\/\/api.github.com\/users\/czq1999\/repos","events_url":"https:\/\/api.github.com\/users\/czq1999\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/czq1999\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["![image](https:\/\/user-images.githubusercontent.com\/49108847\/165700611-915b4343-766f-4b81-bdaa-b31950250f06.png)\r\n","Thanks for reporting @czq1999.\r\n\r\nNote that the dataset viewer uses the dataset in streaming mode and that not all datasets support streaming yet.\r\n\r\nThat is the case for XGLUE dataset (as the error message points out): this must be refactored to support streaming. ","Fixed, thanks @albertvillanova !\r\n\r\nhttps:\/\/huggingface.co\/datasets\/xglue\r\n\r\n<img width=\"824\" alt=\"Capture d\u2019e\u0301cran 2022-04-29 a\u0300 10 23 14\" src=\"https:\/\/user-images.githubusercontent.com\/1676121\/165909391-9f98d98a-665a-4e57-822d-8baa2dc9b7c9.png\">\r\n"],"created_at":1651131050000,"updated_at":1651220608000,"closed_at":1651162083000,"author_association":"NONE","active_lock_reason":null,"body":"It seems that something wrong with the data previvew of XGLUE","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4246","id":1218320293,"node_id":"PR_kwDODunzps427NiD","number":4246,"title":"Support to load dataset with TSV files by passing only dataset name","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651131015000,"updated_at":1651826308000,"closed_at":1651824847000,"author_association":"MEMBER","active_lock_reason":null,"body":"This PR implements support to load a dataset (w\/o script) containing TSV files by passing only the dataset name (no need to pass `sep='\\t'`):\r\n```python\r\nds = load_dataset(\"dataset\/name\")\r\n```\r\n\r\nThe refactoring allows for future builder kwargs customizations based on file extension.\r\n\r\nRelated to #4238.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4246","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4246","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4246.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4246.patch","merged_at":1651824847000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4245","id":1217959400,"node_id":"PR_kwDODunzps426AUR","number":4245,"title":"Add code examples for DatasetDict","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651099942000,"updated_at":1651256374000,"closed_at":1651255983000,"author_association":"MEMBER","active_lock_reason":null,"body":"This PR adds code examples for `DatasetDict` in the API reference :)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4245","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4245","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4245.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4245.patch","merged_at":1651255983000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4244","id":1217732221,"node_id":"PR_kwDODunzps425Po6","number":4244,"title":"task id update","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Reverted the multi-input-text-classification tag from task_categories and added it as task_ids @lhoestq ","_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651084094000,"updated_at":1651661033000,"closed_at":1651660597000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"changed multi input text classification as task id instead of category","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4244","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4244","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4244.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4244.patch","merged_at":1651660597000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4243","id":1217689909,"node_id":"PR_kwDODunzps425Gkn","number":4243,"title":"WIP: Initial shades loading script and readme","user":{"login":"shayne-longpre","id":69018523,"node_id":"MDQ6VXNlcjY5MDE4NTIz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/69018523?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/shayne-longpre","html_url":"https:\/\/github.com\/shayne-longpre","followers_url":"https:\/\/api.github.com\/users\/shayne-longpre\/followers","following_url":"https:\/\/api.github.com\/users\/shayne-longpre\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/shayne-longpre\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/shayne-longpre\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/shayne-longpre\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/shayne-longpre\/orgs","repos_url":"https:\/\/api.github.com\/users\/shayne-longpre\/repos","events_url":"https:\/\/api.github.com\/users\/shayne-longpre\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/shayne-longpre\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1651081543000,"updated_at":1651081543000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4243","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4243","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4243.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4243.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4242","id":1217665960,"node_id":"PR_kwDODunzps425BYf","number":4242,"title":"Update auth when mirroring datasets on the hub","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651080151000,"updated_at":1651081024000,"closed_at":1651080642000,"author_association":"MEMBER","active_lock_reason":null,"body":"We don't need to use extraHeaders anymore for rate limits anymore. Anyway extraHeaders was not working with git LFS because it was passing the wrong auth to S3.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4242","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4242","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4242.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4242.patch","merged_at":1651080642000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4241","id":1217423686,"node_id":"I_kwDODunzps5IkGlG","number":4241,"title":"NonMatchingChecksumError when attempting to download GLUE","user":{"login":"drussellmrichie","id":9650729,"node_id":"MDQ6VXNlcjk2NTA3Mjk=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/9650729?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/drussellmrichie","html_url":"https:\/\/github.com\/drussellmrichie","followers_url":"https:\/\/api.github.com\/users\/drussellmrichie\/followers","following_url":"https:\/\/api.github.com\/users\/drussellmrichie\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/drussellmrichie\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/drussellmrichie\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/drussellmrichie\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/drussellmrichie\/orgs","repos_url":"https:\/\/api.github.com\/users\/drussellmrichie\/repos","events_url":"https:\/\/api.github.com\/users\/drussellmrichie\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/drussellmrichie\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi :)\r\n\r\nI think your issue may be related to the older `nlp` library. I was able to download `glue` with the latest version of `datasets`. Can you try updating with:\r\n\r\n```py\r\npip install -U datasets\r\n```\r\n\r\nThen you can download:\r\n\r\n```py\r\nfrom datasets import load_dataset\r\nds = load_dataset(\"glue\", \"rte\")\r\n```","This appears to work. Thank you!\n\nOn Wed, Apr 27, 2022, 1:18 PM Steven Liu ***@***.***> wrote:\n\n> Hi :)\n>\n> I think your issue may be related to the older nlp library. I was able to\n> download glue with the latest version of datasets. Can you try updating\n> with:\n>\n> pip install -U datasets\n>\n> Then you can download:\n>\n> from datasets import load_datasetds = load_dataset(\"glue\", \"rte\")\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/huggingface\/datasets\/issues\/4241#issuecomment-1111267650>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/ACJUEKLUP2EL7ES3RRWJRPTVHFZHBANCNFSM5UPJBYXA>\n> .\n> You are receiving this because you authored the thread.Message ID:\n> ***@***.***>\n>\n"],"created_at":1651068861000,"updated_at":1651131927000,"closed_at":1651131927000,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\nI am trying to download the GLUE dataset from the NLP module but get an error (see below).\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport nlp\r\nnlp.__version__ # '0.2.0'\r\nnlp.load_dataset('glue', name=\"rte\", download_mode=\"force_redownload\")\r\n```\r\n\r\n## Expected results\r\nI expect the dataset to download without an error.\r\n\r\n## Actual results\r\n```\r\nINFO:nlp.load:Checking \/home\/richier\/.cache\/huggingface\/datasets\/5fe6ab0df8a32a3371b2e6a969d31d855a19563724fb0d0f163748c270c0ac60.2ea96febf19981fae5f13f0a43d4e2aa58bc619bc23acf06de66675f425a5538.py for additional imports.\r\nINFO:nlp.load:Found main folder for dataset https:\/\/s3.amazonaws.com\/datasets.huggingface.co\/nlp\/datasets\/glue\/glue.py at \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\r\nINFO:nlp.load:Found specific version folder for dataset https:\/\/s3.amazonaws.com\/datasets.huggingface.co\/nlp\/datasets\/glue\/glue.py at \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4\r\nINFO:nlp.load:Found script file from https:\/\/s3.amazonaws.com\/datasets.huggingface.co\/nlp\/datasets\/glue\/glue.py to \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4\/glue.py\r\nINFO:nlp.load:Found dataset infos file from https:\/\/s3.amazonaws.com\/datasets.huggingface.co\/nlp\/datasets\/glue\/dataset_infos.json to \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4\/dataset_infos.json\r\nINFO:nlp.load:Found metadata file for dataset https:\/\/s3.amazonaws.com\/datasets.huggingface.co\/nlp\/datasets\/glue\/glue.py at \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4\/glue.json\r\nINFO:nlp.info:Loading Dataset Infos from \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4\r\nINFO:nlp.builder:Generating dataset glue (\/home\/richier\/.cache\/huggingface\/datasets\/glue\/rte\/1.0.0)\r\nINFO:nlp.builder:Dataset not on Hf google storage. Downloading and preparing it from source\r\nINFO:nlp.utils.file_utils:Couldn't get ETag version for url https:\/\/firebasestorage.googleapis.com\/v0\/b\/mtl-sentence-representations.appspot.com\/o\/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb\r\nINFO:nlp.utils.file_utils:https:\/\/firebasestorage.googleapis.com\/v0\/b\/mtl-sentence-representations.appspot.com\/o\/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb not found in cache or force_download set to True, downloading to \/home\/richier\/.cache\/huggingface\/datasets\/downloads\/tmpldt3n805\r\nDownloading and preparing dataset glue\/rte (download: 680.81 KiB, generated: 1.83 MiB, total: 2.49 MiB) to \/home\/richier\/.cache\/huggingface\/datasets\/glue\/rte\/1.0.0...\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 73.0\/73.0 [00:00<00:00, 73.9kB\/s]\r\nINFO:nlp.utils.file_utils:storing https:\/\/firebasestorage.googleapis.com\/v0\/b\/mtl-sentence-representations.appspot.com\/o\/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb in cache at \/home\/richier\/.cache\/huggingface\/datasets\/downloads\/e8b62ee44e6f8b6aea761935928579ffe1aa55d161808c482e0725abbdcf9c64\r\nINFO:nlp.utils.file_utils:creating metadata file for \/home\/richier\/.cache\/huggingface\/datasets\/downloads\/e8b62ee44e6f8b6aea761935928579ffe1aa55d161808c482e0725abbdcf9c64\r\n---------------------------------------------------------------------------\r\nNonMatchingChecksumError                  Traceback (most recent call last)\r\n<ipython-input-7-669a8343dcc1> in <module>\r\n----> 1 nlp.load_dataset('glue', name=\"rte\", download_mode=\"force_redownload\")\r\n\r\n~\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    518         download_mode=download_mode,\r\n    519         ignore_verifications=ignore_verifications,\r\n--> 520         save_infos=save_infos,\r\n    521     )\r\n    522 \r\n\r\n~\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\r\n    418                 verify_infos = not save_infos and not ignore_verifications\r\n    419                 self._download_and_prepare(\r\n--> 420                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    421                 )\r\n    422                 # Sync info\r\n\r\n~\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    458         # Checksums verification\r\n    459         if verify_infos:\r\n--> 460             verify_checksums(self.info.download_checksums, dl_manager.get_recorded_sizes_checksums())\r\n    461         for split_generator in split_generators:\r\n    462             if str(split_generator.split_info.name).lower() == \"all\":\r\n\r\n~\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/utils\/info_utils.py in verify_checksums(expected_checksums, recorded_checksums)\r\n     34     bad_urls = [url for url in expected_checksums if expected_checksums[url] != recorded_checksums[url]]\r\n     35     if len(bad_urls) > 0:\r\n---> 36         raise NonMatchingChecksumError(str(bad_urls))\r\n     37     logger.info(\"All the checksums matched successfully.\")\r\n     38 \r\n\r\nNonMatchingChecksumError: ['https:\/\/firebasestorage.googleapis.com\/v0\/b\/mtl-sentence-representations.appspot.com\/o\/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb']\r\n```\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0\r\n- Platform: Linux-4.18.0-348.20.1.el8_5.x86_64-x86_64-with-redhat-8.5-Ootpa\r\n- Python version: 3.6.13\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.1.5\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4240","id":1217287594,"node_id":"PR_kwDODunzps423xRl","number":4240,"title":"Fix yield for crd3","user":{"login":"shanyas10","id":21066979,"node_id":"MDQ6VXNlcjIxMDY2OTc5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/21066979?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/shanyas10","html_url":"https:\/\/github.com\/shanyas10","followers_url":"https:\/\/api.github.com\/users\/shanyas10\/followers","following_url":"https:\/\/api.github.com\/users\/shanyas10\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/shanyas10\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/shanyas10\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/shanyas10\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/shanyas10\/orgs","repos_url":"https:\/\/api.github.com\/users\/shanyas10\/repos","events_url":"https:\/\/api.github.com\/users\/shanyas10\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/shanyas10\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["I don't think you need to generate new dummy data, since they're in the same format as the original data.\r\n\r\nThe CI is failing because of this error:\r\n```python\r\n>                       turn[\"names\"] = turn[\"NAMES\"]\r\nE                       TypeError: tuple indices must be integers or slices, not str\r\n```\r\n\r\nDo you know what could cause this ? If I understand correctly, `turn` is supposed to be a list of dictionaries right ?","> ```  \r\n>   \r\n> Do you know what could cause this ? If I understand correctly, turn is supposed to be a list of dictionaries right ?\r\n> ```\r\n\r\nThis is strange. Let me look into this. As per https:\/\/github.com\/RevanthRameshkumar\/CRD3\/blob\/master\/data\/aligned%20data\/c%3D2\/C1E001_2_0.json TURNS is a list of dictionaries. So when we iterate over `row[\"TURNS]\"` each `turn` is essentially a dictionary. Not sure why it's being considered a tuple here."],"created_at":1651062696000,"updated_at":1651236101000,"closed_at":1651236101000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Modified the `_generate_examples` function to consider all the turns for a chunk id as a single example\r\nModified the features accordingly\r\n\r\n```\r\n\"turns\": [\r\n                        {\r\n                            \"names\": datasets.features.Sequence(datasets.Value(\"string\")),\r\n                            \"utterances\": datasets.features.Sequence(datasets.Value(\"string\")),\r\n                            \"number\": datasets.Value(\"int32\"),\r\n                        }\r\n                    ],\r\n                }\r\n```\r\n\r\nI wasn't able to run `datasets-cli dummy_data datasets` command. Is there a workaround for this? ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4240","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4240","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4240.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4240.patch","merged_at":1651236101000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4239","id":1217269689,"node_id":"PR_kwDODunzps423tZr","number":4239,"title":"Small fixes in ROC AUC docs","user":{"login":"wschella","id":9478856,"node_id":"MDQ6VXNlcjk0Nzg4NTY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/9478856?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/wschella","html_url":"https:\/\/github.com\/wschella","followers_url":"https:\/\/api.github.com\/users\/wschella\/followers","following_url":"https:\/\/api.github.com\/users\/wschella\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/wschella\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/wschella\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/wschella\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/wschella\/orgs","repos_url":"https:\/\/api.github.com\/users\/wschella\/repos","events_url":"https:\/\/api.github.com\/users\/wschella\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/wschella\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651061750000,"updated_at":1651498137000,"closed_at":1651497723000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"The list of use cases did not render on GitHub with the prepended spacing.\r\nAdditionally, some typo's we're fixed.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4239","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4239","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4239.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4239.patch","merged_at":1651497723000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4238","id":1217168123,"node_id":"I_kwDODunzps5IjIL7","number":4238,"title":"Dataset caching policy","user":{"login":"loretoparisi","id":163333,"node_id":"MDQ6VXNlcjE2MzMzMw==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/163333?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/loretoparisi","html_url":"https:\/\/github.com\/loretoparisi","followers_url":"https:\/\/api.github.com\/users\/loretoparisi\/followers","following_url":"https:\/\/api.github.com\/users\/loretoparisi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/loretoparisi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/loretoparisi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/loretoparisi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/loretoparisi\/orgs","repos_url":"https:\/\/api.github.com\/users\/loretoparisi\/repos","events_url":"https:\/\/api.github.com\/users\/loretoparisi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/loretoparisi\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi @loretoparisi, thanks for reporting.\r\n\r\nThere is an option to force the redownload of the data files (and thus not using previously download and cached data files): `load_dataset(..., download_mode=\"force_redownload\")`.\r\n\r\nPlease, let me know if this fixes your problem.\r\n\r\nI can confirm you that your dataset loads without any problem for me:\r\n```python\r\nIn [2]: ds = load_dataset(\"loretoparisi\/tatoeba-sentences\", data_files={\"train\": \"train.csv\", \"test\": \"test.csv\"}, delimiter=\"\\t\", column_names=['label', 'text'])\r\n\r\nIn [3]: ds\r\nOut[3]: \r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['label', 'text'],\r\n        num_rows: 8256449\r\n    })\r\n    test: Dataset({\r\n        features: ['label', 'text'],\r\n        num_rows: 2061204\r\n    })\r\n})\r\n``` ","@albertvillanova thank you, it seems it still does not work using:\r\n\r\n```python\r\nsentences = load_dataset(\r\n     \"loretoparisi\/tatoeba-sentences\",\r\n     data_files=data_files,\r\n     delimiter='\\t', \r\n     column_names=['label', 'text'],\r\n     download_mode=\"force_redownload\"\r\n)\r\n```\r\n[This](https:\/\/colab.research.google.com\/drive\/1EA6FWo5pHxU8rPHHRn24NlHqRPiOlPTr?usp=sharing) is my notebook!\r\n\r\nThe problem is that the download file's revision for `test.csv` is not correctly parsed\r\n\r\n![Schermata 2022-04-27 alle 18 09 41](https:\/\/user-images.githubusercontent.com\/163333\/165563507-0be53eb6-8f61-49b0-b959-306e59281de3.png)\r\n\r\nIf you download that file `test.csv` from the repo, the line `\\\\N` is not there anymore (it was there at the first file upload).\r\n\r\nMy impression is that the Apache Arrow file is still cached - so server side, despite of enabling a forced download. For what I can see I get those two arrow files, but I cannot grep the bad line (`\\\\N`) since are binary files:\r\n\r\n```\r\n!ls -l \/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-efeff8965c730a2c\/0.0.0\/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\r\n!ls -l \/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-efeff8965c730a2c\/0.0.0\/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\/csv-test.arrow\r\n!head \/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-efeff8965c730a2c\/0.0.0\/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\/dataset_info.json\r\n```\r\n","SOLVED! The problem was the with the file itself, using caching parameter helped indeed.\r\nThanks for helping!"],"created_at":1651056131000,"updated_at":1651076965000,"closed_at":1651076930000,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\nI cannot clean cache of my datasets files, despite I have updated the `csv` files on the repository [here](https:\/\/huggingface.co\/datasets\/loretoparisi\/tatoeba-sentences). The original file had a line with bad characters, causing the following error\r\n\r\n```\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/features\/features.py](https:\/\/localhost:8080\/#) in str2int(self, values)\r\n    852                 if value not in self._str2int:\r\n    853                     value = str(value).strip()\r\n--> 854                 output.append(self._str2int[str(value)])\r\n    855             else:\r\n    856                 # No names provided, try to integerize\r\n\r\nKeyError: '\\\\N'\r\n```\r\n\r\nThe file now is cleanup up, but I still get the error. This happens even if I inspect the local cached contents, and cleanup the files locally:\r\n\r\n```python\r\nfrom datasets import load_dataset_builder\r\ndataset_builder = load_dataset_builder(\"loretoparisi\/tatoeba-sentences\")\r\nprint(dataset_builder.cache_dir)\r\nprint(dataset_builder.info.features)\r\nprint(dataset_builder.info.splits)\r\n```\r\n\r\n```\r\nUsing custom data configuration loretoparisi--tatoeba-sentences-e59b8ad92f1bb8dd\r\n\/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-e59b8ad92f1bb8dd\/0.0.0\/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\r\nNone\r\nNone\r\n```\r\n\r\nand removing files located at `\/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-*`.\r\n Is there any remote file caching policy in place? If so, is it possibile to programmatically disable it? \r\nCurrently it seems that the file `test.csv` on the repo [here](https:\/\/huggingface.co\/datasets\/loretoparisi\/tatoeba-sentences\/blob\/main\/test.csv) is cached remotely. In fact I download locally the file from raw link, the file is up-to-date; but If I use it within `datasets` as shown above, it gives to me always the first revision of the file, not the last.\r\n\r\nThank you.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset,Features,Value,ClassLabel\r\n\r\nclass_names = [\"cmn\",\"deu\",\"rus\",\"fra\",\"eng\",\"jpn\",\"spa\",\"ita\",\"kor\",\"vie\",\"nld\",\"epo\",\"por\",\"tur\",\"heb\",\"hun\",\"ell\",\"ind\",\"ara\",\"arz\",\"fin\",\"bul\",\"yue\",\"swe\",\"ukr\",\"bel\",\"que\",\"ces\",\"swh\",\"nno\",\"wuu\",\"nob\",\"zsm\",\"est\",\"kat\",\"pol\",\"lat\",\"urd\",\"sqi\",\"isl\",\"fry\",\"afr\",\"ron\",\"fao\",\"san\",\"bre\",\"tat\",\"yid\",\"uig\",\"uzb\",\"srp\",\"qya\",\"dan\",\"pes\",\"slk\",\"eus\",\"cycl\",\"acm\",\"tgl\",\"lvs\",\"kaz\",\"hye\",\"hin\",\"lit\",\"ben\",\"cat\",\"bos\",\"hrv\",\"tha\",\"orv\",\"cha\",\"mon\",\"lzh\",\"scn\",\"gle\",\"mkd\",\"slv\",\"frm\",\"glg\",\"vol\",\"ain\",\"jbo\",\"tok\",\"ina\",\"nds\",\"mal\",\"tlh\",\"roh\",\"ltz\",\"oss\",\"ido\",\"gla\",\"mlt\",\"sco\",\"ast\",\"jav\",\"oci\",\"ile\",\"ota\",\"xal\",\"tel\",\"sjn\",\"nov\",\"khm\",\"tpi\",\"ang\",\"aze\",\"tgk\",\"tuk\",\"chv\",\"hsb\",\"dsb\",\"bod\",\"sme\",\"cym\",\"mri\",\"ksh\",\"kmr\",\"ewe\",\"kab\",\"ber\",\"tpw\",\"udm\",\"lld\",\"pms\",\"lad\",\"grn\",\"mlg\",\"xho\",\"pnb\",\"grc\",\"hat\",\"lao\",\"npi\",\"cor\",\"nah\",\"avk\",\"mar\",\"guj\",\"pan\",\"kir\",\"myv\",\"prg\",\"sux\",\"crs\",\"ckt\",\"bak\",\"zlm\",\"hil\",\"cbk\",\"chr\",\"nav\",\"lkt\",\"enm\",\"arq\",\"lin\",\"abk\",\"pcd\",\"rom\",\"gsw\",\"tam\",\"zul\",\"awa\",\"wln\",\"amh\",\"bar\",\"hbo\",\"mhr\",\"bho\",\"mrj\",\"ckb\",\"osx\",\"pfl\",\"mgm\",\"sna\",\"mah\",\"hau\",\"kan\",\"nog\",\"sin\",\"glv\",\"dng\",\"kal\",\"liv\",\"vro\",\"apc\",\"jdt\",\"fur\",\"che\",\"haw\",\"yor\",\"crh\",\"pdc\",\"ppl\",\"kin\",\"shs\",\"mnw\",\"tet\",\"sah\",\"kum\",\"ngt\",\"nya\",\"pus\",\"hif\",\"mya\",\"moh\",\"wol\",\"tir\",\"ton\",\"lzz\",\"oar\",\"lug\",\"brx\",\"non\",\"mww\",\"hak\",\"nlv\",\"ngu\",\"bua\",\"aym\",\"vec\",\"ibo\",\"tkl\",\"bam\",\"kha\",\"ceb\",\"lou\",\"fuc\",\"smo\",\"gag\",\"lfn\",\"arg\",\"umb\",\"tyv\",\"kjh\",\"oji\",\"cyo\",\"urh\",\"kzj\",\"pam\",\"srd\",\"lmo\",\"swg\",\"mdf\",\"gil\",\"snd\",\"tso\",\"sot\",\"zza\",\"tsn\",\"pau\",\"som\",\"egl\",\"ady\",\"asm\",\"ori\",\"dtp\",\"cho\",\"max\",\"kam\",\"niu\",\"sag\",\"ilo\",\"kaa\",\"fuv\",\"nch\",\"hoc\",\"iba\",\"gbm\",\"sun\",\"war\",\"mvv\",\"pap\",\"ary\",\"kxi\",\"csb\",\"pag\",\"cos\",\"rif\",\"kek\",\"krc\",\"aii\",\"ban\",\"ssw\",\"tvl\",\"mfe\",\"tah\",\"bvy\",\"bcl\",\"hnj\",\"nau\",\"nst\",\"afb\",\"quc\",\"min\",\"tmw\",\"mad\",\"bjn\",\"mai\",\"cjy\",\"got\",\"hsn\",\"gan\",\"tzl\",\"dws\",\"ldn\",\"afh\",\"sgs\",\"krl\",\"vep\",\"rue\",\"tly\",\"mic\",\"ext\",\"izh\",\"sma\",\"jam\",\"cmo\",\"mwl\",\"kpv\",\"koi\",\"bis\",\"ike\",\"run\",\"evn\",\"ryu\",\"mnc\",\"aoz\",\"otk\",\"kas\",\"aln\",\"akl\",\"yua\",\"shy\",\"fkv\",\"gos\",\"fij\",\"thv\",\"zgh\",\"gcf\",\"cay\",\"xmf\",\"tig\",\"div\",\"lij\",\"rap\",\"hrx\",\"cpi\",\"tts\",\"gaa\",\"tmr\",\"iii\",\"ltg\",\"bzt\",\"syc\",\"emx\",\"gom\",\"chg\",\"osp\",\"stq\",\"frr\",\"fro\",\"nys\",\"toi\",\"new\",\"phn\",\"jpa\",\"rel\",\"drt\",\"chn\",\"pli\",\"laa\",\"bal\",\"hdn\",\"hax\",\"mik\",\"ajp\",\"xqa\",\"pal\",\"crk\",\"mni\",\"lut\",\"ayl\",\"ood\",\"sdh\",\"ofs\",\"nus\",\"kiu\",\"diq\",\"qxq\",\"alt\",\"bfz\",\"klj\",\"mus\",\"srn\",\"guc\",\"lim\",\"zea\",\"shi\",\"mnr\",\"bom\",\"sat\",\"szl\"]\r\nfeatures = Features({ 'label': ClassLabel(names=class_names), 'text': Value('string')})\r\nnum_labels = features['label'].num_classes\r\ndata_files = { \"train\": \"train.csv\", \"test\": \"test.csv\" }\r\nsentences = load_dataset(\r\n     \"loretoparisi\/tatoeba-sentences\",\r\n     data_files=data_files,\r\n     delimiter='\\t', \r\n     column_names=['label', 'text'],\r\n)\r\n# You can make this part faster with num_proc=<some int>\r\nsentences = sentences.map(lambda ex: {\"label\" : features[\"label\"].str2int(ex[\"label\"]) if ex[\"label\"] is not None else None}, features=features)\r\nsentences = sentences.shuffle()\r\n```\r\n\r\n## Expected results\r\nProperly tokenize dataset file `test.csv` without issues.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n```\r\nDownloading data files: 100%\r\n2\/2 [00:16<00:00, 7.34s\/it]\r\nDownloading data: 100%\r\n391M\/391M [00:12<00:00, 36.6MB\/s]\r\nDownloading data: 100%\r\n92.4M\/92.4M [00:02<00:00, 40.0MB\/s]\r\nExtracting data files: 100%\r\n2\/2 [00:00<00:00, 47.66it\/s]\r\nDataset csv downloaded and prepared to \/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-efeff8965c730a2c\/0.0.0\/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\r\n100%\r\n2\/2 [00:00<00:00, 25.94it\/s]\r\n11%\r\n942339\/8256449 [01:55<13:11, 9245.85ex\/s]\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n[<ipython-input-3-6a9867fad8d6>](https:\/\/localhost:8080\/#) in <module>()\r\n     12 )\r\n     13 # You can make this part faster with num_proc=<some int>\r\n---> 14 sentences = sentences.map(lambda ex: {\"label\" : features[\"label\"].str2int(ex[\"label\"]) if ex[\"label\"] is not None else None}, features=features)\r\n     15 sentences = sentences.shuffle()\r\n\r\n10 frames\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/features\/features.py](https:\/\/localhost:8080\/#) in str2int(self, values)\r\n    852                 if value not in self._str2int:\r\n    853                     value = str(value).strip()\r\n--> 854                 output.append(self._str2int[str(value)])\r\n    855             else:\r\n    856                 # No names provided, try to integerize\r\n\r\nKeyError: '\\\\N'\r\n```\r\n\r\n## Environment info\r\n```\r\n- `datasets` version: 2.1.0\r\n- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.3.5\r\n- ```\r\n\r\n\r\n```\r\n- `transformers` version: 4.18.0\r\n- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- Huggingface_hub version: 0.5.1\r\n- PyTorch version (GPU?): 1.11.0+cu113 (True)\r\n- Tensorflow version (GPU?): 2.8.0 (True)\r\n- Flax version (CPU?\/GPU?\/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n- ```\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4237","id":1217121044,"node_id":"I_kwDODunzps5Ii8sU","number":4237,"title":"Common Voice 8 doesn't show datasets viewer","user":{"login":"patrickvonplaten","id":23423619,"node_id":"MDQ6VXNlcjIzNDIzNjE5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/23423619?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/patrickvonplaten","html_url":"https:\/\/github.com\/patrickvonplaten","followers_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/followers","following_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/orgs","repos_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/repos","events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/received_events","type":"User","site_admin":false},"labels":[{"id":3470211881,"node_id":"LA_kwDODunzps7O1zsp","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer","name":"dataset-viewer","color":"E5583E","default":false,"description":"Related to the dataset viewer on huggingface.co"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Thanks for reporting. I understand it's an error in the dataset script. To reproduce:\r\n\r\n```python\r\n>>> import datasets as ds\r\n>>> split_names = ds.get_dataset_split_names(\"mozilla-foundation\/common_voice_8_0\", use_auth_token=\"**********\")\r\nDownloading builder script: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.9k\/10.9k [00:00<00:00, 10.9MB\/s]\r\nDownloading extra modules: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.98k\/2.98k [00:00<00:00, 3.36MB\/s]\r\nDownloading extra modules: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 53.1k\/53.1k [00:00<00:00, 650kB\/s]\r\nNo config specified, defaulting to: common_voice\/en\r\nTraceback (most recent call last):\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/libs\/libmodels\/.venv\/lib\/python3.9\/site-packages\/datasets\/inspect.py\", line 280, in get_dataset_config_info\r\n    for split_generator in builder._split_generators(\r\n  File \"\/home\/slesage\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/mozilla-foundation--common_voice_8_0\/720589e6e5ad674019008b719053303a71716db1b27e63c9846df02fdf93f2f3\/common_voice_8_0.py\", line 153, in _split_generators\r\n    self._log_download(self.config.name, bundle_version, hf_auth_token)\r\n  File \"\/home\/slesage\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/mozilla-foundation--common_voice_8_0\/720589e6e5ad674019008b719053303a71716db1b27e63c9846df02fdf93f2f3\/common_voice_8_0.py\", line 139, in _log_download\r\n    email = HfApi().whoami(auth_token)[\"email\"]\r\nKeyError: 'email'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/libs\/libmodels\/.venv\/lib\/python3.9\/site-packages\/datasets\/inspect.py\", line 323, in get_dataset_split_names\r\n    info = get_dataset_config_info(\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/libs\/libmodels\/.venv\/lib\/python3.9\/site-packages\/datasets\/inspect.py\", line 285, in get_dataset_config_info\r\n    raise SplitsNotFoundError(\"The split names could not be parsed from the dataset config.\") from err\r\ndatasets.inspect.SplitsNotFoundError: The split names could not be parsed from the dataset config.\r\n```","Thanks for reporting @patrickvonplaten and thanks for the investigation @severo.\r\n\r\nUnfortunately I'm not able to reproduce the error.\r\n\r\nI think the error has to do with authentication with `huggingface_hub`, because the exception is thrown from these code lines: https:\/\/huggingface.co\/datasets\/mozilla-foundation\/common_voice_8_0\/blob\/main\/common_voice_8_0.py#L137-L139\r\n```python\r\nfrom huggingface_hub import HfApi, HfFolder\r\n\r\nif isinstance(auth_token, bool):\r\n    email = HfApi().whoami(auth_token)\r\nemail = HfApi().whoami(auth_token)[\"email\"]\r\n```\r\n\r\nCould you please verify the previous code with the `auth_token` you pass to `load_dataset(..., use_auth_token=auth_token,...`?","OK, thanks for digging a bit into it. Indeed, the error occurs with the dataset-viewer, but not with a normal user token, because we use an app token, and it does not have a related email!\r\n\r\n```python\r\n>>> from huggingface_hub import HfApi, HfFolder\r\n>>> auth_token = \"hf_app_******\"\r\n>>> t = HfApi().whoami(auth_token)\r\n>>> t\r\n{'type': 'app', 'name': 'dataset-preview-backend'}\r\n>>> t[\"email\"]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nKeyError: 'email'\r\n```\r\n\r\nNote also that the doc (https:\/\/huggingface.co\/docs\/huggingface_hub\/package_reference\/hf_api#huggingface_hub.HfApi.whoami) does not state that `whoami` should return an `email` key.\r\n\r\n@SBrandeis @julien-c: do you think the app token should have an email associated, like the users?","We can workaround this with\r\n```python\r\nemail = HfApi().whoami(auth_token).get(\"email\", \"system@huggingface.co\")\r\n```\r\nin the common voice scripts","Hmmm, does this mean that any person who downloads the common voice dataset will be logged as \"system@huggingface.co\"? If so, it would defeat the purpose of sending the user's email to the commonvoice API, right?","I agree with @severo: we cannot set our system email as default, allowing anybody not authenticated to by-pass the Common Voice usage policy.\r\n\r\nAdditionally, looking at the code, I think we should implement a more robust way to send user email to Common Voice: currently anybody can tweak the script and send somebody else email instead.\r\n\r\nCC: @patrickvonplaten @lhoestq @SBrandeis @julien-c ","Hmm I don't agree here. \r\n\r\nAnybody can always just bypass the system by setting whatever email. As soon as someone has access to the downloading script it's trivial to tweak the code to not send the \"correct\" email but to just whatever and it would work.\r\n\r\nNote that someone only has visibility on the code after having \"signed\" the access-mechanism so I think we can expect the users to have agreed to not do anything malicious. \r\n\r\nI'm fine with both @lhoestq's solution or we find a way that forces the user to be logged in + being able to load the data for the datasets viewer. Wdyt @lhoestq @severo @albertvillanova ?","> Additionally, looking at the code, I think we should implement a more robust way to send user email to Common Voice: currently anybody can tweak the script and send somebody else email instead.\r\n\r\nYes, I agree we can forget about this @patrickvonplaten. After having had a look at Common Voice website, I've seen they only require sending an email (no auth is inplace on their side, contrary to what I had previously thought). Therefore, currently we impose stronger requirements than them: we require the user having logged in and accepted the access mechanism.\r\n\r\nCurrently the script as it is already requires the user being logged in:\r\n```python\r\nHfApi().whoami(auth_token)\r\n```\r\nthrows an exception if None\/invalid auth_token is passed.\r\n\r\nOn the other hand, we should agree on the way to allow the viewer to stream the data."],"created_at":1651053920000,"updated_at":1651489642000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"https:\/\/huggingface.co\/datasets\/mozilla-foundation\/common_voice_8_0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4236","id":1217115691,"node_id":"PR_kwDODunzps423MOc","number":4236,"title":"Replace data URL in big_patent dataset and support streaming","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","I first uploaded the data files to the Hub: I think it is a good option because we have git lfs to track versions and changes. Moreover people will be able to make PRs to propose updates on the data files.\r\n- I would have preferred to upload it it to the \"data\" org namespace, but it is already taken (although not used): might be possible to take it?\r\n\r\nAs an alternative (and to be consistent with previous datasets), I also uploaded the data files to our AWS bucket.\r\n\r\nWe should decide which to use (now and for future datasets) and set it here before merging. We should remove the data files for the non-chosen option.\r\n\r\nCC: @lhoestq @mariosasko @polinaeterna ","Would it make sense to make the dataset a community one (so, create an organization for it) and store the script and the data in a single repository? Just as it is for most of the datasets. That way we can also access the data using a relative path inside the repo (that's not the point though). The point is that to me it seems a bit more straightforward to store everything in one place. \r\n\r\nI guess the strong argument against this logic is that in this case the canonical version won't work... But maybe there is some redirecting mechanism I don't know about? :)\r\n\r\nAnyway, I'm in favor of hosting data on the Hub instead of AWS :) ","I also think storing everything in one place\/single repository is the best option.\r\n\r\n@polinaeterna Canonical datasets also support data files (see the [`red_caps` repo](https:\/\/huggingface.co\/datasets\/red_caps\/tree\/main) for instance) ","Thanks @polinaeterna and @mariosasko for your comments.\r\n\r\nYes, definitely it is much better to have everything in the same repo. \r\n\r\nI'm transferring their data files to the Hub under \"big_patent\" and deleting them from the other repo and AWS."],"created_at":1651053673000,"updated_at":1651826314000,"closed_at":1651515675000,"author_association":"MEMBER","active_lock_reason":null,"body":"This PR replaces the Google Drive URL with by our Hub one, once the data owners have approved to host their data on the Hub.\r\n\r\nMoreover, this PR makes the dataset streamable.\r\n\r\nFix #4217.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4236","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4236","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4236.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4236.patch","merged_at":1651515675000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4235","id":1216952640,"node_id":"I_kwDODunzps5IiTlA","number":4235,"title":"How to load VERY LARGE dataset?","user":{"login":"CaoYiqingT","id":45160643,"node_id":"MDQ6VXNlcjQ1MTYwNjQz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/45160643?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/CaoYiqingT","html_url":"https:\/\/github.com\/CaoYiqingT","followers_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/followers","following_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/orgs","repos_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/repos","events_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The `Trainer` support `IterableDataset`, not just datasets."],"created_at":1651045813000,"updated_at":1651059857000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### System Info\n\n```shell\nI am using transformer trainer while meeting the issue.\r\nThe trainer requests torch.utils.data.Dataset as input, which loads the whole dataset into the memory at once. Therefore, when the dataset is too large to load, there's nothing I can do except using IterDataset, which loads samples of data seperately, and results in low efficiency. \r\nI wonder if there are any tricks like Sharding in huggingface trainer.\r\nLooking forward to your reply.\n```\n\n\n### Who can help?\n\nTrainer: @sgugger\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nNone\n\n### Expected behavior\n\n```shell\nI wonder if there are any tricks like fairseq Sharding very large datasets https:\/\/fairseq.readthedocs.io\/en\/latest\/getting_started.html.\r\nThanks a lot!\n```\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4234","id":1216818846,"node_id":"PR_kwDODunzps422Mwn","number":4234,"title":"Autoeval config","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Related to: https:\/\/github.com\/huggingface\/autonlp-backend\/issues\/414 and https:\/\/github.com\/huggingface\/autonlp-backend\/issues\/424","The tests are failing due to the changed metadata:\r\n\r\n```\r\ngot an unexpected keyword argument 'train-eval-index'\r\n```\r\n\r\nI think you can fix this by updating the `DatasetMetadata` class and implementing an appropriate `validate_train_eval_index()` function\r\n\r\n@lhoestq we are working with an arbitrary set of tags for `autoeval config`. See https:\/\/github.com\/huggingface\/autonlp-backend\/issues\/414\r\nI need to add a validator function though for the tests to pass. Our set is not well-defined as in the rest https:\/\/github.com\/huggingface\/datasets\/tree\/master\/src\/datasets\/utils\/resources. What's a workaround for this?","On the question of validating the `train-eval-index` metadata, I think the simplest approach would be to validate that the required fields exist and not worry about their values (which are open-ended).\r\n\r\nFor me, the required fields include:\r\n\r\n* `config`\r\n* `task`\r\n* `task_id`\r\n* `splits` (train \/ validation \/ eval)\r\n* `col_mapping`\r\n* `metrics` (checking that each one has `type`, `name`) \r\n\r\nHere I'm using the spec defined in https:\/\/github.com\/huggingface\/autonlp-backend\/issues\/414 as a guide.\r\n\r\nWDYT @lhoestq ?","Makes sense ! Currently the metadata type validator doesn't support subfields - let me open a PR to add it","I ended up improving the metadata validation in this PR x)\r\n\r\nIn particular:\r\n- I added support YAML keys with dashes instead of underscores for `train-eval-index`\r\n- I added `train-eval-index` validation  with `validate_train_eval_index`. It does nothing fancy, it just checks that it is a list if it exists in the YAML, but feel free to improve it if you want\r\n\r\nLet me know if it sounds good to you ! I think we can improve `validate_train_eval_index` in another PR","Come on windows... I didn't do anything advanced...\r\n\r\nAnyway, will try to fix this when I get back home x)","> Come on windows... I didn't do anything advanced...\r\n> \r\n> Anyway, will try to fix this when I get back home x)\r\n\r\nHehe, thanks!","Thanks, @lhoestq this is great! ","Did I just fix it for windows and now it fails on linux ? xD","> Did I just fix it for windows and now it fails on linux ? xD\r\n\r\nLooks like the Heisenberg uncertainty principle is at play here - you cannot simultaneously have unit tests passing in both Linux and Windows \ud83d\ude05 ","The worst is that the tests pass locally both on my windows and my linux x)","Ok fixed it, the issue came from python 3.6 that doesn't return the right `__origin__` for Dict and List types","> Alright thanks for adding the first Autoeval config ! :D\r\n\r\nWoohoo! Thank you so much \ud83e\udd17 ","This is cool!"],"created_at":1651037530000,"updated_at":1651843231000,"closed_at":1651774858000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Added autoeval config to imdb as pilot","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4234","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4234","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4234.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4234.patch","merged_at":1651774858000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4233","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4233\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4233\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4233\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4233","id":1216665044,"node_id":"PR_kwDODunzps421r-6","number":4233,"title":"Autoeval","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4233). All of your documentation changes will be reflected on that endpoint."],"created_at":1651023129000,"updated_at":1651037370000,"closed_at":1651023143000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4233\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4233\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4233","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4233","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4233.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4233.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4232","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4232\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4232\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4232\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4232","id":1216659444,"node_id":"PR_kwDODunzps421qz4","number":4232,"title":"adding new tag to tasks.json and modified for existing datasets","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","closing in favor of https:\/\/github.com\/huggingface\/datasets\/pull\/4244"],"created_at":1651022469000,"updated_at":1651587836000,"closed_at":1651587399000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4232\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4232\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4232","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4232","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4232.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4232.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4231","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4231\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4231\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4231\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4231","id":1216651960,"node_id":"PR_kwDODunzps421pUX","number":4231,"title":"Fix invalid url to CC-Aligned dataset","user":{"login":"juntang-zhuang","id":44451229,"node_id":"MDQ6VXNlcjQ0NDUxMjI5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/44451229?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/juntang-zhuang","html_url":"https:\/\/github.com\/juntang-zhuang","followers_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/followers","following_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/orgs","repos_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/repos","events_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1651021621000,"updated_at":1651021689000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"The CC-Aligned dataset url has changed to  https:\/\/data.statmt.org\/cc-aligned\/, the old address http:\/\/www.statmt.org\/cc-aligned\/ is no longer valid","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4231\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4231\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4231","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4231","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4231.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4231.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4230","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4230\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4230\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4230\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4230","id":1216643661,"node_id":"I_kwDODunzps5IhIJN","number":4230,"title":"Why the `conll2003` dataset on huggingface only contains the `en` subset? Where is the  German data?","user":{"login":"beyondguo","id":37113676,"node_id":"MDQ6VXNlcjM3MTEzNjc2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/37113676?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/beyondguo","html_url":"https:\/\/github.com\/beyondguo","followers_url":"https:\/\/api.github.com\/users\/beyondguo\/followers","following_url":"https:\/\/api.github.com\/users\/beyondguo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/beyondguo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/beyondguo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/beyondguo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/beyondguo\/orgs","repos_url":"https:\/\/api.github.com\/users\/beyondguo\/repos","events_url":"https:\/\/api.github.com\/users\/beyondguo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/beyondguo\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Thanks for reporting @beyondguo.\r\n\r\nIndeed, we generate this dataset from this raw data file URL: https:\/\/data.deepai.org\/conll2003.zip\r\nAnd that URL only contains the English version."],"created_at":1651020832000,"updated_at":1651164914000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"![image](https:\/\/user-images.githubusercontent.com\/37113676\/165416606-96b5db18-b16c-4b6b-928c-de8620fd943e.png)\r\n\r\nBut on huggingface datasets:\r\n![image](https:\/\/user-images.githubusercontent.com\/37113676\/165416649-8fd77980-ca0d-43f0-935e-f398ba8323a4.png)\r\n\r\nWhere is the  German data?","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4230\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4230\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4229","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4229\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4229\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4229\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4229","id":1216638968,"node_id":"PR_kwDODunzps421mjM","number":4229,"title":"new task tag","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1651020428000,"updated_at":1651020508000,"closed_at":1651020497000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"multi-input-text-classification tag for classification datasets that take more than one input","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4229\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4229\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4229","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4229","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4229.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4229.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4228","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4228\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4228\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4228\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4228","id":1216523043,"node_id":"PR_kwDODunzps421NKL","number":4228,"title":"new task tag","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1651010433000,"updated_at":1651020511000,"closed_at":1651020391000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"multi-input-text-classification tag for classification datasets that take more than one input","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4228\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4228\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4228","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4228","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4228.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4228.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4227","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4227\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4227\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4227\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4227","id":1216455316,"node_id":"PR_kwDODunzps420-mc","number":4227,"title":"Add f1 metric card, update docstring in py file","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651005663000,"updated_at":1651582223000,"closed_at":1651581813000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4227\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4227\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4227","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4227","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4227.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4227.patch","merged_at":1651581813000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4226","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4226\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4226\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4226\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4226","id":1216331073,"node_id":"PR_kwDODunzps420kAv","number":4226,"title":"Add pearsonr mc, update functionality to match the original docs","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","thank you @lhoestq!! :hugs: "],"created_at":1650997846000,"updated_at":1651597764000,"closed_at":1651597348000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"- adds pearsonr metric card\r\n- adds ability to return p-value\r\n    - p-value was mentioned in the original docs as a return value, but there was no option to return it. I updated the _compute function slightly to have an option to return the p-value.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4226\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4226\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4226","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4226","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4226.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4226.patch","merged_at":1651597348000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4225","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4225\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4225\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4225\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4225","id":1216213464,"node_id":"PR_kwDODunzps420LNM","number":4225,"title":"autoeval config","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1650991114000,"updated_at":1651020511000,"closed_at":1651010426000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"add train eval index for autoeval","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4225\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4225\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4225","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4225","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4225.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4225.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4224","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4224\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4224\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4224\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4224","id":1216209667,"node_id":"PR_kwDODunzps420KX2","number":4224,"title":"autoeval config","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1650990919000,"updated_at":1650991005000,"closed_at":1650991005000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"add train eval index for autoeval","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4224\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4224\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4224","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4224","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4224.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4224.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4223","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4223\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4223\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4223\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4223","id":1216107082,"node_id":"PR_kwDODunzps42z0YV","number":4223,"title":"Add Accuracy Metric Card","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650985846000,"updated_at":1651588065000,"closed_at":1651587647000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"- adds accuracy metric card\r\n- updates docstring in accuracy.py\r\n- adds .json file with metric card and docstring information","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4223\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4223\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4223","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4223","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4223.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4223.patch","merged_at":1651587647000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4222","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4222\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4222\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4222\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4222","id":1216056439,"node_id":"PR_kwDODunzps42zpcd","number":4222,"title":"Fix description links in dataset cards","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Non passing tests are due to other pre-existing errors in dataset cards: not related to this PR."],"created_at":1650983785000,"updated_at":1651826318000,"closed_at":1650991949000,"author_association":"MEMBER","active_lock_reason":null,"body":"I noticed many links were not properly displayed (only text, no link) on the Hub because of wrong syntax, e.g.: https:\/\/huggingface.co\/datasets\/big_patent\r\n\r\nThis PR fixes all description links in dataset cards.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4222\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4222\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4222","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4222","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4222.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4222.patch","merged_at":1650991949000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4221","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4221\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4221\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4221\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4221","id":1215911182,"node_id":"I_kwDODunzps5IeVUO","number":4221,"title":"Dictionary Feature","user":{"login":"jordiae","id":2944532,"node_id":"MDQ6VXNlcjI5NDQ1MzI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/2944532?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jordiae","html_url":"https:\/\/github.com\/jordiae","followers_url":"https:\/\/api.github.com\/users\/jordiae\/followers","following_url":"https:\/\/api.github.com\/users\/jordiae\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jordiae\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jordiae\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jordiae\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jordiae\/orgs","repos_url":"https:\/\/api.github.com\/users\/jordiae\/repos","events_url":"https:\/\/api.github.com\/users\/jordiae\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jordiae\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892912,"node_id":"MDU6TGFiZWwxOTM1ODkyOTEy","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/question","name":"question","color":"d876e3","default":true,"description":"Further information is requested"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi @jordiae,\r\n\r\nInstead of the `Sequence` feature, you can use just a regular list: put the dict between `[` and `]`:\r\n```python\r\n\"list_of_dict_feature\": [\r\n    {\r\n        \"key1_in_dict\": datasets.Value(\"string\"),\r\n        \"key2_in_dict\": datasets.Value(\"int32\"),\r\n        ...\r\n    }\r\n],\r\n```\r\n\r\nFeel free to re-open this issue if that does not work for your use case.","> Hi @jordiae,\r\n> \r\n> Instead of the `Sequence` feature, you can use just a regular list: put the dict between `[` and `]`:\r\n> \r\n> ```python\r\n> \"list_of_dict_feature\": [\r\n>     {\r\n>         \"key1_in_dict\": datasets.Value(\"string\"),\r\n>         \"key2_in_dict\": datasets.Value(\"int32\"),\r\n>         ...\r\n>     }\r\n> ],\r\n> ```\r\n> \r\n> Feel free to re-open this issue if that does not work for your use case.\r\n\r\nThank you"],"created_at":1650977418000,"updated_at":1651243939000,"closed_at":1651165498000,"author_association":"NONE","active_lock_reason":null,"body":"Hi, I'm trying to create the loading script for a dataset in which one feature is a list of dictionaries, which afaik doesn't fit very well the values and structures supported by Value and Sequence. Is there any suggested workaround, am I missing something?\r\n\r\nThank you in advance.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4221\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4221\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4220","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4220\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4220\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4220\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4220","id":1215225802,"node_id":"PR_kwDODunzps42w5YO","number":4220,"title":"Altered faiss installation comment","user":{"login":"vishalsrao","id":36671559,"node_id":"MDQ6VXNlcjM2NjcxNTU5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36671559?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/vishalsrao","html_url":"https:\/\/github.com\/vishalsrao","followers_url":"https:\/\/api.github.com\/users\/vishalsrao\/followers","following_url":"https:\/\/api.github.com\/users\/vishalsrao\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/vishalsrao\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/vishalsrao\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/vishalsrao\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/vishalsrao\/orgs","repos_url":"https:\/\/api.github.com\/users\/vishalsrao\/repos","events_url":"https:\/\/api.github.com\/users\/vishalsrao\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/vishalsrao\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4220). All of your documentation changes will be reflected on that endpoint.","Hi ! Can you explain why this change is needed ?","Facebook recommends installing FAISS using conda (https:\/\/github.com\/facebookresearch\/faiss\/blob\/main\/INSTALL.md). pip does not seem to have the latest version of FAISS. The latest version of faiss is 1.7.2 (https:\/\/anaconda.org\/conda-forge\/faiss), but the latest one available through pip is 1.5.3 (https:\/\/pypi.org\/project\/faiss\/). "],"created_at":1650936043000,"updated_at":1651834011000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4220\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4220\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4220","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4220","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4220.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4220.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4219","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4219\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4219\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4219\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4219","id":1214934025,"node_id":"PR_kwDODunzps42v6rE","number":4219,"title":"Add F1 Metric Card","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650914096000,"updated_at":1651005858000,"closed_at":1651005466000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4219\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4219\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4219","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4219","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4219.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4219.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4218","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4218\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4218\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4218\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4218","id":1214748226,"node_id":"PR_kwDODunzps42vTA0","number":4218,"title":"Make code for image downloading from image urls cacheable","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650903479000,"updated_at":1650992424000,"closed_at":1650980306000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #4199 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4218\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4218\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4218","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4218","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4218.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4218.patch","merged_at":1650980306000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4217","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4217\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4217\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4217\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4217","id":1214688141,"node_id":"I_kwDODunzps5IZquN","number":4217,"title":"Big_Patent dataset broken","user":{"login":"Matthew-Larsen","id":54189843,"node_id":"MDQ6VXNlcjU0MTg5ODQz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/54189843?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Matthew-Larsen","html_url":"https:\/\/github.com\/Matthew-Larsen","followers_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/followers","following_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/orgs","repos_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/repos","events_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/received_events","type":"User","site_admin":false},"labels":[{"id":4069435429,"node_id":"LA_kwDODunzps7yjqgl","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/hosted-on-google-drive","name":"hosted-on-google-drive","color":"8B51EF","default":false,"description":""}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Thanks for reporting. The issue seems not to be directly related to the dataset viewer or the `datasets` library, but instead to it being hosted on Google Drive.\r\n\r\nSee related issues: https:\/\/github.com\/huggingface\/datasets\/issues?q=is%3Aissue+is%3Aopen+drive.google.com\r\n\r\nTo quote [@lhoestq](https:\/\/github.com\/huggingface\/datasets\/issues\/4075#issuecomment-1087362551):\r\n\r\n> PS: if possible, please try to not use Google Drive links in your dataset script, since Google Drive has download quotas and is not always reliable.\r\n\r\n","We should find out if the dataset license allows redistribution and contact the data owners to propose them to host their data on our Hub.","The data owners have agreed on hosting their data on the Hub."],"created_at":1650900705000,"updated_at":1651515675000,"closed_at":1651515675000,"author_association":"NONE","active_lock_reason":null,"body":"## Dataset viewer issue for '*big_patent*'\r\n\r\n**Link:** *[link to the dataset viewer page](https:\/\/huggingface.co\/datasets\/big_patent\/viewer\/all\/train)*\r\n\r\n*Unable to view because it says FileNotFound, also cannot download it through the python API*\r\n\r\nAm I the one who added this dataset ? No\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4217\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4217\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4216","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4216\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4216\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4216\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4216","id":1214614029,"node_id":"PR_kwDODunzps42u1_w","number":4216,"title":"Avoid recursion error in map if example is returned as dict value","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650897632000,"updated_at":1651684806000,"closed_at":1651684372000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"I noticed this bug while answering [this question](https:\/\/discuss.huggingface.co\/t\/correct-way-to-create-a-dataset-from-a-csv-file\/15686\/11?u=mariosasko). \r\n\r\nThis code replicates the bug:\r\n```python\r\nfrom datasets import Dataset\r\ndset = Dataset.from_dict({\"en\": [\"aa\", \"bb\"], \"fr\": [\"cc\", \"dd\"]})\r\ndset.map(lambda ex: {\"translation\": ex})\r\n```\r\nand this is the fix for it (before this PR):\r\n```python\r\nfrom datasets import Dataset\r\ndset = Dataset.from_dict({\"en\": [\"aa\", \"bb\"], \"fr\": [\"cc\", \"dd\"]})\r\ndset.map(lambda ex: {\"translation\": dict(ex)})\r\n```\r\n\r\nInternally, this can be fixed by merging two dicts via dict unpacking (instead of `dict.update) `in `Dataset.map`, which avoids creating recursive dictionaries.\r\n\r\nP.S. `{**a, **b}` is slightly more performant than `a.update(b)` in my bencmarks.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4216\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4216\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4216","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4216","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4216.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4216.patch","merged_at":1651684372000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4215","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4215\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4215\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4215\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4215","id":1214579162,"node_id":"PR_kwDODunzps42uuhY","number":4215,"title":"Add `drop_last_batch` to `IterableDataset.map`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650896119000,"updated_at":1651593367000,"closed_at":1651592934000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Addresses this comment: https:\/\/github.com\/huggingface\/datasets\/pull\/3801#pullrequestreview-901736921","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4215\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4215\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4215","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4215","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4215.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4215.patch","merged_at":1651592934000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4214","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4214\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4214\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4214\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4214","id":1214572430,"node_id":"PR_kwDODunzps42utC5","number":4214,"title":"Skip checksum computation in Imagefolder by default","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650895841000,"updated_at":1651591712000,"closed_at":1651591289000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Avoids having to set `ignore_verifications=True` in `load_dataset(\"imagefolder\", ...)` to skip checksum verification and speed up loading.\r\n\r\nThe user can still pass `DownloadConfig(record_checksums=True)` to not skip this part. ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4214\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4214\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4214","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4214","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4214.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4214.patch","merged_at":1651591289000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4213","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4213\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4213\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4213\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4213","id":1214510010,"node_id":"PR_kwDODunzps42uft_","number":4213,"title":"ETT time series dataset","user":{"login":"kashif","id":8100,"node_id":"MDQ6VXNlcjgxMDA=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8100?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kashif","html_url":"https:\/\/github.com\/kashif","followers_url":"https:\/\/api.github.com\/users\/kashif\/followers","following_url":"https:\/\/api.github.com\/users\/kashif\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kashif\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kashif\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kashif\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kashif\/orgs","repos_url":"https:\/\/api.github.com\/users\/kashif\/repos","events_url":"https:\/\/api.github.com\/users\/kashif\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kashif\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","thank you!\r\n"],"created_at":1650893178000,"updated_at":1651753161000,"closed_at":1651752635000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Ready for review.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4213\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4213\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4213","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4213","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4213.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4213.patch","merged_at":1651752635000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4212","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4212\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4212\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4212\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4212","id":1214498582,"node_id":"PR_kwDODunzps42udRf","number":4212,"title":"[Common Voice] Make sure bytes are correctly deleted if `path` exists","user":{"login":"patrickvonplaten","id":23423619,"node_id":"MDQ6VXNlcjIzNDIzNjE5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/23423619?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/patrickvonplaten","html_url":"https:\/\/github.com\/patrickvonplaten","followers_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/followers","following_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/orgs","repos_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/repos","events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","cool that you noticed that we store unnecessary bytes again :D "],"created_at":1650892706000,"updated_at":1651013668000,"closed_at":1651013307000,"author_association":"MEMBER","active_lock_reason":null,"body":"`path` should be set to local path inside audio feature if exist so that bytes can correctly be deleted.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4212\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4212\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4212","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4212","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4212.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4212.patch","merged_at":1651013307000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4211","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4211\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4211\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4211\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4211","id":1214361837,"node_id":"I_kwDODunzps5IYbDt","number":4211,"title":"DatasetDict containing Datasets with different features when pushed to hub gets remapped features","user":{"login":"pietrolesci","id":61748653,"node_id":"MDQ6VXNlcjYxNzQ4NjUz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/61748653?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/pietrolesci","html_url":"https:\/\/github.com\/pietrolesci","followers_url":"https:\/\/api.github.com\/users\/pietrolesci\/followers","following_url":"https:\/\/api.github.com\/users\/pietrolesci\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/pietrolesci\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/pietrolesci\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/pietrolesci\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/pietrolesci\/orgs","repos_url":"https:\/\/api.github.com\/users\/pietrolesci\/repos","events_url":"https:\/\/api.github.com\/users\/pietrolesci\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/pietrolesci\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi @pietrolesci, thanks for reporting.\r\n\r\nPlease note that this is a design purpose: a `DatasetDict` has the same features for all its datasets. Normally, a `DatasetDict` is composed of several sub-datasets each corresponding to a different **split**.\r\n\r\nTo handle sub-datasets with different features, we use another approach: use different **configurations** instead of **splits**.\r\n\r\nHowever, for the moment `push_to_hub` does not support specifying different configurations. IMHO, we should implement this.","Hi @albertvillanova,\r\n\r\nThanks a lot for your reply! I got it now. The strange thing for me was to have it correctly working (i.e., DatasetDict with different features in some datasets) locally and not on the Hub. It would be great to have configuration supported by `push_to_hub`. Personally, this latter functionality allowed me to iterate rather quickly on dataset curation.\r\n\r\nAgain, thanks for your time @albertvillanova!\r\n\r\nBest,\r\nPietro","Hi! Yes, we should override `DatasetDict.__setitem__` and throw an error if features dictionaries are different. `DatasetDict` is a subclass of `dict`, so `DatasetDict.{update\/setdefault}` need to be overridden as well. We could avoid this by subclassing `UserDict`, but then we would get the name collision - `DatasetDict.data` vs. `UserDict.data`. This makes me think we should rename the `data` attribute of `DatasetDict`\/`Dataset` for easier dict subclassing (would also simplify https:\/\/github.com\/huggingface\/datasets\/pull\/3997) and to follow good Python practices. Another option is to have a custom `UserDict` class in `py_utils`, but it can be hard to keep this class consistent with the built-in `UserDict`. \r\n\r\n@albertvillanova @lhoestq wdyt?","I would keep things simple and keep subclassing dict. Regarding the features check, I guess this can be done only for `push_to_hub` right ? It is the only function right now that requires the underlying datasets to be splits (e.g. train\/test) and have the same features.\r\n\r\nNote that later you will be able to push datasets with different features as different dataset **configurations** (similarly to the [GLUE subsets](https:\/\/huggingface.co\/datasets\/glue) for example). We will work on this soon"],"created_at":1650885774000,"updated_at":1650990732000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi there,\r\n\r\nI am trying to load a dataset to the Hub. This dataset is a `DatasetDict` composed of various splits. Some splits have a different `Feature` mapping. Locally, the DatasetDict preserves the individual features but if I `push_to_hub` and then `load_dataset`, the features are all the same.\r\n\r\nDataset and code to reproduce available [here](https:\/\/huggingface.co\/datasets\/pietrolesci\/robust_nli).\r\n\r\nIn short:\r\n\r\nI have 3 feature mapping\r\n```python\r\nTri_features = Features(\r\n    {\r\n        \"idx\": Value(dtype=\"int64\"),\r\n        \"premise\": Value(dtype=\"string\"),\r\n        \"hypothesis\": Value(dtype=\"string\"),\r\n        \"label\": ClassLabel(num_classes=3, names=[\"entailment\", \"neutral\", \"contradiction\"]),\r\n    }\r\n)\r\n\r\nEnt_features = Features(\r\n    {\r\n        \"idx\": Value(dtype=\"int64\"),\r\n        \"premise\": Value(dtype=\"string\"),\r\n        \"hypothesis\": Value(dtype=\"string\"),\r\n        \"label\": ClassLabel(num_classes=2, names=[\"non-entailment\", \"entailment\"]),\r\n    }\r\n)\r\n\r\nCon_features = Features(\r\n    {\r\n        \"idx\": Value(dtype=\"int64\"),\r\n        \"premise\": Value(dtype=\"string\"),\r\n        \"hypothesis\": Value(dtype=\"string\"),\r\n        \"label\": ClassLabel(num_classes=2, names=[\"non-contradiction\", \"contradiction\"]),\r\n    }\r\n)\r\n```\r\n\r\nThen I create different datasets\r\n\r\n```python\r\ndataset_splits = {}\r\n\r\nfor split in df[\"split\"].unique():\r\n    print(split)\r\n    df_split = df.loc[df[\"split\"] == split].copy()\r\n    \r\n    if split in Tri_dataset:\r\n        df_split[\"label\"] = df_split[\"label\"].map({\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2})\r\n        ds = Dataset.from_pandas(df_split, features=Tri_features)\r\n    \r\n    elif split in Ent_bin_dataset:\r\n        df_split[\"label\"] = df_split[\"label\"].map({\"non-entailment\": 0, \"entailment\": 1})\r\n        ds = Dataset.from_pandas(df_split, features=Ent_features)\r\n    \r\n    elif split in Con_bin_dataset:\r\n        df_split[\"label\"] = df_split[\"label\"].map({\"non-contradiction\": 0, \"contradiction\": 1})\r\n        ds = Dataset.from_pandas(df_split, features=Con_features)\r\n\r\n    else:\r\n        print(\"ERROR:\", split)\r\n    dataset_splits[split] = ds\r\ndatasets = DatasetDict(dataset_splits)\r\n```\r\n\r\nI then push to hub\r\n\r\n```python\r\ndatasets.push_to_hub(\"pietrolesci\/robust_nli\", token=\"<token>\")\r\n```\r\n\r\nFinally, I load it from the hub\r\n\r\n```python\r\ndatasets_loaded_from_hub = load_dataset(\"pietrolesci\/robust_nli\")\r\n```\r\n\r\nAnd I get that\r\n\r\n```python\r\ndatasets[\"LI_TS\"].features != datasets_loaded_from_hub[\"LI_TS\"].features\r\n```\r\n\r\nsince \r\n\r\n```python\r\n\"label\": ClassLabel(num_classes=2, names=[\"non-contradiction\", \"contradiction\"])\r\n```\r\n\r\ngets remapped to \r\n\r\n```python\r\n \"label\": ClassLabel(num_classes=3, names=[\"entailment\", \"neutral\", \"contradiction\"])\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4211\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4211\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4210","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4210\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4210\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4210\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4210","id":1214089130,"node_id":"I_kwDODunzps5IXYeq","number":4210,"title":"TypeError: Cannot cast array data from dtype('O') to dtype('int64') according to the rule 'safe'","user":{"login":"loretoparisi","id":163333,"node_id":"MDQ6VXNlcjE2MzMzMw==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/163333?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/loretoparisi","html_url":"https:\/\/github.com\/loretoparisi","followers_url":"https:\/\/api.github.com\/users\/loretoparisi\/followers","following_url":"https:\/\/api.github.com\/users\/loretoparisi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/loretoparisi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/loretoparisi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/loretoparisi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/loretoparisi\/orgs","repos_url":"https:\/\/api.github.com\/users\/loretoparisi\/repos","events_url":"https:\/\/api.github.com\/users\/loretoparisi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/loretoparisi\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi! Casting class labels from strings is currently not supported in the CSV loader, but you can get the same result with an additional map as follows:\r\n```python\r\nfrom datasets import load_dataset,Features,Value,ClassLabel\r\nclass_names = [\"cmn\",\"deu\",\"rus\",\"fra\",\"eng\",\"jpn\",\"spa\",\"ita\",\"kor\",\"vie\",\"nld\",\"epo\",\"por\",\"tur\",\"heb\",\"hun\",\"ell\",\"ind\",\"ara\",\"arz\",\"fin\",\"bul\",\"yue\",\"swe\",\"ukr\",\"bel\",\"que\",\"ces\",\"swh\",\"nno\",\"wuu\",\"nob\",\"zsm\",\"est\",\"kat\",\"pol\",\"lat\",\"urd\",\"sqi\",\"isl\",\"fry\",\"afr\",\"ron\",\"fao\",\"san\",\"bre\",\"tat\",\"yid\",\"uig\",\"uzb\",\"srp\",\"qya\",\"dan\",\"pes\",\"slk\",\"eus\",\"cycl\",\"acm\",\"tgl\",\"lvs\",\"kaz\",\"hye\",\"hin\",\"lit\",\"ben\",\"cat\",\"bos\",\"hrv\",\"tha\",\"orv\",\"cha\",\"mon\",\"lzh\",\"scn\",\"gle\",\"mkd\",\"slv\",\"frm\",\"glg\",\"vol\",\"ain\",\"jbo\",\"tok\",\"ina\",\"nds\",\"mal\",\"tlh\",\"roh\",\"ltz\",\"oss\",\"ido\",\"gla\",\"mlt\",\"sco\",\"ast\",\"jav\",\"oci\",\"ile\",\"ota\",\"xal\",\"tel\",\"sjn\",\"nov\",\"khm\",\"tpi\",\"ang\",\"aze\",\"tgk\",\"tuk\",\"chv\",\"hsb\",\"dsb\",\"bod\",\"sme\",\"cym\",\"mri\",\"ksh\",\"kmr\",\"ewe\",\"kab\",\"ber\",\"tpw\",\"udm\",\"lld\",\"pms\",\"lad\",\"grn\",\"mlg\",\"xho\",\"pnb\",\"grc\",\"hat\",\"lao\",\"npi\",\"cor\",\"nah\",\"avk\",\"mar\",\"guj\",\"pan\",\"kir\",\"myv\",\"prg\",\"sux\",\"crs\",\"ckt\",\"bak\",\"zlm\",\"hil\",\"cbk\",\"chr\",\"nav\",\"lkt\",\"enm\",\"arq\",\"lin\",\"abk\",\"pcd\",\"rom\",\"gsw\",\"tam\",\"zul\",\"awa\",\"wln\",\"amh\",\"bar\",\"hbo\",\"mhr\",\"bho\",\"mrj\",\"ckb\",\"osx\",\"pfl\",\"mgm\",\"sna\",\"mah\",\"hau\",\"kan\",\"nog\",\"sin\",\"glv\",\"dng\",\"kal\",\"liv\",\"vro\",\"apc\",\"jdt\",\"fur\",\"che\",\"haw\",\"yor\",\"crh\",\"pdc\",\"ppl\",\"kin\",\"shs\",\"mnw\",\"tet\",\"sah\",\"kum\",\"ngt\",\"nya\",\"pus\",\"hif\",\"mya\",\"moh\",\"wol\",\"tir\",\"ton\",\"lzz\",\"oar\",\"lug\",\"brx\",\"non\",\"mww\",\"hak\",\"nlv\",\"ngu\",\"bua\",\"aym\",\"vec\",\"ibo\",\"tkl\",\"bam\",\"kha\",\"ceb\",\"lou\",\"fuc\",\"smo\",\"gag\",\"lfn\",\"arg\",\"umb\",\"tyv\",\"kjh\",\"oji\",\"cyo\",\"urh\",\"kzj\",\"pam\",\"srd\",\"lmo\",\"swg\",\"mdf\",\"gil\",\"snd\",\"tso\",\"sot\",\"zza\",\"tsn\",\"pau\",\"som\",\"egl\",\"ady\",\"asm\",\"ori\",\"dtp\",\"cho\",\"max\",\"kam\",\"niu\",\"sag\",\"ilo\",\"kaa\",\"fuv\",\"nch\",\"hoc\",\"iba\",\"gbm\",\"sun\",\"war\",\"mvv\",\"pap\",\"ary\",\"kxi\",\"csb\",\"pag\",\"cos\",\"rif\",\"kek\",\"krc\",\"aii\",\"ban\",\"ssw\",\"tvl\",\"mfe\",\"tah\",\"bvy\",\"bcl\",\"hnj\",\"nau\",\"nst\",\"afb\",\"quc\",\"min\",\"tmw\",\"mad\",\"bjn\",\"mai\",\"cjy\",\"got\",\"hsn\",\"gan\",\"tzl\",\"dws\",\"ldn\",\"afh\",\"sgs\",\"krl\",\"vep\",\"rue\",\"tly\",\"mic\",\"ext\",\"izh\",\"sma\",\"jam\",\"cmo\",\"mwl\",\"kpv\",\"koi\",\"bis\",\"ike\",\"run\",\"evn\",\"ryu\",\"mnc\",\"aoz\",\"otk\",\"kas\",\"aln\",\"akl\",\"yua\",\"shy\",\"fkv\",\"gos\",\"fij\",\"thv\",\"zgh\",\"gcf\",\"cay\",\"xmf\",\"tig\",\"div\",\"lij\",\"rap\",\"hrx\",\"cpi\",\"tts\",\"gaa\",\"tmr\",\"iii\",\"ltg\",\"bzt\",\"syc\",\"emx\",\"gom\",\"chg\",\"osp\",\"stq\",\"frr\",\"fro\",\"nys\",\"toi\",\"new\",\"phn\",\"jpa\",\"rel\",\"drt\",\"chn\",\"pli\",\"laa\",\"bal\",\"hdn\",\"hax\",\"mik\",\"ajp\",\"xqa\",\"pal\",\"crk\",\"mni\",\"lut\",\"ayl\",\"ood\",\"sdh\",\"ofs\",\"nus\",\"kiu\",\"diq\",\"qxq\",\"alt\",\"bfz\",\"klj\",\"mus\",\"srn\",\"guc\",\"lim\",\"zea\",\"shi\",\"mnr\",\"bom\",\"sat\",\"szl\"]\r\nfeatures = Features({ 'label': ClassLabel(names=class_names), 'text': Value('string')})\r\nnum_labels = features['label'].num_classes\r\ndata_files = { \"train\": \"train.csv\", \"test\": \"test.csv\" }\r\nsentences = load_dataset(\r\n     \"loretoparisi\/tatoeba-sentences\",\r\n     data_files=data_files,\r\n     delimiter='\\t', \r\n     column_names=['label', 'text'],\r\n)\r\n# You can make this part faster with num_proc=<some int>\r\nsentences = sentences.map(lambda ex: features[\"label\"].str2int(ex[\"label\"]) if ex[\"label\"] is not None else None, features=features)\r\n```\r\n\r\n@lhoestq IIRC, I suggested adding `cast_to_storage` to `ClassLabel`  + `table_cast` to the packaged loaders if the `ClassLabel`\/`Image`\/`Audio` type is present in `features` to avoid this kind of error, but your concern was speed. IMO shouldn't be a problem if we do `table_cast` only when these features are present.","I agree packaged loaders should support `ClassLabel` feature without throwing an error.","@albertvillanova @mariosasko thank you, with that change now I get\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n[<ipython-input-9-eeb68eeb9bec>](https:\/\/localhost:8080\/#) in <module>()\r\n     11 )\r\n     12 # You can make this part faster with num_proc=<some int>\r\n---> 13 sentences = sentences.map(lambda ex: features[\"label\"].str2int(ex[\"label\"]) if ex[\"label\"] is not None else None, features=features)\r\n     14 sentences = sentences.shuffle()\r\n\r\n8 frames\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in validate_function_output(processed_inputs, indices)\r\n   2193             if processed_inputs is not None and not isinstance(processed_inputs, (Mapping, pa.Table)):\r\n   2194                 raise TypeError(\r\n-> 2195                     f\"Provided `function` which is applied to all elements of table returns a variable of type {type(processed_inputs)}. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects.\"\r\n   2196                 )\r\n   2197             elif isinstance(indices, list) and isinstance(processed_inputs, Mapping):\r\n\r\nTypeError: Provided `function` which is applied to all elements of table returns a variable of type <class 'int'>. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects.\r\n```\r\n\r\nthe error is raised by [this](https:\/\/github.com\/huggingface\/datasets\/blob\/master\/src\/datasets\/arrow_dataset.py#L2221)\r\n\r\n```\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in validate_function_output(processed_inputs, indices)\r\n```","@mariosasko changed it like\r\n\r\n```python\r\nsentences = sentences.map(lambda ex: {\"label\" : features[\"label\"].str2int(ex[\"label\"]) if ex[\"label\"] is not None else None}, features=features)\r\n```\r\n\r\nto avoid the above errorr.","Any update on this? Is this correct ?\r\n> @mariosasko changed it like\r\n> \r\n> ```python\r\n> sentences = sentences.map(lambda ex: {\"label\" : features[\"label\"].str2int(ex[\"label\"]) if ex[\"label\"] is not None else None}, features=features)\r\n> ```\r\n> \r\n> to avoid the above errorr.\r\n\r\n"],"created_at":1650871722000,"updated_at":1651247976000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### System Info\r\n\r\n```shell\r\n- `transformers` version: 4.18.0\r\n- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- Huggingface_hub version: 0.5.1\r\n- PyTorch version (GPU?): 1.10.0+cu111 (True)\r\n- Tensorflow version (GPU?): 2.8.0 (True)\r\n- Flax version (CPU?\/GPU?\/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n```\r\n\r\n\r\n### Who can help?\r\n\r\n@LysandreJik \r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [X] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n```python\r\nfrom datasets import load_dataset,Features,Value,ClassLabel\r\n\r\nclass_names = [\"cmn\",\"deu\",\"rus\",\"fra\",\"eng\",\"jpn\",\"spa\",\"ita\",\"kor\",\"vie\",\"nld\",\"epo\",\"por\",\"tur\",\"heb\",\"hun\",\"ell\",\"ind\",\"ara\",\"arz\",\"fin\",\"bul\",\"yue\",\"swe\",\"ukr\",\"bel\",\"que\",\"ces\",\"swh\",\"nno\",\"wuu\",\"nob\",\"zsm\",\"est\",\"kat\",\"pol\",\"lat\",\"urd\",\"sqi\",\"isl\",\"fry\",\"afr\",\"ron\",\"fao\",\"san\",\"bre\",\"tat\",\"yid\",\"uig\",\"uzb\",\"srp\",\"qya\",\"dan\",\"pes\",\"slk\",\"eus\",\"cycl\",\"acm\",\"tgl\",\"lvs\",\"kaz\",\"hye\",\"hin\",\"lit\",\"ben\",\"cat\",\"bos\",\"hrv\",\"tha\",\"orv\",\"cha\",\"mon\",\"lzh\",\"scn\",\"gle\",\"mkd\",\"slv\",\"frm\",\"glg\",\"vol\",\"ain\",\"jbo\",\"tok\",\"ina\",\"nds\",\"mal\",\"tlh\",\"roh\",\"ltz\",\"oss\",\"ido\",\"gla\",\"mlt\",\"sco\",\"ast\",\"jav\",\"oci\",\"ile\",\"ota\",\"xal\",\"tel\",\"sjn\",\"nov\",\"khm\",\"tpi\",\"ang\",\"aze\",\"tgk\",\"tuk\",\"chv\",\"hsb\",\"dsb\",\"bod\",\"sme\",\"cym\",\"mri\",\"ksh\",\"kmr\",\"ewe\",\"kab\",\"ber\",\"tpw\",\"udm\",\"lld\",\"pms\",\"lad\",\"grn\",\"mlg\",\"xho\",\"pnb\",\"grc\",\"hat\",\"lao\",\"npi\",\"cor\",\"nah\",\"avk\",\"mar\",\"guj\",\"pan\",\"kir\",\"myv\",\"prg\",\"sux\",\"crs\",\"ckt\",\"bak\",\"zlm\",\"hil\",\"cbk\",\"chr\",\"nav\",\"lkt\",\"enm\",\"arq\",\"lin\",\"abk\",\"pcd\",\"rom\",\"gsw\",\"tam\",\"zul\",\"awa\",\"wln\",\"amh\",\"bar\",\"hbo\",\"mhr\",\"bho\",\"mrj\",\"ckb\",\"osx\",\"pfl\",\"mgm\",\"sna\",\"mah\",\"hau\",\"kan\",\"nog\",\"sin\",\"glv\",\"dng\",\"kal\",\"liv\",\"vro\",\"apc\",\"jdt\",\"fur\",\"che\",\"haw\",\"yor\",\"crh\",\"pdc\",\"ppl\",\"kin\",\"shs\",\"mnw\",\"tet\",\"sah\",\"kum\",\"ngt\",\"nya\",\"pus\",\"hif\",\"mya\",\"moh\",\"wol\",\"tir\",\"ton\",\"lzz\",\"oar\",\"lug\",\"brx\",\"non\",\"mww\",\"hak\",\"nlv\",\"ngu\",\"bua\",\"aym\",\"vec\",\"ibo\",\"tkl\",\"bam\",\"kha\",\"ceb\",\"lou\",\"fuc\",\"smo\",\"gag\",\"lfn\",\"arg\",\"umb\",\"tyv\",\"kjh\",\"oji\",\"cyo\",\"urh\",\"kzj\",\"pam\",\"srd\",\"lmo\",\"swg\",\"mdf\",\"gil\",\"snd\",\"tso\",\"sot\",\"zza\",\"tsn\",\"pau\",\"som\",\"egl\",\"ady\",\"asm\",\"ori\",\"dtp\",\"cho\",\"max\",\"kam\",\"niu\",\"sag\",\"ilo\",\"kaa\",\"fuv\",\"nch\",\"hoc\",\"iba\",\"gbm\",\"sun\",\"war\",\"mvv\",\"pap\",\"ary\",\"kxi\",\"csb\",\"pag\",\"cos\",\"rif\",\"kek\",\"krc\",\"aii\",\"ban\",\"ssw\",\"tvl\",\"mfe\",\"tah\",\"bvy\",\"bcl\",\"hnj\",\"nau\",\"nst\",\"afb\",\"quc\",\"min\",\"tmw\",\"mad\",\"bjn\",\"mai\",\"cjy\",\"got\",\"hsn\",\"gan\",\"tzl\",\"dws\",\"ldn\",\"afh\",\"sgs\",\"krl\",\"vep\",\"rue\",\"tly\",\"mic\",\"ext\",\"izh\",\"sma\",\"jam\",\"cmo\",\"mwl\",\"kpv\",\"koi\",\"bis\",\"ike\",\"run\",\"evn\",\"ryu\",\"mnc\",\"aoz\",\"otk\",\"kas\",\"aln\",\"akl\",\"yua\",\"shy\",\"fkv\",\"gos\",\"fij\",\"thv\",\"zgh\",\"gcf\",\"cay\",\"xmf\",\"tig\",\"div\",\"lij\",\"rap\",\"hrx\",\"cpi\",\"tts\",\"gaa\",\"tmr\",\"iii\",\"ltg\",\"bzt\",\"syc\",\"emx\",\"gom\",\"chg\",\"osp\",\"stq\",\"frr\",\"fro\",\"nys\",\"toi\",\"new\",\"phn\",\"jpa\",\"rel\",\"drt\",\"chn\",\"pli\",\"laa\",\"bal\",\"hdn\",\"hax\",\"mik\",\"ajp\",\"xqa\",\"pal\",\"crk\",\"mni\",\"lut\",\"ayl\",\"ood\",\"sdh\",\"ofs\",\"nus\",\"kiu\",\"diq\",\"qxq\",\"alt\",\"bfz\",\"klj\",\"mus\",\"srn\",\"guc\",\"lim\",\"zea\",\"shi\",\"mnr\",\"bom\",\"sat\",\"szl\"]\r\nfeatures = Features({ 'label': ClassLabel(names=class_names), 'text': Value('string')})\r\nnum_labels = features['label'].num_classes\r\ndata_files = { \"train\": \"train.csv\", \"test\": \"test.csv\" }\r\nsentences = load_dataset(\"loretoparisi\/tatoeba-sentences\",\r\n                             data_files=data_files,\r\n                             delimiter='\\t', \r\n                             column_names=['label', 'text'],\r\n                             features = features\r\n```\r\n\r\nERROR:\r\n```\r\nClassLabel(num_classes=403, names=['cmn', 'deu', 'rus', 'fra', 'eng', 'jpn', 'spa', 'ita', 'kor', 'vie', 'nld', 'epo', 'por', 'tur', 'heb', 'hun', 'ell', 'ind', 'ara', 'arz', 'fin', 'bul', 'yue', 'swe', 'ukr', 'bel', 'que', 'ces', 'swh', 'nno', 'wuu', 'nob', 'zsm', 'est', 'kat', 'pol', 'lat', 'urd', 'sqi', 'isl', 'fry', 'afr', 'ron', 'fao', 'san', 'bre', 'tat', 'yid', 'uig', 'uzb', 'srp', 'qya', 'dan', 'pes', 'slk', 'eus', 'cycl', 'acm', 'tgl', 'lvs', 'kaz', 'hye', 'hin', 'lit', 'ben', 'cat', 'bos', 'hrv', 'tha', 'orv', 'cha', 'mon', 'lzh', 'scn', 'gle', 'mkd', 'slv', 'frm', 'glg', 'vol', 'ain', 'jbo', 'tok', 'ina', 'nds', 'mal', 'tlh', 'roh', 'ltz', 'oss', 'ido', 'gla', 'mlt', 'sco', 'ast', 'jav', 'oci', 'ile', 'ota', 'xal', 'tel', 'sjn', 'nov', 'khm', 'tpi', 'ang', 'aze', 'tgk', 'tuk', 'chv', 'hsb', 'dsb', 'bod', 'sme', 'cym', 'mri', 'ksh', 'kmr', 'ewe', 'kab', 'ber', 'tpw', 'udm', 'lld', 'pms', 'lad', 'grn', 'mlg', 'xho', 'pnb', 'grc', 'hat', 'lao', 'npi', 'cor', 'nah', 'avk', 'mar', 'guj', 'pan', 'kir', 'myv', 'prg', 'sux', 'crs', 'ckt', 'bak', 'zlm', 'hil', 'cbk', 'chr', 'nav', 'lkt', 'enm', 'arq', 'lin', 'abk', 'pcd', 'rom', 'gsw', 'tam', 'zul', 'awa', 'wln', 'amh', 'bar', 'hbo', 'mhr', 'bho', 'mrj', 'ckb', 'osx', 'pfl', 'mgm', 'sna', 'mah', 'hau', 'kan', 'nog', 'sin', 'glv', 'dng', 'kal', 'liv', 'vro', 'apc', 'jdt', 'fur', 'che', 'haw', 'yor', 'crh', 'pdc', 'ppl', 'kin', 'shs', 'mnw', 'tet', 'sah', 'kum', 'ngt', 'nya', 'pus', 'hif', 'mya', 'moh', 'wol', 'tir', 'ton', 'lzz', 'oar', 'lug', 'brx', 'non', 'mww', 'hak', 'nlv', 'ngu', 'bua', 'aym', 'vec', 'ibo', 'tkl', 'bam', 'kha', 'ceb', 'lou', 'fuc', 'smo', 'gag', 'lfn', 'arg', 'umb', 'tyv', 'kjh', 'oji', 'cyo', 'urh', 'kzj', 'pam', 'srd', 'lmo', 'swg', 'mdf', 'gil', 'snd', 'tso', 'sot', 'zza', 'tsn', 'pau', 'som', 'egl', 'ady', 'asm', 'ori', 'dtp', 'cho', 'max', 'kam', 'niu', 'sag', 'ilo', 'kaa', 'fuv', 'nch', 'hoc', 'iba', 'gbm', 'sun', 'war', 'mvv', 'pap', 'ary', 'kxi', 'csb', 'pag', 'cos', 'rif', 'kek', 'krc', 'aii', 'ban', 'ssw', 'tvl', 'mfe', 'tah', 'bvy', 'bcl', 'hnj', 'nau', 'nst', 'afb', 'quc', 'min', 'tmw', 'mad', 'bjn', 'mai', 'cjy', 'got', 'hsn', 'gan', 'tzl', 'dws', 'ldn', 'afh', 'sgs', 'krl', 'vep', 'rue', 'tly', 'mic', 'ext', 'izh', 'sma', 'jam', 'cmo', 'mwl', 'kpv', 'koi', 'bis', 'ike', 'run', 'evn', 'ryu', 'mnc', 'aoz', 'otk', 'kas', 'aln', 'akl', 'yua', 'shy', 'fkv', 'gos', 'fij', 'thv', 'zgh', 'gcf', 'cay', 'xmf', 'tig', 'div', 'lij', 'rap', 'hrx', 'cpi', 'tts', 'gaa', 'tmr', 'iii', 'ltg', 'bzt', 'syc', 'emx', 'gom', 'chg', 'osp', 'stq', 'frr', 'fro', 'nys', 'toi', 'new', 'phn', 'jpa', 'rel', 'drt', 'chn', 'pli', 'laa', 'bal', 'hdn', 'hax', 'mik', 'ajp', 'xqa', 'pal', 'crk', 'mni', 'lut', 'ayl', 'ood', 'sdh', 'ofs', 'nus', 'kiu', 'diq', 'qxq', 'alt', 'bfz', 'klj', 'mus', 'srn', 'guc', 'lim', 'zea', 'shi', 'mnr', 'bom', 'sat', 'szl'], id=None)\r\nValue(dtype='string', id=None)\r\nUsing custom data configuration loretoparisi--tatoeba-sentences-7b2c5e991f398f39\r\nDownloading and preparing dataset csv\/loretoparisi--tatoeba-sentences to \/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-7b2c5e991f398f39\/0.0.0\/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\r\nDownloading data files: 100%\r\n2\/2 [00:18<00:00, 8.06s\/it]\r\nDownloading data: 100%\r\n391M\/391M [00:13<00:00, 35.3MB\/s]\r\nDownloading data: 100%\r\n92.4M\/92.4M [00:02<00:00, 36.5MB\/s]\r\nFailed to read file '\/root\/.cache\/huggingface\/datasets\/downloads\/933132df9905194ea9faeb30cabca8c49318795612f6495fcb941a290191dd5d' with error <class 'ValueError'>: invalid literal for int() with base 10: 'cmn'\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader._convert_tokens()\r\n\r\nTypeError: Cannot cast array data from dtype('O') to dtype('int64') according to the rule 'safe'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n15 frames\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader._convert_tokens()\r\n\r\nValueError: invalid literal for int() with base 10: 'cmn'\r\n```\r\n\r\nwhile loading without `features` it loads without errors\r\n\r\n```\r\nsentences = load_dataset(\"loretoparisi\/tatoeba-sentences\",\r\n                             data_files=data_files,\r\n                             delimiter='\\t', \r\n                             column_names=['label', 'text']\r\n                         )\r\n```\r\n\r\nbut the `label` col seems to be wrong (without the `ClassLabel` object):\r\n\r\n```\r\nsentences['train'].features\r\n{'label': Value(dtype='string', id=None),\r\n 'text': Value(dtype='string', id=None)}\r\n```\r\n\r\nThe dataset was https:\/\/huggingface.co\/datasets\/loretoparisi\/tatoeba-sentences\r\n\r\n\r\nDataset format is:\r\n\r\n```\r\nces\tNechci v\u011bd\u011bt, co je tam uvnit\u0159.\r\nces\tKdo o tom chce sly\u0161et?\r\ndeu\tTom sagte, er f\u00fchle sich nicht wohl.\r\nber\tMel-iyi-d anida-t tura ?\r\nhun\tGondom lesz r\u00e1 r\u00f6gt\u00f6n.\r\nber\tMel-iyi-d anida-tt tura ?\r\ndeu\tIch will dich nicht reden h\u00f6ren.\r\n```\r\n\r\n### Expected behavior\r\n\r\n```shell\r\ncorrectly load train and test files.\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4210\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4210\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4208","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4208\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4208\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4208\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4208","id":1213716426,"node_id":"PR_kwDODunzps42r7bW","number":4208,"title":"Add CMU MoCap Dataset","user":{"login":"dnaveenr","id":17746528,"node_id":"MDQ6VXNlcjE3NzQ2NTI4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17746528?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/dnaveenr","html_url":"https:\/\/github.com\/dnaveenr","followers_url":"https:\/\/api.github.com\/users\/dnaveenr\/followers","following_url":"https:\/\/api.github.com\/users\/dnaveenr\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/dnaveenr\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/dnaveenr\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/dnaveenr\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/dnaveenr\/orgs","repos_url":"https:\/\/api.github.com\/users\/dnaveenr\/repos","events_url":"https:\/\/api.github.com\/users\/dnaveenr\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/dnaveenr\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4208). All of your documentation changes will be reflected on that endpoint.","- Updated the readme.\r\n- Added dummy_data.zip and ran the all the tests.\r\n\r\nThe dataset works for \"asf\/amc\" and \"avi\" formats which have a single download link for the complete dataset. But \"c3d\" and \"mpg\" have multiple download links, can we combine and host these links on the Hub since the dataset is free to use ?"],"created_at":1650821468000,"updated_at":1651555452000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Resolves #3457 \r\n\r\nDataset Request : Add CMU Graphics Lab Motion Capture dataset [#3457](https:\/\/github.com\/huggingface\/datasets\/issues\/3457)\r\nThis PR adds the CMU MoCap Dataset.\r\n\r\nThe authors didn't respond even after multiple follow ups, so I ended up crawling the website to get categories, subcategories and description information. Some of the subjects do not have category\/subcategory\/description as well. I am using a subject to categories, subcategories and description map (metadata file).\r\n\r\nCurrently the loading of the dataset works for \"asf\/amc\" and \"avi\" formats since they have a single download link. But \"c3d\" and \"mpg\" have multiple download links (part archives) and dl_manager.download_and_extract() extracts the files to multiple paths, is there a way to extract these multiple archives into one folder ? Any other way to go about this ?\r\nAny suggestions\/inputs on this would be helpful. Thank you.\r\n\r\n\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4208\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4208\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4208","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4208","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4208.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4208.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4207","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4207\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4207\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4207\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4207","id":1213604615,"node_id":"PR_kwDODunzps42rmbK","number":4207,"title":"[Minor edit] Fix typo in class name","user":{"login":"cakiki","id":3664563,"node_id":"MDQ6VXNlcjM2NjQ1NjM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3664563?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/cakiki","html_url":"https:\/\/github.com\/cakiki","followers_url":"https:\/\/api.github.com\/users\/cakiki\/followers","following_url":"https:\/\/api.github.com\/users\/cakiki\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/cakiki\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/cakiki\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/cakiki\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/cakiki\/orgs","repos_url":"https:\/\/api.github.com\/users\/cakiki\/repos","events_url":"https:\/\/api.github.com\/users\/cakiki\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/cakiki\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1650793777000,"updated_at":1651756667000,"closed_at":1651756667000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Typo: `datasets.DatsetDict` -> `datasets.DatasetDict`","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4207\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4207\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4207","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4207","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4207.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4207.patch","merged_at":1651756667000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4206","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4206\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4206\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4206\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4206","id":1212715581,"node_id":"PR_kwDODunzps42pJQW","number":4206,"title":"Add Nerval Metric","user":{"login":"mdadda","id":49372461,"node_id":"MDQ6VXNlcjQ5MzcyNDYx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/49372461?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mdadda","html_url":"https:\/\/github.com\/mdadda","followers_url":"https:\/\/api.github.com\/users\/mdadda\/followers","following_url":"https:\/\/api.github.com\/users\/mdadda\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mdadda\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mdadda\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mdadda\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mdadda\/orgs","repos_url":"https:\/\/api.github.com\/users\/mdadda\/repos","events_url":"https:\/\/api.github.com\/users\/mdadda\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mdadda\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1650656700000,"updated_at":1651404219000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"This PR adds readme.md and ner_val.py to metrics.\r\nNerval is a python package that helps evaluate NER models. It creates classification report and confusion matrix at entity level.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4206\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4206\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4206","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4206","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4206.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4206.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4205","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4205\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4205\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4205\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4205","id":1212466138,"node_id":"PR_kwDODunzps42oVFE","number":4205,"title":"Fix `convert_file_size_to_int` for kilobits and megabits","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650639381000,"updated_at":1651591722000,"closed_at":1651591308000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Minor change to fully align this function with the recent change in Transformers (https:\/\/github.com\/huggingface\/transformers\/pull\/16891) ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4205\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4205\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4205","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4205","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4205.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4205.patch","merged_at":1651591308000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4204","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4204\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4204\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4204\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4204","id":1212431764,"node_id":"PR_kwDODunzps42oN0j","number":4204,"title":"Add Recall Metric Card","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","This looks good to me! "],"created_at":1650637466000,"updated_at":1651584203000,"closed_at":1651583784000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"What this PR mainly does:\r\n- add metric card for recall metric\r\n- update docs in recall python file\r\n\r\nNote: I've also included a .json file with all of the metric card information. I've started compiling the relevant information in this type of .json files, and then using a script I wrote to generate the formatted metric card, as well as the docs to go in the .py file. I figured I'd upload the .json because it could be useful, especially if I also make a PR with the script I'm using (let me know if that's something you think would be beneficial!)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4204\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4204\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4204","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4204","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4204.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4204.patch","merged_at":1651583784000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4203","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4203\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4203\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4203\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4203","id":1212431067,"node_id":"PR_kwDODunzps42oNrS","number":4203,"title":"Add Precision Metric Card","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650637428000,"updated_at":1651587820000,"closed_at":1651587406000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"What this PR mainly does:\r\n- add metric card for precision metric\r\n- update docs in precision python file\r\n\r\nNote: I've also included a .json file with all of the metric card information. I've started compiling the relevant information in this type of .json files, and then using a script I wrote to generate the formatted metric card, as well as the docs to go in the .py file. I figured I'd upload the .json because it could be useful, especially if I also make a PR with the script I'm using (let me know if that's something you think would be beneficial!)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4203\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4203\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4203","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4203","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4203.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4203.patch","merged_at":1651587405000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4202","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4202\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4202\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4202\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4202","id":1212326288,"node_id":"PR_kwDODunzps42n278","number":4202,"title":"Fix some type annotation in doc","user":{"login":"thomasw21","id":24695242,"node_id":"MDQ6VXNlcjI0Njk1MjQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/24695242?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/thomasw21","html_url":"https:\/\/github.com\/thomasw21","followers_url":"https:\/\/api.github.com\/users\/thomasw21\/followers","following_url":"https:\/\/api.github.com\/users\/thomasw21\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/thomasw21\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/thomasw21\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/thomasw21\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/thomasw21\/orgs","repos_url":"https:\/\/api.github.com\/users\/thomasw21\/repos","events_url":"https:\/\/api.github.com\/users\/thomasw21\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/thomasw21\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650632011000,"updated_at":1650639780000,"closed_at":1650639403000,"author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4202\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4202\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4202","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4202","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4202.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4202.patch","merged_at":1650639403000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4201","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4201\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4201\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4201\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4201","id":1212086420,"node_id":"PR_kwDODunzps42nIRm","number":4201,"title":"Update GH template for dataset viewer issues","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","You can see rendering at: https:\/\/github.com\/huggingface\/datasets\/blob\/6b48fedbdafe12a42c7b6edcecc32820af1a4822\/.github\/ISSUE_TEMPLATE\/dataset-viewer.yml"],"created_at":1650620084000,"updated_at":1651826323000,"closed_at":1650962755000,"author_association":"MEMBER","active_lock_reason":null,"body":"Update template to use new issue forms instead.\r\n\r\nWith this PR we can check if this new feature is useful for us.\r\n\r\nOnce validated, we can update the other templates.\r\n\r\nCC: @severo ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4201\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4201\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4201","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4201","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4201.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4201.patch","merged_at":1650962755000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4200","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4200\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4200\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4200\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4200","id":1211980110,"node_id":"PR_kwDODunzps42mz0w","number":4200,"title":"Add to docs how to load from local script","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650614905000,"updated_at":1651826365000,"closed_at":1650692845000,"author_association":"MEMBER","active_lock_reason":null,"body":"This option was missing from the docs guide (it was only explained in the docstring of `load_dataset`). Although this is an infrequent use case, there might be some users interested in it.\r\n\r\nRelated to #4192\r\n\r\nCC: @stevhliu ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4200\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4200\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4200","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4200","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4200.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4200.patch","merged_at":1650692844000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4199","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4199\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4199\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4199\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4199","id":1211953308,"node_id":"I_kwDODunzps5IPPCc","number":4199,"title":"Cache miss during reload for datasets using image fetch utilities through map ","user":{"login":"apsdehal","id":3616806,"node_id":"MDQ6VXNlcjM2MTY4MDY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3616806?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/apsdehal","html_url":"https:\/\/github.com\/apsdehal","followers_url":"https:\/\/api.github.com\/users\/apsdehal\/followers","following_url":"https:\/\/api.github.com\/users\/apsdehal\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/apsdehal\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/apsdehal\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/apsdehal\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/apsdehal\/orgs","repos_url":"https:\/\/api.github.com\/users\/apsdehal\/repos","events_url":"https:\/\/api.github.com\/users\/apsdehal\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/apsdehal\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"assignees":[{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi ! Maybe one of the objects in the function is not deterministic across sessions ? You can read more about it and how to investigate here: https:\/\/huggingface.co\/docs\/datasets\/about_cache","Hi @apsdehal! Can you verify that replacing\r\n```python\r\ndef fetch_single_image(image_url, timeout=None, retries=0):\r\n    for _ in range(retries + 1):\r\n        try:\r\n            request = urllib.request.Request(\r\n                image_url,\r\n                data=None,\r\n                headers={\"user-agent\": get_datasets_user_agent()},\r\n            )\r\n            with urllib.request.urlopen(request, timeout=timeout) as req:\r\n                image = PIL.Image.open(io.BytesIO(req.read()))\r\n            break\r\n        except Exception:\r\n            image = None\r\n    return image\r\n```\r\nwith \r\n```python\r\nUSER_AGENT = get_datasets_user_agent()\r\n\r\ndef fetch_single_image(image_url, timeout=None, retries=0):\r\n    for _ in range(retries + 1):\r\n        try:\r\n            request = urllib.request.Request(\r\n                image_url,\r\n                data=None,\r\n                headers={\"user-agent\": USER_AGENT},\r\n            )\r\n            with urllib.request.urlopen(request, timeout=timeout) as req:\r\n                image = PIL.Image.open(io.BytesIO(req.read()))\r\n            break\r\n        except Exception:\r\n            image = None\r\n    return image\r\n```\r\nfixes the issue?","Thanks @mariosasko. That does fix the issue. In general, I think these image downloading utilities since they are being used by a lot of image dataset should be provided as a part of `datasets` library right to keep the logic consistent and READMEs smaller? If they already exists, that is also great, please point me to those. I saw that `http_get` does exist.","You can find my rationale (and a proposed solution) for why these utilities are not a part of `datasets` here: https:\/\/github.com\/huggingface\/datasets\/pull\/4100#issuecomment-1097994003.","Makes sense. But, I think as the number of image datasets as grow, more people are copying pasting original code from docs to work as it is while we make fixes to them later. I think we do need a central place for these to avoid that confusion as well as more easier access to image datasets. Should we restart that discussion, possible on slack?"],"created_at":1650613628000,"updated_at":1650992432000,"closed_at":1650980306000,"author_association":"MEMBER","active_lock_reason":null,"body":"## Describe the bug\r\n\r\nIt looks like that result of `.map` operation dataset are missing the cache when you reload the script and always run from scratch. In same interpretor session, they are able to find the cache and reload it. But, when you exit the interpretor and reload it, the downloading starts from scratch.\r\n\r\n## Steps to reproduce the bug\r\n\r\nUsing the example provided in `red_caps` dataset.\r\n```python\r\nfrom concurrent.futures import ThreadPoolExecutor\r\nfrom functools import partial\r\nimport io\r\nimport urllib\r\n\r\nimport PIL.Image\r\n\r\nimport datasets\r\nfrom datasets import load_dataset\r\nfrom datasets.utils.file_utils import get_datasets_user_agent\r\n\r\n\r\ndef fetch_single_image(image_url, timeout=None, retries=0):\r\n    for _ in range(retries + 1):\r\n        try:\r\n            request = urllib.request.Request(\r\n                image_url,\r\n                data=None,\r\n                headers={\"user-agent\": get_datasets_user_agent()},\r\n            )\r\n            with urllib.request.urlopen(request, timeout=timeout) as req:\r\n                image = PIL.Image.open(io.BytesIO(req.read()))\r\n            break\r\n        except Exception:\r\n            image = None\r\n    return image\r\n\r\n\r\ndef fetch_images(batch, num_threads, timeout=None, retries=0):\r\n    fetch_single_image_with_args = partial(fetch_single_image, timeout=timeout, retries=retries)\r\n    with ThreadPoolExecutor(max_workers=num_threads) as executor:\r\n        batch[\"image\"] = list(executor.map(lambda image_urls: [fetch_single_image_with_args(image_url) for image_url in image_urls], batch[\"image_url\"]))\r\n    return batch\r\n\r\n\r\ndef process_image_urls(batch):\r\n    processed_batch_image_urls = []\r\n    for image_url in batch[\"image_url\"]:\r\n        processed_example_image_urls = []\r\n        image_url_splits = re.findall(r\"http\\S+\", image_url)\r\n        for image_url_split in image_url_splits:\r\n            if \"imgur\" in image_url_split and \",\" in image_url_split:\r\n                for image_url_part in image_url_split.split(\",\"):\r\n                    if not image_url_part:\r\n                        continue\r\n                    image_url_part = image_url_part.strip()\r\n                    root, ext = os.path.splitext(image_url_part)\r\n                    if not root.startswith(\"http\"):\r\n                      root = \"http:\/\/i.imgur.com\/\" + root\r\n                    root = root.split(\"#\")[0]\r\n                    if not ext:\r\n                      ext = \".jpg\"\r\n                    ext = re.split(r\"[?%]\", ext)[0]\r\n                    image_url_part = root + ext\r\n                    processed_example_image_urls.append(image_url_part)\r\n            else:\r\n                processed_example_image_urls.append(image_url_split)\r\n        processed_batch_image_urls.append(processed_example_image_urls)\r\n    batch[\"image_url\"] = processed_batch_image_urls\r\n    return batch\r\n\r\n\r\ndset = load_dataset(\"red_caps\", \"jellyfish\")\r\ndset = dset.map(process_image_urls, batched=True, num_proc=4)\r\nfeatures = dset[\"train\"].features.copy()\r\nfeatures[\"image\"] = datasets.Sequence(datasets.Image())\r\nnum_threads = 5\r\ndset = dset.map(fetch_images, batched=True, batch_size=50, features=features, fn_kwargs={\"num_threads\": num_threads})\r\n```\r\n\r\nRun this in an interpretor or as a script twice and see that the cache is missed the second time.\r\n\r\n## Expected results\r\nAt reload there should not be any cache miss\r\n\r\n## Actual results\r\nEvery time script is run, cache is missed and dataset is built from scratch.\r\n\r\n## Environment info\r\n- `datasets` version: 2.1.1.dev0\r\n- Platform: Linux-4.19.0-20-cloud-amd64-x86_64-with-glibc2.10\r\n- Python version: 3.8.13\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.1\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4199\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4199\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4198","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4198\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4198\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4198\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4198","id":1211456559,"node_id":"I_kwDODunzps5INVwv","number":4198,"title":"There is no dataset","user":{"login":"wilfoderek","id":1625647,"node_id":"MDQ6VXNlcjE2MjU2NDc=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1625647?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/wilfoderek","html_url":"https:\/\/github.com\/wilfoderek","followers_url":"https:\/\/api.github.com\/users\/wilfoderek\/followers","following_url":"https:\/\/api.github.com\/users\/wilfoderek\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/wilfoderek\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/wilfoderek\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/wilfoderek\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/wilfoderek\/orgs","repos_url":"https:\/\/api.github.com\/users\/wilfoderek\/repos","events_url":"https:\/\/api.github.com\/users\/wilfoderek\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/wilfoderek\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1650568766000,"updated_at":1651577345000,"closed_at":1650607945000,"author_association":"NONE","active_lock_reason":null,"body":"## Dataset viewer issue for '*name of the dataset*'\r\n\r\n**Link:** *link to the dataset viewer page*\r\n\r\n*short description of the issue*\r\n\r\nAm I the one who added this dataset ? Yes-No\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4198\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4198\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4197","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4197\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4197\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4197\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4197","id":1211342558,"node_id":"PR_kwDODunzps42kyXD","number":4197,"title":"Add remove_columns=True","user":{"login":"thomasw21","id":24695242,"node_id":"MDQ6VXNlcjI0Njk1MjQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/24695242?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/thomasw21","html_url":"https:\/\/github.com\/thomasw21","followers_url":"https:\/\/api.github.com\/users\/thomasw21\/followers","following_url":"https:\/\/api.github.com\/users\/thomasw21\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/thomasw21\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/thomasw21\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/thomasw21\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/thomasw21\/orgs","repos_url":"https:\/\/api.github.com\/users\/thomasw21\/repos","events_url":"https:\/\/api.github.com\/users\/thomasw21\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/thomasw21\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Any reason why we can't just do `[inputs.copy()]` in this line for in-place operations to not have effects anymore:\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/bf432011ff9155a5bc16c03956bc63e514baf80d\/src\/datasets\/arrow_dataset.py#L2232.\r\n\r\n(in the `batched` case, we can also copy the inputs' values (list objects) to ignore in-place modifications to the inputs' columns)\r\n\r\nI think `remove_columns=True` has no meaning, so I'm not a fan of this change.","@mariosasko copy does have a cost associated with it ... and plus you'll have to consider `deepcopy` Imagine columnds that are list of list of list of list .... Though I have to agree that `remove_columns=True` doesn't make sense (but, IMO, neither does it in its current use-case as it should refer to `input_columns`) ","Okay closing this PR for the following reasons:\r\n - `remove_columns=True` was expected to keep the `.update`-like operator for `.map`. I initially thought it would be a good way to ignore function side effects and only keep output of that function (cf. PR description).\r\n - expected `remove_columns=True` is a bad API according to @mariosasko and introduces unecessary changes for little gain (strictly equivalent to `remove_columns=dset.column_names`)"],"created_at":1650562093000,"updated_at":1650639101000,"closed_at":1650638730000,"author_association":"MEMBER","active_lock_reason":null,"body":"This should fix all the issue we have with in place operations in mapping functions. This is crucial as where we do some weird things like:\r\n```\r\ndef apply(batch):\r\n    batch_size = len(batch[\"id\"])\r\n    batch[\"text\"] = [\"potato\" for _ range(batch_size)]\r\n    return {}\r\n\r\n# Columns are: {\"id\": int}\r\ndset.map(apply, batched=True, remove_columns=\"text\") # crashes because `text` is not in the original columns\r\ndset.map(apply, batched=True) # mapped datasets has `text` column\r\n```\r\n\r\nIn this PR we suggest to have `remove_columns=True` so that we ignore the input completely, and just use the output to generate mapped dataset. This means that inplace operations won't have any effects anymore.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4197\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4197\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4197","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4197","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4197.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4197.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4196","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4196\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4196\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4196\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4196","id":1211271261,"node_id":"I_kwDODunzps5IMohd","number":4196,"title":"Embed image and audio files in `save_to_disk`","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1650558318000,"updated_at":1650558318000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Following https:\/\/github.com\/huggingface\/datasets\/pull\/4184, currently a dataset saved using `save_to_disk` doesn't actually contain the bytes of the image or audio files. Instead it stores the path to your local files. \r\n\r\nAdding `embed_external_files` and set it to True by default to save_to_disk would be kind of a breaking change since some users will get bigger Arrow files when updating the lib, but the advantages are nice:\r\n\r\n- the resulting dataset is self contained, in case you want to delete your cache for example or share it with someone else\r\n- users also upload these Arrow files to cloud storage via the fs parameter, and in this case they would expect to upload a self-contained dataset\r\n- consistency with push_to_hub\r\n\r\nThis can be implemented at the same time as sharding for `save_to_disk` for efficiency, and reuse the helpers from `push_to_hub` to embed the external files.\r\n\r\ncc @mariosasko ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4196\/reactions","total_count":5,"+1":5,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4196\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4194","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4194\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4194\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4194\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4194","id":1210958602,"node_id":"PR_kwDODunzps42jjD3","number":4194,"title":"Support lists of multi-dimensional numpy arrays","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4194). All of your documentation changes will be reflected on that endpoint."],"created_at":1650543746000,"updated_at":1650693116000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Fix #4191.\r\n\r\nCC: @SaulLu ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4194\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4194\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4194","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4194","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4194.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4194.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4193","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4193\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4193\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4193\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4193","id":1210734701,"node_id":"PR_kwDODunzps42izQG","number":4193,"title":"Document save_to_disk and push_to_hub on images and audio files","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Good catch, I updated the docstrings"],"created_at":1650531876000,"updated_at":1650621355000,"closed_at":1650620971000,"author_association":"MEMBER","active_lock_reason":null,"body":"Following https:\/\/github.com\/huggingface\/datasets\/pull\/4187, I explained in the documentation of `save_to_disk` and `push_to_hub` how they handle image and audio data.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4193\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4193\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4193","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4193","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4193.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4193.patch","merged_at":1650620971000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4192","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4192\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4192\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4192\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4192","id":1210692554,"node_id":"I_kwDODunzps5IKbPK","number":4192,"title":"load_dataset can't load local dataset,Unable to find ...","user":{"login":"ahf876828330","id":33253979,"node_id":"MDQ6VXNlcjMzMjUzOTc5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/33253979?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ahf876828330","html_url":"https:\/\/github.com\/ahf876828330","followers_url":"https:\/\/api.github.com\/users\/ahf876828330\/followers","following_url":"https:\/\/api.github.com\/users\/ahf876828330\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ahf876828330\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ahf876828330\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ahf876828330\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ahf876828330\/orgs","repos_url":"https:\/\/api.github.com\/users\/ahf876828330\/repos","events_url":"https:\/\/api.github.com\/users\/ahf876828330\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ahf876828330\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi! :)\r\n\r\nI believe that should work unless `dataset_infos.json` isn't actually a dataset. For Hugging Face datasets, there is usually a file named `dataset_infos.json` which contains metadata about the dataset (eg. the dataset citation, license, description, etc). Can you double-check that `dataset_infos.json` isn't just metadata please?","Hi @ahf876828330, \r\n\r\nAs @stevhliu pointed out, the proper way to load a dataset is not trying to load its metadata file.\r\n\r\nIn your case, as the dataset script is local, you should better point to your local loading script:\r\n```python\r\ndataset = load_dataset(\"dataset\/opus_books.py\")\r\n```\r\n\r\nPlease, feel free to re-open this issue if the previous code snippet does not work for you.","> Hi! :)\r\n> \r\n> I believe that should work unless `dataset_infos.json` isn't actually a dataset. For Hugging Face datasets, there is usually a file named `dataset_infos.json` which contains metadata about the dataset (eg. the dataset citation, license, description, etc). Can you double-check that `dataset_infos.json` isn't just metadata please?\r\n\r\nYes\uff0cyou are right!So if I have a metadata dataset local,How can I turn it to a dataset that can be used by the load_dataset() function\uff1fAre there some examples?","The metadata file isn't a dataset so you can't turn it into one. You should try @albertvillanova's code snippet above (now merged in the docs [here](https:\/\/huggingface.co\/docs\/datasets\/master\/en\/loading#local-loading-script)), which uses your local loading script `opus_books.py` to:\r\n\r\n1. Download the actual dataset. \r\n2. Once the dataset is downloaded, `load_dataset` will load it for you."],"created_at":1650529738000,"updated_at":1650905517000,"closed_at":1650613193000,"author_association":"NONE","active_lock_reason":null,"body":"\r\nTraceback (most recent call last):\r\n  File \"\/home\/gs603\/ahf\/pretrained\/model.py\", line 48, in <module>\r\n    dataset = load_dataset(\"json\",data_files=\"dataset\/dataset_infos.json\")\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/load.py\", line 1675, in load_dataset\r\n    **config_kwargs,\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/load.py\", line 1496, in load_dataset_builder\r\n    data_files=data_files,\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/load.py\", line 1155, in dataset_module_factory\r\n    download_mode=download_mode,\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/load.py\", line 800, in get_module\r\n    data_files = DataFilesDict.from_local_or_remote(patterns, use_auth_token=self.downnload_config.use_auth_token)\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/data_files.py\", line 582, in from_local_or_remote\r\n    if not isinstance(patterns_for_key, DataFilesList)\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/data_files.py\", line 544, in from_local_or_remote\r\n    data_files = resolve_patterns_locally_or_by_urls(base_path, patterns, allowed_extensions)\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/data_files.py\", line 194, in resolve_patterns_locally_or_by_urls\r\n    for path in _resolve_single_pattern_locally(base_path, pattern, allowed_extensions):\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/data_files.py\", line 144, in _resolve_single_pattern_locally\r\n    raise FileNotFoundError(error_msg)\r\nFileNotFoundError: Unable to find '\/home\/gs603\/ahf\/pretrained\/dataset\/dataset_infos.json' at \/home\/gs603\/ahf\/pretrained\r\n\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/33253979\/164413285-84ea65ac-9126-408f-9cd2-ce4751a5dd73.png)\r\n![image](https:\/\/user-images.githubusercontent.com\/33253979\/164413338-4735142f-408b-41d9-ab87-8484de2be54f.png)\r\n\r\nthe code is in the model.py,why I can't use the load_dataset function to load my local dataset?","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4192\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4192\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4191","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4191\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4191\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4191\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4191","id":1210028090,"node_id":"I_kwDODunzps5IH5A6","number":4191,"title":"feat: create an `Array3D` column from a list of arrays of dimension 2","user":{"login":"SaulLu","id":55560583,"node_id":"MDQ6VXNlcjU1NTYwNTgz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/55560583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/SaulLu","html_url":"https:\/\/github.com\/SaulLu","followers_url":"https:\/\/api.github.com\/users\/SaulLu\/followers","following_url":"https:\/\/api.github.com\/users\/SaulLu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/SaulLu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/SaulLu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/SaulLu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/SaulLu\/orgs","repos_url":"https:\/\/api.github.com\/users\/SaulLu\/repos","events_url":"https:\/\/api.github.com\/users\/SaulLu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/SaulLu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi @SaulLu, thanks for your proposal.\r\n\r\nJust I got a bit confused about the dimensions...\r\n- For the 2D case, you mention it is possible to create an `Array2D` from a list of arrays of dimension 1\r\n- However, you give an example of creating an `Array2D` from arrays of dimension 2:\r\n  - the values of `data_map` are arrays of dimension 2\r\n  - the outer list in `prepare_dataset_2D` should not be taken into account in the dimension counting, as it is used because in `map` you pass `batched=True`\r\n\r\nNote that for the 3D alternatives you mention:\r\n- In `prepare_dataset_3D_ter`, you create an `Array3D` from arrays of dimension 3:\r\n  - the array `data_map[index][np.newaxis, :, :]` has dimension 3\r\n  - the outer list in `prepare_dataset_3D_ter` is the one used by `batched=True`\r\n- In `prepare_dataset_3D_bis`, you create an `Array3D` from a list of list of lists:\r\n  - the value of `data_map[index].tolist()` is a list of lists\r\n  - it is enclosed by another list `[data_map[index].tolist()]`, thus giving a list of list of lists\r\n  - the outer list is the one used by `batched=True`\r\n\r\nTherefore, if I understand correctly, your request would be to be able to create an `Array3D` from a list of an array of dimension 2:\r\n- In `prepare_dataset_3D`, `data_map[index]` is an array of dimension 2\r\n- it is enclosed by a list `[data_map[index]]`, thus giving a list of an array of dimension 2\r\n- the outer list is the one used by `batched=True`\r\n\r\nPlease, feel free to tell me if I did not understand you correctly.","Hi @albertvillanova ,\r\n\r\nIndeed my message was confusing and you guessed right :smile: : I think would be interesting to be able to create an Array3D from a list of an array of dimension 2. \r\n\r\nFor the 2D case I should have given as a \"similar\" example:\r\n```python\r\n\r\ndata_map_1D = {\r\n    1: np.array([0.2, 0.4]),\r\n    2: np.array([0.1, 0.4]),\r\n}\r\n\r\ndef prepare_dataset_2D(batch):\r\n    batch[\"pixel_values\"] = [[data_map_1D[index]] for index in batch[\"id\"]]\r\n    return batch\r\n    \r\nds_2D = ds.map(\r\n    prepare_dataset_2D, \r\n    batched=True, \r\n    remove_columns=ds.column_names, \r\n    features=features.Features({\"pixel_values\": features.Array2D(shape=(1, 2), dtype=\"float32\")})\r\n)\r\n```"],"created_at":1650477872000,"updated_at":1650543737000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Is your feature request related to a problem? Please describe.**\r\n\r\nIt is possible to create an `Array2D` column from a list of arrays of dimension 1. Similarly, I think it might be nice to be able to create a `Array3D` column from a list of lists of arrays of dimension 1.\r\n\r\nTo illustrate my proposal, let's take the following toy dataset t:\r\n```python\r\nimport numpy as np\r\nfrom datasets import Dataset, features\r\n\r\ndata_map = {\r\n    1: np.array([[0.2, 0,4],[0.19, 0,3]]),\r\n    2: np.array([[0.1, 0,4],[0.19, 0,3]]),\r\n}\r\n\r\ndef create_toy_ds():\r\n    my_dict = {\"id\":[1, 2]}\r\n    return Dataset.from_dict(my_dict)\r\n\r\nds = create_toy_ds()\r\n```\r\n\r\nThe following 2D processing works without any errors raised:\r\n```python\r\ndef prepare_dataset_2D(batch):\r\n    batch[\"pixel_values\"] = [data_map[index] for index in batch[\"id\"]]\r\n    return batch\r\n    \r\nds_2D = ds.map(\r\n    prepare_dataset_2D, \r\n    batched=True, \r\n    remove_columns=ds.column_names, \r\n    features=features.Features({\"pixel_values\": features.Array2D(shape=(2, 3), dtype=\"float32\")})\r\n)\r\n```\r\n\r\nThe following 3D processing doesn't work:\r\n```python\r\ndef prepare_dataset_3D(batch):\r\n    batch[\"pixel_values\"] = [[data_map[index]] for index in batch[\"id\"]]\r\n    return batch\r\n    \r\nds_3D = ds.map(\r\n    prepare_dataset_3D, \r\n    batched=True, \r\n    remove_columns=ds.column_names, \r\n    features=features.Features({\"pixel_values\": features.Array3D(shape=(1,  2, 3, dtype=\"float32\")})\r\n)\r\n```\r\nThe error raised is:\r\n```\r\n---------------------------------------------------------------------------\r\nArrowInvalid                              Traceback (most recent call last)\r\n[<ipython-input-6-676547e4cd41>](https:\/\/localhost:8080\/#) in <module>()\r\n      3     batched=True,\r\n      4     remove_columns=ds.column_names,\r\n----> 5     features=features.Features({\"pixel_values\": features.Array3D(shape=(1, 2, 3), dtype=\"float32\")})\r\n      6 )\r\n\r\n12 frames\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\r\n   1971                 new_fingerprint=new_fingerprint,\r\n   1972                 disable_tqdm=disable_tqdm,\r\n-> 1973                 desc=desc,\r\n   1974             )\r\n   1975         else:\r\n\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in wrapper(*args, **kwargs)\r\n    518             self: \"Dataset\" = kwargs.pop(\"self\")\r\n    519         # apply actual function\r\n--> 520         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    521         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    522         for dataset in datasets:\r\n\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in wrapper(*args, **kwargs)\r\n    485         }\r\n    486         # apply actual function\r\n--> 487         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    488         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    489         # re-apply format to the output\r\n\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/fingerprint.py](https:\/\/localhost:8080\/#) in wrapper(*args, **kwargs)\r\n    456             # Call actual function\r\n    457 \r\n--> 458             out = func(self, *args, **kwargs)\r\n    459 \r\n    460             # Update fingerprint of in-place transforms + update in-place history of transforms\r\n\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in _map_single(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\r\n   2354                                 writer.write_table(batch)\r\n   2355                             else:\r\n-> 2356                                 writer.write_batch(batch)\r\n   2357                 if update_data and writer is not None:\r\n   2358                     writer.finalize()  # close_stream=bool(buf_writer is None))  # We only close if we are writing in a file\r\n\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_writer.py](https:\/\/localhost:8080\/#) in write_batch(self, batch_examples, writer_batch_size)\r\n    505             col_try_type = try_features[col] if try_features is not None and col in try_features else None\r\n    506             typed_sequence = OptimizedTypedSequence(batch_examples[col], type=col_type, try_type=col_try_type, col=col)\r\n--> 507             arrays.append(pa.array(typed_sequence))\r\n    508             inferred_features[col] = typed_sequence.get_inferred_type()\r\n    509         schema = inferred_features.arrow_schema if self.pa_writer is None else self.schema\r\n\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pyarrow\/array.pxi in pyarrow.lib.array()\r\n\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pyarrow\/array.pxi in pyarrow.lib._handle_arrow_array_protocol()\r\n\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_writer.py](https:\/\/localhost:8080\/#) in __arrow_array__(self, type)\r\n    175                     storage = list_of_np_array_to_pyarrow_listarray(data, type=pa_type.value_type)\r\n    176                 else:\r\n--> 177                     storage = pa.array(data, pa_type.storage_dtype)\r\n    178                 return pa.ExtensionArray.from_storage(pa_type, storage)\r\n    179 \r\n\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pyarrow\/array.pxi in pyarrow.lib.array()\r\n\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pyarrow\/array.pxi in pyarrow.lib._sequence_to_array()\r\n\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pyarrow\/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pyarrow\/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: Can only convert 1-dimensional array values\r\n```\r\n\r\n**Describe the solution you'd like**\r\nNo error in the second scenario and an identical result to the following snippets.\r\n\r\n**Describe alternatives you've considered**\r\nThere are other alternatives that work such as:\r\n```python\r\n\r\ndef prepare_dataset_3D_bis(batch):\r\n    batch[\"pixel_values\"] = [[data_map[index].tolist()] for index in batch[\"id\"]]\r\n    return batch\r\n\r\nds_3D_bis = ds.map(\r\n    prepare_dataset_3D_bis, \r\n    batched=True, \r\n    remove_columns=ds.column_names, \r\n    features=features.Features({\"pixel_values\": features.Array3D(shape=(1, 2, 3), dtype=\"float32\")})\r\n)\r\n```\r\nor\r\n```python\r\ndef prepare_dataset_3D_ter(batch):\r\n    batch[\"pixel_values\"] = [data_map[index][np.newaxis, :, :] for index in batch[\"id\"]]\r\n    return batch\r\n\r\nds_3D_ter = ds.map(\r\n    prepare_dataset_3D_ter, \r\n    batched=True, \r\n    remove_columns=ds.column_names, \r\n    features=features.Features({\"pixel_values\": features.Array3D(shape=(1,  2, 3), dtype=\"float32\")})\r\n)\r\n```\r\nBut both solutions require the user to be aware that `data_map[index]` is an `np.array` type.\r\n\r\ncc @lhoestq as we discuss this offline :smile: ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4191\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4191\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4190","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4190\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4190\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4190\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4190","id":1209901677,"node_id":"PR_kwDODunzps42gK3y","number":4190,"title":"Deprecate `shard_size` in `push_to_hub` in favor of `max_shard_size`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650470881000,"updated_at":1650635905000,"closed_at":1650635520000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"This PR adds a `max_shard_size` param to `push_to_hub` and deprecates `shard_size` in favor of this new param to have a more descriptive name (a shard has at most the `shard_size` bytes in `push_to_hub`) for the param and to align the API with [Transformers](https:\/\/github.com\/huggingface\/transformers\/blob\/ff06b177917384137af2d9585697d2d76c40cdfc\/src\/transformers\/modeling_utils.py#L1350).\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4190\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4190\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4190","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4190","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4190.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4190.patch","merged_at":1650635520000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4189","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4189\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4189\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4189\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4189","id":1209881351,"node_id":"PR_kwDODunzps42gGv5","number":4189,"title":"Document how to use FAISS index for special operations","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650469916000,"updated_at":1651826590000,"closed_at":1651826152000,"author_association":"MEMBER","active_lock_reason":null,"body":"Document how to use FAISS index for special operations, by accessing the index itself.\r\n\r\nClose #4029.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4189\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4189\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4189","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4189","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4189.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4189.patch","merged_at":1651826152000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4188","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4188\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4188\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4188\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4188","id":1209740957,"node_id":"PR_kwDODunzps42fpMv","number":4188,"title":"Support streaming cnn_dailymail dataset","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650463476000,"updated_at":1651826377000,"closed_at":1650469969000,"author_association":"MEMBER","active_lock_reason":null,"body":"Support streaming cnn_dailymail dataset.\r\n\r\nFix #3969.\r\n\r\nCC: @severo ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4188\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4188\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4188","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4188","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4188.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4188.patch","merged_at":1650469969000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4187","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4187\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4187\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4187\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4187","id":1209721532,"node_id":"PR_kwDODunzps42flGp","number":4187,"title":"Don't duplicate data when encoding audio or image","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","I'm not familiar with the concept of streaming vs non-streaming in HF datasets. I just wonder that you have the distinction here. Why doesn't it work to always make use of `bytes`? \"using a local file - which is often required for audio\" - why would that be?\r\n\r\nThe `path` would always point to some location in the `cache_dir`? I think this can be problematic. I would have expected that after I did `dataset.save_to_disk(...)` that I can remove the cache dir. But maybe just because I'm not familiar with HF. Or maybe the docs can be improved to clarify this.\r\n","We could always load every data file into `bytes` and save it this way the audio as bytes in `arrow` format, but the problem then would be that it makes the `file` column useless, *i.e.* people cannot inspect the audio file locally anymore or else they would need to first save bytes as a file which is not evident. This either breaks backwards compatibility or forces the user to stored 2x the required size locally. There was a longer discussion here: https:\/\/github.com\/huggingface\/datasets\/issues\/3663\r\n\r\nIt's a good argument though that `dataset.save_to_disk(...)` should save everything that is needed to the disk and should be independent of other folders, but I do think the arguments of #3663 to not break backwards compatibility and to allow people to inspect the downloaded audio files locally are a bit more important here. \r\n\r\nBut maybe, we could add a flag, `save_files_as_bytes` or `make_independent`, `make_self_contained` or a better name to `save_to_disk(...)` and `push_to_hub(...)` that would allow to make the resulting folder completely independent. ","What do you think @mariosasko @lhoestq @polinaeterna @anton-l ?\r\n","For context: you can either store the path to local images or audio files, or the bytes of those files.\r\n\r\nIf your images and audio files are local files, then the arrow file from `save_to_disk` will store paths to these files.\r\nIf you want to include the bytes or your images or audio files instead, you must `read()` those files first.\r\nThis can be done by storing the \"bytes\" instead of the \"path\" of the images or audio files.\r\n\r\nOn the other hand, the resulting Parquet files from `push_to_hub` are self-contained, so that anyone can reload the dataset from the Hub. If your dataset contains image or audio data, the Parquet files will store the bytes of your images or audio files.\r\n\r\nFor now I just updated the documentation: https:\/\/github.com\/huggingface\/datasets\/pull\/4193. Maybe we can also embed the image and audio bytes in `save_to_disk` when we implement sharding, so that is can be done as efficiently as `push_to_hub`.\r\n\r\nAnyway, merging this one :)"],"created_at":1650462637000,"updated_at":1650532620000,"closed_at":1650532247000,"author_association":"MEMBER","active_lock_reason":null,"body":"Right now if you pass both the `bytes` and a local `path` for audio or image data, then the `bytes` are unnecessarily written in the Arrow file, while we could just keep the local `path`.\r\n\r\nThis PR discards the `bytes` when the audio or image file exists locally.\r\n\r\nIn particular it's common for audio datasets builders to provide both the bytes and the local path in order to work for both streaming (using the bytes) and non-streaming mode (using a local file - which is often required for audio).\r\n\r\ncc @patrickvonplaten ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4187\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4187\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4187","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4187","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4187.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4187.patch","merged_at":1650532247000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4291","id":1227777500,"node_id":"I_kwDODunzps5JLmXc","number":4291,"title":"Dataset Viewer issue for strombergnlp\/ipm_nel : preview is empty, no error message","user":{"login":"leondz","id":121934,"node_id":"MDQ6VXNlcjEyMTkzNA==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/121934?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/leondz","html_url":"https:\/\/github.com\/leondz","followers_url":"https:\/\/api.github.com\/users\/leondz\/followers","following_url":"https:\/\/api.github.com\/users\/leondz\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/leondz\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/leondz\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/leondz\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/leondz\/orgs","repos_url":"https:\/\/api.github.com\/users\/leondz\/repos","events_url":"https:\/\/api.github.com\/users\/leondz\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/leondz\/received_events","type":"User","site_admin":false},"labels":[{"id":3470211881,"node_id":"LA_kwDODunzps7O1zsp","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer","name":"dataset-viewer","color":"E5583E","default":false,"description":"Related to the dataset viewer on huggingface.co"}],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi @leondz, thanks for reporting.\r\n\r\nIndeed, the dataset viewer relies on the dataset being streamable (passing `streaming=True` to `load_dataset`). Whereas most of the datastes are streamable out of the box (thanks to our implementation of streaming), there are still some exceptions.\r\n\r\nIn particular, in your case, that is due to the data file being TAR. This format is not streamable out of the box (it does not allow random access to the archived files), but we use a trick to allow streaming: using `dl_manager.iter_archive`.\r\n\r\nLet me know if you need some help: I could push a commit to your repo with the fix."],"created_at":1651838607000,"updated_at":1651848811000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Link\n\nhttps:\/\/huggingface.co\/datasets\/strombergnlp\/ipm_nel\/viewer\/ipm_nel\/train\n\n### Description\n\nThe viewer is blank. I tried my best to emulate a dataset with a working viewer, but this one just doesn't seem to want to come up. What did I miss?\n\n### Owner\n\nYes","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4291\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4290","id":1227592826,"node_id":"PR_kwDODunzps43Zr08","number":4290,"title":"Update README.md","user":{"login":"monk1337","id":17107749,"node_id":"MDQ6VXNlcjE3MTA3NzQ5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17107749?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/monk1337","html_url":"https:\/\/github.com\/monk1337","followers_url":"https:\/\/api.github.com\/users\/monk1337\/followers","following_url":"https:\/\/api.github.com\/users\/monk1337\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/monk1337\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/monk1337\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/monk1337\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/monk1337\/orgs","repos_url":"https:\/\/api.github.com\/users\/monk1337\/repos","events_url":"https:\/\/api.github.com\/users\/monk1337\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/monk1337\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4290). All of your documentation changes will be reflected on that endpoint."],"created_at":1651827171000,"updated_at":1651827848000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Updating readme in medmcqa dataset.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4290\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4290","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4290","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4290.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4290.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4288","id":1226821732,"node_id":"PR_kwDODunzps43XLKi","number":4288,"title":"Add missing `faiss` import to fix https:\/\/github.com\/huggingface\/datasets\/issues\/4287","user":{"login":"alvarobartt","id":36760800,"node_id":"MDQ6VXNlcjM2NzYwODAw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36760800?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alvarobartt","html_url":"https:\/\/github.com\/alvarobartt","followers_url":"https:\/\/api.github.com\/users\/alvarobartt\/followers","following_url":"https:\/\/api.github.com\/users\/alvarobartt\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alvarobartt\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alvarobartt\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alvarobartt\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alvarobartt\/orgs","repos_url":"https:\/\/api.github.com\/users\/alvarobartt\/repos","events_url":"https:\/\/api.github.com\/users\/alvarobartt\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alvarobartt\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1651764109000,"updated_at":1651764109000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"This PR fixes the issue recently mentioned in https:\/\/github.com\/huggingface\/datasets\/issues\/4287 \ud83e\udd17 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4288\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4288","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4288","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4288.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4288.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4287","id":1226806652,"node_id":"I_kwDODunzps5JH5V8","number":4287,"title":"\"NameError: name 'faiss' is not defined\" on `.add_faiss_index` when `device` is not None","user":{"login":"alvarobartt","id":36760800,"node_id":"MDQ6VXNlcjM2NzYwODAw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36760800?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alvarobartt","html_url":"https:\/\/github.com\/alvarobartt","followers_url":"https:\/\/api.github.com\/users\/alvarobartt\/followers","following_url":"https:\/\/api.github.com\/users\/alvarobartt\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alvarobartt\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alvarobartt\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alvarobartt\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alvarobartt\/orgs","repos_url":"https:\/\/api.github.com\/users\/alvarobartt\/repos","events_url":"https:\/\/api.github.com\/users\/alvarobartt\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alvarobartt\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["So I managed to solve this by adding a missing `import faiss` in the `@staticmethod` defined in https:\/\/github.com\/huggingface\/datasets\/blob\/f51b6994db27ea69261ef919fb7775928f9ec10b\/src\/datasets\/search.py#L305, triggered from https:\/\/github.com\/huggingface\/datasets\/blob\/f51b6994db27ea69261ef919fb7775928f9ec10b\/src\/datasets\/search.py#L249 when trying to `ds_with_embeddings.add_faiss_index(column='embeddings', device=0)` with the code above.\r\n\r\nAs it seems that the `@staticmethod` doesn't recognize the `import faiss` defined in https:\/\/github.com\/huggingface\/datasets\/blob\/f51b6994db27ea69261ef919fb7775928f9ec10b\/src\/datasets\/search.py#L261, so whenever the value of `device` is not None in https:\/\/github.com\/huggingface\/datasets\/blob\/71f76e0bdeaddadedc4f9c8d15cfff5a36d62f66\/src\/datasets\/search.py#L438, that exception is triggered.\r\n\r\nSo on, adding `import faiss` inside https:\/\/github.com\/huggingface\/datasets\/blob\/71f76e0bdeaddadedc4f9c8d15cfff5a36d62f66\/src\/datasets\/search.py#L305 right after the check of `device`'s value, solves the issue and lets you calculate the indices in GPU.\r\n\r\nI'll add the code in a PR linked to this issue in case you want to merge it!","Adding here the complete error traceback!\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/home\/alvarobartt\/lol.py\", line 12, in <module>\r\n    ds_with_embeddings.add_faiss_index(column='embeddings', device=0) # default `device=None`\r\n  File \"\/home\/alvarobartt\/.local\/lib\/python3.9\/site-packages\/datasets\/arrow_dataset.py\", line 3656, in add_faiss_index\r\n    super().add_faiss_index(\r\n  File \"\/home\/alvarobartt\/.local\/lib\/python3.9\/site-packages\/datasets\/search.py\", line 478, in add_faiss_index\r\n    faiss_index.add_vectors(self, column=column, train_size=train_size, faiss_verbose=True)\r\n  File \"\/home\/alvarobartt\/.local\/lib\/python3.9\/site-packages\/datasets\/search.py\", line 281, in add_vectors\r\n    self.faiss_index = self._faiss_index_to_device(index, self.device)\r\n  File \"\/home\/alvarobartt\/.local\/lib\/python3.9\/site-packages\/datasets\/search.py\", line 327, in _faiss_index_to_device\r\n    faiss_res = faiss.StandardGpuResources()\r\nNameError: name 'faiss' is not defined\r\n```"],"created_at":1651763385000,"updated_at":1651837063000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\n\r\nWhen using `datasets` to calculate the FAISS indices of a dataset, the exception `NameError: name 'faiss' is not defined` is triggered when trying to calculate those on a device (GPU), so `.add_faiss_index(..., device=0)` fails with that exception.\r\n\r\nAll that assuming that `datasets` is properly installed and `faiss-gpu` too, as well as all the CUDA drivers required.\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\n# Sample code to reproduce the bug\r\nfrom transformers import DPRContextEncoder, DPRContextEncoderTokenizer\r\nimport torch\r\ntorch.set_grad_enabled(False)\r\nctx_encoder = DPRContextEncoder.from_pretrained(\"facebook\/dpr-ctx_encoder-single-nq-base\")\r\nctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook\/dpr-ctx_encoder-single-nq-base\")\r\n\r\nfrom datasets import load_dataset\r\nds = load_dataset('crime_and_punish', split='train[:100]')\r\nds_with_embeddings = ds.map(lambda example: {'embeddings': ctx_encoder(**ctx_tokenizer(example[\"line\"], return_tensors=\"pt\"))[0][0].numpy()})\r\n\r\nds_with_embeddings.add_faiss_index(column='embeddings', device=0) # default `device=None`\r\n```\r\n\r\n## Expected results\r\n\r\nA new column named `embeddings` in the dataset that we're adding the index to.\r\n\r\n## Actual results\r\n\r\nAn exception is triggered with the following message `NameError: name 'faiss' is not defined`.\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 2.1.0\r\n- Platform: Linux-5.13.0-1022-azure-x86_64-with-glibc2.31\r\n- Python version: 3.9.12\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.2\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4287\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4286","id":1226758621,"node_id":"PR_kwDODunzps43W-DI","number":4286,"title":"Add Lahnda language tag","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4286). All of your documentation changes will be reflected on that endpoint."],"created_at":1651761260000,"updated_at":1651762010000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"This language is present in [Wikimedia's WIT](https:\/\/huggingface.co\/datasets\/wikimedia\/wit_base) dataset.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4286\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4286","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4286","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4286.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4286.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4285","id":1226374831,"node_id":"PR_kwDODunzps43VtEa","number":4285,"title":"Update LexGLUE README.md","user":{"login":"iliaschalkidis","id":1626984,"node_id":"MDQ6VXNlcjE2MjY5ODQ=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1626984?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/iliaschalkidis","html_url":"https:\/\/github.com\/iliaschalkidis","followers_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/followers","following_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/orgs","repos_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/repos","events_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/iliaschalkidis\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651739810000,"updated_at":1651757944000,"closed_at":1651757615000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Update the leaderboard based on the latest results presented in the ACL 2022 version of the article.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4285\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4285","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4285","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4285.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4285.patch","merged_at":1651757615000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4284","id":1226200727,"node_id":"I_kwDODunzps5JFlaX","number":4284,"title":"Issues in processing very large datasets","user":{"login":"sajastu","id":10419055,"node_id":"MDQ6VXNlcjEwNDE5MDU1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10419055?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sajastu","html_url":"https:\/\/github.com\/sajastu","followers_url":"https:\/\/api.github.com\/users\/sajastu\/followers","following_url":"https:\/\/api.github.com\/users\/sajastu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sajastu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sajastu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sajastu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sajastu\/orgs","repos_url":"https:\/\/api.github.com\/users\/sajastu\/repos","events_url":"https:\/\/api.github.com\/users\/sajastu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sajastu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1651726869000,"updated_at":1651726869000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\nI'm trying to add a feature called \"subgraph\" to CNN\/DM dataset (modifications on run_summarization.py of Huggingface Transformers script) --- I'm not quite sure if I'm doing it the right way, though--- but the main problem appears when the training starts where the error ` [OSError: [Errno 12] Cannot allocate memory]`  appears. I suppose this problem roots in RAM issues and how the dataset is loaded during training, but I have no clue of what I can do to fix it.  Observing the dataset's cache directory, I see that it takes ~600GB of memory and that's why I believe special care is needed when loading it into the memory. \r\n\r\n\r\nHere are my modifications to `run_summarization.py` code. \r\n\r\n\r\n```\r\n# loading pre-computed dictionary where keys are 'id' of article and values are corresponding subgraph\r\ngraph_data_train = get_graph_data('train') \r\ngraph_data_validation = get_graph_data('val')\r\n...\r\n...\r\n\r\n\r\nwith training_args.main_process_first(desc=\"train dataset map pre-processing\"):\r\n    train_dataset = train_dataset.map(\r\n        preprocess_function_train,\r\n        batched=True,\r\n        num_proc=data_args.preprocessing_num_workers,\r\n        remove_columns=column_names,\r\n        load_from_cache_file=not data_args.overwrite_cache,\r\n        desc=\"Running tokenizer on train dataset\",\r\n    )\r\n\r\n```\r\n\r\n\r\nAnd here is the modified preprocessed function:\r\n\r\n```\r\ndef preprocess_function_train(examples):\r\n        inputs, targets, sub_graphs, ids = [], [], [], []\r\n        for i in range(len(examples[text_column])):\r\n            if examples[text_column][i] is not None and examples[summary_column][i] is not None:\r\n                # if examples['doc_id'][i] in graph_data.keys():\r\n                inputs.append(examples[text_column][i])\r\n                targets.append(examples[summary_column][i])\r\n                sub_graphs.append(graph_data_train[examples['id'][i]])\r\n                ids.append(examples['id'][i])\r\n\r\n        inputs = [prefix + inp for inp in inputs]\r\n        model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True,\r\n                                 sub_graphs=sub_graphs, ids=ids)\r\n\r\n            # Setup the tokenizer for targets\r\n        with tokenizer.as_target_tokenizer():\r\n            labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\r\n\r\n        # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\r\n        # padding in the loss.\r\n        if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\r\n            labels[\"input_ids\"] = [\r\n                [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\r\n            ]\r\n\r\n        model_inputs[\"labels\"] = labels[\"input_ids\"]\r\n        return model_inputs\r\n```\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:  2.1.0\r\n- Platform: Linux Ubuntu\r\n- Python version: 3.6\r\n- PyArrow version: 6.0.1\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4284\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4283","id":1225686988,"node_id":"PR_kwDODunzps43Tnxo","number":4283,"title":"Fix filesystem docstring","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651686162000,"updated_at":1651854722000,"closed_at":1651818137000,"author_association":"MEMBER","active_lock_reason":null,"body":"This PR untangles the `S3FileSystem` docstring so the [parameters](https:\/\/huggingface.co\/docs\/datasets\/master\/en\/package_reference\/main_classes#parameters) are properly displayed.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4283\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4283","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4283","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4283.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4283.patch","merged_at":1651818137000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4282","id":1225616545,"node_id":"PR_kwDODunzps43TZYL","number":4282,"title":"Don't do unnecessary list type casting to avoid replacing None values by empty lists","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Quick question about the message in the warning. You say \"will be fixed in a future major version\" but don't you mean \"will raise an error in a future major version\"?","Right ! Good catch, thanks, I updated the message to say \"will raise an error in a future major version\""],"created_at":1651682221000,"updated_at":1651833838000,"closed_at":1651833420000,"author_association":"MEMBER","active_lock_reason":null,"body":"In certain cases, `None` values are replaced by empty lists when casting feature types.\r\n\r\nIt happens every time you cast an array of nested lists like [None, [0, 1, 2, 3]] to a different type (to change the integer precision for example). In this case you'd get [[], [0, 1, 2, 3]] for example. This issue comes from PyArrow, see the discussion in https:\/\/github.com\/huggingface\/datasets\/issues\/3676\r\n\r\nThis issue also happens when no type casting is needed, because casting is supposed to be a no-op in this case. But as https:\/\/github.com\/huggingface\/datasets\/issues\/3676 shown, it's not the case and `None` are replaced by empty lists even if we cast to the exact same type.\r\n\r\nIn this PR I just workaround this bug in the case where no type casting is needed. In particular, I only call `pa.ListArray.from_arrays` only when necessary.\r\n\r\nI also added a warning when some `None` are effectively replaced by empty lists. I wanted to raise an error in this case, but maybe we should wait a major update to do so\r\n\r\nThis PR fixes this particular case, that is occurring in `run_qa.py` in `transformers`:\r\n```python\r\nfrom datasets import Dataset\r\n\r\nds = Dataset.from_dict({\"a\": range(4)})\r\nds = ds.map(lambda x: {\"b\": [[None, [0]]]}, batched=True, batch_size=1, remove_columns=[\"a\"])\r\nprint(ds.to_pandas())\r\n# before:\r\n#              b\r\n# 0  [None, [0]]\r\n# 1    [[], [0]]\r\n# 2    [[], [0]]\r\n# 3    [[], [0]]\r\n#\r\n# now:\r\n#              b\r\n# 0  [None, [0]]\r\n# 1  [None, [0]]\r\n# 2  [None, [0]]\r\n# 3  [None, [0]]\r\n```\r\n\r\ncc @sgugger ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4282\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4282","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4282","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4282.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4282.patch","merged_at":1651833420000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4281","id":1225556939,"node_id":"PR_kwDODunzps43TNBm","number":4281,"title":"Remove a copy-paste sentence in dataset cards","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","The non-passing tests have nothing to do with this PR."],"created_at":1651678915000,"updated_at":1651826283000,"closed_at":1651689196000,"author_association":"MEMBER","active_lock_reason":null,"body":"Remove the following copy-paste sentence from dataset cards:\r\n```\r\nWe show detailed information for up to 5 configurations of the dataset.\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4281\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4281","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4281","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4281.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4281.patch","merged_at":1651689196000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4280","id":1225446844,"node_id":"PR_kwDODunzps43S2xg","number":4280,"title":"Add missing features to commonsense_qa dataset","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","@albertvillanova it adds question_concept and id which is great. I suppose we'll talk about staying true to the format on another PR. ","Yes, let's merge this PR as it is: it adds missing features.\r\n\r\nA subsequent PR may address the request on changing the dataset feature structure."],"created_at":1651674266000,"updated_at":1651847037000,"closed_at":1651846606000,"author_association":"MEMBER","active_lock_reason":null,"body":"Fix partially #4275.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4280\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4280","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4280","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4280.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4280.patch","merged_at":1651846606000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4279","id":1225300273,"node_id":"PR_kwDODunzps43SXw5","number":4279,"title":"Update minimal PyArrow version warning","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651667169000,"updated_at":1651740658000,"closed_at":1651740227000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Update the minimal PyArrow version warning (should've been part of #4250). ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4279\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4279","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4279","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4279.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4279.patch","merged_at":1651740227000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4278","id":1225122123,"node_id":"PR_kwDODunzps43RyTs","number":4278,"title":"Add missing features to openbookqa dataset for additional config","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Let's merge this PR as it is: it adds missing features.\r\n\r\nA subsequent PR may address the request on changing the data feature structure."],"created_at":1651656170000,"updated_at":1651842800000,"closed_at":1651842361000,"author_association":"MEMBER","active_lock_reason":null,"body":"Fix partially #4276.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4278\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4278","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4278","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4278.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4278.patch","merged_at":1651842361000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4277","id":1225002286,"node_id":"PR_kwDODunzps43RZV9","number":4277,"title":"Enable label alignment for token classification datasets","user":{"login":"lewtun","id":26859204,"node_id":"MDQ6VXNlcjI2ODU5MjA0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/26859204?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lewtun","html_url":"https:\/\/github.com\/lewtun","followers_url":"https:\/\/api.github.com\/users\/lewtun\/followers","following_url":"https:\/\/api.github.com\/users\/lewtun\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lewtun\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lewtun\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lewtun\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lewtun\/orgs","repos_url":"https:\/\/api.github.com\/users\/lewtun\/repos","events_url":"https:\/\/api.github.com\/users\/lewtun\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lewtun\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Hmm, not sure why the Windows tests are failing with:\r\n\r\n```\r\nDid not find path entry C:\\tools\\miniconda3\\bin\r\nC:\\tools\\miniconda3\\envs\\py37\\python.exe: No module named pytest\r\n```\r\n\r\nEdit: running the CI again fixed the problem \ud83d\ude43 ","> One last nit and we can merge then\r\n\r\nThanks, done!"],"created_at":1651648516000,"updated_at":1651851735000,"closed_at":1651851391000,"author_association":"MEMBER","active_lock_reason":null,"body":"This PR extends the `Dataset.align_labels_with_mapping()` method to support alignment of label mappings between datasets and models for token classification (e.g. NER).\r\n\r\nExample of usage:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nner_ds = load_dataset(\"conll2003\", split=\"train\")\r\n# returns [3, 0, 7, 0, 0, 0, 7, 0, 0]\r\nner_ds[0][\"ner_tags\"]\r\n# hypothetical model mapping with O <--> B-LOC\r\nlabel2id = {\r\n    \"B-LOC\": \"0\",\r\n    \"B-MISC\": \"7\",\r\n    \"B-ORG\": \"3\",\r\n    \"B-PER\": \"1\",\r\n    \"I-LOC\": \"6\",\r\n    \"I-MISC\": \"8\",\r\n    \"I-ORG\": \"4\",\r\n    \"I-PER\": \"2\",\r\n    \"O\": \"5\"\r\n  }\r\nner_aligned_ds = ner_ds.align_labels_with_mapping(label2id, \"ner_tags\")\r\n# returns [3, 5, 7, 5, 5, 5, 7, 5, 5]\r\nner_aligned_ds[0][\"ner_tags\"]\r\n```\r\n\r\nContext: we need this in AutoTrain to automatically align datasets \/ models during evaluation. cc @abhishekkrthakur ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4277\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4277","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4277","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4277.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4277.patch","merged_at":1651851391000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4276","id":1224949252,"node_id":"I_kwDODunzps5JAz4E","number":4276,"title":"OpenBookQA has missing and inconsistent field names","user":{"login":"vblagoje","id":458335,"node_id":"MDQ6VXNlcjQ1ODMzNQ==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/458335?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/vblagoje","html_url":"https:\/\/github.com\/vblagoje","followers_url":"https:\/\/api.github.com\/users\/vblagoje\/followers","following_url":"https:\/\/api.github.com\/users\/vblagoje\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/vblagoje\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/vblagoje\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/vblagoje\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/vblagoje\/orgs","repos_url":"https:\/\/api.github.com\/users\/vblagoje\/repos","events_url":"https:\/\/api.github.com\/users\/vblagoje\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/vblagoje\/received_events","type":"User","site_admin":false},"labels":[{"id":2067388877,"node_id":"MDU6TGFiZWwyMDY3Mzg4ODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20bug","name":"dataset bug","color":"2edb81","default":false,"description":"A bug in a dataset script provided in the library"}],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Thanks for reporting, @vblagoje.\r\n\r\nIndeed, I noticed some of these issues while reviewing this PR:\r\n- #4259 \r\n\r\nThis is in my TODO list. ","Ok, awesome @albertvillanova How about #4275 ?","On the other hand, I am not sure if we should always preserve the original nested structure. I think we should also consider other factors as convenience or consistency.\r\n\r\nFor example, other datasets also flatten \"question.stem\" into \"question\":\r\n- ai2_arc:\r\n  ```python\r\n  question = data[\"question\"][\"stem\"]\r\n  choices = data[\"question\"][\"choices\"]\r\n  text_choices = [choice[\"text\"] for choice in choices]\r\n  label_choices = [choice[\"label\"] for choice in choices]\r\n  yield id_, {\r\n      \"id\": id_,\r\n      \"answerKey\": answerkey,\r\n      \"question\": question,\r\n      \"choices\": {\"text\": text_choices, \"label\": label_choices},\r\n  }\r\n  ```\r\n- commonsense_qa:\r\n  ```python\r\n  question = data[\"question\"]\r\n  stem = question[\"stem\"]\r\n  yield id_, {\r\n      \"answerKey\": answerkey,\r\n      \"question\": stem,\r\n      \"choices\": {\"label\": labels, \"text\": texts},\r\n  }\r\n  ```\r\n- cos_e:\r\n  ```python\r\n  \"question\": cqa[\"question\"][\"stem\"],\r\n  ```\r\n- qasc\r\n- quartz\r\n- wiqa\r\n\r\nExceptions:\r\n- exams\r\n\r\nI think we should agree on a CONVENIENT format for QA and use always CONSISTENTLY the same.","@albertvillanova I agree that we should be consistent. In the last month, I have come across tons of code that deals with OpenBookQA and CommonSenseQA and all of that code relies on the original data format structure. We can't expect users to adopt HF Datasets if we arbitrarily change the structure of the format just because we think something makes more sense. I am in that position now (downloading original data rather than using HF Datasets) and undoubtedly it hinders HF Datasets' widespread use and adoption. Missing fields like in the case of #4275 is definitely bad and not even up for a discussion IMHO! cc @lhoestq ","I'm opening a PR that adds the missing fields.\r\n\r\nLet's agree on the feature structure: @lhoestq @mariosasko @polinaeterna ","IMO we should always try to preserve the original structure unless there is a good reason not to (and I don't see one in this case).","I agree with @mariosasko . The transition to the original format could be done in one PR for the next minor release, clearly documenting all dataset changes just as @albertvillanova outlined them above and perhaps even providing a per dataset util method to convert the new valid format to the old for backward compatibility. Users who relied on the old format will update their code with either the util method for a quick fix or slightly more elaborate for the new. ","I don't have a strong opinion on this, besides the fact that whatever decision we agree on, should be applied to all datasets.\r\n\r\nThere is always the tension between:\r\n- preserving each dataset original structure (which has the advantage of not forcing users to learn other structure for the same dataset),\r\n-  and on the other hand performing some king of standardization\/harmonization depending on the task (this has the advantage that once learnt, the same structure applies to all datasets; this has been done for e.g.  POS tagging: all datasets have been adapted to a certain \"standard\" structure).\r\n   - Another advantage: datasets can easily be interchanged (or joined) to be used by the same model\r\n\r\nRecently, in the BigScience BioMedical hackathon, they adopted a different approach:\r\n- they implement a \"source\" config, respecting the original structure as much as possible\r\n- they implement additional config for each task, with a \"standard\" nested structure per task, which is most useful for users."],"created_at":1651643512000,"updated_at":1651842081000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"## Describe the bug\r\nOpenBookQA implementation is inconsistent with the original dataset.\r\n\r\nWe need to:\r\n\r\n1. The dataset field [question][stem] is flattened into question_stem. Unflatten it to match the original format.\r\n2. Add missing additional fields:\r\n    - 'fact1': row['fact1'],\r\n    - 'humanScore': row['humanScore'],\r\n    - 'clarity': row['clarity'],\r\n    - 'turkIdAnonymized': row['turkIdAnonymized']\r\n3. Ensure the structure and every data item in the original OpenBookQA matches our OpenBookQA version.\r\n\r\n## Expected results\r\nThe structure and every data item in the original OpenBookQA matches our OpenBookQA version.\r\n\r\n## Actual results\r\nTBD\r\n\r\n## Environment info\r\n- `datasets` version: 2.1.0\r\n- Platform: macOS-10.15.7-x86_64-i386-64bit\r\n- Python version: 3.8.13\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4276\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4275","id":1224943414,"node_id":"I_kwDODunzps5JAyc2","number":4275,"title":"CommonSenseQA has missing and inconsistent field names","user":{"login":"vblagoje","id":458335,"node_id":"MDQ6VXNlcjQ1ODMzNQ==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/458335?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/vblagoje","html_url":"https:\/\/github.com\/vblagoje","followers_url":"https:\/\/api.github.com\/users\/vblagoje\/followers","following_url":"https:\/\/api.github.com\/users\/vblagoje\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/vblagoje\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/vblagoje\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/vblagoje\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/vblagoje\/orgs","repos_url":"https:\/\/api.github.com\/users\/vblagoje\/repos","events_url":"https:\/\/api.github.com\/users\/vblagoje\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/vblagoje\/received_events","type":"User","site_admin":false},"labels":[{"id":2067388877,"node_id":"MDU6TGFiZWwyMDY3Mzg4ODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20bug","name":"dataset bug","color":"2edb81","default":false,"description":"A bug in a dataset script provided in the library"}],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Thanks for reporting, @vblagoje.\r\n\r\nI'm opening a PR to address this. "],"created_at":1651642739000,"updated_at":1651664478000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"## Describe the bug\r\nIn short, CommonSenseQA implementation is inconsistent with the original dataset.\r\n\r\nMore precisely, we need to:\r\n\r\n1. Add the dataset matching \"id\" field. The current dataset, instead, regenerates monotonically increasing id.  \r\n2. The [\u201cquestion\u201d][\u201cstem\u201d] field is flattened into \"question\". We should match the original dataset and unflatten it\r\n3. Add the missing \"question_concept\" field in the question tree node\r\n4. Anything else? Go over the data structure of the newly repaired CommonSenseQA and make sure it matches the original\r\n\r\n## Expected results\r\nEvery data item of the CommonSenseQA should structurally and data-wise match the original CommonSenseQA dataset.\r\n\r\n## Actual results\r\nTBD\r\n\r\n## Environment info\r\n- `datasets` version: 2.1.0\r\n- Platform: macOS-10.15.7-x86_64-i386-64bit\r\n- Python version: 3.8.13\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4275\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4274","id":1224740303,"node_id":"PR_kwDODunzps43Qm2w","number":4274,"title":"Add API code examples for IterableDataset","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651617857000,"updated_at":1651681772000,"closed_at":1651681324000,"author_association":"MEMBER","active_lock_reason":null,"body":"This PR adds API code examples for `IterableDataset` and `IterableDatasetDicts`.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4274\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4274","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4274","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4274.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4274.patch","merged_at":1651681324000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4273","id":1224681036,"node_id":"PR_kwDODunzps43QaA6","number":4273,"title":"leadboard info added for TNE","user":{"login":"yanaiela","id":8031035,"node_id":"MDQ6VXNlcjgwMzEwMzU=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8031035?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yanaiela","html_url":"https:\/\/github.com\/yanaiela","followers_url":"https:\/\/api.github.com\/users\/yanaiela\/followers","following_url":"https:\/\/api.github.com\/users\/yanaiela\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yanaiela\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yanaiela\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yanaiela\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yanaiela\/orgs","repos_url":"https:\/\/api.github.com\/users\/yanaiela\/repos","events_url":"https:\/\/api.github.com\/users\/yanaiela\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yanaiela\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651613741000,"updated_at":1651757124000,"closed_at":1651756693000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4273\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4273","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4273","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4273.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4273.patch","merged_at":1651756693000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4272","id":1224635660,"node_id":"PR_kwDODunzps43QQQt","number":4272,"title":"Fix typo in logging docs","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","> This PR fixes #4271.\r\n\r\nThings have not changed when searching \"tqdm\" in the Dataset document. The second result still performs as \"Enable\".","Hi @jiangwy99, the fix will appear on the `main` version of the docs:\r\n\r\n![Screen Shot 2022-05-04 at 8 38 29 AM](https:\/\/user-images.githubusercontent.com\/59462357\/166718225-6848ab91-87d1-4572-9912-40a909af6cb9.png)\r\n","Fixed now, thanks."],"created_at":1651610877000,"updated_at":1651678947000,"closed_at":1651647516000,"author_association":"MEMBER","active_lock_reason":null,"body":"This PR fixes #4271.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4272\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4272","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4272","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4272.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4272.patch","merged_at":1651647515000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4271","id":1224404403,"node_id":"I_kwDODunzps5I-u2z","number":4271,"title":"A typo in docs of datasets.disable_progress_bar","user":{"login":"jiangwy99","id":39762734,"node_id":"MDQ6VXNlcjM5NzYyNzM0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/39762734?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jiangwy99","html_url":"https:\/\/github.com\/jiangwy99","followers_url":"https:\/\/api.github.com\/users\/jiangwy99\/followers","following_url":"https:\/\/api.github.com\/users\/jiangwy99\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jiangwy99\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jiangwy99\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jiangwy99\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jiangwy99\/orgs","repos_url":"https:\/\/api.github.com\/users\/jiangwy99\/repos","events_url":"https:\/\/api.github.com\/users\/jiangwy99\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jiangwy99\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"assignees":[{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi! Thanks for catching and reporting the typo, a PR has been opened to fix it :)"],"created_at":1651599896000,"updated_at":1651647515000,"closed_at":1651647515000,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\nin the docs of V2.1.0 datasets.disable_progress_bar, we should replace \"enable\" with \"disable\".","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4271\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4270","id":1224244460,"node_id":"PR_kwDODunzps43PC5V","number":4270,"title":"Fix style in openbookqa dataset","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651591294000,"updated_at":1651826286000,"closed_at":1651594852000,"author_association":"MEMBER","active_lock_reason":null,"body":"CI in PR:\r\n- #4259 \r\n\r\nwas green, but after merging it to master, a code quality error appeared.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4270\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4270","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4270","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4270.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4270.patch","merged_at":1651594852000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4269","id":1223865145,"node_id":"PR_kwDODunzps43Nzwh","number":4269,"title":"Add license and point of contact to big_patent dataset","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651569847000,"updated_at":1651826289000,"closed_at":1651576579000,"author_association":"MEMBER","active_lock_reason":null,"body":"Update metadata of big_patent dataset with:\r\n- license\r\n- point of contact","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4269\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4269","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4269","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4269.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4269.patch","merged_at":1651576579000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4268","id":1223331964,"node_id":"I_kwDODunzps5I6pB8","number":4268,"title":"error downloading bigscience-catalogue-lm-data\/lm_en_wiktionary_filtered","user":{"login":"i-am-neo","id":102043285,"node_id":"U_kgDOBhUOlQ","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/102043285?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/i-am-neo","html_url":"https:\/\/github.com\/i-am-neo","followers_url":"https:\/\/api.github.com\/users\/i-am-neo\/followers","following_url":"https:\/\/api.github.com\/users\/i-am-neo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/i-am-neo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/i-am-neo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/i-am-neo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/i-am-neo\/orgs","repos_url":"https:\/\/api.github.com\/users\/i-am-neo\/repos","events_url":"https:\/\/api.github.com\/users\/i-am-neo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/i-am-neo\/received_events","type":"User","site_admin":false},"labels":[{"id":2067388877,"node_id":"MDU6TGFiZWwyMDY3Mzg4ODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20bug","name":"dataset bug","color":"2edb81","default":false,"description":"A bug in a dataset script provided in the library"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["It would help a lot to be able to preview the dataset - I'd like to see if the pronunciations are in the dataset, eg. for [\"word\"](https:\/\/en.wiktionary.org\/wiki\/word),\r\n\r\nPronunciation\r\n([Received Pronunciation](https:\/\/en.wikipedia.org\/wiki\/Received_Pronunciation)) [IPA](https:\/\/en.wiktionary.org\/wiki\/Wiktionary:International_Phonetic_Alphabet)([key](https:\/\/en.wiktionary.org\/wiki\/Appendix:English_pronunciation)): \/w\u025c\u02d0d\/\r\n([General American](https:\/\/en.wikipedia.org\/wiki\/General_American)) [enPR](https:\/\/en.wiktionary.org\/wiki\/Appendix:English_pronunciation): w\u00fbrd, [IPA](https:\/\/en.wiktionary.org\/wiki\/Wiktionary:International_Phonetic_Alphabet)([key](https:\/\/en.wiktionary.org\/wiki\/Appendix:English_pronunciation)): \/w\u025dd\/","Hi @i-am-neo, thanks for reporting.\r\n\r\nNormally this dataset should be private and not accessible for public use. @cakiki, @lvwerra, any reason why is it public? I see many other Wikimedia datasets are also public.\r\n\r\nAlso note that last commit \"Add metadata\" (https:\/\/huggingface.co\/datasets\/bigscience-catalogue-lm-data\/lm_en_wiktionary_filtered\/commit\/dc2f458dab50e00f35c94efb3cd4009996858609) introduced buggy data files (`data\/file-01.jsonl.gz.lock`, `data\/file-01.jsonl.gz.lock.lock`). The same bug appears in other datasets as well.\r\n\r\n@i-am-neo, please note that in the near future we are planning to make public all datasets used for the BigScience project (at least all of them whose license allows to do that). Once public, they will be accessible for all the NLP community.","Ah this must be a bug introduced at creation time since the repos were created programmatically; I'll go ahead and make them private; sorry about that!","All datasets are private now. \r\n\r\nRe:that bug I think we're currently avoiding it by avoiding verifications. (i.e. `ignore_verifications=True`)","Thanks a lot, @cakiki.\r\n\r\n@i-am-neo, I'm closing this issue for now because the dataset is not publicly available yet. Just stay tuned, as we will soon release all the BigScience open-license datasets.  ","Thanks for letting me know, @albertvillanova @cakiki.\r\nAny chance of having a subset alpha version in the meantime? \r\nI only need two dicts out of wiktionary: 1) phoneme(as key): word, and 2) word(as key): its phonemes.\r\n\r\nWould like to use it for a mini-poc [Robust ASR](https:\/\/github.com\/huggingface\/transformers\/issues\/13162#issuecomment-1096881290) decoding, cc @patrickvonplaten. \r\n\r\n(Patrick, possible to email you so as not to litter github with comments? I have some observations after experiments training hubert on some YT AMI-like data (11.44% wer).  Also wonder if a robust ASR is on your\/HG's roadmap).  Thanks!","Hey @i-am-neo,\r\n\r\nCool to hear that you're working on Robust ASR! Feel free to drop me a mail :-)","@i-am-neo This particular subset of the dataset was taken from the [CirrusSearch dumps](https:\/\/dumps.wikimedia.org\/other\/cirrussearch\/current\/)\r\nYou're specifically after the [enwiktionary-20220425-cirrussearch-content.json.gz](https:\/\/dumps.wikimedia.org\/other\/cirrussearch\/current\/enwiktionary-20220425-cirrussearch-content.json.gz) file","thanks @cakiki !  <del>I could access the gz file yesterday (but neglected to tuck it away somewhere safe), and today the link is throwing a 404. Can you help? <\/del>  Never mind, got it!","thanks @patrickvonplaten.  will do - getting my observations together."],"created_at":1651523665000,"updated_at":1651852410000,"closed_at":1651577028000,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\nError generated when attempting to download dataset\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"bigscience-catalogue-lm-data\/lm_en_wiktionary_filtered\")\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\n```\r\nExpectedMoreDownloadedFiles               Traceback (most recent call last)\r\n\r\n[<ipython-input-62-4ac5cf959477>](https:\/\/localhost:8080\/#) in <module>()\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 dataset = load_dataset(\"bigscience-catalogue-lm-data\/lm_en_wiktionary_filtered\")\r\n\r\n3 frames\r\n\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/utils\/info_utils.py](https:\/\/localhost:8080\/#) in verify_checksums(expected_checksums, recorded_checksums, verification_name)\r\n     31         return\r\n     32     if len(set(expected_checksums) - set(recorded_checksums)) > 0:\r\n---> 33         raise ExpectedMoreDownloadedFiles(str(set(expected_checksums) - set(recorded_checksums)))\r\n     34     if len(set(recorded_checksums) - set(expected_checksums)) > 0:\r\n     35         raise UnexpectedDownloadedFile(str(set(recorded_checksums) - set(expected_checksums)))\r\n\r\nExpectedMoreDownloadedFiles: {'\/home\/leandro\/catalogue_data\/datasets\/lm_en_wiktionary_filtered\/data\/file-01.jsonl.gz', '\/home\/leandro\/catalogue_data\/datasets\/lm_en_wiktionary_filtered\/data\/file-01.jsonl.gz.lock'}\r\n```\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.3\r\n- Platform: Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- PyArrow version: 6.0.1\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4268\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4267","id":1223214275,"node_id":"PR_kwDODunzps43LzOR","number":4267,"title":"Replace data URL in SAMSum dataset within the same repository","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651516688000,"updated_at":1651826293000,"closed_at":1651518229000,"author_association":"MEMBER","active_lock_reason":null,"body":"Replace data URL with one in the same repository.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4267\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4267","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4267","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4267.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4267.patch","merged_at":1651518229000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4266","id":1223116436,"node_id":"PR_kwDODunzps43LeXK","number":4266,"title":"Add HF Speech Bench to Librispeech Dataset Card","user":{"login":"sanchit-gandhi","id":93869735,"node_id":"U_kgDOBZhWpw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/93869735?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sanchit-gandhi","html_url":"https:\/\/github.com\/sanchit-gandhi","followers_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/followers","following_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/orgs","repos_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/repos","events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651510771000,"updated_at":1651740440000,"closed_at":1651740009000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Adds the HF Speech Bench to Librispeech Dataset Card in place of the Papers With Code Leaderboard. Should improve usage and visibility of this leaderboard! Wondering whether this can also be done for [Common Voice 7](https:\/\/huggingface.co\/datasets\/mozilla-foundation\/common_voice_7_0) and [8](https:\/\/huggingface.co\/datasets\/mozilla-foundation\/common_voice_8_0) through someone with permissions? \r\n\r\ncc @patrickvonplaten: more leaderboard promotion!","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4266\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4266","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4266","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4266.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4266.patch","merged_at":1651740009000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4263","id":1222723083,"node_id":"PR_kwDODunzps43KLnD","number":4263,"title":"Rename imagenet2012 -> imagenet-1k","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["> Later we can add imagenet-21k as a new dataset if we want.\r\n\r\nisn't it what models refer to as `imagenet` already?","> isn't it what models refer to as imagenet already?\r\n\r\nI wasn't sure, but it looks like it indeed. Therefore having a dataset `imagenet` for ImageNet 21k makes sense actually.\r\n\r\nEDIT: actually not all `imagenet` tag refer to ImageNet 21k - we will need to correct some of them","_The documentation is not available anymore as the PR was closed or merged._","should we remove the repo mirror on the hub side or will you do it?"],"created_at":1651487181000,"updated_at":1651513846000,"closed_at":1651509177000,"author_association":"MEMBER","active_lock_reason":null,"body":"On the Hugging Face Hub, users refer to imagenet2012 (from #4178 ) as imagenet-1k in their model tags.\r\n\r\nTo correctly link models to imagenet, we should rename this dataset `imagenet-1k`.\r\n\r\nLater we can add `imagenet-21k` as a new dataset if we want.\r\n\r\nOnce this one is merged we can delete the `imagenet2012` dataset repository on the Hub.\r\n\r\nEDIT: to complete the rationale on why we should name it `imagenet-1k`:\r\nIf users specifically added the tag `imagenet-1k` , then it could be for two reasons (not sure which one is predominant), either they\r\n- wanted to make it explicit that it\u2019s not 21k -> the distinction is important for the community\r\n- or they have been following this convention from other models -> the convention implicitly exists already","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263\/reactions","total_count":3,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":1,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4263\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4263","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4263","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4263.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4263.patch","merged_at":1651509177000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4262","id":1222130749,"node_id":"PR_kwDODunzps43IOye","number":4262,"title":"Add YAML tags to Dataset Card rotten tomatoes","user":{"login":"mo6zes","id":10004251,"node_id":"MDQ6VXNlcjEwMDA0MjUx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10004251?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mo6zes","html_url":"https:\/\/github.com\/mo6zes","followers_url":"https:\/\/api.github.com\/users\/mo6zes\/followers","following_url":"https:\/\/api.github.com\/users\/mo6zes\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mo6zes\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mo6zes\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mo6zes\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mo6zes\/orgs","repos_url":"https:\/\/api.github.com\/users\/mo6zes\/repos","events_url":"https:\/\/api.github.com\/users\/mo6zes\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mo6zes\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651406348000,"updated_at":1651588053000,"closed_at":1651587635000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"The dataset card for the rotten tomatoes \/ MR movie review dataset had some missing YAML tags. Hopefully, this also improves the visibility of this dataset now that paperswithcode and huggingface link to eachother.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262\/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4262\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4262","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4262","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4262.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4262.patch","merged_at":1651587635000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4261","id":1221883779,"node_id":"I_kwDODunzps5I1HeD","number":4261,"title":"data leakage in `webis\/conclugen` dataset","user":{"login":"xflashxx","id":54585776,"node_id":"MDQ6VXNlcjU0NTg1Nzc2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/54585776?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/xflashxx","html_url":"https:\/\/github.com\/xflashxx","followers_url":"https:\/\/api.github.com\/users\/xflashxx\/followers","following_url":"https:\/\/api.github.com\/users\/xflashxx\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/xflashxx\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/xflashxx\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/xflashxx\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/xflashxx\/orgs","repos_url":"https:\/\/api.github.com\/users\/xflashxx\/repos","events_url":"https:\/\/api.github.com\/users\/xflashxx\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/xflashxx\/received_events","type":"User","site_admin":false},"labels":[{"id":2067388877,"node_id":"MDU6TGFiZWwyMDY3Mzg4ODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20bug","name":"dataset bug","color":"2edb81","default":false,"description":"A bug in a dataset script provided in the library"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi @xflashxx, thanks for reporting.\r\n\r\nPlease note that this dataset was generated and shared by Webis Group: https:\/\/huggingface.co\/webis\r\n\r\nWe are contacting the dataset owners to inform them about the issue you found. We'll keep you updated of their reply.","i'd suggest just pinging the authors here in the issue if possible?","Thanks for reporting this @xflashxx. I'll have a look and get back to you on this.","Hi @xflashxx and @albertvillanova,\r\n\r\nI have updated the files with de-duplicated splits. Apparently the debate portals from which part of the examples were sourced had unique timestamps for some examples (up to 6%; updated counts in the README) without any actual content updated that lead to \"new\" items. The length of `ids_validation` and `ids_testing` is zero.\r\n\r\nRegarding impact on scores:\r\n1. We employed automatic evaluation (on a separate set of 1000 examples) only to justify the exclusion of the smaller models for manual evaluation (due to budget constraints). I am confident the ranking still stands (unsurprisingly, the bigger models doing better than those trained on the smaller splits). We also highlight this in the paper. \r\n\r\n2. The examples used for manual evaluation have no overlap with any splits (also because they do not have any ground truth as we applied the trained models on an unlabeled sample to test its practical usage). I've added these two files to the dataset repository.\r\n\r\nHope this helps!","Thanks @shahbazsyed for your fast fix.\r\n\r\nAs a side note:\r\n- Your email appearing as Point of Contact in the dataset README has a typo: @uni.leipzig.de instead of @uni-leipzig.de\r\n- Your commits on the Hub are not linked to your profile on the Hub: this is because we use the email address to make this link; the email address used in your commit author and the email address set on your Hub account settings."],"created_at":1651340617000,"updated_at":1651557866000,"closed_at":1651557866000,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\nSome samples (argument-conclusion pairs) in the *training* split of the `webis\/conclugen` dataset are present in both the *validation* and *test* splits, creating data leakage and distorting model results.\r\nFurthermore, all splits contain duplicate samples.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ntraining = load_dataset(\"webis\/conclugen\", \"base\", split=\"train\")\r\nvalidation = load_dataset(\"webis\/conclugen\", \"base\", split=\"validation\")\r\ntesting = load_dataset(\"webis\/conclugen\", \"base\", split=\"test\")\r\n\r\n# collect which sample id's are present in the training split\r\nids_validation = list()\r\nids_testing = list()\r\n\r\nfor train_sample in training:\r\n    train_argument = train_sample[\"argument\"]\r\n    train_conclusion = train_sample[\"conclusion\"]\r\n    train_id = train_sample[\"id\"]\r\n    \r\n    # test if current sample is in validation split\r\n    if train_argument in validation[\"argument\"]:\r\n        for validation_sample in validation:\r\n            validation_argument = validation_sample[\"argument\"]\r\n            validation_conclusion = validation_sample[\"conclusion\"]\r\n            validation_id = validation_sample[\"id\"]\r\n            if train_argument == validation_argument and train_conclusion == validation_conclusion:\r\n                ids_validation.append(validation_id)\r\n    \r\n    # test if current sample is in test split\r\n    if train_argument in testing[\"argument\"]:\r\n        for testing_sample in testing:\r\n            testing_argument = testing_sample[\"argument\"]\r\n            testing_conclusion = testing_sample[\"conclusion\"]\r\n            testing_id = testing_sample[\"id\"]\r\n            if train_argument == testing_argument and train_conclusion == testing_conclusion:\r\n                ids_testing.append(testing_id)\r\n```\r\n\r\n## Expected results\r\nLength of both lists `ids_validation` and `ids_testing` should be zero.\r\n\r\n## Actual results\r\nLength of `ids_validation` = `2556`\r\nLength of `ids_testing` = `287`\r\n\r\nFurthermore, there seems to be duplicate samples in (at least) the *training* split, since:\r\n`print(len(set(ids_validation)))` = `950`\r\n`print(len(set(ids_testing)))` = `101`\r\n\r\nAll in all, around 7% of the samples of each the *validation* and *test* split seems to be present in the *training* split.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.4\r\n- Platform: macOS-12.3.1-arm64-arm-64bit\r\n- Python version: 3.9.10\r\n- PyArrow version: 7.0.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4261\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4260","id":1221830292,"node_id":"PR_kwDODunzps43HSfs","number":4260,"title":"Add mr_polarity movie review sentiment classification","user":{"login":"mo6zes","id":10004251,"node_id":"MDQ6VXNlcjEwMDA0MjUx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10004251?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mo6zes","html_url":"https:\/\/github.com\/mo6zes","followers_url":"https:\/\/api.github.com\/users\/mo6zes\/followers","following_url":"https:\/\/api.github.com\/users\/mo6zes\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mo6zes\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mo6zes\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mo6zes\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mo6zes\/orgs","repos_url":"https:\/\/api.github.com\/users\/mo6zes\/repos","events_url":"https:\/\/api.github.com\/users\/mo6zes\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mo6zes\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["whoops just found https:\/\/huggingface.co\/datasets\/rotten_tomatoes"],"created_at":1651324773000,"updated_at":1651328185000,"closed_at":1651328185000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Add the MR (Movie Review) dataset. The original dataset contains sentences from Rotten Tomatoes labeled as either \"positive\" or  \"negative\". \r\n\r\nHomepage: [https:\/\/www.cs.cornell.edu\/people\/pabo\/movie-review-data\/](https:\/\/www.cs.cornell.edu\/people\/pabo\/movie-review-data\/)\r\npaperswithcode: [https:\/\/paperswithcode.com\/dataset\/mr](https:\/\/paperswithcode.com\/dataset\/mr)\r\n\r\n- [ ] I was not able to generate dummy data, the original dataset files have \".pos\" and \".neg\" as file extensions so the auto-generator does not work. Is it fine like this or should dummy data be added?\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4260\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4260","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4260","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4260.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4260.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4259","id":1221768025,"node_id":"PR_kwDODunzps43HHGc","number":4259,"title":"Fix bug in choices labels in openbookqa dataset","user":{"login":"manandey","id":6687858,"node_id":"MDQ6VXNlcjY2ODc4NTg=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/6687858?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/manandey","html_url":"https:\/\/github.com\/manandey","followers_url":"https:\/\/api.github.com\/users\/manandey\/followers","following_url":"https:\/\/api.github.com\/users\/manandey\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/manandey\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/manandey\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/manandey\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/manandey\/orgs","repos_url":"https:\/\/api.github.com\/users\/manandey\/repos","events_url":"https:\/\/api.github.com\/users\/manandey\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/manandey\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651304499000,"updated_at":1651645891000,"closed_at":1651590861000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"This PR fixes the Bug in the openbookqa dataset as mentioned in this issue #3550.\r\n\r\nFix #3550.\r\n\r\ncc. @lhoestq @mariosasko ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4259\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4259","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4259","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4259.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4259.patch","merged_at":1651590861000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4258","id":1221637727,"node_id":"PR_kwDODunzps43Gstg","number":4258,"title":"Fix\/start token mask issue and update documentation","user":{"login":"TristanThrush","id":20826878,"node_id":"MDQ6VXNlcjIwODI2ODc4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/20826878?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/TristanThrush","html_url":"https:\/\/github.com\/TristanThrush","followers_url":"https:\/\/api.github.com\/users\/TristanThrush\/followers","following_url":"https:\/\/api.github.com\/users\/TristanThrush\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/TristanThrush\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/TristanThrush\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/TristanThrush\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/TristanThrush\/orgs","repos_url":"https:\/\/api.github.com\/users\/TristanThrush\/repos","events_url":"https:\/\/api.github.com\/users\/TristanThrush\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/TristanThrush\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","> Good catch ! Thanks :)\r\n> \r\n> Next time can you describe your fix in the Pull Request description please ?\r\n\r\nThanks. Also whoops, sorry about not being very descriptive. I updated the pull request description, and will keep this in mind for future PRs."],"created_at":1651272164000,"updated_at":1651509200000,"closed_at":1651508772000,"author_association":"MEMBER","active_lock_reason":null,"body":"This pr fixes a couple bugs:\r\n\r\n1) the perplexity was calculated with a 0 in the attention mask for the start token, which was causing high perplexity scores that were not correct\r\n2) the documentation was not updated","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4258\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4258","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4258","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4258.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4258.patch","merged_at":1651508772000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4257","id":1221393137,"node_id":"PR_kwDODunzps43GATC","number":4257,"title":"Create metric card for Mahalanobis Distance","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651257447000,"updated_at":1651503018000,"closed_at":1651502604000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"proposing a metric card to better explain how Mahalanobis distance works (last one for now :sweat_smile:","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4257\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4257","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4257","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4257.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4257.patch","merged_at":1651502604000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4256","id":1221379625,"node_id":"PR_kwDODunzps43F9Zw","number":4256,"title":"Create metric card for MSE","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651256482000,"updated_at":1651503342000,"closed_at":1651502927000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Proposing a metric card for Mean Squared Error","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4256\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4256","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4256","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4256.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4256.patch","merged_at":1651502927000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4255","id":1221142899,"node_id":"PR_kwDODunzps43FHgR","number":4255,"title":"No google drive URL for pubmed_qa","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","CI is failing because some sections are missing in the dataset card, but this is unrelated to this PR - Merging !"],"created_at":1651247746000,"updated_at":1651249495000,"closed_at":1651249136000,"author_association":"MEMBER","active_lock_reason":null,"body":"I hosted the data files in https:\/\/huggingface.co\/datasets\/pubmed_qa. This is allowed because the data is under the MIT license.\r\n\r\ncc @stas00 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4255\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4255","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4255","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4255.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4255.patch","merged_at":1651249136000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4254","id":1220204395,"node_id":"PR_kwDODunzps43Bwnj","number":4254,"title":"Replace data URL in SAMSum dataset and support streaming","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651220503000,"updated_at":1651826296000,"closed_at":1651249569000,"author_association":"MEMBER","active_lock_reason":null,"body":"This PR replaces data URL in SAMSum dataset:\r\n- original host (arxiv.org) does not allow HTTP Range requests\r\n- we have hosted the data on the Hub (license: CC BY-NC-ND 4.0)\r\n\r\nMoreover, it implements support for streaming.\r\n\r\nFix #4146.\r\nRelated to: #4236.\r\n\r\nCC: @severo ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4254\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4254","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4254","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4254.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4254.patch","merged_at":1651249568000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4253","id":1219286408,"node_id":"PR_kwDODunzps42-c8Q","number":4253,"title":"Create metric cards for mean IOU","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651179507000,"updated_at":1651254287000,"closed_at":1651253886000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Proposing a metric card for mIoU :rocket:\r\n\r\nsorry for spamming you with review requests, @albertvillanova ! :hugs: ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4253\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4253","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4253","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4253.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4253.patch","merged_at":1651253886000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4252","id":1219151100,"node_id":"PR_kwDODunzps429--I","number":4252,"title":"Creating metric card for MAE","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651172673000,"updated_at":1651251551000,"closed_at":1651251150000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Initial proposal for MAE metric card","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4252\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4252","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4252","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4252.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4252.patch","merged_at":1651251150000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4251","id":1219116354,"node_id":"PR_kwDODunzps4293dB","number":4251,"title":"Metric card for the XTREME-S dataset","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651170739000,"updated_at":1651250771000,"closed_at":1651250326000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Proposing a metric card for the XTREME-S dataset :hugs:","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4251\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4251","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4251","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4251.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4251.patch","merged_at":1651250326000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4250","id":1219093830,"node_id":"PR_kwDODunzps429yjN","number":4250,"title":"Bump PyArrow Version to 6","user":{"login":"dnaveenr","id":17746528,"node_id":"MDQ6VXNlcjE3NzQ2NTI4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17746528?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/dnaveenr","html_url":"https:\/\/github.com\/dnaveenr","followers_url":"https:\/\/api.github.com\/users\/dnaveenr\/followers","following_url":"https:\/\/api.github.com\/users\/dnaveenr\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/dnaveenr\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/dnaveenr\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/dnaveenr\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/dnaveenr\/orgs","repos_url":"https:\/\/api.github.com\/users\/dnaveenr\/repos","events_url":"https:\/\/api.github.com\/users\/dnaveenr\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/dnaveenr\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Updated meta.yaml as well. Thanks.","I'm OK with bumping PyArrow to version 6 to match the version in Colab, but maybe a better solution would be to stop using extension types in our codebase to avoid similar issues.","> but maybe a better solution would be to stop using extension types in our codebase to avoid similar issues.\r\n\r\nI agree, not much attention has been payed to extension arrays in the latest developments of Arrow anyway.\r\n\r\nLet's not use them more that what we do right now, and try to remove them at one point"],"created_at":1651169450000,"updated_at":1651657012000,"closed_at":1651656586000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fixes #4152 \r\n\r\nThis PR updates the PyArrow version to 6 in setup.py, CI job files .circleci\/config.yaml and .github\/workflows\/benchmarks.yaml files.\r\nThis will fix ArrayND error which exists in pyarrow 5.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4250\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4250","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4250","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4250.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4250.patch","merged_at":1651656586000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4249","id":1218524424,"node_id":"PR_kwDODunzps42742y","number":4249,"title":"Support streaming XGLUE dataset","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651141643000,"updated_at":1651826301000,"closed_at":1651162083000,"author_association":"MEMBER","active_lock_reason":null,"body":"Support streaming XGLUE dataset.\r\n\r\nFix #4247.\r\n\r\nCC: @severo ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4249\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4249","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4249","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4249.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4249.patch","merged_at":1651162083000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4248","id":1218460444,"node_id":"I_kwDODunzps5IoDsc","number":4248,"title":"conll2003 dataset loads original data.","user":{"login":"sue991","id":26458611,"node_id":"MDQ6VXNlcjI2NDU4NjEx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/26458611?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sue991","html_url":"https:\/\/github.com\/sue991","followers_url":"https:\/\/api.github.com\/users\/sue991\/followers","following_url":"https:\/\/api.github.com\/users\/sue991\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sue991\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sue991\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sue991\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sue991\/orgs","repos_url":"https:\/\/api.github.com\/users\/sue991\/repos","events_url":"https:\/\/api.github.com\/users\/sue991\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sue991\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Thanks for reporting @sue99.\r\n\r\nUnfortunately. I'm not able to reproduce your problem:\r\n```python\r\nIn [1]: import datasets\r\n   ...: from datasets import load_dataset\r\n   ...: dataset = load_dataset(\"conll2003\")\r\n\r\nIn [2]: dataset\r\nOut[2]: \r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\r\n        num_rows: 14042\r\n    })\r\n    validation: Dataset({\r\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\r\n        num_rows: 3251\r\n    })\r\n    test: Dataset({\r\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\r\n        num_rows: 3454\r\n    })\r\n})\r\n\r\nIn [3]: dataset[\"train\"][0]\r\nOut[3]: \r\n{'id': '0',\r\n 'tokens': ['EU',\r\n  'rejects',\r\n  'German',\r\n  'call',\r\n  'to',\r\n  'boycott',\r\n  'British',\r\n  'lamb',\r\n  '.'],\r\n 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\r\n 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\r\n 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\r\n```\r\n\r\nJust guessing: might be the case that you are calling `load_dataset` from a working directory that contains a local folder named `conll2003` (containing the raw data files)? If that is the case, `datasets` library gives precedence to the local folder over the dataset on the Hub. "],"created_at":1651138411000,"updated_at":1651163473000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\nI load `conll2003` dataset to use refined data like [this](https:\/\/huggingface.co\/datasets\/conll2003\/viewer\/conll2003\/train)  preview, but it is original data that contains `'-DOCSTART- -X- -X- O'` text.\r\n\r\nIs this a bug or should I use another dataset_name like `lhoestq\/conll2003` ?\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"conll2003\")\r\n```\r\n\r\n## Expected results\r\n{\r\n    \"chunk_tags\": [11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0],\r\n    \"id\": \"0\",\r\n    \"ner_tags\": [0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\r\n    \"pos_tags\": [12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7],\r\n    \"tokens\": [\"The\", \"European\", \"Commission\", \"said\", \"on\", \"Thursday\", \"it\", \"disagreed\", \"with\", \"German\", \"advice\", \"to\", \"consumers\", \"to\", \"shun\", \"British\", \"lamb\", \"until\", \"scientists\", \"determine\", \"whether\", \"mad\", \"cow\", \"disease\", \"can\", \"be\", \"transmitted\", \"to\", \"sheep\", \".\"]\r\n}\r\n\r\n## Actual results\r\n```python\r\nprint(dataset)\r\n\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['text'],\r\n        num_rows: 219554\r\n    })\r\n    test: Dataset({\r\n        features: ['text'],\r\n        num_rows: 50350\r\n    })\r\n    validation: Dataset({\r\n        features: ['text'],\r\n        num_rows: 55044\r\n    })\r\n})\r\n```\r\n\r\n```python\r\nfor i in range(20):\r\n    print(dataset['train'][i])\r\n\r\n{'text': '-DOCSTART- -X- -X- O'}\r\n{'text': ''}\r\n{'text': 'EU NNP B-NP B-ORG'}\r\n{'text': 'rejects VBZ B-VP O'}\r\n{'text': 'German JJ B-NP B-MISC'}\r\n{'text': 'call NN I-NP O'}\r\n{'text': 'to TO B-VP O'}\r\n{'text': 'boycott VB I-VP O'}\r\n{'text': 'British JJ B-NP B-MISC'}\r\n{'text': 'lamb NN I-NP O'}\r\n{'text': '. . O O'}\r\n{'text': ''}\r\n{'text': 'Peter NNP B-NP B-PER'}\r\n{'text': 'Blackburn NNP I-NP I-PER'}\r\n{'text': ''}\r\n{'text': 'BRUSSELS NNP B-NP B-LOC'}\r\n{'text': '1996-08-22 CD I-NP O'}\r\n{'text': ''}\r\n{'text': 'The DT B-NP O'}\r\n{'text': 'European NNP I-NP B-ORG'}\r\n```\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4248\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4247","id":1218320882,"node_id":"I_kwDODunzps5Inhny","number":4247,"title":"The data preview of XGLUE","user":{"login":"czq1999","id":49108847,"node_id":"MDQ6VXNlcjQ5MTA4ODQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/49108847?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/czq1999","html_url":"https:\/\/github.com\/czq1999","followers_url":"https:\/\/api.github.com\/users\/czq1999\/followers","following_url":"https:\/\/api.github.com\/users\/czq1999\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/czq1999\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/czq1999\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/czq1999\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/czq1999\/orgs","repos_url":"https:\/\/api.github.com\/users\/czq1999\/repos","events_url":"https:\/\/api.github.com\/users\/czq1999\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/czq1999\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["![image](https:\/\/user-images.githubusercontent.com\/49108847\/165700611-915b4343-766f-4b81-bdaa-b31950250f06.png)\r\n","Thanks for reporting @czq1999.\r\n\r\nNote that the dataset viewer uses the dataset in streaming mode and that not all datasets support streaming yet.\r\n\r\nThat is the case for XGLUE dataset (as the error message points out): this must be refactored to support streaming. ","Fixed, thanks @albertvillanova !\r\n\r\nhttps:\/\/huggingface.co\/datasets\/xglue\r\n\r\n<img width=\"824\" alt=\"Capture d\u2019e\u0301cran 2022-04-29 a\u0300 10 23 14\" src=\"https:\/\/user-images.githubusercontent.com\/1676121\/165909391-9f98d98a-665a-4e57-822d-8baa2dc9b7c9.png\">\r\n"],"created_at":1651131050000,"updated_at":1651220608000,"closed_at":1651162083000,"author_association":"NONE","active_lock_reason":null,"body":"It seems that something wrong with the data previvew of XGLUE","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4247\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4246","id":1218320293,"node_id":"PR_kwDODunzps427NiD","number":4246,"title":"Support to load dataset with TSV files by passing only dataset name","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651131015000,"updated_at":1651826308000,"closed_at":1651824847000,"author_association":"MEMBER","active_lock_reason":null,"body":"This PR implements support to load a dataset (w\/o script) containing TSV files by passing only the dataset name (no need to pass `sep='\\t'`):\r\n```python\r\nds = load_dataset(\"dataset\/name\")\r\n```\r\n\r\nThe refactoring allows for future builder kwargs customizations based on file extension.\r\n\r\nRelated to #4238.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4246\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4246","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4246","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4246.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4246.patch","merged_at":1651824847000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4245","id":1217959400,"node_id":"PR_kwDODunzps426AUR","number":4245,"title":"Add code examples for DatasetDict","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651099942000,"updated_at":1651256374000,"closed_at":1651255983000,"author_association":"MEMBER","active_lock_reason":null,"body":"This PR adds code examples for `DatasetDict` in the API reference :)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4245\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4245","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4245","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4245.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4245.patch","merged_at":1651255983000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4244","id":1217732221,"node_id":"PR_kwDODunzps425Po6","number":4244,"title":"task id update","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Reverted the multi-input-text-classification tag from task_categories and added it as task_ids @lhoestq ","_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651084094000,"updated_at":1651661033000,"closed_at":1651660597000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"changed multi input text classification as task id instead of category","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4244\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4244","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4244","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4244.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4244.patch","merged_at":1651660597000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4243","id":1217689909,"node_id":"PR_kwDODunzps425Gkn","number":4243,"title":"WIP: Initial shades loading script and readme","user":{"login":"shayne-longpre","id":69018523,"node_id":"MDQ6VXNlcjY5MDE4NTIz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/69018523?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/shayne-longpre","html_url":"https:\/\/github.com\/shayne-longpre","followers_url":"https:\/\/api.github.com\/users\/shayne-longpre\/followers","following_url":"https:\/\/api.github.com\/users\/shayne-longpre\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/shayne-longpre\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/shayne-longpre\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/shayne-longpre\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/shayne-longpre\/orgs","repos_url":"https:\/\/api.github.com\/users\/shayne-longpre\/repos","events_url":"https:\/\/api.github.com\/users\/shayne-longpre\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/shayne-longpre\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1651081543000,"updated_at":1651081543000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4243\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4243","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4243","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4243.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4243.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4242","id":1217665960,"node_id":"PR_kwDODunzps425BYf","number":4242,"title":"Update auth when mirroring datasets on the hub","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651080151000,"updated_at":1651081024000,"closed_at":1651080642000,"author_association":"MEMBER","active_lock_reason":null,"body":"We don't need to use extraHeaders anymore for rate limits anymore. Anyway extraHeaders was not working with git LFS because it was passing the wrong auth to S3.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4242\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4242","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4242","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4242.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4242.patch","merged_at":1651080642000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4241","id":1217423686,"node_id":"I_kwDODunzps5IkGlG","number":4241,"title":"NonMatchingChecksumError when attempting to download GLUE","user":{"login":"drussellmrichie","id":9650729,"node_id":"MDQ6VXNlcjk2NTA3Mjk=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/9650729?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/drussellmrichie","html_url":"https:\/\/github.com\/drussellmrichie","followers_url":"https:\/\/api.github.com\/users\/drussellmrichie\/followers","following_url":"https:\/\/api.github.com\/users\/drussellmrichie\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/drussellmrichie\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/drussellmrichie\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/drussellmrichie\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/drussellmrichie\/orgs","repos_url":"https:\/\/api.github.com\/users\/drussellmrichie\/repos","events_url":"https:\/\/api.github.com\/users\/drussellmrichie\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/drussellmrichie\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi :)\r\n\r\nI think your issue may be related to the older `nlp` library. I was able to download `glue` with the latest version of `datasets`. Can you try updating with:\r\n\r\n```py\r\npip install -U datasets\r\n```\r\n\r\nThen you can download:\r\n\r\n```py\r\nfrom datasets import load_dataset\r\nds = load_dataset(\"glue\", \"rte\")\r\n```","This appears to work. Thank you!\n\nOn Wed, Apr 27, 2022, 1:18 PM Steven Liu ***@***.***> wrote:\n\n> Hi :)\n>\n> I think your issue may be related to the older nlp library. I was able to\n> download glue with the latest version of datasets. Can you try updating\n> with:\n>\n> pip install -U datasets\n>\n> Then you can download:\n>\n> from datasets import load_datasetds = load_dataset(\"glue\", \"rte\")\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/huggingface\/datasets\/issues\/4241#issuecomment-1111267650>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/ACJUEKLUP2EL7ES3RRWJRPTVHFZHBANCNFSM5UPJBYXA>\n> .\n> You are receiving this because you authored the thread.Message ID:\n> ***@***.***>\n>\n"],"created_at":1651068861000,"updated_at":1651131927000,"closed_at":1651131927000,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\nI am trying to download the GLUE dataset from the NLP module but get an error (see below).\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport nlp\r\nnlp.__version__ # '0.2.0'\r\nnlp.load_dataset('glue', name=\"rte\", download_mode=\"force_redownload\")\r\n```\r\n\r\n## Expected results\r\nI expect the dataset to download without an error.\r\n\r\n## Actual results\r\n```\r\nINFO:nlp.load:Checking \/home\/richier\/.cache\/huggingface\/datasets\/5fe6ab0df8a32a3371b2e6a969d31d855a19563724fb0d0f163748c270c0ac60.2ea96febf19981fae5f13f0a43d4e2aa58bc619bc23acf06de66675f425a5538.py for additional imports.\r\nINFO:nlp.load:Found main folder for dataset https:\/\/s3.amazonaws.com\/datasets.huggingface.co\/nlp\/datasets\/glue\/glue.py at \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\r\nINFO:nlp.load:Found specific version folder for dataset https:\/\/s3.amazonaws.com\/datasets.huggingface.co\/nlp\/datasets\/glue\/glue.py at \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4\r\nINFO:nlp.load:Found script file from https:\/\/s3.amazonaws.com\/datasets.huggingface.co\/nlp\/datasets\/glue\/glue.py to \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4\/glue.py\r\nINFO:nlp.load:Found dataset infos file from https:\/\/s3.amazonaws.com\/datasets.huggingface.co\/nlp\/datasets\/glue\/dataset_infos.json to \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4\/dataset_infos.json\r\nINFO:nlp.load:Found metadata file for dataset https:\/\/s3.amazonaws.com\/datasets.huggingface.co\/nlp\/datasets\/glue\/glue.py at \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4\/glue.json\r\nINFO:nlp.info:Loading Dataset Infos from \/home\/richier\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/datasets\/glue\/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4\r\nINFO:nlp.builder:Generating dataset glue (\/home\/richier\/.cache\/huggingface\/datasets\/glue\/rte\/1.0.0)\r\nINFO:nlp.builder:Dataset not on Hf google storage. Downloading and preparing it from source\r\nINFO:nlp.utils.file_utils:Couldn't get ETag version for url https:\/\/firebasestorage.googleapis.com\/v0\/b\/mtl-sentence-representations.appspot.com\/o\/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb\r\nINFO:nlp.utils.file_utils:https:\/\/firebasestorage.googleapis.com\/v0\/b\/mtl-sentence-representations.appspot.com\/o\/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb not found in cache or force_download set to True, downloading to \/home\/richier\/.cache\/huggingface\/datasets\/downloads\/tmpldt3n805\r\nDownloading and preparing dataset glue\/rte (download: 680.81 KiB, generated: 1.83 MiB, total: 2.49 MiB) to \/home\/richier\/.cache\/huggingface\/datasets\/glue\/rte\/1.0.0...\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 73.0\/73.0 [00:00<00:00, 73.9kB\/s]\r\nINFO:nlp.utils.file_utils:storing https:\/\/firebasestorage.googleapis.com\/v0\/b\/mtl-sentence-representations.appspot.com\/o\/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb in cache at \/home\/richier\/.cache\/huggingface\/datasets\/downloads\/e8b62ee44e6f8b6aea761935928579ffe1aa55d161808c482e0725abbdcf9c64\r\nINFO:nlp.utils.file_utils:creating metadata file for \/home\/richier\/.cache\/huggingface\/datasets\/downloads\/e8b62ee44e6f8b6aea761935928579ffe1aa55d161808c482e0725abbdcf9c64\r\n---------------------------------------------------------------------------\r\nNonMatchingChecksumError                  Traceback (most recent call last)\r\n<ipython-input-7-669a8343dcc1> in <module>\r\n----> 1 nlp.load_dataset('glue', name=\"rte\", download_mode=\"force_redownload\")\r\n\r\n~\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    518         download_mode=download_mode,\r\n    519         ignore_verifications=ignore_verifications,\r\n--> 520         save_infos=save_infos,\r\n    521     )\r\n    522 \r\n\r\n~\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\r\n    418                 verify_infos = not save_infos and not ignore_verifications\r\n    419                 self._download_and_prepare(\r\n--> 420                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    421                 )\r\n    422                 # Sync info\r\n\r\n~\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    458         # Checksums verification\r\n    459         if verify_infos:\r\n--> 460             verify_checksums(self.info.download_checksums, dl_manager.get_recorded_sizes_checksums())\r\n    461         for split_generator in split_generators:\r\n    462             if str(split_generator.split_info.name).lower() == \"all\":\r\n\r\n~\/anaconda3\/envs\/py36_bert_ee_torch1_11\/lib\/python3.6\/site-packages\/nlp\/utils\/info_utils.py in verify_checksums(expected_checksums, recorded_checksums)\r\n     34     bad_urls = [url for url in expected_checksums if expected_checksums[url] != recorded_checksums[url]]\r\n     35     if len(bad_urls) > 0:\r\n---> 36         raise NonMatchingChecksumError(str(bad_urls))\r\n     37     logger.info(\"All the checksums matched successfully.\")\r\n     38 \r\n\r\nNonMatchingChecksumError: ['https:\/\/firebasestorage.googleapis.com\/v0\/b\/mtl-sentence-representations.appspot.com\/o\/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb']\r\n```\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0\r\n- Platform: Linux-4.18.0-348.20.1.el8_5.x86_64-x86_64-with-redhat-8.5-Ootpa\r\n- Python version: 3.6.13\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.1.5\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4241\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4240","id":1217287594,"node_id":"PR_kwDODunzps423xRl","number":4240,"title":"Fix yield for crd3","user":{"login":"shanyas10","id":21066979,"node_id":"MDQ6VXNlcjIxMDY2OTc5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/21066979?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/shanyas10","html_url":"https:\/\/github.com\/shanyas10","followers_url":"https:\/\/api.github.com\/users\/shanyas10\/followers","following_url":"https:\/\/api.github.com\/users\/shanyas10\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/shanyas10\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/shanyas10\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/shanyas10\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/shanyas10\/orgs","repos_url":"https:\/\/api.github.com\/users\/shanyas10\/repos","events_url":"https:\/\/api.github.com\/users\/shanyas10\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/shanyas10\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["I don't think you need to generate new dummy data, since they're in the same format as the original data.\r\n\r\nThe CI is failing because of this error:\r\n```python\r\n>                       turn[\"names\"] = turn[\"NAMES\"]\r\nE                       TypeError: tuple indices must be integers or slices, not str\r\n```\r\n\r\nDo you know what could cause this ? If I understand correctly, `turn` is supposed to be a list of dictionaries right ?","> ```  \r\n>   \r\n> Do you know what could cause this ? If I understand correctly, turn is supposed to be a list of dictionaries right ?\r\n> ```\r\n\r\nThis is strange. Let me look into this. As per https:\/\/github.com\/RevanthRameshkumar\/CRD3\/blob\/master\/data\/aligned%20data\/c%3D2\/C1E001_2_0.json TURNS is a list of dictionaries. So when we iterate over `row[\"TURNS]\"` each `turn` is essentially a dictionary. Not sure why it's being considered a tuple here."],"created_at":1651062696000,"updated_at":1651236101000,"closed_at":1651236101000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Modified the `_generate_examples` function to consider all the turns for a chunk id as a single example\r\nModified the features accordingly\r\n\r\n```\r\n\"turns\": [\r\n                        {\r\n                            \"names\": datasets.features.Sequence(datasets.Value(\"string\")),\r\n                            \"utterances\": datasets.features.Sequence(datasets.Value(\"string\")),\r\n                            \"number\": datasets.Value(\"int32\"),\r\n                        }\r\n                    ],\r\n                }\r\n```\r\n\r\nI wasn't able to run `datasets-cli dummy_data datasets` command. Is there a workaround for this? ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4240\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4240","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4240","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4240.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4240.patch","merged_at":1651236101000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4239","id":1217269689,"node_id":"PR_kwDODunzps423tZr","number":4239,"title":"Small fixes in ROC AUC docs","user":{"login":"wschella","id":9478856,"node_id":"MDQ6VXNlcjk0Nzg4NTY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/9478856?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/wschella","html_url":"https:\/\/github.com\/wschella","followers_url":"https:\/\/api.github.com\/users\/wschella\/followers","following_url":"https:\/\/api.github.com\/users\/wschella\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/wschella\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/wschella\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/wschella\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/wschella\/orgs","repos_url":"https:\/\/api.github.com\/users\/wschella\/repos","events_url":"https:\/\/api.github.com\/users\/wschella\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/wschella\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651061750000,"updated_at":1651498137000,"closed_at":1651497723000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"The list of use cases did not render on GitHub with the prepended spacing.\r\nAdditionally, some typo's we're fixed.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4239\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4239","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4239","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4239.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4239.patch","merged_at":1651497723000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4238","id":1217168123,"node_id":"I_kwDODunzps5IjIL7","number":4238,"title":"Dataset caching policy","user":{"login":"loretoparisi","id":163333,"node_id":"MDQ6VXNlcjE2MzMzMw==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/163333?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/loretoparisi","html_url":"https:\/\/github.com\/loretoparisi","followers_url":"https:\/\/api.github.com\/users\/loretoparisi\/followers","following_url":"https:\/\/api.github.com\/users\/loretoparisi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/loretoparisi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/loretoparisi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/loretoparisi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/loretoparisi\/orgs","repos_url":"https:\/\/api.github.com\/users\/loretoparisi\/repos","events_url":"https:\/\/api.github.com\/users\/loretoparisi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/loretoparisi\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi @loretoparisi, thanks for reporting.\r\n\r\nThere is an option to force the redownload of the data files (and thus not using previously download and cached data files): `load_dataset(..., download_mode=\"force_redownload\")`.\r\n\r\nPlease, let me know if this fixes your problem.\r\n\r\nI can confirm you that your dataset loads without any problem for me:\r\n```python\r\nIn [2]: ds = load_dataset(\"loretoparisi\/tatoeba-sentences\", data_files={\"train\": \"train.csv\", \"test\": \"test.csv\"}, delimiter=\"\\t\", column_names=['label', 'text'])\r\n\r\nIn [3]: ds\r\nOut[3]: \r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['label', 'text'],\r\n        num_rows: 8256449\r\n    })\r\n    test: Dataset({\r\n        features: ['label', 'text'],\r\n        num_rows: 2061204\r\n    })\r\n})\r\n``` ","@albertvillanova thank you, it seems it still does not work using:\r\n\r\n```python\r\nsentences = load_dataset(\r\n     \"loretoparisi\/tatoeba-sentences\",\r\n     data_files=data_files,\r\n     delimiter='\\t', \r\n     column_names=['label', 'text'],\r\n     download_mode=\"force_redownload\"\r\n)\r\n```\r\n[This](https:\/\/colab.research.google.com\/drive\/1EA6FWo5pHxU8rPHHRn24NlHqRPiOlPTr?usp=sharing) is my notebook!\r\n\r\nThe problem is that the download file's revision for `test.csv` is not correctly parsed\r\n\r\n![Schermata 2022-04-27 alle 18 09 41](https:\/\/user-images.githubusercontent.com\/163333\/165563507-0be53eb6-8f61-49b0-b959-306e59281de3.png)\r\n\r\nIf you download that file `test.csv` from the repo, the line `\\\\N` is not there anymore (it was there at the first file upload).\r\n\r\nMy impression is that the Apache Arrow file is still cached - so server side, despite of enabling a forced download. For what I can see I get those two arrow files, but I cannot grep the bad line (`\\\\N`) since are binary files:\r\n\r\n```\r\n!ls -l \/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-efeff8965c730a2c\/0.0.0\/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\r\n!ls -l \/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-efeff8965c730a2c\/0.0.0\/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\/csv-test.arrow\r\n!head \/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-efeff8965c730a2c\/0.0.0\/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\/dataset_info.json\r\n```\r\n","SOLVED! The problem was the with the file itself, using caching parameter helped indeed.\r\nThanks for helping!"],"created_at":1651056131000,"updated_at":1651076965000,"closed_at":1651076930000,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\nI cannot clean cache of my datasets files, despite I have updated the `csv` files on the repository [here](https:\/\/huggingface.co\/datasets\/loretoparisi\/tatoeba-sentences). The original file had a line with bad characters, causing the following error\r\n\r\n```\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/features\/features.py](https:\/\/localhost:8080\/#) in str2int(self, values)\r\n    852                 if value not in self._str2int:\r\n    853                     value = str(value).strip()\r\n--> 854                 output.append(self._str2int[str(value)])\r\n    855             else:\r\n    856                 # No names provided, try to integerize\r\n\r\nKeyError: '\\\\N'\r\n```\r\n\r\nThe file now is cleanup up, but I still get the error. This happens even if I inspect the local cached contents, and cleanup the files locally:\r\n\r\n```python\r\nfrom datasets import load_dataset_builder\r\ndataset_builder = load_dataset_builder(\"loretoparisi\/tatoeba-sentences\")\r\nprint(dataset_builder.cache_dir)\r\nprint(dataset_builder.info.features)\r\nprint(dataset_builder.info.splits)\r\n```\r\n\r\n```\r\nUsing custom data configuration loretoparisi--tatoeba-sentences-e59b8ad92f1bb8dd\r\n\/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-e59b8ad92f1bb8dd\/0.0.0\/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\r\nNone\r\nNone\r\n```\r\n\r\nand removing files located at `\/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-*`.\r\n Is there any remote file caching policy in place? If so, is it possibile to programmatically disable it? \r\nCurrently it seems that the file `test.csv` on the repo [here](https:\/\/huggingface.co\/datasets\/loretoparisi\/tatoeba-sentences\/blob\/main\/test.csv) is cached remotely. In fact I download locally the file from raw link, the file is up-to-date; but If I use it within `datasets` as shown above, it gives to me always the first revision of the file, not the last.\r\n\r\nThank you.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset,Features,Value,ClassLabel\r\n\r\nclass_names = [\"cmn\",\"deu\",\"rus\",\"fra\",\"eng\",\"jpn\",\"spa\",\"ita\",\"kor\",\"vie\",\"nld\",\"epo\",\"por\",\"tur\",\"heb\",\"hun\",\"ell\",\"ind\",\"ara\",\"arz\",\"fin\",\"bul\",\"yue\",\"swe\",\"ukr\",\"bel\",\"que\",\"ces\",\"swh\",\"nno\",\"wuu\",\"nob\",\"zsm\",\"est\",\"kat\",\"pol\",\"lat\",\"urd\",\"sqi\",\"isl\",\"fry\",\"afr\",\"ron\",\"fao\",\"san\",\"bre\",\"tat\",\"yid\",\"uig\",\"uzb\",\"srp\",\"qya\",\"dan\",\"pes\",\"slk\",\"eus\",\"cycl\",\"acm\",\"tgl\",\"lvs\",\"kaz\",\"hye\",\"hin\",\"lit\",\"ben\",\"cat\",\"bos\",\"hrv\",\"tha\",\"orv\",\"cha\",\"mon\",\"lzh\",\"scn\",\"gle\",\"mkd\",\"slv\",\"frm\",\"glg\",\"vol\",\"ain\",\"jbo\",\"tok\",\"ina\",\"nds\",\"mal\",\"tlh\",\"roh\",\"ltz\",\"oss\",\"ido\",\"gla\",\"mlt\",\"sco\",\"ast\",\"jav\",\"oci\",\"ile\",\"ota\",\"xal\",\"tel\",\"sjn\",\"nov\",\"khm\",\"tpi\",\"ang\",\"aze\",\"tgk\",\"tuk\",\"chv\",\"hsb\",\"dsb\",\"bod\",\"sme\",\"cym\",\"mri\",\"ksh\",\"kmr\",\"ewe\",\"kab\",\"ber\",\"tpw\",\"udm\",\"lld\",\"pms\",\"lad\",\"grn\",\"mlg\",\"xho\",\"pnb\",\"grc\",\"hat\",\"lao\",\"npi\",\"cor\",\"nah\",\"avk\",\"mar\",\"guj\",\"pan\",\"kir\",\"myv\",\"prg\",\"sux\",\"crs\",\"ckt\",\"bak\",\"zlm\",\"hil\",\"cbk\",\"chr\",\"nav\",\"lkt\",\"enm\",\"arq\",\"lin\",\"abk\",\"pcd\",\"rom\",\"gsw\",\"tam\",\"zul\",\"awa\",\"wln\",\"amh\",\"bar\",\"hbo\",\"mhr\",\"bho\",\"mrj\",\"ckb\",\"osx\",\"pfl\",\"mgm\",\"sna\",\"mah\",\"hau\",\"kan\",\"nog\",\"sin\",\"glv\",\"dng\",\"kal\",\"liv\",\"vro\",\"apc\",\"jdt\",\"fur\",\"che\",\"haw\",\"yor\",\"crh\",\"pdc\",\"ppl\",\"kin\",\"shs\",\"mnw\",\"tet\",\"sah\",\"kum\",\"ngt\",\"nya\",\"pus\",\"hif\",\"mya\",\"moh\",\"wol\",\"tir\",\"ton\",\"lzz\",\"oar\",\"lug\",\"brx\",\"non\",\"mww\",\"hak\",\"nlv\",\"ngu\",\"bua\",\"aym\",\"vec\",\"ibo\",\"tkl\",\"bam\",\"kha\",\"ceb\",\"lou\",\"fuc\",\"smo\",\"gag\",\"lfn\",\"arg\",\"umb\",\"tyv\",\"kjh\",\"oji\",\"cyo\",\"urh\",\"kzj\",\"pam\",\"srd\",\"lmo\",\"swg\",\"mdf\",\"gil\",\"snd\",\"tso\",\"sot\",\"zza\",\"tsn\",\"pau\",\"som\",\"egl\",\"ady\",\"asm\",\"ori\",\"dtp\",\"cho\",\"max\",\"kam\",\"niu\",\"sag\",\"ilo\",\"kaa\",\"fuv\",\"nch\",\"hoc\",\"iba\",\"gbm\",\"sun\",\"war\",\"mvv\",\"pap\",\"ary\",\"kxi\",\"csb\",\"pag\",\"cos\",\"rif\",\"kek\",\"krc\",\"aii\",\"ban\",\"ssw\",\"tvl\",\"mfe\",\"tah\",\"bvy\",\"bcl\",\"hnj\",\"nau\",\"nst\",\"afb\",\"quc\",\"min\",\"tmw\",\"mad\",\"bjn\",\"mai\",\"cjy\",\"got\",\"hsn\",\"gan\",\"tzl\",\"dws\",\"ldn\",\"afh\",\"sgs\",\"krl\",\"vep\",\"rue\",\"tly\",\"mic\",\"ext\",\"izh\",\"sma\",\"jam\",\"cmo\",\"mwl\",\"kpv\",\"koi\",\"bis\",\"ike\",\"run\",\"evn\",\"ryu\",\"mnc\",\"aoz\",\"otk\",\"kas\",\"aln\",\"akl\",\"yua\",\"shy\",\"fkv\",\"gos\",\"fij\",\"thv\",\"zgh\",\"gcf\",\"cay\",\"xmf\",\"tig\",\"div\",\"lij\",\"rap\",\"hrx\",\"cpi\",\"tts\",\"gaa\",\"tmr\",\"iii\",\"ltg\",\"bzt\",\"syc\",\"emx\",\"gom\",\"chg\",\"osp\",\"stq\",\"frr\",\"fro\",\"nys\",\"toi\",\"new\",\"phn\",\"jpa\",\"rel\",\"drt\",\"chn\",\"pli\",\"laa\",\"bal\",\"hdn\",\"hax\",\"mik\",\"ajp\",\"xqa\",\"pal\",\"crk\",\"mni\",\"lut\",\"ayl\",\"ood\",\"sdh\",\"ofs\",\"nus\",\"kiu\",\"diq\",\"qxq\",\"alt\",\"bfz\",\"klj\",\"mus\",\"srn\",\"guc\",\"lim\",\"zea\",\"shi\",\"mnr\",\"bom\",\"sat\",\"szl\"]\r\nfeatures = Features({ 'label': ClassLabel(names=class_names), 'text': Value('string')})\r\nnum_labels = features['label'].num_classes\r\ndata_files = { \"train\": \"train.csv\", \"test\": \"test.csv\" }\r\nsentences = load_dataset(\r\n     \"loretoparisi\/tatoeba-sentences\",\r\n     data_files=data_files,\r\n     delimiter='\\t', \r\n     column_names=['label', 'text'],\r\n)\r\n# You can make this part faster with num_proc=<some int>\r\nsentences = sentences.map(lambda ex: {\"label\" : features[\"label\"].str2int(ex[\"label\"]) if ex[\"label\"] is not None else None}, features=features)\r\nsentences = sentences.shuffle()\r\n```\r\n\r\n## Expected results\r\nProperly tokenize dataset file `test.csv` without issues.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n```\r\nDownloading data files: 100%\r\n2\/2 [00:16<00:00, 7.34s\/it]\r\nDownloading data: 100%\r\n391M\/391M [00:12<00:00, 36.6MB\/s]\r\nDownloading data: 100%\r\n92.4M\/92.4M [00:02<00:00, 40.0MB\/s]\r\nExtracting data files: 100%\r\n2\/2 [00:00<00:00, 47.66it\/s]\r\nDataset csv downloaded and prepared to \/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-efeff8965c730a2c\/0.0.0\/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\r\n100%\r\n2\/2 [00:00<00:00, 25.94it\/s]\r\n11%\r\n942339\/8256449 [01:55<13:11, 9245.85ex\/s]\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n[<ipython-input-3-6a9867fad8d6>](https:\/\/localhost:8080\/#) in <module>()\r\n     12 )\r\n     13 # You can make this part faster with num_proc=<some int>\r\n---> 14 sentences = sentences.map(lambda ex: {\"label\" : features[\"label\"].str2int(ex[\"label\"]) if ex[\"label\"] is not None else None}, features=features)\r\n     15 sentences = sentences.shuffle()\r\n\r\n10 frames\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/features\/features.py](https:\/\/localhost:8080\/#) in str2int(self, values)\r\n    852                 if value not in self._str2int:\r\n    853                     value = str(value).strip()\r\n--> 854                 output.append(self._str2int[str(value)])\r\n    855             else:\r\n    856                 # No names provided, try to integerize\r\n\r\nKeyError: '\\\\N'\r\n```\r\n\r\n## Environment info\r\n```\r\n- `datasets` version: 2.1.0\r\n- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.3.5\r\n- ```\r\n\r\n\r\n```\r\n- `transformers` version: 4.18.0\r\n- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- Huggingface_hub version: 0.5.1\r\n- PyTorch version (GPU?): 1.11.0+cu113 (True)\r\n- Tensorflow version (GPU?): 2.8.0 (True)\r\n- Flax version (CPU?\/GPU?\/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n- ```\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4238\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4237","id":1217121044,"node_id":"I_kwDODunzps5Ii8sU","number":4237,"title":"Common Voice 8 doesn't show datasets viewer","user":{"login":"patrickvonplaten","id":23423619,"node_id":"MDQ6VXNlcjIzNDIzNjE5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/23423619?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/patrickvonplaten","html_url":"https:\/\/github.com\/patrickvonplaten","followers_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/followers","following_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/orgs","repos_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/repos","events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/received_events","type":"User","site_admin":false},"labels":[{"id":3470211881,"node_id":"LA_kwDODunzps7O1zsp","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer","name":"dataset-viewer","color":"E5583E","default":false,"description":"Related to the dataset viewer on huggingface.co"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Thanks for reporting. I understand it's an error in the dataset script. To reproduce:\r\n\r\n```python\r\n>>> import datasets as ds\r\n>>> split_names = ds.get_dataset_split_names(\"mozilla-foundation\/common_voice_8_0\", use_auth_token=\"**********\")\r\nDownloading builder script: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.9k\/10.9k [00:00<00:00, 10.9MB\/s]\r\nDownloading extra modules: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.98k\/2.98k [00:00<00:00, 3.36MB\/s]\r\nDownloading extra modules: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 53.1k\/53.1k [00:00<00:00, 650kB\/s]\r\nNo config specified, defaulting to: common_voice\/en\r\nTraceback (most recent call last):\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/libs\/libmodels\/.venv\/lib\/python3.9\/site-packages\/datasets\/inspect.py\", line 280, in get_dataset_config_info\r\n    for split_generator in builder._split_generators(\r\n  File \"\/home\/slesage\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/mozilla-foundation--common_voice_8_0\/720589e6e5ad674019008b719053303a71716db1b27e63c9846df02fdf93f2f3\/common_voice_8_0.py\", line 153, in _split_generators\r\n    self._log_download(self.config.name, bundle_version, hf_auth_token)\r\n  File \"\/home\/slesage\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/mozilla-foundation--common_voice_8_0\/720589e6e5ad674019008b719053303a71716db1b27e63c9846df02fdf93f2f3\/common_voice_8_0.py\", line 139, in _log_download\r\n    email = HfApi().whoami(auth_token)[\"email\"]\r\nKeyError: 'email'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/libs\/libmodels\/.venv\/lib\/python3.9\/site-packages\/datasets\/inspect.py\", line 323, in get_dataset_split_names\r\n    info = get_dataset_config_info(\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/libs\/libmodels\/.venv\/lib\/python3.9\/site-packages\/datasets\/inspect.py\", line 285, in get_dataset_config_info\r\n    raise SplitsNotFoundError(\"The split names could not be parsed from the dataset config.\") from err\r\ndatasets.inspect.SplitsNotFoundError: The split names could not be parsed from the dataset config.\r\n```","Thanks for reporting @patrickvonplaten and thanks for the investigation @severo.\r\n\r\nUnfortunately I'm not able to reproduce the error.\r\n\r\nI think the error has to do with authentication with `huggingface_hub`, because the exception is thrown from these code lines: https:\/\/huggingface.co\/datasets\/mozilla-foundation\/common_voice_8_0\/blob\/main\/common_voice_8_0.py#L137-L139\r\n```python\r\nfrom huggingface_hub import HfApi, HfFolder\r\n\r\nif isinstance(auth_token, bool):\r\n    email = HfApi().whoami(auth_token)\r\nemail = HfApi().whoami(auth_token)[\"email\"]\r\n```\r\n\r\nCould you please verify the previous code with the `auth_token` you pass to `load_dataset(..., use_auth_token=auth_token,...`?","OK, thanks for digging a bit into it. Indeed, the error occurs with the dataset-viewer, but not with a normal user token, because we use an app token, and it does not have a related email!\r\n\r\n```python\r\n>>> from huggingface_hub import HfApi, HfFolder\r\n>>> auth_token = \"hf_app_******\"\r\n>>> t = HfApi().whoami(auth_token)\r\n>>> t\r\n{'type': 'app', 'name': 'dataset-preview-backend'}\r\n>>> t[\"email\"]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nKeyError: 'email'\r\n```\r\n\r\nNote also that the doc (https:\/\/huggingface.co\/docs\/huggingface_hub\/package_reference\/hf_api#huggingface_hub.HfApi.whoami) does not state that `whoami` should return an `email` key.\r\n\r\n@SBrandeis @julien-c: do you think the app token should have an email associated, like the users?","We can workaround this with\r\n```python\r\nemail = HfApi().whoami(auth_token).get(\"email\", \"system@huggingface.co\")\r\n```\r\nin the common voice scripts","Hmmm, does this mean that any person who downloads the common voice dataset will be logged as \"system@huggingface.co\"? If so, it would defeat the purpose of sending the user's email to the commonvoice API, right?","I agree with @severo: we cannot set our system email as default, allowing anybody not authenticated to by-pass the Common Voice usage policy.\r\n\r\nAdditionally, looking at the code, I think we should implement a more robust way to send user email to Common Voice: currently anybody can tweak the script and send somebody else email instead.\r\n\r\nCC: @patrickvonplaten @lhoestq @SBrandeis @julien-c ","Hmm I don't agree here. \r\n\r\nAnybody can always just bypass the system by setting whatever email. As soon as someone has access to the downloading script it's trivial to tweak the code to not send the \"correct\" email but to just whatever and it would work.\r\n\r\nNote that someone only has visibility on the code after having \"signed\" the access-mechanism so I think we can expect the users to have agreed to not do anything malicious. \r\n\r\nI'm fine with both @lhoestq's solution or we find a way that forces the user to be logged in + being able to load the data for the datasets viewer. Wdyt @lhoestq @severo @albertvillanova ?","> Additionally, looking at the code, I think we should implement a more robust way to send user email to Common Voice: currently anybody can tweak the script and send somebody else email instead.\r\n\r\nYes, I agree we can forget about this @patrickvonplaten. After having had a look at Common Voice website, I've seen they only require sending an email (no auth is inplace on their side, contrary to what I had previously thought). Therefore, currently we impose stronger requirements than them: we require the user having logged in and accepted the access mechanism.\r\n\r\nCurrently the script as it is already requires the user being logged in:\r\n```python\r\nHfApi().whoami(auth_token)\r\n```\r\nthrows an exception if None\/invalid auth_token is passed.\r\n\r\nOn the other hand, we should agree on the way to allow the viewer to stream the data."],"created_at":1651053920000,"updated_at":1651489642000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"https:\/\/huggingface.co\/datasets\/mozilla-foundation\/common_voice_8_0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4237\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4236","id":1217115691,"node_id":"PR_kwDODunzps423MOc","number":4236,"title":"Replace data URL in big_patent dataset and support streaming","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","I first uploaded the data files to the Hub: I think it is a good option because we have git lfs to track versions and changes. Moreover people will be able to make PRs to propose updates on the data files.\r\n- I would have preferred to upload it it to the \"data\" org namespace, but it is already taken (although not used): might be possible to take it?\r\n\r\nAs an alternative (and to be consistent with previous datasets), I also uploaded the data files to our AWS bucket.\r\n\r\nWe should decide which to use (now and for future datasets) and set it here before merging. We should remove the data files for the non-chosen option.\r\n\r\nCC: @lhoestq @mariosasko @polinaeterna ","Would it make sense to make the dataset a community one (so, create an organization for it) and store the script and the data in a single repository? Just as it is for most of the datasets. That way we can also access the data using a relative path inside the repo (that's not the point though). The point is that to me it seems a bit more straightforward to store everything in one place. \r\n\r\nI guess the strong argument against this logic is that in this case the canonical version won't work... But maybe there is some redirecting mechanism I don't know about? :)\r\n\r\nAnyway, I'm in favor of hosting data on the Hub instead of AWS :) ","I also think storing everything in one place\/single repository is the best option.\r\n\r\n@polinaeterna Canonical datasets also support data files (see the [`red_caps` repo](https:\/\/huggingface.co\/datasets\/red_caps\/tree\/main) for instance) ","Thanks @polinaeterna and @mariosasko for your comments.\r\n\r\nYes, definitely it is much better to have everything in the same repo. \r\n\r\nI'm transferring their data files to the Hub under \"big_patent\" and deleting them from the other repo and AWS."],"created_at":1651053673000,"updated_at":1651826314000,"closed_at":1651515675000,"author_association":"MEMBER","active_lock_reason":null,"body":"This PR replaces the Google Drive URL with by our Hub one, once the data owners have approved to host their data on the Hub.\r\n\r\nMoreover, this PR makes the dataset streamable.\r\n\r\nFix #4217.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4236\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4236","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4236","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4236.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4236.patch","merged_at":1651515675000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4235","id":1216952640,"node_id":"I_kwDODunzps5IiTlA","number":4235,"title":"How to load VERY LARGE dataset?","user":{"login":"CaoYiqingT","id":45160643,"node_id":"MDQ6VXNlcjQ1MTYwNjQz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/45160643?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/CaoYiqingT","html_url":"https:\/\/github.com\/CaoYiqingT","followers_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/followers","following_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/orgs","repos_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/repos","events_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/CaoYiqingT\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The `Trainer` support `IterableDataset`, not just datasets."],"created_at":1651045813000,"updated_at":1651059857000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### System Info\n\n```shell\nI am using transformer trainer while meeting the issue.\r\nThe trainer requests torch.utils.data.Dataset as input, which loads the whole dataset into the memory at once. Therefore, when the dataset is too large to load, there's nothing I can do except using IterDataset, which loads samples of data seperately, and results in low efficiency. \r\nI wonder if there are any tricks like Sharding in huggingface trainer.\r\nLooking forward to your reply.\n```\n\n\n### Who can help?\n\nTrainer: @sgugger\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nNone\n\n### Expected behavior\n\n```shell\nI wonder if there are any tricks like fairseq Sharding very large datasets https:\/\/fairseq.readthedocs.io\/en\/latest\/getting_started.html.\r\nThanks a lot!\n```\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4235\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4234","id":1216818846,"node_id":"PR_kwDODunzps422Mwn","number":4234,"title":"Autoeval config","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Related to: https:\/\/github.com\/huggingface\/autonlp-backend\/issues\/414 and https:\/\/github.com\/huggingface\/autonlp-backend\/issues\/424","The tests are failing due to the changed metadata:\r\n\r\n```\r\ngot an unexpected keyword argument 'train-eval-index'\r\n```\r\n\r\nI think you can fix this by updating the `DatasetMetadata` class and implementing an appropriate `validate_train_eval_index()` function\r\n\r\n@lhoestq we are working with an arbitrary set of tags for `autoeval config`. See https:\/\/github.com\/huggingface\/autonlp-backend\/issues\/414\r\nI need to add a validator function though for the tests to pass. Our set is not well-defined as in the rest https:\/\/github.com\/huggingface\/datasets\/tree\/master\/src\/datasets\/utils\/resources. What's a workaround for this?","On the question of validating the `train-eval-index` metadata, I think the simplest approach would be to validate that the required fields exist and not worry about their values (which are open-ended).\r\n\r\nFor me, the required fields include:\r\n\r\n* `config`\r\n* `task`\r\n* `task_id`\r\n* `splits` (train \/ validation \/ eval)\r\n* `col_mapping`\r\n* `metrics` (checking that each one has `type`, `name`) \r\n\r\nHere I'm using the spec defined in https:\/\/github.com\/huggingface\/autonlp-backend\/issues\/414 as a guide.\r\n\r\nWDYT @lhoestq ?","Makes sense ! Currently the metadata type validator doesn't support subfields - let me open a PR to add it","I ended up improving the metadata validation in this PR x)\r\n\r\nIn particular:\r\n- I added support YAML keys with dashes instead of underscores for `train-eval-index`\r\n- I added `train-eval-index` validation  with `validate_train_eval_index`. It does nothing fancy, it just checks that it is a list if it exists in the YAML, but feel free to improve it if you want\r\n\r\nLet me know if it sounds good to you ! I think we can improve `validate_train_eval_index` in another PR","Come on windows... I didn't do anything advanced...\r\n\r\nAnyway, will try to fix this when I get back home x)","> Come on windows... I didn't do anything advanced...\r\n> \r\n> Anyway, will try to fix this when I get back home x)\r\n\r\nHehe, thanks!","Thanks, @lhoestq this is great! ","Did I just fix it for windows and now it fails on linux ? xD","> Did I just fix it for windows and now it fails on linux ? xD\r\n\r\nLooks like the Heisenberg uncertainty principle is at play here - you cannot simultaneously have unit tests passing in both Linux and Windows \ud83d\ude05 ","The worst is that the tests pass locally both on my windows and my linux x)","Ok fixed it, the issue came from python 3.6 that doesn't return the right `__origin__` for Dict and List types","> Alright thanks for adding the first Autoeval config ! :D\r\n\r\nWoohoo! Thank you so much \ud83e\udd17 ","This is cool!"],"created_at":1651037530000,"updated_at":1651843231000,"closed_at":1651774858000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Added autoeval config to imdb as pilot","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4234\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4234","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4234","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4234.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4234.patch","merged_at":1651774858000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4233","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4233\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4233\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4233\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4233","id":1216665044,"node_id":"PR_kwDODunzps421r-6","number":4233,"title":"Autoeval","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4233). All of your documentation changes will be reflected on that endpoint."],"created_at":1651023129000,"updated_at":1651037370000,"closed_at":1651023143000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4233\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4233\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4233","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4233","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4233.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4233.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4232","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4232\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4232\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4232\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4232","id":1216659444,"node_id":"PR_kwDODunzps421qz4","number":4232,"title":"adding new tag to tasks.json and modified for existing datasets","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","closing in favor of https:\/\/github.com\/huggingface\/datasets\/pull\/4244"],"created_at":1651022469000,"updated_at":1651587836000,"closed_at":1651587399000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4232\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4232\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4232","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4232","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4232.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4232.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4231","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4231\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4231\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4231\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4231","id":1216651960,"node_id":"PR_kwDODunzps421pUX","number":4231,"title":"Fix invalid url to CC-Aligned dataset","user":{"login":"juntang-zhuang","id":44451229,"node_id":"MDQ6VXNlcjQ0NDUxMjI5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/44451229?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/juntang-zhuang","html_url":"https:\/\/github.com\/juntang-zhuang","followers_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/followers","following_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/orgs","repos_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/repos","events_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/juntang-zhuang\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1651021621000,"updated_at":1651021689000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"The CC-Aligned dataset url has changed to  https:\/\/data.statmt.org\/cc-aligned\/, the old address http:\/\/www.statmt.org\/cc-aligned\/ is no longer valid","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4231\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4231\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4231","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4231","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4231.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4231.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4230","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4230\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4230\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4230\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4230","id":1216643661,"node_id":"I_kwDODunzps5IhIJN","number":4230,"title":"Why the `conll2003` dataset on huggingface only contains the `en` subset? Where is the  German data?","user":{"login":"beyondguo","id":37113676,"node_id":"MDQ6VXNlcjM3MTEzNjc2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/37113676?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/beyondguo","html_url":"https:\/\/github.com\/beyondguo","followers_url":"https:\/\/api.github.com\/users\/beyondguo\/followers","following_url":"https:\/\/api.github.com\/users\/beyondguo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/beyondguo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/beyondguo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/beyondguo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/beyondguo\/orgs","repos_url":"https:\/\/api.github.com\/users\/beyondguo\/repos","events_url":"https:\/\/api.github.com\/users\/beyondguo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/beyondguo\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Thanks for reporting @beyondguo.\r\n\r\nIndeed, we generate this dataset from this raw data file URL: https:\/\/data.deepai.org\/conll2003.zip\r\nAnd that URL only contains the English version."],"created_at":1651020832000,"updated_at":1651164914000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"![image](https:\/\/user-images.githubusercontent.com\/37113676\/165416606-96b5db18-b16c-4b6b-928c-de8620fd943e.png)\r\n\r\nBut on huggingface datasets:\r\n![image](https:\/\/user-images.githubusercontent.com\/37113676\/165416649-8fd77980-ca0d-43f0-935e-f398ba8323a4.png)\r\n\r\nWhere is the  German data?","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4230\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4230\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4229","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4229\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4229\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4229\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4229","id":1216638968,"node_id":"PR_kwDODunzps421mjM","number":4229,"title":"new task tag","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1651020428000,"updated_at":1651020508000,"closed_at":1651020497000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"multi-input-text-classification tag for classification datasets that take more than one input","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4229\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4229\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4229","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4229","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4229.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4229.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4228","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4228\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4228\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4228\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4228","id":1216523043,"node_id":"PR_kwDODunzps421NKL","number":4228,"title":"new task tag","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1651010433000,"updated_at":1651020511000,"closed_at":1651020391000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"multi-input-text-classification tag for classification datasets that take more than one input","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4228\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4228\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4228","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4228","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4228.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4228.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4227","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4227\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4227\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4227\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4227","id":1216455316,"node_id":"PR_kwDODunzps420-mc","number":4227,"title":"Add f1 metric card, update docstring in py file","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1651005663000,"updated_at":1651582223000,"closed_at":1651581813000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4227\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4227\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4227","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4227","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4227.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4227.patch","merged_at":1651581813000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4226","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4226\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4226\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4226\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4226","id":1216331073,"node_id":"PR_kwDODunzps420kAv","number":4226,"title":"Add pearsonr mc, update functionality to match the original docs","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","thank you @lhoestq!! :hugs: "],"created_at":1650997846000,"updated_at":1651597764000,"closed_at":1651597348000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"- adds pearsonr metric card\r\n- adds ability to return p-value\r\n    - p-value was mentioned in the original docs as a return value, but there was no option to return it. I updated the _compute function slightly to have an option to return the p-value.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4226\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4226\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4226","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4226","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4226.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4226.patch","merged_at":1651597348000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4225","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4225\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4225\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4225\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4225","id":1216213464,"node_id":"PR_kwDODunzps420LNM","number":4225,"title":"autoeval config","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1650991114000,"updated_at":1651020511000,"closed_at":1651010426000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"add train eval index for autoeval","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4225\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4225\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4225","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4225","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4225.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4225.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4224","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4224\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4224\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4224\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4224","id":1216209667,"node_id":"PR_kwDODunzps420KX2","number":4224,"title":"autoeval config","user":{"login":"nrajani","id":3278583,"node_id":"MDQ6VXNlcjMyNzg1ODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3278583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nrajani","html_url":"https:\/\/github.com\/nrajani","followers_url":"https:\/\/api.github.com\/users\/nrajani\/followers","following_url":"https:\/\/api.github.com\/users\/nrajani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nrajani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nrajani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nrajani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nrajani\/orgs","repos_url":"https:\/\/api.github.com\/users\/nrajani\/repos","events_url":"https:\/\/api.github.com\/users\/nrajani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nrajani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1650990919000,"updated_at":1650991005000,"closed_at":1650991005000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"add train eval index for autoeval","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4224\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4224\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4224","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4224","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4224.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4224.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4223","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4223\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4223\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4223\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4223","id":1216107082,"node_id":"PR_kwDODunzps42z0YV","number":4223,"title":"Add Accuracy Metric Card","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650985846000,"updated_at":1651588065000,"closed_at":1651587647000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"- adds accuracy metric card\r\n- updates docstring in accuracy.py\r\n- adds .json file with metric card and docstring information","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4223\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4223\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4223","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4223","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4223.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4223.patch","merged_at":1651587647000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4222","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4222\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4222\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4222\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4222","id":1216056439,"node_id":"PR_kwDODunzps42zpcd","number":4222,"title":"Fix description links in dataset cards","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Non passing tests are due to other pre-existing errors in dataset cards: not related to this PR."],"created_at":1650983785000,"updated_at":1651826318000,"closed_at":1650991949000,"author_association":"MEMBER","active_lock_reason":null,"body":"I noticed many links were not properly displayed (only text, no link) on the Hub because of wrong syntax, e.g.: https:\/\/huggingface.co\/datasets\/big_patent\r\n\r\nThis PR fixes all description links in dataset cards.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4222\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4222\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4222","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4222","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4222.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4222.patch","merged_at":1650991949000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4221","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4221\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4221\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4221\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4221","id":1215911182,"node_id":"I_kwDODunzps5IeVUO","number":4221,"title":"Dictionary Feature","user":{"login":"jordiae","id":2944532,"node_id":"MDQ6VXNlcjI5NDQ1MzI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/2944532?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jordiae","html_url":"https:\/\/github.com\/jordiae","followers_url":"https:\/\/api.github.com\/users\/jordiae\/followers","following_url":"https:\/\/api.github.com\/users\/jordiae\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jordiae\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jordiae\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jordiae\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jordiae\/orgs","repos_url":"https:\/\/api.github.com\/users\/jordiae\/repos","events_url":"https:\/\/api.github.com\/users\/jordiae\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jordiae\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892912,"node_id":"MDU6TGFiZWwxOTM1ODkyOTEy","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/question","name":"question","color":"d876e3","default":true,"description":"Further information is requested"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi @jordiae,\r\n\r\nInstead of the `Sequence` feature, you can use just a regular list: put the dict between `[` and `]`:\r\n```python\r\n\"list_of_dict_feature\": [\r\n    {\r\n        \"key1_in_dict\": datasets.Value(\"string\"),\r\n        \"key2_in_dict\": datasets.Value(\"int32\"),\r\n        ...\r\n    }\r\n],\r\n```\r\n\r\nFeel free to re-open this issue if that does not work for your use case.","> Hi @jordiae,\r\n> \r\n> Instead of the `Sequence` feature, you can use just a regular list: put the dict between `[` and `]`:\r\n> \r\n> ```python\r\n> \"list_of_dict_feature\": [\r\n>     {\r\n>         \"key1_in_dict\": datasets.Value(\"string\"),\r\n>         \"key2_in_dict\": datasets.Value(\"int32\"),\r\n>         ...\r\n>     }\r\n> ],\r\n> ```\r\n> \r\n> Feel free to re-open this issue if that does not work for your use case.\r\n\r\nThank you"],"created_at":1650977418000,"updated_at":1651243939000,"closed_at":1651165498000,"author_association":"NONE","active_lock_reason":null,"body":"Hi, I'm trying to create the loading script for a dataset in which one feature is a list of dictionaries, which afaik doesn't fit very well the values and structures supported by Value and Sequence. Is there any suggested workaround, am I missing something?\r\n\r\nThank you in advance.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4221\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4221\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4220","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4220\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4220\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4220\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4220","id":1215225802,"node_id":"PR_kwDODunzps42w5YO","number":4220,"title":"Altered faiss installation comment","user":{"login":"vishalsrao","id":36671559,"node_id":"MDQ6VXNlcjM2NjcxNTU5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36671559?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/vishalsrao","html_url":"https:\/\/github.com\/vishalsrao","followers_url":"https:\/\/api.github.com\/users\/vishalsrao\/followers","following_url":"https:\/\/api.github.com\/users\/vishalsrao\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/vishalsrao\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/vishalsrao\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/vishalsrao\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/vishalsrao\/orgs","repos_url":"https:\/\/api.github.com\/users\/vishalsrao\/repos","events_url":"https:\/\/api.github.com\/users\/vishalsrao\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/vishalsrao\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4220). All of your documentation changes will be reflected on that endpoint.","Hi ! Can you explain why this change is needed ?","Facebook recommends installing FAISS using conda (https:\/\/github.com\/facebookresearch\/faiss\/blob\/main\/INSTALL.md). pip does not seem to have the latest version of FAISS. The latest version of faiss is 1.7.2 (https:\/\/anaconda.org\/conda-forge\/faiss), but the latest one available through pip is 1.5.3 (https:\/\/pypi.org\/project\/faiss\/). "],"created_at":1650936043000,"updated_at":1651834011000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4220\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4220\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4220","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4220","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4220.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4220.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4219","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4219\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4219\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4219\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4219","id":1214934025,"node_id":"PR_kwDODunzps42v6rE","number":4219,"title":"Add F1 Metric Card","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650914096000,"updated_at":1651005858000,"closed_at":1651005466000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4219\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4219\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4219","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4219","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4219.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4219.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4218","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4218\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4218\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4218\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4218","id":1214748226,"node_id":"PR_kwDODunzps42vTA0","number":4218,"title":"Make code for image downloading from image urls cacheable","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650903479000,"updated_at":1650992424000,"closed_at":1650980306000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #4199 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4218\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4218\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4218","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4218","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4218.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4218.patch","merged_at":1650980306000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4217","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4217\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4217\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4217\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4217","id":1214688141,"node_id":"I_kwDODunzps5IZquN","number":4217,"title":"Big_Patent dataset broken","user":{"login":"Matthew-Larsen","id":54189843,"node_id":"MDQ6VXNlcjU0MTg5ODQz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/54189843?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Matthew-Larsen","html_url":"https:\/\/github.com\/Matthew-Larsen","followers_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/followers","following_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/orgs","repos_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/repos","events_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Matthew-Larsen\/received_events","type":"User","site_admin":false},"labels":[{"id":4069435429,"node_id":"LA_kwDODunzps7yjqgl","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/hosted-on-google-drive","name":"hosted-on-google-drive","color":"8B51EF","default":false,"description":""}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Thanks for reporting. The issue seems not to be directly related to the dataset viewer or the `datasets` library, but instead to it being hosted on Google Drive.\r\n\r\nSee related issues: https:\/\/github.com\/huggingface\/datasets\/issues?q=is%3Aissue+is%3Aopen+drive.google.com\r\n\r\nTo quote [@lhoestq](https:\/\/github.com\/huggingface\/datasets\/issues\/4075#issuecomment-1087362551):\r\n\r\n> PS: if possible, please try to not use Google Drive links in your dataset script, since Google Drive has download quotas and is not always reliable.\r\n\r\n","We should find out if the dataset license allows redistribution and contact the data owners to propose them to host their data on our Hub.","The data owners have agreed on hosting their data on the Hub."],"created_at":1650900705000,"updated_at":1651515675000,"closed_at":1651515675000,"author_association":"NONE","active_lock_reason":null,"body":"## Dataset viewer issue for '*big_patent*'\r\n\r\n**Link:** *[link to the dataset viewer page](https:\/\/huggingface.co\/datasets\/big_patent\/viewer\/all\/train)*\r\n\r\n*Unable to view because it says FileNotFound, also cannot download it through the python API*\r\n\r\nAm I the one who added this dataset ? No\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4217\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4217\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4216","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4216\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4216\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4216\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4216","id":1214614029,"node_id":"PR_kwDODunzps42u1_w","number":4216,"title":"Avoid recursion error in map if example is returned as dict value","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650897632000,"updated_at":1651684806000,"closed_at":1651684372000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"I noticed this bug while answering [this question](https:\/\/discuss.huggingface.co\/t\/correct-way-to-create-a-dataset-from-a-csv-file\/15686\/11?u=mariosasko). \r\n\r\nThis code replicates the bug:\r\n```python\r\nfrom datasets import Dataset\r\ndset = Dataset.from_dict({\"en\": [\"aa\", \"bb\"], \"fr\": [\"cc\", \"dd\"]})\r\ndset.map(lambda ex: {\"translation\": ex})\r\n```\r\nand this is the fix for it (before this PR):\r\n```python\r\nfrom datasets import Dataset\r\ndset = Dataset.from_dict({\"en\": [\"aa\", \"bb\"], \"fr\": [\"cc\", \"dd\"]})\r\ndset.map(lambda ex: {\"translation\": dict(ex)})\r\n```\r\n\r\nInternally, this can be fixed by merging two dicts via dict unpacking (instead of `dict.update) `in `Dataset.map`, which avoids creating recursive dictionaries.\r\n\r\nP.S. `{**a, **b}` is slightly more performant than `a.update(b)` in my bencmarks.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4216\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4216\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4216","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4216","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4216.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4216.patch","merged_at":1651684372000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4215","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4215\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4215\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4215\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4215","id":1214579162,"node_id":"PR_kwDODunzps42uuhY","number":4215,"title":"Add `drop_last_batch` to `IterableDataset.map`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650896119000,"updated_at":1651593367000,"closed_at":1651592934000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Addresses this comment: https:\/\/github.com\/huggingface\/datasets\/pull\/3801#pullrequestreview-901736921","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4215\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4215\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4215","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4215","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4215.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4215.patch","merged_at":1651592934000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4214","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4214\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4214\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4214\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4214","id":1214572430,"node_id":"PR_kwDODunzps42utC5","number":4214,"title":"Skip checksum computation in Imagefolder by default","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650895841000,"updated_at":1651591712000,"closed_at":1651591289000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Avoids having to set `ignore_verifications=True` in `load_dataset(\"imagefolder\", ...)` to skip checksum verification and speed up loading.\r\n\r\nThe user can still pass `DownloadConfig(record_checksums=True)` to not skip this part. ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4214\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4214\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4214","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4214","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4214.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4214.patch","merged_at":1651591289000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4213","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4213\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4213\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4213\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4213","id":1214510010,"node_id":"PR_kwDODunzps42uft_","number":4213,"title":"ETT time series dataset","user":{"login":"kashif","id":8100,"node_id":"MDQ6VXNlcjgxMDA=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8100?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kashif","html_url":"https:\/\/github.com\/kashif","followers_url":"https:\/\/api.github.com\/users\/kashif\/followers","following_url":"https:\/\/api.github.com\/users\/kashif\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kashif\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kashif\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kashif\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kashif\/orgs","repos_url":"https:\/\/api.github.com\/users\/kashif\/repos","events_url":"https:\/\/api.github.com\/users\/kashif\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kashif\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","thank you!\r\n"],"created_at":1650893178000,"updated_at":1651753161000,"closed_at":1651752635000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Ready for review.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4213\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4213\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4213","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4213","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4213.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4213.patch","merged_at":1651752635000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4212","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4212\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4212\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4212\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4212","id":1214498582,"node_id":"PR_kwDODunzps42udRf","number":4212,"title":"[Common Voice] Make sure bytes are correctly deleted if `path` exists","user":{"login":"patrickvonplaten","id":23423619,"node_id":"MDQ6VXNlcjIzNDIzNjE5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/23423619?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/patrickvonplaten","html_url":"https:\/\/github.com\/patrickvonplaten","followers_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/followers","following_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/orgs","repos_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/repos","events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","cool that you noticed that we store unnecessary bytes again :D "],"created_at":1650892706000,"updated_at":1651013668000,"closed_at":1651013307000,"author_association":"MEMBER","active_lock_reason":null,"body":"`path` should be set to local path inside audio feature if exist so that bytes can correctly be deleted.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4212\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4212\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4212","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4212","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4212.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4212.patch","merged_at":1651013307000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4211","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4211\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4211\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4211\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4211","id":1214361837,"node_id":"I_kwDODunzps5IYbDt","number":4211,"title":"DatasetDict containing Datasets with different features when pushed to hub gets remapped features","user":{"login":"pietrolesci","id":61748653,"node_id":"MDQ6VXNlcjYxNzQ4NjUz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/61748653?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/pietrolesci","html_url":"https:\/\/github.com\/pietrolesci","followers_url":"https:\/\/api.github.com\/users\/pietrolesci\/followers","following_url":"https:\/\/api.github.com\/users\/pietrolesci\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/pietrolesci\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/pietrolesci\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/pietrolesci\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/pietrolesci\/orgs","repos_url":"https:\/\/api.github.com\/users\/pietrolesci\/repos","events_url":"https:\/\/api.github.com\/users\/pietrolesci\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/pietrolesci\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi @pietrolesci, thanks for reporting.\r\n\r\nPlease note that this is a design purpose: a `DatasetDict` has the same features for all its datasets. Normally, a `DatasetDict` is composed of several sub-datasets each corresponding to a different **split**.\r\n\r\nTo handle sub-datasets with different features, we use another approach: use different **configurations** instead of **splits**.\r\n\r\nHowever, for the moment `push_to_hub` does not support specifying different configurations. IMHO, we should implement this.","Hi @albertvillanova,\r\n\r\nThanks a lot for your reply! I got it now. The strange thing for me was to have it correctly working (i.e., DatasetDict with different features in some datasets) locally and not on the Hub. It would be great to have configuration supported by `push_to_hub`. Personally, this latter functionality allowed me to iterate rather quickly on dataset curation.\r\n\r\nAgain, thanks for your time @albertvillanova!\r\n\r\nBest,\r\nPietro","Hi! Yes, we should override `DatasetDict.__setitem__` and throw an error if features dictionaries are different. `DatasetDict` is a subclass of `dict`, so `DatasetDict.{update\/setdefault}` need to be overridden as well. We could avoid this by subclassing `UserDict`, but then we would get the name collision - `DatasetDict.data` vs. `UserDict.data`. This makes me think we should rename the `data` attribute of `DatasetDict`\/`Dataset` for easier dict subclassing (would also simplify https:\/\/github.com\/huggingface\/datasets\/pull\/3997) and to follow good Python practices. Another option is to have a custom `UserDict` class in `py_utils`, but it can be hard to keep this class consistent with the built-in `UserDict`. \r\n\r\n@albertvillanova @lhoestq wdyt?","I would keep things simple and keep subclassing dict. Regarding the features check, I guess this can be done only for `push_to_hub` right ? It is the only function right now that requires the underlying datasets to be splits (e.g. train\/test) and have the same features.\r\n\r\nNote that later you will be able to push datasets with different features as different dataset **configurations** (similarly to the [GLUE subsets](https:\/\/huggingface.co\/datasets\/glue) for example). We will work on this soon"],"created_at":1650885774000,"updated_at":1650990732000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi there,\r\n\r\nI am trying to load a dataset to the Hub. This dataset is a `DatasetDict` composed of various splits. Some splits have a different `Feature` mapping. Locally, the DatasetDict preserves the individual features but if I `push_to_hub` and then `load_dataset`, the features are all the same.\r\n\r\nDataset and code to reproduce available [here](https:\/\/huggingface.co\/datasets\/pietrolesci\/robust_nli).\r\n\r\nIn short:\r\n\r\nI have 3 feature mapping\r\n```python\r\nTri_features = Features(\r\n    {\r\n        \"idx\": Value(dtype=\"int64\"),\r\n        \"premise\": Value(dtype=\"string\"),\r\n        \"hypothesis\": Value(dtype=\"string\"),\r\n        \"label\": ClassLabel(num_classes=3, names=[\"entailment\", \"neutral\", \"contradiction\"]),\r\n    }\r\n)\r\n\r\nEnt_features = Features(\r\n    {\r\n        \"idx\": Value(dtype=\"int64\"),\r\n        \"premise\": Value(dtype=\"string\"),\r\n        \"hypothesis\": Value(dtype=\"string\"),\r\n        \"label\": ClassLabel(num_classes=2, names=[\"non-entailment\", \"entailment\"]),\r\n    }\r\n)\r\n\r\nCon_features = Features(\r\n    {\r\n        \"idx\": Value(dtype=\"int64\"),\r\n        \"premise\": Value(dtype=\"string\"),\r\n        \"hypothesis\": Value(dtype=\"string\"),\r\n        \"label\": ClassLabel(num_classes=2, names=[\"non-contradiction\", \"contradiction\"]),\r\n    }\r\n)\r\n```\r\n\r\nThen I create different datasets\r\n\r\n```python\r\ndataset_splits = {}\r\n\r\nfor split in df[\"split\"].unique():\r\n    print(split)\r\n    df_split = df.loc[df[\"split\"] == split].copy()\r\n    \r\n    if split in Tri_dataset:\r\n        df_split[\"label\"] = df_split[\"label\"].map({\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2})\r\n        ds = Dataset.from_pandas(df_split, features=Tri_features)\r\n    \r\n    elif split in Ent_bin_dataset:\r\n        df_split[\"label\"] = df_split[\"label\"].map({\"non-entailment\": 0, \"entailment\": 1})\r\n        ds = Dataset.from_pandas(df_split, features=Ent_features)\r\n    \r\n    elif split in Con_bin_dataset:\r\n        df_split[\"label\"] = df_split[\"label\"].map({\"non-contradiction\": 0, \"contradiction\": 1})\r\n        ds = Dataset.from_pandas(df_split, features=Con_features)\r\n\r\n    else:\r\n        print(\"ERROR:\", split)\r\n    dataset_splits[split] = ds\r\ndatasets = DatasetDict(dataset_splits)\r\n```\r\n\r\nI then push to hub\r\n\r\n```python\r\ndatasets.push_to_hub(\"pietrolesci\/robust_nli\", token=\"<token>\")\r\n```\r\n\r\nFinally, I load it from the hub\r\n\r\n```python\r\ndatasets_loaded_from_hub = load_dataset(\"pietrolesci\/robust_nli\")\r\n```\r\n\r\nAnd I get that\r\n\r\n```python\r\ndatasets[\"LI_TS\"].features != datasets_loaded_from_hub[\"LI_TS\"].features\r\n```\r\n\r\nsince \r\n\r\n```python\r\n\"label\": ClassLabel(num_classes=2, names=[\"non-contradiction\", \"contradiction\"])\r\n```\r\n\r\ngets remapped to \r\n\r\n```python\r\n \"label\": ClassLabel(num_classes=3, names=[\"entailment\", \"neutral\", \"contradiction\"])\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4211\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4211\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4210","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4210\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4210\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4210\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4210","id":1214089130,"node_id":"I_kwDODunzps5IXYeq","number":4210,"title":"TypeError: Cannot cast array data from dtype('O') to dtype('int64') according to the rule 'safe'","user":{"login":"loretoparisi","id":163333,"node_id":"MDQ6VXNlcjE2MzMzMw==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/163333?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/loretoparisi","html_url":"https:\/\/github.com\/loretoparisi","followers_url":"https:\/\/api.github.com\/users\/loretoparisi\/followers","following_url":"https:\/\/api.github.com\/users\/loretoparisi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/loretoparisi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/loretoparisi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/loretoparisi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/loretoparisi\/orgs","repos_url":"https:\/\/api.github.com\/users\/loretoparisi\/repos","events_url":"https:\/\/api.github.com\/users\/loretoparisi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/loretoparisi\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi! Casting class labels from strings is currently not supported in the CSV loader, but you can get the same result with an additional map as follows:\r\n```python\r\nfrom datasets import load_dataset,Features,Value,ClassLabel\r\nclass_names = [\"cmn\",\"deu\",\"rus\",\"fra\",\"eng\",\"jpn\",\"spa\",\"ita\",\"kor\",\"vie\",\"nld\",\"epo\",\"por\",\"tur\",\"heb\",\"hun\",\"ell\",\"ind\",\"ara\",\"arz\",\"fin\",\"bul\",\"yue\",\"swe\",\"ukr\",\"bel\",\"que\",\"ces\",\"swh\",\"nno\",\"wuu\",\"nob\",\"zsm\",\"est\",\"kat\",\"pol\",\"lat\",\"urd\",\"sqi\",\"isl\",\"fry\",\"afr\",\"ron\",\"fao\",\"san\",\"bre\",\"tat\",\"yid\",\"uig\",\"uzb\",\"srp\",\"qya\",\"dan\",\"pes\",\"slk\",\"eus\",\"cycl\",\"acm\",\"tgl\",\"lvs\",\"kaz\",\"hye\",\"hin\",\"lit\",\"ben\",\"cat\",\"bos\",\"hrv\",\"tha\",\"orv\",\"cha\",\"mon\",\"lzh\",\"scn\",\"gle\",\"mkd\",\"slv\",\"frm\",\"glg\",\"vol\",\"ain\",\"jbo\",\"tok\",\"ina\",\"nds\",\"mal\",\"tlh\",\"roh\",\"ltz\",\"oss\",\"ido\",\"gla\",\"mlt\",\"sco\",\"ast\",\"jav\",\"oci\",\"ile\",\"ota\",\"xal\",\"tel\",\"sjn\",\"nov\",\"khm\",\"tpi\",\"ang\",\"aze\",\"tgk\",\"tuk\",\"chv\",\"hsb\",\"dsb\",\"bod\",\"sme\",\"cym\",\"mri\",\"ksh\",\"kmr\",\"ewe\",\"kab\",\"ber\",\"tpw\",\"udm\",\"lld\",\"pms\",\"lad\",\"grn\",\"mlg\",\"xho\",\"pnb\",\"grc\",\"hat\",\"lao\",\"npi\",\"cor\",\"nah\",\"avk\",\"mar\",\"guj\",\"pan\",\"kir\",\"myv\",\"prg\",\"sux\",\"crs\",\"ckt\",\"bak\",\"zlm\",\"hil\",\"cbk\",\"chr\",\"nav\",\"lkt\",\"enm\",\"arq\",\"lin\",\"abk\",\"pcd\",\"rom\",\"gsw\",\"tam\",\"zul\",\"awa\",\"wln\",\"amh\",\"bar\",\"hbo\",\"mhr\",\"bho\",\"mrj\",\"ckb\",\"osx\",\"pfl\",\"mgm\",\"sna\",\"mah\",\"hau\",\"kan\",\"nog\",\"sin\",\"glv\",\"dng\",\"kal\",\"liv\",\"vro\",\"apc\",\"jdt\",\"fur\",\"che\",\"haw\",\"yor\",\"crh\",\"pdc\",\"ppl\",\"kin\",\"shs\",\"mnw\",\"tet\",\"sah\",\"kum\",\"ngt\",\"nya\",\"pus\",\"hif\",\"mya\",\"moh\",\"wol\",\"tir\",\"ton\",\"lzz\",\"oar\",\"lug\",\"brx\",\"non\",\"mww\",\"hak\",\"nlv\",\"ngu\",\"bua\",\"aym\",\"vec\",\"ibo\",\"tkl\",\"bam\",\"kha\",\"ceb\",\"lou\",\"fuc\",\"smo\",\"gag\",\"lfn\",\"arg\",\"umb\",\"tyv\",\"kjh\",\"oji\",\"cyo\",\"urh\",\"kzj\",\"pam\",\"srd\",\"lmo\",\"swg\",\"mdf\",\"gil\",\"snd\",\"tso\",\"sot\",\"zza\",\"tsn\",\"pau\",\"som\",\"egl\",\"ady\",\"asm\",\"ori\",\"dtp\",\"cho\",\"max\",\"kam\",\"niu\",\"sag\",\"ilo\",\"kaa\",\"fuv\",\"nch\",\"hoc\",\"iba\",\"gbm\",\"sun\",\"war\",\"mvv\",\"pap\",\"ary\",\"kxi\",\"csb\",\"pag\",\"cos\",\"rif\",\"kek\",\"krc\",\"aii\",\"ban\",\"ssw\",\"tvl\",\"mfe\",\"tah\",\"bvy\",\"bcl\",\"hnj\",\"nau\",\"nst\",\"afb\",\"quc\",\"min\",\"tmw\",\"mad\",\"bjn\",\"mai\",\"cjy\",\"got\",\"hsn\",\"gan\",\"tzl\",\"dws\",\"ldn\",\"afh\",\"sgs\",\"krl\",\"vep\",\"rue\",\"tly\",\"mic\",\"ext\",\"izh\",\"sma\",\"jam\",\"cmo\",\"mwl\",\"kpv\",\"koi\",\"bis\",\"ike\",\"run\",\"evn\",\"ryu\",\"mnc\",\"aoz\",\"otk\",\"kas\",\"aln\",\"akl\",\"yua\",\"shy\",\"fkv\",\"gos\",\"fij\",\"thv\",\"zgh\",\"gcf\",\"cay\",\"xmf\",\"tig\",\"div\",\"lij\",\"rap\",\"hrx\",\"cpi\",\"tts\",\"gaa\",\"tmr\",\"iii\",\"ltg\",\"bzt\",\"syc\",\"emx\",\"gom\",\"chg\",\"osp\",\"stq\",\"frr\",\"fro\",\"nys\",\"toi\",\"new\",\"phn\",\"jpa\",\"rel\",\"drt\",\"chn\",\"pli\",\"laa\",\"bal\",\"hdn\",\"hax\",\"mik\",\"ajp\",\"xqa\",\"pal\",\"crk\",\"mni\",\"lut\",\"ayl\",\"ood\",\"sdh\",\"ofs\",\"nus\",\"kiu\",\"diq\",\"qxq\",\"alt\",\"bfz\",\"klj\",\"mus\",\"srn\",\"guc\",\"lim\",\"zea\",\"shi\",\"mnr\",\"bom\",\"sat\",\"szl\"]\r\nfeatures = Features({ 'label': ClassLabel(names=class_names), 'text': Value('string')})\r\nnum_labels = features['label'].num_classes\r\ndata_files = { \"train\": \"train.csv\", \"test\": \"test.csv\" }\r\nsentences = load_dataset(\r\n     \"loretoparisi\/tatoeba-sentences\",\r\n     data_files=data_files,\r\n     delimiter='\\t', \r\n     column_names=['label', 'text'],\r\n)\r\n# You can make this part faster with num_proc=<some int>\r\nsentences = sentences.map(lambda ex: features[\"label\"].str2int(ex[\"label\"]) if ex[\"label\"] is not None else None, features=features)\r\n```\r\n\r\n@lhoestq IIRC, I suggested adding `cast_to_storage` to `ClassLabel`  + `table_cast` to the packaged loaders if the `ClassLabel`\/`Image`\/`Audio` type is present in `features` to avoid this kind of error, but your concern was speed. IMO shouldn't be a problem if we do `table_cast` only when these features are present.","I agree packaged loaders should support `ClassLabel` feature without throwing an error.","@albertvillanova @mariosasko thank you, with that change now I get\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n[<ipython-input-9-eeb68eeb9bec>](https:\/\/localhost:8080\/#) in <module>()\r\n     11 )\r\n     12 # You can make this part faster with num_proc=<some int>\r\n---> 13 sentences = sentences.map(lambda ex: features[\"label\"].str2int(ex[\"label\"]) if ex[\"label\"] is not None else None, features=features)\r\n     14 sentences = sentences.shuffle()\r\n\r\n8 frames\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in validate_function_output(processed_inputs, indices)\r\n   2193             if processed_inputs is not None and not isinstance(processed_inputs, (Mapping, pa.Table)):\r\n   2194                 raise TypeError(\r\n-> 2195                     f\"Provided `function` which is applied to all elements of table returns a variable of type {type(processed_inputs)}. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects.\"\r\n   2196                 )\r\n   2197             elif isinstance(indices, list) and isinstance(processed_inputs, Mapping):\r\n\r\nTypeError: Provided `function` which is applied to all elements of table returns a variable of type <class 'int'>. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects.\r\n```\r\n\r\nthe error is raised by [this](https:\/\/github.com\/huggingface\/datasets\/blob\/master\/src\/datasets\/arrow_dataset.py#L2221)\r\n\r\n```\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in validate_function_output(processed_inputs, indices)\r\n```","@mariosasko changed it like\r\n\r\n```python\r\nsentences = sentences.map(lambda ex: {\"label\" : features[\"label\"].str2int(ex[\"label\"]) if ex[\"label\"] is not None else None}, features=features)\r\n```\r\n\r\nto avoid the above errorr.","Any update on this? Is this correct ?\r\n> @mariosasko changed it like\r\n> \r\n> ```python\r\n> sentences = sentences.map(lambda ex: {\"label\" : features[\"label\"].str2int(ex[\"label\"]) if ex[\"label\"] is not None else None}, features=features)\r\n> ```\r\n> \r\n> to avoid the above errorr.\r\n\r\n"],"created_at":1650871722000,"updated_at":1651247976000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### System Info\r\n\r\n```shell\r\n- `transformers` version: 4.18.0\r\n- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- Huggingface_hub version: 0.5.1\r\n- PyTorch version (GPU?): 1.10.0+cu111 (True)\r\n- Tensorflow version (GPU?): 2.8.0 (True)\r\n- Flax version (CPU?\/GPU?\/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n```\r\n\r\n\r\n### Who can help?\r\n\r\n@LysandreJik \r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [X] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n```python\r\nfrom datasets import load_dataset,Features,Value,ClassLabel\r\n\r\nclass_names = [\"cmn\",\"deu\",\"rus\",\"fra\",\"eng\",\"jpn\",\"spa\",\"ita\",\"kor\",\"vie\",\"nld\",\"epo\",\"por\",\"tur\",\"heb\",\"hun\",\"ell\",\"ind\",\"ara\",\"arz\",\"fin\",\"bul\",\"yue\",\"swe\",\"ukr\",\"bel\",\"que\",\"ces\",\"swh\",\"nno\",\"wuu\",\"nob\",\"zsm\",\"est\",\"kat\",\"pol\",\"lat\",\"urd\",\"sqi\",\"isl\",\"fry\",\"afr\",\"ron\",\"fao\",\"san\",\"bre\",\"tat\",\"yid\",\"uig\",\"uzb\",\"srp\",\"qya\",\"dan\",\"pes\",\"slk\",\"eus\",\"cycl\",\"acm\",\"tgl\",\"lvs\",\"kaz\",\"hye\",\"hin\",\"lit\",\"ben\",\"cat\",\"bos\",\"hrv\",\"tha\",\"orv\",\"cha\",\"mon\",\"lzh\",\"scn\",\"gle\",\"mkd\",\"slv\",\"frm\",\"glg\",\"vol\",\"ain\",\"jbo\",\"tok\",\"ina\",\"nds\",\"mal\",\"tlh\",\"roh\",\"ltz\",\"oss\",\"ido\",\"gla\",\"mlt\",\"sco\",\"ast\",\"jav\",\"oci\",\"ile\",\"ota\",\"xal\",\"tel\",\"sjn\",\"nov\",\"khm\",\"tpi\",\"ang\",\"aze\",\"tgk\",\"tuk\",\"chv\",\"hsb\",\"dsb\",\"bod\",\"sme\",\"cym\",\"mri\",\"ksh\",\"kmr\",\"ewe\",\"kab\",\"ber\",\"tpw\",\"udm\",\"lld\",\"pms\",\"lad\",\"grn\",\"mlg\",\"xho\",\"pnb\",\"grc\",\"hat\",\"lao\",\"npi\",\"cor\",\"nah\",\"avk\",\"mar\",\"guj\",\"pan\",\"kir\",\"myv\",\"prg\",\"sux\",\"crs\",\"ckt\",\"bak\",\"zlm\",\"hil\",\"cbk\",\"chr\",\"nav\",\"lkt\",\"enm\",\"arq\",\"lin\",\"abk\",\"pcd\",\"rom\",\"gsw\",\"tam\",\"zul\",\"awa\",\"wln\",\"amh\",\"bar\",\"hbo\",\"mhr\",\"bho\",\"mrj\",\"ckb\",\"osx\",\"pfl\",\"mgm\",\"sna\",\"mah\",\"hau\",\"kan\",\"nog\",\"sin\",\"glv\",\"dng\",\"kal\",\"liv\",\"vro\",\"apc\",\"jdt\",\"fur\",\"che\",\"haw\",\"yor\",\"crh\",\"pdc\",\"ppl\",\"kin\",\"shs\",\"mnw\",\"tet\",\"sah\",\"kum\",\"ngt\",\"nya\",\"pus\",\"hif\",\"mya\",\"moh\",\"wol\",\"tir\",\"ton\",\"lzz\",\"oar\",\"lug\",\"brx\",\"non\",\"mww\",\"hak\",\"nlv\",\"ngu\",\"bua\",\"aym\",\"vec\",\"ibo\",\"tkl\",\"bam\",\"kha\",\"ceb\",\"lou\",\"fuc\",\"smo\",\"gag\",\"lfn\",\"arg\",\"umb\",\"tyv\",\"kjh\",\"oji\",\"cyo\",\"urh\",\"kzj\",\"pam\",\"srd\",\"lmo\",\"swg\",\"mdf\",\"gil\",\"snd\",\"tso\",\"sot\",\"zza\",\"tsn\",\"pau\",\"som\",\"egl\",\"ady\",\"asm\",\"ori\",\"dtp\",\"cho\",\"max\",\"kam\",\"niu\",\"sag\",\"ilo\",\"kaa\",\"fuv\",\"nch\",\"hoc\",\"iba\",\"gbm\",\"sun\",\"war\",\"mvv\",\"pap\",\"ary\",\"kxi\",\"csb\",\"pag\",\"cos\",\"rif\",\"kek\",\"krc\",\"aii\",\"ban\",\"ssw\",\"tvl\",\"mfe\",\"tah\",\"bvy\",\"bcl\",\"hnj\",\"nau\",\"nst\",\"afb\",\"quc\",\"min\",\"tmw\",\"mad\",\"bjn\",\"mai\",\"cjy\",\"got\",\"hsn\",\"gan\",\"tzl\",\"dws\",\"ldn\",\"afh\",\"sgs\",\"krl\",\"vep\",\"rue\",\"tly\",\"mic\",\"ext\",\"izh\",\"sma\",\"jam\",\"cmo\",\"mwl\",\"kpv\",\"koi\",\"bis\",\"ike\",\"run\",\"evn\",\"ryu\",\"mnc\",\"aoz\",\"otk\",\"kas\",\"aln\",\"akl\",\"yua\",\"shy\",\"fkv\",\"gos\",\"fij\",\"thv\",\"zgh\",\"gcf\",\"cay\",\"xmf\",\"tig\",\"div\",\"lij\",\"rap\",\"hrx\",\"cpi\",\"tts\",\"gaa\",\"tmr\",\"iii\",\"ltg\",\"bzt\",\"syc\",\"emx\",\"gom\",\"chg\",\"osp\",\"stq\",\"frr\",\"fro\",\"nys\",\"toi\",\"new\",\"phn\",\"jpa\",\"rel\",\"drt\",\"chn\",\"pli\",\"laa\",\"bal\",\"hdn\",\"hax\",\"mik\",\"ajp\",\"xqa\",\"pal\",\"crk\",\"mni\",\"lut\",\"ayl\",\"ood\",\"sdh\",\"ofs\",\"nus\",\"kiu\",\"diq\",\"qxq\",\"alt\",\"bfz\",\"klj\",\"mus\",\"srn\",\"guc\",\"lim\",\"zea\",\"shi\",\"mnr\",\"bom\",\"sat\",\"szl\"]\r\nfeatures = Features({ 'label': ClassLabel(names=class_names), 'text': Value('string')})\r\nnum_labels = features['label'].num_classes\r\ndata_files = { \"train\": \"train.csv\", \"test\": \"test.csv\" }\r\nsentences = load_dataset(\"loretoparisi\/tatoeba-sentences\",\r\n                             data_files=data_files,\r\n                             delimiter='\\t', \r\n                             column_names=['label', 'text'],\r\n                             features = features\r\n```\r\n\r\nERROR:\r\n```\r\nClassLabel(num_classes=403, names=['cmn', 'deu', 'rus', 'fra', 'eng', 'jpn', 'spa', 'ita', 'kor', 'vie', 'nld', 'epo', 'por', 'tur', 'heb', 'hun', 'ell', 'ind', 'ara', 'arz', 'fin', 'bul', 'yue', 'swe', 'ukr', 'bel', 'que', 'ces', 'swh', 'nno', 'wuu', 'nob', 'zsm', 'est', 'kat', 'pol', 'lat', 'urd', 'sqi', 'isl', 'fry', 'afr', 'ron', 'fao', 'san', 'bre', 'tat', 'yid', 'uig', 'uzb', 'srp', 'qya', 'dan', 'pes', 'slk', 'eus', 'cycl', 'acm', 'tgl', 'lvs', 'kaz', 'hye', 'hin', 'lit', 'ben', 'cat', 'bos', 'hrv', 'tha', 'orv', 'cha', 'mon', 'lzh', 'scn', 'gle', 'mkd', 'slv', 'frm', 'glg', 'vol', 'ain', 'jbo', 'tok', 'ina', 'nds', 'mal', 'tlh', 'roh', 'ltz', 'oss', 'ido', 'gla', 'mlt', 'sco', 'ast', 'jav', 'oci', 'ile', 'ota', 'xal', 'tel', 'sjn', 'nov', 'khm', 'tpi', 'ang', 'aze', 'tgk', 'tuk', 'chv', 'hsb', 'dsb', 'bod', 'sme', 'cym', 'mri', 'ksh', 'kmr', 'ewe', 'kab', 'ber', 'tpw', 'udm', 'lld', 'pms', 'lad', 'grn', 'mlg', 'xho', 'pnb', 'grc', 'hat', 'lao', 'npi', 'cor', 'nah', 'avk', 'mar', 'guj', 'pan', 'kir', 'myv', 'prg', 'sux', 'crs', 'ckt', 'bak', 'zlm', 'hil', 'cbk', 'chr', 'nav', 'lkt', 'enm', 'arq', 'lin', 'abk', 'pcd', 'rom', 'gsw', 'tam', 'zul', 'awa', 'wln', 'amh', 'bar', 'hbo', 'mhr', 'bho', 'mrj', 'ckb', 'osx', 'pfl', 'mgm', 'sna', 'mah', 'hau', 'kan', 'nog', 'sin', 'glv', 'dng', 'kal', 'liv', 'vro', 'apc', 'jdt', 'fur', 'che', 'haw', 'yor', 'crh', 'pdc', 'ppl', 'kin', 'shs', 'mnw', 'tet', 'sah', 'kum', 'ngt', 'nya', 'pus', 'hif', 'mya', 'moh', 'wol', 'tir', 'ton', 'lzz', 'oar', 'lug', 'brx', 'non', 'mww', 'hak', 'nlv', 'ngu', 'bua', 'aym', 'vec', 'ibo', 'tkl', 'bam', 'kha', 'ceb', 'lou', 'fuc', 'smo', 'gag', 'lfn', 'arg', 'umb', 'tyv', 'kjh', 'oji', 'cyo', 'urh', 'kzj', 'pam', 'srd', 'lmo', 'swg', 'mdf', 'gil', 'snd', 'tso', 'sot', 'zza', 'tsn', 'pau', 'som', 'egl', 'ady', 'asm', 'ori', 'dtp', 'cho', 'max', 'kam', 'niu', 'sag', 'ilo', 'kaa', 'fuv', 'nch', 'hoc', 'iba', 'gbm', 'sun', 'war', 'mvv', 'pap', 'ary', 'kxi', 'csb', 'pag', 'cos', 'rif', 'kek', 'krc', 'aii', 'ban', 'ssw', 'tvl', 'mfe', 'tah', 'bvy', 'bcl', 'hnj', 'nau', 'nst', 'afb', 'quc', 'min', 'tmw', 'mad', 'bjn', 'mai', 'cjy', 'got', 'hsn', 'gan', 'tzl', 'dws', 'ldn', 'afh', 'sgs', 'krl', 'vep', 'rue', 'tly', 'mic', 'ext', 'izh', 'sma', 'jam', 'cmo', 'mwl', 'kpv', 'koi', 'bis', 'ike', 'run', 'evn', 'ryu', 'mnc', 'aoz', 'otk', 'kas', 'aln', 'akl', 'yua', 'shy', 'fkv', 'gos', 'fij', 'thv', 'zgh', 'gcf', 'cay', 'xmf', 'tig', 'div', 'lij', 'rap', 'hrx', 'cpi', 'tts', 'gaa', 'tmr', 'iii', 'ltg', 'bzt', 'syc', 'emx', 'gom', 'chg', 'osp', 'stq', 'frr', 'fro', 'nys', 'toi', 'new', 'phn', 'jpa', 'rel', 'drt', 'chn', 'pli', 'laa', 'bal', 'hdn', 'hax', 'mik', 'ajp', 'xqa', 'pal', 'crk', 'mni', 'lut', 'ayl', 'ood', 'sdh', 'ofs', 'nus', 'kiu', 'diq', 'qxq', 'alt', 'bfz', 'klj', 'mus', 'srn', 'guc', 'lim', 'zea', 'shi', 'mnr', 'bom', 'sat', 'szl'], id=None)\r\nValue(dtype='string', id=None)\r\nUsing custom data configuration loretoparisi--tatoeba-sentences-7b2c5e991f398f39\r\nDownloading and preparing dataset csv\/loretoparisi--tatoeba-sentences to \/root\/.cache\/huggingface\/datasets\/csv\/loretoparisi--tatoeba-sentences-7b2c5e991f398f39\/0.0.0\/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\r\nDownloading data files: 100%\r\n2\/2 [00:18<00:00, 8.06s\/it]\r\nDownloading data: 100%\r\n391M\/391M [00:13<00:00, 35.3MB\/s]\r\nDownloading data: 100%\r\n92.4M\/92.4M [00:02<00:00, 36.5MB\/s]\r\nFailed to read file '\/root\/.cache\/huggingface\/datasets\/downloads\/933132df9905194ea9faeb30cabca8c49318795612f6495fcb941a290191dd5d' with error <class 'ValueError'>: invalid literal for int() with base 10: 'cmn'\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader._convert_tokens()\r\n\r\nTypeError: Cannot cast array data from dtype('O') to dtype('int64') according to the rule 'safe'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n15 frames\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader._convert_tokens()\r\n\r\nValueError: invalid literal for int() with base 10: 'cmn'\r\n```\r\n\r\nwhile loading without `features` it loads without errors\r\n\r\n```\r\nsentences = load_dataset(\"loretoparisi\/tatoeba-sentences\",\r\n                             data_files=data_files,\r\n                             delimiter='\\t', \r\n                             column_names=['label', 'text']\r\n                         )\r\n```\r\n\r\nbut the `label` col seems to be wrong (without the `ClassLabel` object):\r\n\r\n```\r\nsentences['train'].features\r\n{'label': Value(dtype='string', id=None),\r\n 'text': Value(dtype='string', id=None)}\r\n```\r\n\r\nThe dataset was https:\/\/huggingface.co\/datasets\/loretoparisi\/tatoeba-sentences\r\n\r\n\r\nDataset format is:\r\n\r\n```\r\nces\tNechci v\u011bd\u011bt, co je tam uvnit\u0159.\r\nces\tKdo o tom chce sly\u0161et?\r\ndeu\tTom sagte, er f\u00fchle sich nicht wohl.\r\nber\tMel-iyi-d anida-t tura ?\r\nhun\tGondom lesz r\u00e1 r\u00f6gt\u00f6n.\r\nber\tMel-iyi-d anida-tt tura ?\r\ndeu\tIch will dich nicht reden h\u00f6ren.\r\n```\r\n\r\n### Expected behavior\r\n\r\n```shell\r\ncorrectly load train and test files.\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4210\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4210\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4208","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4208\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4208\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4208\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4208","id":1213716426,"node_id":"PR_kwDODunzps42r7bW","number":4208,"title":"Add CMU MoCap Dataset","user":{"login":"dnaveenr","id":17746528,"node_id":"MDQ6VXNlcjE3NzQ2NTI4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17746528?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/dnaveenr","html_url":"https:\/\/github.com\/dnaveenr","followers_url":"https:\/\/api.github.com\/users\/dnaveenr\/followers","following_url":"https:\/\/api.github.com\/users\/dnaveenr\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/dnaveenr\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/dnaveenr\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/dnaveenr\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/dnaveenr\/orgs","repos_url":"https:\/\/api.github.com\/users\/dnaveenr\/repos","events_url":"https:\/\/api.github.com\/users\/dnaveenr\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/dnaveenr\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4208). All of your documentation changes will be reflected on that endpoint.","- Updated the readme.\r\n- Added dummy_data.zip and ran the all the tests.\r\n\r\nThe dataset works for \"asf\/amc\" and \"avi\" formats which have a single download link for the complete dataset. But \"c3d\" and \"mpg\" have multiple download links, can we combine and host these links on the Hub since the dataset is free to use ?"],"created_at":1650821468000,"updated_at":1651555452000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Resolves #3457 \r\n\r\nDataset Request : Add CMU Graphics Lab Motion Capture dataset [#3457](https:\/\/github.com\/huggingface\/datasets\/issues\/3457)\r\nThis PR adds the CMU MoCap Dataset.\r\n\r\nThe authors didn't respond even after multiple follow ups, so I ended up crawling the website to get categories, subcategories and description information. Some of the subjects do not have category\/subcategory\/description as well. I am using a subject to categories, subcategories and description map (metadata file).\r\n\r\nCurrently the loading of the dataset works for \"asf\/amc\" and \"avi\" formats since they have a single download link. But \"c3d\" and \"mpg\" have multiple download links (part archives) and dl_manager.download_and_extract() extracts the files to multiple paths, is there a way to extract these multiple archives into one folder ? Any other way to go about this ?\r\nAny suggestions\/inputs on this would be helpful. Thank you.\r\n\r\n\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4208\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4208\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4208","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4208","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4208.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4208.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4207","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4207\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4207\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4207\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4207","id":1213604615,"node_id":"PR_kwDODunzps42rmbK","number":4207,"title":"[Minor edit] Fix typo in class name","user":{"login":"cakiki","id":3664563,"node_id":"MDQ6VXNlcjM2NjQ1NjM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3664563?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/cakiki","html_url":"https:\/\/github.com\/cakiki","followers_url":"https:\/\/api.github.com\/users\/cakiki\/followers","following_url":"https:\/\/api.github.com\/users\/cakiki\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/cakiki\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/cakiki\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/cakiki\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/cakiki\/orgs","repos_url":"https:\/\/api.github.com\/users\/cakiki\/repos","events_url":"https:\/\/api.github.com\/users\/cakiki\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/cakiki\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1650793777000,"updated_at":1651756667000,"closed_at":1651756667000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Typo: `datasets.DatsetDict` -> `datasets.DatasetDict`","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4207\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4207\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4207","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4207","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4207.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4207.patch","merged_at":1651756667000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4206","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4206\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4206\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4206\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4206","id":1212715581,"node_id":"PR_kwDODunzps42pJQW","number":4206,"title":"Add Nerval Metric","user":{"login":"mdadda","id":49372461,"node_id":"MDQ6VXNlcjQ5MzcyNDYx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/49372461?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mdadda","html_url":"https:\/\/github.com\/mdadda","followers_url":"https:\/\/api.github.com\/users\/mdadda\/followers","following_url":"https:\/\/api.github.com\/users\/mdadda\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mdadda\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mdadda\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mdadda\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mdadda\/orgs","repos_url":"https:\/\/api.github.com\/users\/mdadda\/repos","events_url":"https:\/\/api.github.com\/users\/mdadda\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mdadda\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1650656700000,"updated_at":1651404219000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"This PR adds readme.md and ner_val.py to metrics.\r\nNerval is a python package that helps evaluate NER models. It creates classification report and confusion matrix at entity level.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4206\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4206\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4206","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4206","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4206.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4206.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4205","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4205\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4205\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4205\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4205","id":1212466138,"node_id":"PR_kwDODunzps42oVFE","number":4205,"title":"Fix `convert_file_size_to_int` for kilobits and megabits","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650639381000,"updated_at":1651591722000,"closed_at":1651591308000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Minor change to fully align this function with the recent change in Transformers (https:\/\/github.com\/huggingface\/transformers\/pull\/16891) ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4205\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4205\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4205","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4205","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4205.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4205.patch","merged_at":1651591308000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4204","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4204\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4204\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4204\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4204","id":1212431764,"node_id":"PR_kwDODunzps42oN0j","number":4204,"title":"Add Recall Metric Card","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","This looks good to me! "],"created_at":1650637466000,"updated_at":1651584203000,"closed_at":1651583784000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"What this PR mainly does:\r\n- add metric card for recall metric\r\n- update docs in recall python file\r\n\r\nNote: I've also included a .json file with all of the metric card information. I've started compiling the relevant information in this type of .json files, and then using a script I wrote to generate the formatted metric card, as well as the docs to go in the .py file. I figured I'd upload the .json because it could be useful, especially if I also make a PR with the script I'm using (let me know if that's something you think would be beneficial!)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4204\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4204\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4204","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4204","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4204.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4204.patch","merged_at":1651583784000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4203","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4203\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4203\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4203\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4203","id":1212431067,"node_id":"PR_kwDODunzps42oNrS","number":4203,"title":"Add Precision Metric Card","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650637428000,"updated_at":1651587820000,"closed_at":1651587406000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"What this PR mainly does:\r\n- add metric card for precision metric\r\n- update docs in precision python file\r\n\r\nNote: I've also included a .json file with all of the metric card information. I've started compiling the relevant information in this type of .json files, and then using a script I wrote to generate the formatted metric card, as well as the docs to go in the .py file. I figured I'd upload the .json because it could be useful, especially if I also make a PR with the script I'm using (let me know if that's something you think would be beneficial!)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4203\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4203\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4203","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4203","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4203.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4203.patch","merged_at":1651587405000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4202","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4202\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4202\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4202\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4202","id":1212326288,"node_id":"PR_kwDODunzps42n278","number":4202,"title":"Fix some type annotation in doc","user":{"login":"thomasw21","id":24695242,"node_id":"MDQ6VXNlcjI0Njk1MjQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/24695242?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/thomasw21","html_url":"https:\/\/github.com\/thomasw21","followers_url":"https:\/\/api.github.com\/users\/thomasw21\/followers","following_url":"https:\/\/api.github.com\/users\/thomasw21\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/thomasw21\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/thomasw21\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/thomasw21\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/thomasw21\/orgs","repos_url":"https:\/\/api.github.com\/users\/thomasw21\/repos","events_url":"https:\/\/api.github.com\/users\/thomasw21\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/thomasw21\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650632011000,"updated_at":1650639780000,"closed_at":1650639403000,"author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4202\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4202\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4202","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4202","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4202.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4202.patch","merged_at":1650639403000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4201","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4201\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4201\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4201\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4201","id":1212086420,"node_id":"PR_kwDODunzps42nIRm","number":4201,"title":"Update GH template for dataset viewer issues","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","You can see rendering at: https:\/\/github.com\/huggingface\/datasets\/blob\/6b48fedbdafe12a42c7b6edcecc32820af1a4822\/.github\/ISSUE_TEMPLATE\/dataset-viewer.yml"],"created_at":1650620084000,"updated_at":1651826323000,"closed_at":1650962755000,"author_association":"MEMBER","active_lock_reason":null,"body":"Update template to use new issue forms instead.\r\n\r\nWith this PR we can check if this new feature is useful for us.\r\n\r\nOnce validated, we can update the other templates.\r\n\r\nCC: @severo ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4201\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4201\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4201","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4201","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4201.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4201.patch","merged_at":1650962755000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4200","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4200\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4200\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4200\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4200","id":1211980110,"node_id":"PR_kwDODunzps42mz0w","number":4200,"title":"Add to docs how to load from local script","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650614905000,"updated_at":1651826365000,"closed_at":1650692845000,"author_association":"MEMBER","active_lock_reason":null,"body":"This option was missing from the docs guide (it was only explained in the docstring of `load_dataset`). Although this is an infrequent use case, there might be some users interested in it.\r\n\r\nRelated to #4192\r\n\r\nCC: @stevhliu ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4200\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4200\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4200","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4200","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4200.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4200.patch","merged_at":1650692844000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4199","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4199\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4199\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4199\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4199","id":1211953308,"node_id":"I_kwDODunzps5IPPCc","number":4199,"title":"Cache miss during reload for datasets using image fetch utilities through map ","user":{"login":"apsdehal","id":3616806,"node_id":"MDQ6VXNlcjM2MTY4MDY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3616806?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/apsdehal","html_url":"https:\/\/github.com\/apsdehal","followers_url":"https:\/\/api.github.com\/users\/apsdehal\/followers","following_url":"https:\/\/api.github.com\/users\/apsdehal\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/apsdehal\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/apsdehal\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/apsdehal\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/apsdehal\/orgs","repos_url":"https:\/\/api.github.com\/users\/apsdehal\/repos","events_url":"https:\/\/api.github.com\/users\/apsdehal\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/apsdehal\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"assignees":[{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi ! Maybe one of the objects in the function is not deterministic across sessions ? You can read more about it and how to investigate here: https:\/\/huggingface.co\/docs\/datasets\/about_cache","Hi @apsdehal! Can you verify that replacing\r\n```python\r\ndef fetch_single_image(image_url, timeout=None, retries=0):\r\n    for _ in range(retries + 1):\r\n        try:\r\n            request = urllib.request.Request(\r\n                image_url,\r\n                data=None,\r\n                headers={\"user-agent\": get_datasets_user_agent()},\r\n            )\r\n            with urllib.request.urlopen(request, timeout=timeout) as req:\r\n                image = PIL.Image.open(io.BytesIO(req.read()))\r\n            break\r\n        except Exception:\r\n            image = None\r\n    return image\r\n```\r\nwith \r\n```python\r\nUSER_AGENT = get_datasets_user_agent()\r\n\r\ndef fetch_single_image(image_url, timeout=None, retries=0):\r\n    for _ in range(retries + 1):\r\n        try:\r\n            request = urllib.request.Request(\r\n                image_url,\r\n                data=None,\r\n                headers={\"user-agent\": USER_AGENT},\r\n            )\r\n            with urllib.request.urlopen(request, timeout=timeout) as req:\r\n                image = PIL.Image.open(io.BytesIO(req.read()))\r\n            break\r\n        except Exception:\r\n            image = None\r\n    return image\r\n```\r\nfixes the issue?","Thanks @mariosasko. That does fix the issue. In general, I think these image downloading utilities since they are being used by a lot of image dataset should be provided as a part of `datasets` library right to keep the logic consistent and READMEs smaller? If they already exists, that is also great, please point me to those. I saw that `http_get` does exist.","You can find my rationale (and a proposed solution) for why these utilities are not a part of `datasets` here: https:\/\/github.com\/huggingface\/datasets\/pull\/4100#issuecomment-1097994003.","Makes sense. But, I think as the number of image datasets as grow, more people are copying pasting original code from docs to work as it is while we make fixes to them later. I think we do need a central place for these to avoid that confusion as well as more easier access to image datasets. Should we restart that discussion, possible on slack?"],"created_at":1650613628000,"updated_at":1650992432000,"closed_at":1650980306000,"author_association":"MEMBER","active_lock_reason":null,"body":"## Describe the bug\r\n\r\nIt looks like that result of `.map` operation dataset are missing the cache when you reload the script and always run from scratch. In same interpretor session, they are able to find the cache and reload it. But, when you exit the interpretor and reload it, the downloading starts from scratch.\r\n\r\n## Steps to reproduce the bug\r\n\r\nUsing the example provided in `red_caps` dataset.\r\n```python\r\nfrom concurrent.futures import ThreadPoolExecutor\r\nfrom functools import partial\r\nimport io\r\nimport urllib\r\n\r\nimport PIL.Image\r\n\r\nimport datasets\r\nfrom datasets import load_dataset\r\nfrom datasets.utils.file_utils import get_datasets_user_agent\r\n\r\n\r\ndef fetch_single_image(image_url, timeout=None, retries=0):\r\n    for _ in range(retries + 1):\r\n        try:\r\n            request = urllib.request.Request(\r\n                image_url,\r\n                data=None,\r\n                headers={\"user-agent\": get_datasets_user_agent()},\r\n            )\r\n            with urllib.request.urlopen(request, timeout=timeout) as req:\r\n                image = PIL.Image.open(io.BytesIO(req.read()))\r\n            break\r\n        except Exception:\r\n            image = None\r\n    return image\r\n\r\n\r\ndef fetch_images(batch, num_threads, timeout=None, retries=0):\r\n    fetch_single_image_with_args = partial(fetch_single_image, timeout=timeout, retries=retries)\r\n    with ThreadPoolExecutor(max_workers=num_threads) as executor:\r\n        batch[\"image\"] = list(executor.map(lambda image_urls: [fetch_single_image_with_args(image_url) for image_url in image_urls], batch[\"image_url\"]))\r\n    return batch\r\n\r\n\r\ndef process_image_urls(batch):\r\n    processed_batch_image_urls = []\r\n    for image_url in batch[\"image_url\"]:\r\n        processed_example_image_urls = []\r\n        image_url_splits = re.findall(r\"http\\S+\", image_url)\r\n        for image_url_split in image_url_splits:\r\n            if \"imgur\" in image_url_split and \",\" in image_url_split:\r\n                for image_url_part in image_url_split.split(\",\"):\r\n                    if not image_url_part:\r\n                        continue\r\n                    image_url_part = image_url_part.strip()\r\n                    root, ext = os.path.splitext(image_url_part)\r\n                    if not root.startswith(\"http\"):\r\n                      root = \"http:\/\/i.imgur.com\/\" + root\r\n                    root = root.split(\"#\")[0]\r\n                    if not ext:\r\n                      ext = \".jpg\"\r\n                    ext = re.split(r\"[?%]\", ext)[0]\r\n                    image_url_part = root + ext\r\n                    processed_example_image_urls.append(image_url_part)\r\n            else:\r\n                processed_example_image_urls.append(image_url_split)\r\n        processed_batch_image_urls.append(processed_example_image_urls)\r\n    batch[\"image_url\"] = processed_batch_image_urls\r\n    return batch\r\n\r\n\r\ndset = load_dataset(\"red_caps\", \"jellyfish\")\r\ndset = dset.map(process_image_urls, batched=True, num_proc=4)\r\nfeatures = dset[\"train\"].features.copy()\r\nfeatures[\"image\"] = datasets.Sequence(datasets.Image())\r\nnum_threads = 5\r\ndset = dset.map(fetch_images, batched=True, batch_size=50, features=features, fn_kwargs={\"num_threads\": num_threads})\r\n```\r\n\r\nRun this in an interpretor or as a script twice and see that the cache is missed the second time.\r\n\r\n## Expected results\r\nAt reload there should not be any cache miss\r\n\r\n## Actual results\r\nEvery time script is run, cache is missed and dataset is built from scratch.\r\n\r\n## Environment info\r\n- `datasets` version: 2.1.1.dev0\r\n- Platform: Linux-4.19.0-20-cloud-amd64-x86_64-with-glibc2.10\r\n- Python version: 3.8.13\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.1\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4199\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4199\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4198","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4198\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4198\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4198\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4198","id":1211456559,"node_id":"I_kwDODunzps5INVwv","number":4198,"title":"There is no dataset","user":{"login":"wilfoderek","id":1625647,"node_id":"MDQ6VXNlcjE2MjU2NDc=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1625647?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/wilfoderek","html_url":"https:\/\/github.com\/wilfoderek","followers_url":"https:\/\/api.github.com\/users\/wilfoderek\/followers","following_url":"https:\/\/api.github.com\/users\/wilfoderek\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/wilfoderek\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/wilfoderek\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/wilfoderek\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/wilfoderek\/orgs","repos_url":"https:\/\/api.github.com\/users\/wilfoderek\/repos","events_url":"https:\/\/api.github.com\/users\/wilfoderek\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/wilfoderek\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1650568766000,"updated_at":1651577345000,"closed_at":1650607945000,"author_association":"NONE","active_lock_reason":null,"body":"## Dataset viewer issue for '*name of the dataset*'\r\n\r\n**Link:** *link to the dataset viewer page*\r\n\r\n*short description of the issue*\r\n\r\nAm I the one who added this dataset ? Yes-No\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4198\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4198\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4197","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4197\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4197\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4197\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4197","id":1211342558,"node_id":"PR_kwDODunzps42kyXD","number":4197,"title":"Add remove_columns=True","user":{"login":"thomasw21","id":24695242,"node_id":"MDQ6VXNlcjI0Njk1MjQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/24695242?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/thomasw21","html_url":"https:\/\/github.com\/thomasw21","followers_url":"https:\/\/api.github.com\/users\/thomasw21\/followers","following_url":"https:\/\/api.github.com\/users\/thomasw21\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/thomasw21\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/thomasw21\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/thomasw21\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/thomasw21\/orgs","repos_url":"https:\/\/api.github.com\/users\/thomasw21\/repos","events_url":"https:\/\/api.github.com\/users\/thomasw21\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/thomasw21\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Any reason why we can't just do `[inputs.copy()]` in this line for in-place operations to not have effects anymore:\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/bf432011ff9155a5bc16c03956bc63e514baf80d\/src\/datasets\/arrow_dataset.py#L2232.\r\n\r\n(in the `batched` case, we can also copy the inputs' values (list objects) to ignore in-place modifications to the inputs' columns)\r\n\r\nI think `remove_columns=True` has no meaning, so I'm not a fan of this change.","@mariosasko copy does have a cost associated with it ... and plus you'll have to consider `deepcopy` Imagine columnds that are list of list of list of list .... Though I have to agree that `remove_columns=True` doesn't make sense (but, IMO, neither does it in its current use-case as it should refer to `input_columns`) ","Okay closing this PR for the following reasons:\r\n - `remove_columns=True` was expected to keep the `.update`-like operator for `.map`. I initially thought it would be a good way to ignore function side effects and only keep output of that function (cf. PR description).\r\n - expected `remove_columns=True` is a bad API according to @mariosasko and introduces unecessary changes for little gain (strictly equivalent to `remove_columns=dset.column_names`)"],"created_at":1650562093000,"updated_at":1650639101000,"closed_at":1650638730000,"author_association":"MEMBER","active_lock_reason":null,"body":"This should fix all the issue we have with in place operations in mapping functions. This is crucial as where we do some weird things like:\r\n```\r\ndef apply(batch):\r\n    batch_size = len(batch[\"id\"])\r\n    batch[\"text\"] = [\"potato\" for _ range(batch_size)]\r\n    return {}\r\n\r\n# Columns are: {\"id\": int}\r\ndset.map(apply, batched=True, remove_columns=\"text\") # crashes because `text` is not in the original columns\r\ndset.map(apply, batched=True) # mapped datasets has `text` column\r\n```\r\n\r\nIn this PR we suggest to have `remove_columns=True` so that we ignore the input completely, and just use the output to generate mapped dataset. This means that inplace operations won't have any effects anymore.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4197\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4197\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4197","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4197","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4197.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4197.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4196","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4196\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4196\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4196\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4196","id":1211271261,"node_id":"I_kwDODunzps5IMohd","number":4196,"title":"Embed image and audio files in `save_to_disk`","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1650558318000,"updated_at":1650558318000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Following https:\/\/github.com\/huggingface\/datasets\/pull\/4184, currently a dataset saved using `save_to_disk` doesn't actually contain the bytes of the image or audio files. Instead it stores the path to your local files. \r\n\r\nAdding `embed_external_files` and set it to True by default to save_to_disk would be kind of a breaking change since some users will get bigger Arrow files when updating the lib, but the advantages are nice:\r\n\r\n- the resulting dataset is self contained, in case you want to delete your cache for example or share it with someone else\r\n- users also upload these Arrow files to cloud storage via the fs parameter, and in this case they would expect to upload a self-contained dataset\r\n- consistency with push_to_hub\r\n\r\nThis can be implemented at the same time as sharding for `save_to_disk` for efficiency, and reuse the helpers from `push_to_hub` to embed the external files.\r\n\r\ncc @mariosasko ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4196\/reactions","total_count":5,"+1":5,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4196\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4194","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4194\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4194\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4194\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4194","id":1210958602,"node_id":"PR_kwDODunzps42jjD3","number":4194,"title":"Support lists of multi-dimensional numpy arrays","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4194). All of your documentation changes will be reflected on that endpoint."],"created_at":1650543746000,"updated_at":1650693116000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Fix #4191.\r\n\r\nCC: @SaulLu ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4194\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4194\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4194","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4194","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4194.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4194.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4193","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4193\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4193\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4193\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4193","id":1210734701,"node_id":"PR_kwDODunzps42izQG","number":4193,"title":"Document save_to_disk and push_to_hub on images and audio files","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Good catch, I updated the docstrings"],"created_at":1650531876000,"updated_at":1650621355000,"closed_at":1650620971000,"author_association":"MEMBER","active_lock_reason":null,"body":"Following https:\/\/github.com\/huggingface\/datasets\/pull\/4187, I explained in the documentation of `save_to_disk` and `push_to_hub` how they handle image and audio data.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4193\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4193\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4193","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4193","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4193.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4193.patch","merged_at":1650620971000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4192","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4192\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4192\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4192\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4192","id":1210692554,"node_id":"I_kwDODunzps5IKbPK","number":4192,"title":"load_dataset can't load local dataset,Unable to find ...","user":{"login":"ahf876828330","id":33253979,"node_id":"MDQ6VXNlcjMzMjUzOTc5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/33253979?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ahf876828330","html_url":"https:\/\/github.com\/ahf876828330","followers_url":"https:\/\/api.github.com\/users\/ahf876828330\/followers","following_url":"https:\/\/api.github.com\/users\/ahf876828330\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ahf876828330\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ahf876828330\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ahf876828330\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ahf876828330\/orgs","repos_url":"https:\/\/api.github.com\/users\/ahf876828330\/repos","events_url":"https:\/\/api.github.com\/users\/ahf876828330\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ahf876828330\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi! :)\r\n\r\nI believe that should work unless `dataset_infos.json` isn't actually a dataset. For Hugging Face datasets, there is usually a file named `dataset_infos.json` which contains metadata about the dataset (eg. the dataset citation, license, description, etc). Can you double-check that `dataset_infos.json` isn't just metadata please?","Hi @ahf876828330, \r\n\r\nAs @stevhliu pointed out, the proper way to load a dataset is not trying to load its metadata file.\r\n\r\nIn your case, as the dataset script is local, you should better point to your local loading script:\r\n```python\r\ndataset = load_dataset(\"dataset\/opus_books.py\")\r\n```\r\n\r\nPlease, feel free to re-open this issue if the previous code snippet does not work for you.","> Hi! :)\r\n> \r\n> I believe that should work unless `dataset_infos.json` isn't actually a dataset. For Hugging Face datasets, there is usually a file named `dataset_infos.json` which contains metadata about the dataset (eg. the dataset citation, license, description, etc). Can you double-check that `dataset_infos.json` isn't just metadata please?\r\n\r\nYes\uff0cyou are right!So if I have a metadata dataset local,How can I turn it to a dataset that can be used by the load_dataset() function\uff1fAre there some examples?","The metadata file isn't a dataset so you can't turn it into one. You should try @albertvillanova's code snippet above (now merged in the docs [here](https:\/\/huggingface.co\/docs\/datasets\/master\/en\/loading#local-loading-script)), which uses your local loading script `opus_books.py` to:\r\n\r\n1. Download the actual dataset. \r\n2. Once the dataset is downloaded, `load_dataset` will load it for you."],"created_at":1650529738000,"updated_at":1650905517000,"closed_at":1650613193000,"author_association":"NONE","active_lock_reason":null,"body":"\r\nTraceback (most recent call last):\r\n  File \"\/home\/gs603\/ahf\/pretrained\/model.py\", line 48, in <module>\r\n    dataset = load_dataset(\"json\",data_files=\"dataset\/dataset_infos.json\")\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/load.py\", line 1675, in load_dataset\r\n    **config_kwargs,\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/load.py\", line 1496, in load_dataset_builder\r\n    data_files=data_files,\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/load.py\", line 1155, in dataset_module_factory\r\n    download_mode=download_mode,\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/load.py\", line 800, in get_module\r\n    data_files = DataFilesDict.from_local_or_remote(patterns, use_auth_token=self.downnload_config.use_auth_token)\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/data_files.py\", line 582, in from_local_or_remote\r\n    if not isinstance(patterns_for_key, DataFilesList)\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/data_files.py\", line 544, in from_local_or_remote\r\n    data_files = resolve_patterns_locally_or_by_urls(base_path, patterns, allowed_extensions)\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/data_files.py\", line 194, in resolve_patterns_locally_or_by_urls\r\n    for path in _resolve_single_pattern_locally(base_path, pattern, allowed_extensions):\r\n  File \"\/home\/gs603\/miniconda3\/envs\/coderepair\/lib\/python3.7\/site-packages\/datasets\/data_files.py\", line 144, in _resolve_single_pattern_locally\r\n    raise FileNotFoundError(error_msg)\r\nFileNotFoundError: Unable to find '\/home\/gs603\/ahf\/pretrained\/dataset\/dataset_infos.json' at \/home\/gs603\/ahf\/pretrained\r\n\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/33253979\/164413285-84ea65ac-9126-408f-9cd2-ce4751a5dd73.png)\r\n![image](https:\/\/user-images.githubusercontent.com\/33253979\/164413338-4735142f-408b-41d9-ab87-8484de2be54f.png)\r\n\r\nthe code is in the model.py,why I can't use the load_dataset function to load my local dataset?","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4192\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4192\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4191","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4191\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4191\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4191\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4191","id":1210028090,"node_id":"I_kwDODunzps5IH5A6","number":4191,"title":"feat: create an `Array3D` column from a list of arrays of dimension 2","user":{"login":"SaulLu","id":55560583,"node_id":"MDQ6VXNlcjU1NTYwNTgz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/55560583?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/SaulLu","html_url":"https:\/\/github.com\/SaulLu","followers_url":"https:\/\/api.github.com\/users\/SaulLu\/followers","following_url":"https:\/\/api.github.com\/users\/SaulLu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/SaulLu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/SaulLu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/SaulLu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/SaulLu\/orgs","repos_url":"https:\/\/api.github.com\/users\/SaulLu\/repos","events_url":"https:\/\/api.github.com\/users\/SaulLu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/SaulLu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi @SaulLu, thanks for your proposal.\r\n\r\nJust I got a bit confused about the dimensions...\r\n- For the 2D case, you mention it is possible to create an `Array2D` from a list of arrays of dimension 1\r\n- However, you give an example of creating an `Array2D` from arrays of dimension 2:\r\n  - the values of `data_map` are arrays of dimension 2\r\n  - the outer list in `prepare_dataset_2D` should not be taken into account in the dimension counting, as it is used because in `map` you pass `batched=True`\r\n\r\nNote that for the 3D alternatives you mention:\r\n- In `prepare_dataset_3D_ter`, you create an `Array3D` from arrays of dimension 3:\r\n  - the array `data_map[index][np.newaxis, :, :]` has dimension 3\r\n  - the outer list in `prepare_dataset_3D_ter` is the one used by `batched=True`\r\n- In `prepare_dataset_3D_bis`, you create an `Array3D` from a list of list of lists:\r\n  - the value of `data_map[index].tolist()` is a list of lists\r\n  - it is enclosed by another list `[data_map[index].tolist()]`, thus giving a list of list of lists\r\n  - the outer list is the one used by `batched=True`\r\n\r\nTherefore, if I understand correctly, your request would be to be able to create an `Array3D` from a list of an array of dimension 2:\r\n- In `prepare_dataset_3D`, `data_map[index]` is an array of dimension 2\r\n- it is enclosed by a list `[data_map[index]]`, thus giving a list of an array of dimension 2\r\n- the outer list is the one used by `batched=True`\r\n\r\nPlease, feel free to tell me if I did not understand you correctly.","Hi @albertvillanova ,\r\n\r\nIndeed my message was confusing and you guessed right :smile: : I think would be interesting to be able to create an Array3D from a list of an array of dimension 2. \r\n\r\nFor the 2D case I should have given as a \"similar\" example:\r\n```python\r\n\r\ndata_map_1D = {\r\n    1: np.array([0.2, 0.4]),\r\n    2: np.array([0.1, 0.4]),\r\n}\r\n\r\ndef prepare_dataset_2D(batch):\r\n    batch[\"pixel_values\"] = [[data_map_1D[index]] for index in batch[\"id\"]]\r\n    return batch\r\n    \r\nds_2D = ds.map(\r\n    prepare_dataset_2D, \r\n    batched=True, \r\n    remove_columns=ds.column_names, \r\n    features=features.Features({\"pixel_values\": features.Array2D(shape=(1, 2), dtype=\"float32\")})\r\n)\r\n```"],"created_at":1650477872000,"updated_at":1650543737000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Is your feature request related to a problem? Please describe.**\r\n\r\nIt is possible to create an `Array2D` column from a list of arrays of dimension 1. Similarly, I think it might be nice to be able to create a `Array3D` column from a list of lists of arrays of dimension 1.\r\n\r\nTo illustrate my proposal, let's take the following toy dataset t:\r\n```python\r\nimport numpy as np\r\nfrom datasets import Dataset, features\r\n\r\ndata_map = {\r\n    1: np.array([[0.2, 0,4],[0.19, 0,3]]),\r\n    2: np.array([[0.1, 0,4],[0.19, 0,3]]),\r\n}\r\n\r\ndef create_toy_ds():\r\n    my_dict = {\"id\":[1, 2]}\r\n    return Dataset.from_dict(my_dict)\r\n\r\nds = create_toy_ds()\r\n```\r\n\r\nThe following 2D processing works without any errors raised:\r\n```python\r\ndef prepare_dataset_2D(batch):\r\n    batch[\"pixel_values\"] = [data_map[index] for index in batch[\"id\"]]\r\n    return batch\r\n    \r\nds_2D = ds.map(\r\n    prepare_dataset_2D, \r\n    batched=True, \r\n    remove_columns=ds.column_names, \r\n    features=features.Features({\"pixel_values\": features.Array2D(shape=(2, 3), dtype=\"float32\")})\r\n)\r\n```\r\n\r\nThe following 3D processing doesn't work:\r\n```python\r\ndef prepare_dataset_3D(batch):\r\n    batch[\"pixel_values\"] = [[data_map[index]] for index in batch[\"id\"]]\r\n    return batch\r\n    \r\nds_3D = ds.map(\r\n    prepare_dataset_3D, \r\n    batched=True, \r\n    remove_columns=ds.column_names, \r\n    features=features.Features({\"pixel_values\": features.Array3D(shape=(1,  2, 3, dtype=\"float32\")})\r\n)\r\n```\r\nThe error raised is:\r\n```\r\n---------------------------------------------------------------------------\r\nArrowInvalid                              Traceback (most recent call last)\r\n[<ipython-input-6-676547e4cd41>](https:\/\/localhost:8080\/#) in <module>()\r\n      3     batched=True,\r\n      4     remove_columns=ds.column_names,\r\n----> 5     features=features.Features({\"pixel_values\": features.Array3D(shape=(1, 2, 3), dtype=\"float32\")})\r\n      6 )\r\n\r\n12 frames\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\r\n   1971                 new_fingerprint=new_fingerprint,\r\n   1972                 disable_tqdm=disable_tqdm,\r\n-> 1973                 desc=desc,\r\n   1974             )\r\n   1975         else:\r\n\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in wrapper(*args, **kwargs)\r\n    518             self: \"Dataset\" = kwargs.pop(\"self\")\r\n    519         # apply actual function\r\n--> 520         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    521         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    522         for dataset in datasets:\r\n\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in wrapper(*args, **kwargs)\r\n    485         }\r\n    486         # apply actual function\r\n--> 487         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    488         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    489         # re-apply format to the output\r\n\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/fingerprint.py](https:\/\/localhost:8080\/#) in wrapper(*args, **kwargs)\r\n    456             # Call actual function\r\n    457 \r\n--> 458             out = func(self, *args, **kwargs)\r\n    459 \r\n    460             # Update fingerprint of in-place transforms + update in-place history of transforms\r\n\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in _map_single(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\r\n   2354                                 writer.write_table(batch)\r\n   2355                             else:\r\n-> 2356                                 writer.write_batch(batch)\r\n   2357                 if update_data and writer is not None:\r\n   2358                     writer.finalize()  # close_stream=bool(buf_writer is None))  # We only close if we are writing in a file\r\n\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_writer.py](https:\/\/localhost:8080\/#) in write_batch(self, batch_examples, writer_batch_size)\r\n    505             col_try_type = try_features[col] if try_features is not None and col in try_features else None\r\n    506             typed_sequence = OptimizedTypedSequence(batch_examples[col], type=col_type, try_type=col_try_type, col=col)\r\n--> 507             arrays.append(pa.array(typed_sequence))\r\n    508             inferred_features[col] = typed_sequence.get_inferred_type()\r\n    509         schema = inferred_features.arrow_schema if self.pa_writer is None else self.schema\r\n\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pyarrow\/array.pxi in pyarrow.lib.array()\r\n\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pyarrow\/array.pxi in pyarrow.lib._handle_arrow_array_protocol()\r\n\r\n[\/usr\/local\/lib\/python3.7\/dist-packages\/datasets\/arrow_writer.py](https:\/\/localhost:8080\/#) in __arrow_array__(self, type)\r\n    175                     storage = list_of_np_array_to_pyarrow_listarray(data, type=pa_type.value_type)\r\n    176                 else:\r\n--> 177                     storage = pa.array(data, pa_type.storage_dtype)\r\n    178                 return pa.ExtensionArray.from_storage(pa_type, storage)\r\n    179 \r\n\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pyarrow\/array.pxi in pyarrow.lib.array()\r\n\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pyarrow\/array.pxi in pyarrow.lib._sequence_to_array()\r\n\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pyarrow\/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n\/usr\/local\/lib\/python3.7\/dist-packages\/pyarrow\/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: Can only convert 1-dimensional array values\r\n```\r\n\r\n**Describe the solution you'd like**\r\nNo error in the second scenario and an identical result to the following snippets.\r\n\r\n**Describe alternatives you've considered**\r\nThere are other alternatives that work such as:\r\n```python\r\n\r\ndef prepare_dataset_3D_bis(batch):\r\n    batch[\"pixel_values\"] = [[data_map[index].tolist()] for index in batch[\"id\"]]\r\n    return batch\r\n\r\nds_3D_bis = ds.map(\r\n    prepare_dataset_3D_bis, \r\n    batched=True, \r\n    remove_columns=ds.column_names, \r\n    features=features.Features({\"pixel_values\": features.Array3D(shape=(1, 2, 3), dtype=\"float32\")})\r\n)\r\n```\r\nor\r\n```python\r\ndef prepare_dataset_3D_ter(batch):\r\n    batch[\"pixel_values\"] = [data_map[index][np.newaxis, :, :] for index in batch[\"id\"]]\r\n    return batch\r\n\r\nds_3D_ter = ds.map(\r\n    prepare_dataset_3D_ter, \r\n    batched=True, \r\n    remove_columns=ds.column_names, \r\n    features=features.Features({\"pixel_values\": features.Array3D(shape=(1,  2, 3), dtype=\"float32\")})\r\n)\r\n```\r\nBut both solutions require the user to be aware that `data_map[index]` is an `np.array` type.\r\n\r\ncc @lhoestq as we discuss this offline :smile: ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4191\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4191\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4190","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4190\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4190\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4190\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4190","id":1209901677,"node_id":"PR_kwDODunzps42gK3y","number":4190,"title":"Deprecate `shard_size` in `push_to_hub` in favor of `max_shard_size`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650470881000,"updated_at":1650635905000,"closed_at":1650635520000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"This PR adds a `max_shard_size` param to `push_to_hub` and deprecates `shard_size` in favor of this new param to have a more descriptive name (a shard has at most the `shard_size` bytes in `push_to_hub`) for the param and to align the API with [Transformers](https:\/\/github.com\/huggingface\/transformers\/blob\/ff06b177917384137af2d9585697d2d76c40cdfc\/src\/transformers\/modeling_utils.py#L1350).\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4190\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4190\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4190","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4190","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4190.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4190.patch","merged_at":1650635520000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4189","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4189\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4189\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4189\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4189","id":1209881351,"node_id":"PR_kwDODunzps42gGv5","number":4189,"title":"Document how to use FAISS index for special operations","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650469916000,"updated_at":1651826590000,"closed_at":1651826152000,"author_association":"MEMBER","active_lock_reason":null,"body":"Document how to use FAISS index for special operations, by accessing the index itself.\r\n\r\nClose #4029.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4189\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4189\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4189","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4189","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4189.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4189.patch","merged_at":1651826152000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4188","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4188\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4188\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4188\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4188","id":1209740957,"node_id":"PR_kwDODunzps42fpMv","number":4188,"title":"Support streaming cnn_dailymail dataset","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650463476000,"updated_at":1651826377000,"closed_at":1650469969000,"author_association":"MEMBER","active_lock_reason":null,"body":"Support streaming cnn_dailymail dataset.\r\n\r\nFix #3969.\r\n\r\nCC: @severo ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4188\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4188\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4188","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4188","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4188.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4188.patch","merged_at":1650469969000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4187","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4187\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4187\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4187\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4187","id":1209721532,"node_id":"PR_kwDODunzps42flGp","number":4187,"title":"Don't duplicate data when encoding audio or image","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","I'm not familiar with the concept of streaming vs non-streaming in HF datasets. I just wonder that you have the distinction here. Why doesn't it work to always make use of `bytes`? \"using a local file - which is often required for audio\" - why would that be?\r\n\r\nThe `path` would always point to some location in the `cache_dir`? I think this can be problematic. I would have expected that after I did `dataset.save_to_disk(...)` that I can remove the cache dir. But maybe just because I'm not familiar with HF. Or maybe the docs can be improved to clarify this.\r\n","We could always load every data file into `bytes` and save it this way the audio as bytes in `arrow` format, but the problem then would be that it makes the `file` column useless, *i.e.* people cannot inspect the audio file locally anymore or else they would need to first save bytes as a file which is not evident. This either breaks backwards compatibility or forces the user to stored 2x the required size locally. There was a longer discussion here: https:\/\/github.com\/huggingface\/datasets\/issues\/3663\r\n\r\nIt's a good argument though that `dataset.save_to_disk(...)` should save everything that is needed to the disk and should be independent of other folders, but I do think the arguments of #3663 to not break backwards compatibility and to allow people to inspect the downloaded audio files locally are a bit more important here. \r\n\r\nBut maybe, we could add a flag, `save_files_as_bytes` or `make_independent`, `make_self_contained` or a better name to `save_to_disk(...)` and `push_to_hub(...)` that would allow to make the resulting folder completely independent. ","What do you think @mariosasko @lhoestq @polinaeterna @anton-l ?\r\n","For context: you can either store the path to local images or audio files, or the bytes of those files.\r\n\r\nIf your images and audio files are local files, then the arrow file from `save_to_disk` will store paths to these files.\r\nIf you want to include the bytes or your images or audio files instead, you must `read()` those files first.\r\nThis can be done by storing the \"bytes\" instead of the \"path\" of the images or audio files.\r\n\r\nOn the other hand, the resulting Parquet files from `push_to_hub` are self-contained, so that anyone can reload the dataset from the Hub. If your dataset contains image or audio data, the Parquet files will store the bytes of your images or audio files.\r\n\r\nFor now I just updated the documentation: https:\/\/github.com\/huggingface\/datasets\/pull\/4193. Maybe we can also embed the image and audio bytes in `save_to_disk` when we implement sharding, so that is can be done as efficiently as `push_to_hub`.\r\n\r\nAnyway, merging this one :)"],"created_at":1650462637000,"updated_at":1650532620000,"closed_at":1650532247000,"author_association":"MEMBER","active_lock_reason":null,"body":"Right now if you pass both the `bytes` and a local `path` for audio or image data, then the `bytes` are unnecessarily written in the Arrow file, while we could just keep the local `path`.\r\n\r\nThis PR discards the `bytes` when the audio or image file exists locally.\r\n\r\nIn particular it's common for audio datasets builders to provide both the bytes and the local path in order to work for both streaming (using the bytes) and non-streaming mode (using a local file - which is often required for audio).\r\n\r\ncc @patrickvonplaten ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4187\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4187\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4187","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4187","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4187.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4187.patch","merged_at":1650532247000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4186","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4186\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4186\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4186\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4186","id":1209463599,"node_id":"PR_kwDODunzps42evF5","number":4186,"title":"Fix outdated docstring about default dataset config","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650449091000,"updated_at":1650632084000,"closed_at":1650631711000,"author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4186\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4186\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4186","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4186","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4186.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4186.patch","merged_at":1650631711000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4185","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4185\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4185\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4185\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4185","id":1209429743,"node_id":"I_kwDODunzps5IFm7v","number":4185,"title":"Librispeech documentation, clarification on format","user":{"login":"albertz","id":59132,"node_id":"MDQ6VXNlcjU5MTMy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59132?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertz","html_url":"https:\/\/github.com\/albertz","followers_url":"https:\/\/api.github.com\/users\/albertz\/followers","following_url":"https:\/\/api.github.com\/users\/albertz\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertz\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertz\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertz\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertz\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertz\/repos","events_url":"https:\/\/api.github.com\/users\/albertz\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertz\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["(@patrickvonplaten )","Also cc @lhoestq here","The documentation in the code is definitely outdated - thanks for letting me know, I'll remove it in https:\/\/github.com\/huggingface\/datasets\/pull\/4184 .\r\n\r\nYou're exactly right `audio` `array` already decodes the audio file to the correct waveform. This is done on the fly, which is also why one should **not** do `ds[\"audio\"][\"array\"][0]` as this will decode all dataset samples, but instead `ds[0][\"audio\"][\"array\"]` see: https:\/\/huggingface.co\/docs\/datasets\/audio_process#audio-datasets\r\n\r\n","So, again to clarify: On disk, only the raw flac file content is stored? Is this also the case after `save_to_disk`?\r\n\r\nAnd is it simple to also store it re-encoded as ogg or mp3 instead?\r\n","Hey, \r\n\r\nSorry yeah I was just about to look into this! We actually had an outdated version of Librispeech ASR that didn't save any files, but instead converted the audio files to a byte string, then was then decoded on-the-fly. This however is not very user-friendly so we recently decided to instead show the full path of the audio files with the `path` parameter.\r\n\r\nI'm currently changing this for Librispeech here: https:\/\/github.com\/huggingface\/datasets\/pull\/4184 .\r\nYou should be able to see the audio file in the original `flac` format under `path` then. I don't think it's a good idea to convert to MP3 out-of-the-box, but we could maybe think about some kind of convert function for audio datasets cc @lhoestq ? ","> I don't think it's a good idea to convert to MP3 out-of-the-box, but we could maybe think about some kind of convert function for audio datasets cc @lhoestq ?\r\n\r\nSure, I would expect that `load_dataset(\"librispeech_asr\")` would give you the original (not re-encoded) data (flac or already decoded). So such re-encoding logic would be some separate generic function. So I could do sth like `dataset.reencode_as_ogg(**ogg_encode_opts).save_to_disk(...)` or so.\r\n","A follow-up question: I wonder whether a Parquet dataset is maybe more what we actually want to have? (Following also my comment here: https:\/\/github.com\/huggingface\/datasets\/pull\/4184#issuecomment-1105045491.) Because I think we actually would prefer to embed the data content in the dataset.\r\n\r\nSo, instead of `save_to_disk`\/`load_from_disk`, we would use `to_parquet`,`from_parquet`? Is there any downside? Are arrow files more efficient?\r\n\r\nRelated is also the doc update in #4193.\r\n","`save_to_disk` saves the dataset as an Arrow file, which is the format we use to load a dataset using memory mapping. This way the dataset does not fill your RAM, but is read from your disk instead.\r\n\r\nTherefore you can directly reload a dataset saved with `save_to_disk` using `load_from_disk`.\r\n\r\nParquet files are used for cold storage: to use memory mapping on a Parquet dataset, you first have to convert it to Arrow. We use Parquet to reduce the I\/O when pushing\/downloading data from the Hugging face Hub. When you load a Parquet file from the Hub, it is converted to Arrow on the fly during the download."],"created_at":1650447355000,"updated_at":1650538853000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"https:\/\/github.com\/huggingface\/datasets\/blob\/cd3ce34ab1604118351e1978d26402de57188901\/datasets\/librispeech_asr\/librispeech_asr.py#L53\r\n\r\n> Note that in order to limit the required storage for preparing this dataset, the audio\r\n> is stored in the .flac format and is not converted to a float32 array. To convert, the audio\r\n> file to a float32 array, please make use of the `.map()` function as follows:\r\n> \r\n> ```python\r\n> import soundfile as sf\r\n> def map_to_array(batch):\r\n>     speech_array, _ = sf.read(batch[\"file\"])\r\n>     batch[\"speech\"] = speech_array\r\n>     return batch\r\n> dataset = dataset.map(map_to_array, remove_columns=[\"file\"])\r\n> ```\r\n\r\nIs this still true?\r\n\r\nIn my case, `ds[\"train.100\"]` returns:\r\n```\r\nDataset({\r\n    features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\r\n    num_rows: 28539\r\n})\r\n```\r\nand taking the first instance yields:\r\n```\r\n{'file': '374-180298-0000.flac',\r\n 'audio': {'path': '374-180298-0000.flac',\r\n  'array': array([ 7.01904297e-04,  7.32421875e-04,  7.32421875e-04, ...,\r\n         -2.74658203e-04, -1.83105469e-04, -3.05175781e-05]),\r\n  'sampling_rate': 16000},\r\n 'text': 'CHAPTER SIXTEEN I MIGHT HAVE TOLD YOU OF THE BEGINNING OF THIS LIAISON IN A FEW LINES BUT I WANTED YOU TO SEE EVERY STEP BY WHICH WE CAME I TO AGREE TO WHATEVER MARGUERITE WISHED',\r\n 'speaker_id': 374,\r\n 'chapter_id': 180298,\r\n 'id': '374-180298-0000'}\r\n```\r\n\r\nThe `audio` `array` seems to be already decoded. So such convert\/decode code as mentioned in the doc is wrong?\r\n\r\nBut I wonder, is it actually stored as flac on disk, and the decoding is done on-the-fly? Or was it decoded already during the preparation and is stored as raw samples on disk?\r\n\r\nNote that I also used `datasets.load_dataset(\"librispeech_asr\", \"clean\").save_to_disk(...)` and then `datasets.load_from_disk(...)` in this example. Does this change anything on how it is stored on disk?\r\n\r\nA small related question: Actually I would prefer to even store it as mp3 or ogg on disk. Is this easy to convert?","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4185\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4185\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4184","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4184\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4184\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4184\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4184","id":1208592669,"node_id":"PR_kwDODunzps42cB2j","number":4184,"title":"[Librispeech] Add 'all' config","user":{"login":"patrickvonplaten","id":23423619,"node_id":"MDQ6VXNlcjIzNDIzNjE5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/23423619?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/patrickvonplaten","html_url":"https:\/\/github.com\/patrickvonplaten","followers_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/followers","following_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/orgs","repos_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/repos","events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Fix https:\/\/github.com\/huggingface\/datasets\/issues\/4179","_The documentation is not available anymore as the PR was closed or merged._","Just that I understand: With this change, simply doing `load_dataset(\"librispeech_asr\")` is possible and returns the whole dataset?\r\n\r\nAnd to get the subsets, I do sth like:\r\n```python\r\nds = load_dataset(\"librispeech_asr\")\r\ntrain_ds = ds[\"train\"]\r\ndev_clean_ds = ds[\"dev-clean\"]\r\ndev_other_ds = ds[\"dev-other\"]\r\ntest_clean_ds = ds[\"test-clean\"]\r\ntest_other_ds = ds[\"test-other\"]\r\n```\r\n?\r\n","> Just that I understand: With this change, simply doing `load_dataset(\"librispeech_asr\")` is possible and returns the whole dataset?\r\n> \r\n> And to get the subsets, I do sth like:\r\n> \r\n> ```python\r\n> ds = load_dataset(\"librispeech_asr\")\r\n> train_ds = ds[\"train\"]\r\n> dev_clean_ds = ds[\"dev-clean\"]\r\n> dev_other_ds = ds[\"dev-other\"]\r\n> test_clean_ds = ds[\"test-clean\"]\r\n> test_other_ds = ds[\"test-other\"]\r\n> ```\r\n> \r\n> ?\r\n\r\nYou could do:\r\n\r\n\r\n```python\r\nds = load_dataset(\"librispeech_asr\", \"all\")  # <- note that we have to pass a config\r\ntrain_ds = ds[\"train\"]\r\ndev_clean_ds = ds[\"dev-clean\"]\r\ndev_other_ds = ds[\"dev-other\"]\r\ntest_clean_ds = ds[\"test-clean\"]\r\ntest_other_ds = ds[\"test-other\"]\r\n```","So, `load_dataset(\"librispeech_asr\")` is not possible, it must be `load_dataset(\"librispeech_asr\", \"all\")`?\r\n\r\nWhy is that?\r\n\r\nThe docs say:\r\n```\r\nname: `str` name, optional configuration for the dataset that affects the data generated on disk. Different\r\n    `builder_config`s will have their own subdirectories and versions.\r\n    If not provided, uses the first configuration in self.BUILDER_CONFIGS\r\n```\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/cd3ce34ab1604118351e1978d26402de57188901\/src\/datasets\/builder.py#L228\r\n\r\nOr maybe you could just define `DEFAULT_CONFIG_NAME`?\r\n","> If not provided, uses the first configuration in self.BUILDER_CONFIGS\r\n\r\nOh crap this is outdated documentation. No it doesn't take the first config by default.\r\n\r\nEDIT: opened a PR to fix this: https:\/\/github.com\/huggingface\/datasets\/pull\/4186","> No it doesn't take the first config by default.\r\n\r\nBut defining `DEFAULT_CONFIG_NAME` would work?\r\n\r\nSo should we define `DEFAULT_CONFIG_NAME = \"all\"` here as well? I think this is a reasonable default config.\r\n\r\nDon't most datasets have some default config?\r\n","> But defining DEFAULT_CONFIG_NAME would work?\r\n>\r\n> So should we define DEFAULT_CONFIG_NAME = \"all\" here as well? I think this is a reasonable default config.\r\n\r\nYes that would work, and I also find it reasonable to do it :)\r\n\r\n> Don't most datasets have some default config?\r\n\r\nMost datasets only have one configuration, so the single configuration is the default one. Then other datasets gave several configurations, and whether they have a default one is decided case-by-case.\r\n\r\ne.g. `glue` is a benchmark and doesn't have a default task, one must choose which task of `glue` they want to use explicitely.","Thanks a lot for the feedback! \r\n\r\nUsing `\"all\"` now as the default config. I changed the layout a bit so that there is not a single \"train\", but instead we have multiple \"train.clean.100\", \"train.clean.360\", \"train.other.500\". This way we don't even need to do filtering and it's also cleaner IMO.\r\n\r\n@albertz - you should now be able to do the following:\r\n\r\n```python\r\nload_dataset(\"librispeech_asr\")  # <- run this once to download, prepare dataset and cache everything\r\n\r\n# The following operations will be very fast since all the downloading and processing is already cached\r\ntrain_1 = load_dataset(\"librispeech_asr\", split=\"train.clean.100\")\r\nprint(train_1)\r\ntrain_2 = load_dataset(\"librispeech_asr\", split=\"train.clean.100+train.clean.360\")\r\nprint(train_2)\r\ntrain_full = load_dataset(\"librispeech_asr\", split=\"train.clean.100+train.clean.360+train.other.500\")\r\nprint(train_full)\r\ndev_clean_ds = load_dataset(\"librispeech_asr\", split=\"validation.clean\")\r\nprint(dev_clean_ds)\r\ndev_other_ds = load_dataset(\"librispeech_asr\", split=\"validation.other\")\r\nprint(dev_other_ds)\r\ntest_clean_ds = load_dataset(\"librispeech_asr\", split=\"test.clean\")\r\nprint(test_clean_ds)\r\ntest_other_ds = load_dataset(\"librispeech_asr\", split=\"test.other\")\r\nprint(test_other_ds)\r\n```\r\n\r\n\r\n","Think this way we have the best of both worlds. Also @lhoestq, I think we could highlight better in the docs that it's possible to combine different splits. We do this actually quite a lot for speech. For Common Voice many people include \"validation\" in the training if the data is too small, e.g.: https:\/\/github.com\/huggingface\/transformers\/blob\/ff06b177917384137af2d9585697d2d76c40cdfc\/examples\/pytorch\/speech-recognition\/run_speech_recognition_ctc.py#L147\r\n\r\nShould we maybe add a short section to the loading tutorial here: https:\/\/huggingface.co\/docs\/datasets\/v2.1.0\/en\/loading#hugging-face-hub ? (Happy to do it)","Is there any advantage or difference in calling `load_dataset` multiple times for each split? Or why not just call `load_dataset` once and then access each split?\r\n\r\nNote in our case, we cannot really use the caching mechanism because we have a recipe pipeline used by multiple users (and I think a common cache dir for all users might end up in problems) and we basically would use `load_dataset(\"librispeech_asr\").save_to_disk(...)` and then later `load_from_disk(...)`. (See here: https:\/\/github.com\/rwth-i6\/i6_core\/pull\/253)\r\n\r\nSo with `load_from_disk`, we cannot really provide the split this way, so we anyway would do sth like:\r\n```python\r\nds = datasets.load_from_disk(...)\r\ntrain = ds[\"train\"]\r\n```\r\nOr with your latest proposal, it would look like:\r\n```python\r\nds = datasets.load_from_disk(...)\r\ntrain_ds = datasets.concatenate_datasets(\r\n  [ds[\"train.clean.100\"], ds[\"train.clean.360\"], ds[\"train.other.500\"]])\r\n```\r\nright?\r\n","> Is there any advantage or difference in calling `load_dataset` multiple times for each split? Or why not just call `load_dataset` once and then access each split?\r\n> \r\n> Note in our case, we cannot really use the caching mechanism because we have a recipe pipeline used by multiple users (and I think a common cache dir for all users might end up in problems) and we basically would use `load_dataset(\"librispeech_asr\").save_to_disk(...)` and then later `load_from_disk(...)`. (See here: [rwth-i6\/i6_core#253](https:\/\/github.com\/rwth-i6\/i6_core\/pull\/253))\r\n> \r\n> So with `load_from_disk`, we cannot really provide the split this way, so we anyway would do sth like:\r\n> \r\n> ```python\r\n> ds = datasets.load_from_disk(...)\r\n> train = ds[\"train\"]\r\n> ```\r\n> \r\n> Or with your latest proposal, it would look like:\r\n> \r\n> ```python\r\n> ds = datasets.load_from_disk(...)\r\n> train_ds = datasets.concatenate_datasets(\r\n>   [ds[\"train.clean.100\"], ds[\"train.clean.360\"], ds[\"train.other.500\"]])\r\n> ```\r\n> \r\n> right?\r\n\r\nI see the use case! The only advantage by calling `datasets` multiple times is that one can easily \"merge\" splits with `\"+\"`, but yeah you can do the exact same with `concatenate`.\r\n\r\n@lhoestq what do you think is the best approach with `load_from_disk`? \r\n\r\n@albertz, you could also define the `cache_dir` when doing `load_dataset(...)` which will then put all the relevant `arrow` files int the cache dir that you defined, e.g.:\r\n\r\n```python\r\nload_dataset(\"librispeech_asr\", cache_dir=\"\/easy\/to\/access\/directory\")\r\n```","@albertz, I took a read through https:\/\/github.com\/rwth-i6\/i6_core\/pull\/253 . \r\n\r\nI think the best would be the following:\r\n\r\n1. Do `ds = load_dataset(..., cache_dir=\"\/dir\/that\/is\/easy\/to\/access\")` <- having merged this PR, this will save all the original `.flac` files in the `cache_dir`\r\n2. Do `ds.save_to_disk(\"local\/path\")` this should then only save the `arrow.format` with a `path` string to the audio files which are located in `cache_dir` <- this won't require a lot of memory after https:\/\/github.com\/huggingface\/datasets\/pull\/4184#discussion_r854132740 is fixed and can be done for each person individually.\r\n3. `ds = datasets.load_from_disk(\"local\/path\")` can the be used. An object of `ds` will then have a `path` variable that links to the original audio files in the `cache_dir`. You can change these audio files then easily to `.mp3. You could do this with the `.map(...)` function, e.g. define a function that maps through all audio files, load them and then save them on disk afterward.","@lhoestq - I think this one is good to go","> @albertz, I took a read through [rwth-i6\/i6_core#253](https:\/\/github.com\/rwth-i6\/i6_core\/pull\/253) .\r\n> \r\n> I think the best would be the following:\r\n> \r\n> 1. Do `ds = load_dataset(..., cache_dir=\"\/dir\/that\/is\/easy\/to\/access\")` <- having merged this PR, this will save all the original `.flac` files in the `cache_dir`\r\n> 2. Do `ds.save_to_disk(\"local\/path\")` this should then only save the `arrow.format` with a `path` string to the audio files which are located in `cache_dir` <- this won't require a lot of memory after [[Librispeech] Add 'all' config\u00a0#4184 (comment)](https:\/\/github.com\/huggingface\/datasets\/pull\/4184#discussion_r854132740) is fixed and can be done for each person individually.\r\n> 3. `ds = datasets.load_from_disk(\"local\/path\")` can the be used. An object of `ds` will then have a `path` variable that links to the original audio files in the `cache_dir`. You can change these audio files then easily to `.mp3. You could do this with the `.map(...)` function, e.g. define a function that maps through all audio files, load them and then save them on disk afterward.\r\n\r\nOh, so you say that our current implementation in https:\/\/github.com\/rwth-i6\/i6_core\/pull\/253 is broken? Because our cache dir is just some temp directory which will be removed afterwards, and we just store what we get out of `save_to_disk`. I think it would be good to clarify that in the doc of `save_to_disk`, that this is not enough and can depend on files from the cache dir. (@dthulke)\r\n\r\nSo, you say we anyway need to share the cache dir among users? But we would want to make sure that after the initial download and preparation of the data, this is set to readonly, because we want to make sure that other people will not modify the data in any way. Right?\r\n\r\nBut then, we don't really need the `save_to_disk` and `load_from_disk` at all, right?\r\n","@albertz \r\n\r\n> Oh, so you say that our current implementation in https:\/\/github.com\/rwth-i6\/i6_core\/pull\/253 is broken? Because our cache dir is just some temp directory which will be removed afterwards, and we just store what we get out of save_to_disk. I think it would be good to clarify that in the doc of save_to_disk, that this is not enough and can depend on files from the cache dir. (@dthulke)\r\n\r\nOh, I wasn't aware that audio files are handled this way. Then we should have the cache directory as an additional job output, so that we keep the audio files. \r\n\r\n> So, you say we anyway need to share the cache dir among users?\r\n\r\nNo, the cache dir can still be a directory in the job output folder. Then the audio paths in the corresponding dataset column correspond to the flac files in that directory. This way the \"output\" of the job is contained into the job directory and we don't write files to a global cache directory that is independent of the sisyphus graph.\r\n\r\nIf we want to share the audio data between different users, we can just link to a central instance of the job (similar to how we do it with the `DownloadLibriSpeechCorpusJob`).","@dthulke - that's a good point actually! So you can do both things:\r\n\r\n1. Convert all audio files to bytes. Bytes can be saved by `arrow` so in this case you can do `save_to_disk(...)`, but then you cannot really inspect the audio files locally as they'll just be saved within a large arrow file (this actually used to be the default case but we're changing this now). The problem of this is summarized here a bit:  https:\/\/github.com\/huggingface\/datasets\/issues\/3663 . You can still do this if you'd like, e.g. you could do:\r\n\r\n```python\r\nds = load_dataset(\"librispeech_asr\")\r\n\r\ndef read_file(batch):\r\n    with open(batch[\"file\"], \"r\") as f:\r\n        batch[\"bytes\"] = f.read()   \r\n    return batch\r\n\r\nds = ds.map(read_file)\r\nds.save_to_disk(\"\/path\") <- the saved arrow object will now contain everything you need\r\n```\r\n\r\nhowever this is not recommend - it's should be much easier to just save the path to the downloaded audio files.\r\n\r\n2. Not convert audio files to bytes, but just leave them in their original file format. Then only the path to the original files will be save in arrow. This will be the default case. This means that when you do `load_dataset(...)` both the orginal audio data and the arrow file will be saved in the `cache_dir` (which can be saved locally for every user or in a shared cache - we actually use a shared cache quite a bit at Hugging Face). When do you do `save_to_disk(...)` now only the `path` will be saved in `arrow` format (after this PR is merged, you'll see that the `arrow files should be very light weight` meaning that `save_to_disk(...)` can be done for every user, but has a dependency on the `cache_dir` (because the audio files live there).\r\n\r\n=> Now what you could do as well would be to simply move all the audio files to the folder you want (the `save_to_disk(...)` folder) and then change the path of every sample to this folder (maybe with `map(...)`) and then this folder would be self contained. I do however think it's better to just specific a `cache_dir` and re-use `load_dataset(...)` every time instead of `load_from_disk` or `save_to_disk(...)`. Note that you can even pass the relevant cache files to `load_dataset(...)` here: https:\/\/huggingface.co\/docs\/datasets\/v2.1.0\/en\/package_reference\/loading_methods#datasets.load_dataset.data_files in which case you can be 100% sure that nothing is redownloaded. \r\n\r\nWe discussed storing audio files quite a bit, e.g. see: https:\/\/github.com\/huggingface\/datasets\/issues\/3663 and had (too many) changes around this topic recently, but we've come to the conclusion that the best is to leave the audio format in the format it was originally (`.flac` for Librispeech) so that the user can easily inspect it \/ understand the data. Arrow cannot save data is `.flac` so we'll just save a path to the original data. Curious to hear your guys opinion on this as well.","So what I would suggest here is to do the following:\r\n\r\n1. Do `load_dataset(..., cache_dir=\/a\/read-only\/folder)`\r\n2. \r\n- Either just re-use `load_dataset(..., cache_dir=...)` which should always re-use the data in the `cache_dir` since the hash of the url matches - so there should never be any duplicated downloading \r\n\r\nor \r\n\r\n- If you want to store the files in MP3 locally, first convert the files to MP3 in the read-only folder, then take do `ds.save_to_disk(\/some\/path)` which will save the correct path to the read-only folder to MP3 and then you can easily re-use the small arrow dataset that is saved in `\/some\/path`","> So what I would suggest here is to do the following:\r\n> \r\n> 1. Do `load_dataset(..., cache_dir=\/a\/read-only\/folder)`\r\n> \r\n> * Either just re-use `load_dataset(..., cache_dir=...)` which should always re-use the data in the `cache_dir` since the hash of the url matches - so there should never be any duplicated downloading\r\n> \r\n> or\r\n> \r\n> * If you want to store the files in MP3 locally, first convert the files to MP3 in the read-only folder, then take do `ds.save_to_disk(\/some\/path)` which will save the correct path to the read-only folder to MP3 and then you can easily re-use the small arrow dataset that is saved in `\/some\/path`\r\n\r\nAlso relevant here: https:\/\/github.com\/huggingface\/datasets\/issues\/3663","I also added some documentation about how `save_to_disk` handles audio files here: https:\/\/github.com\/huggingface\/datasets\/pull\/4193","> > So, you say we anyway need to share the cache dir among users?\r\n> \r\n> No, the cache dir can still be a directory in the job output folder.\r\n\r\n@dthulke But this is what I mean. When we share the job output folder, it means we share the cache dir among users.\r\n\r\nI wonder if `load_dataset(..., cache_dir=job_output_cache_dir)` is always save to do then, that it really would not modify the `job_output_cache_dir`.\r\n\r\nWe could enforce that by making the `job_output_cache_dir` read-only afterwards. We currently don't do this.\r\n\r\n@patrickvonplaten @dthulke But in any case, we actually prefer the data content to be inside the dataset (the arrow files). Lots of small files would be very problematic for our cache manager. We have one main copy of the data on NFS, but accessing the NFS directly by all computing nodes is not feasible, so the cache manager will have copies of the files on the nodes. So it means, whenever we access some file, we query the cache manager DB whether the file is already cached somewhere (some other computing node) and if so, it copies it from the other computing node and not from NFS. This works very well when there are not too many files (but the files can be big). So, we want to have only a few but big files. Even for NFS access this is much better.\r\n\r\nI also commented in #3663.\r\n","Hey @albertz @dthulke,\r\n\r\nThanks a lot for your input! \r\n\r\nWe've discussed quite a bit with @lhoestq and we think the best approach is the following:\r\n\r\n\r\na)\r\n`load_dataset(...)` will not store both bytes and the files because this would mean that 3x the size of the dataset would often be needed (1. the compressed `tar.gz` file, 2. the extracted file b, 3. the raw bytes in arrow format). \r\n\r\nFor canonical datasets like librispeech and common voice I think we want to keep the dataset filenames because of i) no breaking changes and ii) reasons explained in #3663\r\n\r\nHowever it's also trivial to write your own datasetset downloading script of librispeech and just not extract the folder e.g. this line: https:\/\/huggingface.co\/datasets\/common_voice\/blob\/main\/common_voice.py#L671\r\n\r\nAnd then it'll be allowed to save the bytes and the dataset will be self-contained out-of-the-box when using `load_dataset(...)`\r\n\r\nb) Now, one major problem that you guys uncovered is that `save_to_disk(...)` is currently not necessarily saving a dataset to be self-contained. We will change that asap. This means that after we've corrected this when you do download the canonical librispeech dataset the following will work:\r\n\r\n```python\r\nds = load_dataset(\"....\")  # <- here we have a dependency on the filepathes\r\nds[0][\"audio\"][\"bytes\"] # <- will not work\r\n\r\nds.save_to_disk(\"\/local\/path\")  # <- now we want to have a self-contained dataset in arrow format, so we load the files into bytes and save it in arrow format\r\n\r\n# now you can delete everything besides \"\/local\/path\"\r\n\r\nds = load_from_disk(\"\/local\/path\")  # <- this will work\r\n```\r\n\r\nSo either option a) where you define your own librispeech data downloading script (you guys could just sign up here: https:\/\/huggingface.co\/join) and upload a dataset loading script in private mode so that no one can see it and you would always store the audio as bytes or b) where you first load then save to disk then delete cache would work. \r\n\r\nHope that fits in your vision :-)\r\n\r\ncc @lhoestq @mariosasko ","@patrickvonplaten sounds like a good approach to me. For b) this could even be configurable with a parameter like `embed_external_files` as you have for `push_to_hub` (if people prefer to keep separate audio files).\r\n","> However it's also trivial to write your own datasetset downloading script of librispeech and just not extract the folder\r\n\r\nI don't exactly understand. In all cases, we need to extract it to prepare the dataset, or not? No matter if we want to store the raw bytes inside the dataset or leaving them as local files. Just in the first case, we can safely delete the extracted files after the dataset preparation.\r\n\r\n> `save_to_disk(...)` is currently not necessarily saving a dataset to be self-contained. We will change that asap.\r\n\r\nFor us, this sounds exactly like what we want.\r\n\r\nBut regarding not introducing breaking changes, wouldn't this maybe also break some setups for users who don't expect this new behavior?\r\n","@albertz I would suggest to move the discussion on implementation details on our side to the following issue: rwth-i6\/i6_core\/issues\/257","I like the idea of adding `embed_external_files` and set it to True by default to `save_to_disk`.\r\nIt's indeed a kind of breaking change since some users will get bigger Arrow files when updating the lib, but the advantages are nice:\r\n1. I like the idea of having it self contained, in case you want to delete your cache\r\n2. users also upload these Arrow files to cloud storage via the `fs` parameter, and in this case they would expect to upload a self-contained dataset\r\n3. consistency with `push_to_hub`\r\n\r\nIf it sounds good to you I'll open an issue to discuss this and track the advancements"],"created_at":1650385676000,"updated_at":1650621099000,"closed_at":1650620717000,"author_association":"MEMBER","active_lock_reason":null,"body":"Add `\"all\"` config to Librispeech","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4184\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4184\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4184","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4184","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4184.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4184.patch","merged_at":1650620717000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4183","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4183\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4183\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4183\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4183","id":1208449335,"node_id":"PR_kwDODunzps42bjXn","number":4183,"title":"Document librispeech configs","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["I think the main purpose of #4179 was how to be able to load both configs into one, so should we maybe add this part of the code: https:\/\/github.com\/huggingface\/datasets\/issues\/4179#issuecomment-1102383717 \r\n\r\nto the doc? \r\n\r\nActually @lhoestq would this work given that they have different split names: https:\/\/huggingface.co\/datasets\/librispeech_asr#data-splits ? ","This doc extension does not explain why I can't simply load the whole dataset. Or what workaround I need to get the whole dataset, which is what people usually want for Librispeech.","_The documentation is not available anymore as the PR was closed or merged._","@lhoestq, I can add a `\"all\"` config to Librispeech have the datasets already cached somewhere ","I'm closing this PR then, feel free to continue the discussion in https:\/\/github.com\/huggingface\/datasets\/issues\/4179\r\n"],"created_at":1650378419000,"updated_at":1650381696000,"closed_at":1650381320000,"author_association":"MEMBER","active_lock_reason":null,"body":"Added an example of how to load one config or the other","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4183\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4183\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4183","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4183","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4183.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4183.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4182","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4182\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4182\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4182\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4182","id":1208285235,"node_id":"I_kwDODunzps5IBPgz","number":4182,"title":"Zenodo.org download is not responding","user":{"login":"dkajtoch","id":32985207,"node_id":"MDQ6VXNlcjMyOTg1MjA3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/32985207?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/dkajtoch","html_url":"https:\/\/github.com\/dkajtoch","followers_url":"https:\/\/api.github.com\/users\/dkajtoch\/followers","following_url":"https:\/\/api.github.com\/users\/dkajtoch\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/dkajtoch\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/dkajtoch\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/dkajtoch\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/dkajtoch\/orgs","repos_url":"https:\/\/api.github.com\/users\/dkajtoch\/repos","events_url":"https:\/\/api.github.com\/users\/dkajtoch\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/dkajtoch\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["[Off topic but related: Is the uptime of S3 provably better than Zenodo's?]","Hi @dkajtoch, please note that at HuggingFace we are not hosting this dataset: we are just using a script to download their data file and create a dataset from it.\r\n\r\nIt was the dataset owners decision to host their data at Zenodo. You can see this on their website: https:\/\/marcobaroni.org\/composes\/sick.html\r\n\r\nAnd yes, you are right: Zenodo is currently having some incidents and people are reporting problems from it.\r\n\r\nOn the other hand, we could contact the data owners and propose them to host their data at our Hugging Face Hub.\r\n\r\n@julien-c I guess so.\r\n","Thanks @albertvillanova. I know that the problem lies in the source data. I just wanted to point out that these kind of problems are unavoidable without having one place where data sources are cached. Websites may go down or data sources may move. Having a copy in Hugging Face Hub would be a great solution. ","Definitely, @dkajtoch! But we have to ask permission to the data owners. And many dataset licenses directly forbid data redistribution: in those cases we are not allowed to host their data on our Hub.","Ahhh good point! License is the problem :("],"created_at":1650371217000,"updated_at":1650438665000,"closed_at":1650438665000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"## Describe the bug\r\nSource download_url from zenodo.org does not respond. \r\n`_DOWNLOAD_URL = \"https:\/\/zenodo.org\/record\/2787612\/files\/SICK.zip?download=1\"`\r\nOther datasets also use zenodo.org to store data and they cannot be downloaded as well.\r\n\r\nIt would be better to actually use more reliable way to store original data like s3 bucket.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nload_dataset(\"sick\")\r\n```\r\n\r\n## Expected results\r\nDataset should be downloaded.\r\n\r\n## Actual results\r\nConnectionError: Couldn't reach https:\/\/zenodo.org\/record\/2787612\/files\/SICK.zip?download=1 (ReadTimeout(ReadTimeoutError(\"HTTPSConnectionPool(host='zenodo.org', port=443): Read timed out. (read timeout=100)\")))\r\n\r\n## Environment info\r\n- `datasets` version: 2.1.0\r\n- Platform: Darwin-21.4.0-x86_64-i386-64bit\r\n- Python version: 3.7.11\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.3.5\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4182\/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4182\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4181","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4181\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4181\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4181\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4181","id":1208194805,"node_id":"I_kwDODunzps5IA5b1","number":4181,"title":"FLEURS","user":{"login":"patrickvonplaten","id":23423619,"node_id":"MDQ6VXNlcjIzNDIzNjE5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/23423619?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/patrickvonplaten","html_url":"https:\/\/github.com\/patrickvonplaten","followers_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/followers","following_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/orgs","repos_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/repos","events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/received_events","type":"User","site_admin":false},"labels":[{"id":2067388877,"node_id":"MDU6TGFiZWwyMDY3Mzg4ODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20bug","name":"dataset bug","color":"2edb81","default":false,"description":"A bug in a dataset script provided in the library"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Yes, you just have to use `dl_manager.iter_archive` instead of `dl_manager.download_and_extract`.\r\n\r\nThat's because `download_and_extract` doesn't support TAR archives in streaming mode.","Tried to make it streamable, but I don't think it's really possible. @lhoestq @polinaeterna maybe you guys can check: \r\nhttps:\/\/huggingface.co\/datasets\/google\/fleurs\/commit\/dcf80160cd77977490a8d32b370c027107f2407b \r\n\r\nreal quick. \r\n\r\nI think the problem is that we cannot ensure that the metadata file is found before the audio. Or is this possible somehow @lhoestq ? ","@patrickvonplaten I think the metadata file should be found first because the audio files are contained in a folder next to the metadata files (just as in common voice), so the metadata files should be \"on top of the list\"   as they are closer to the root in the directories hierarchy ","@patrickvonplaten but apparently it doesn't... I don't really know why.","Yeah! Any ideas what could be the reason here? cc @lhoestq ?","The order of the files is determined when the TAR archive is created, depending on the commands the creator ran.\r\nIf the metadata file is not at the beginning of the file, that makes streaming completely inefficient. In this case the TAR archive needs to be recreated in an appropriate order.","Actually we could maybe just host the metadata file ourselves and then stream the audio data only. Don't think that this would be a problem for the FLEURS authors (I can ask them :-)) "],"created_at":1650366596000,"updated_at":1651094950000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"## Dataset viewer issue for '*name of the dataset*'\r\n\r\nhttps:\/\/huggingface.co\/datasets\/google\/fleurs\r\n\r\n```\r\nStatus code:   400\r\nException:     NotImplementedError\r\nMessage:       Extraction protocol for TAR archives like 'https:\/\/storage.googleapis.com\/xtreme_translations\/FLEURS\/af_za.tar.gz' is not implemented in streaming mode. Please use `dl_manager.iter_archive` instead.\r\n```\r\n\r\nAm I the one who added this dataset ? Yes\r\n\r\nCan I fix this somehow in the script? @lhoestq @severo \r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4181\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4181\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4180","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4180\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4180\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4180\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4180","id":1208042320,"node_id":"I_kwDODunzps5IAUNQ","number":4180,"title":"Add some iteration method on a dataset column (specific for inference)","user":{"login":"Narsil","id":204321,"node_id":"MDQ6VXNlcjIwNDMyMQ==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/204321?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Narsil","html_url":"https:\/\/github.com\/Narsil","followers_url":"https:\/\/api.github.com\/users\/Narsil\/followers","following_url":"https:\/\/api.github.com\/users\/Narsil\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Narsil\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Narsil\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Narsil\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Narsil\/orgs","repos_url":"https:\/\/api.github.com\/users\/Narsil\/repos","events_url":"https:\/\/api.github.com\/users\/Narsil\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Narsil\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Thanks for the suggestion ! I agree it would be nice to have something directly in `datasets` to do something as simple as that\r\n\r\ncc @albertvillanova @mariosasko @polinaeterna What do you think if we have something similar to pandas `Series` that wouldn't bring everything in memory when doing `dataset[\"audio\"]` ? Currently it returns a list with all the decoded audio data in memory.\r\n\r\nIt would be a breaking change though, since `isinstance(dataset[\"audio\"], list)` wouldn't work anymore, but we could implement a `Sequence` so that `dataset[\"audio\"][0]` still works and only loads one item in memory.\r\n\r\nYour alternative suggestion with `iterate` is also sensible, though maybe less satisfactory in terms of experience IMO","I agree that current behavior (decoding all audio file sin the dataset when accessing `dataset[\"audio\"]`) is not useful, IMHO. Indeed in our docs, we are constantly warning our collaborators not to do that.\r\n\r\nTherefore I upvote for a \"useful\" behavior of `dataset[\"audio\"]`. I don't think the breaking change is important in this case, as I guess no many people use it with its current behavior. Therefore, for me it seems reasonable to return a generator (instead of an in-memeory list) for \"special\" features, like Audio\/Image.\r\n\r\n@lhoestq on the other hand I don't understand your proposal about Pandas-like... ","I recall I had the same idea while working on the `Image` feature, so I agree implementing something similar to `pd.Series` that lazily brings elements in memory would be beneficial.","@lhoestq @mariosasko Could you please give a link to that new feature of `pandas.Series`? As far as I remember since I worked with pandas for more than 6 years, there was no lazy in-memory feature; it was everything in-memory; that was the reason why other frameworks were created, like Vaex or Dask, e.g. ","Yea pandas doesn't do lazy loading. I was referring to pandas.Series to say that they have a dedicated class to represent a column ;)"],"created_at":1650359745000,"updated_at":1650537058000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Is your feature request related to a problem? Please describe.**\r\nA clear and concise description of what the problem is.\r\n\r\nCurrently, `dataset[\"audio\"]` will load EVERY element in the dataset in RAM, which can be quite big for an audio dataset.\r\nHaving an iterator (or sequence) type of object, would make inference with `transformers` 's `pipeline` easier to use and not so memory hungry.\r\n\r\n**Describe the solution you'd like**\r\nA clear and concise description of what you want to happen.\r\n\r\nFor a non breaking change:\r\n\r\n```python\r\nfor audio in dataset.iterate(\"audio\"):\r\n    # {\"array\": np.array(...), \"sampling_rate\":...}\r\n```\r\n\r\nFor a  breaking change solution (not necessary), changing the type of `dataset[\"audio\"]` to a sequence type so that\r\n\r\n```python\r\npipe = pipeline(model=\"...\")\r\nfor out in pipe(dataset[\"audio\"]):\r\n    # {\"text\":....}\r\n```\r\ncould work\r\n\r\n**Describe alternatives you've considered**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\n```python\r\ndef iterate(dataset, key):\r\n    for item in dataset:\r\n        yield dataset[key]\r\n\r\nfor out in pipeline(iterate(dataset, \"audio\")):\r\n    # {\"array\": ...}\r\n```\r\n\r\nThis works but requires the helper function which feels slightly clunky.\r\n\r\n**Additional context**\r\nAdd any other context about the feature request here.\r\n\r\nThe context is actually to showcase better integration between  `pipeline` and `datasets` in the Quicktour demo: https:\/\/github.com\/huggingface\/transformers\/pull\/16723\/files\r\n\r\n@lhoestq \r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4180\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4180\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4179","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4179\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4179\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4179\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4179","id":1208001118,"node_id":"I_kwDODunzps5IAKJe","number":4179,"title":"Dataset librispeech_asr fails to load","user":{"login":"albertz","id":59132,"node_id":"MDQ6VXNlcjU5MTMy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59132?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertz","html_url":"https:\/\/github.com\/albertz","followers_url":"https:\/\/api.github.com\/users\/albertz\/followers","following_url":"https:\/\/api.github.com\/users\/albertz\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertz\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertz\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertz\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertz\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertz\/repos","events_url":"https:\/\/api.github.com\/users\/albertz\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertz\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["@patrickvonplaten Hi! I saw that you prepared this? :)","Another thing, but maybe this should be a separate issue: As I see from the code, it would try to use up to 16 simultaneous downloads? This is problematic for Librispeech or anything on OpenSLR. On [the homepage](https:\/\/www.openslr.org\/), it says:\r\n\r\n> If you want to download things from this site, please download them one at a time, and please don't use any fancy software-- just download things from your browser or use 'wget'. We have a firewall rule to drop connections from hosts with more than 5 simultaneous connections, and certain types of download software may activate this rule.\r\n\r\nRelated: https:\/\/github.com\/tensorflow\/datasets\/issues\/3885","Hey @albertz,\r\n\r\nNice to see you here! It's been a while ;-) ","Sorry maybe the docs haven't been super clear here. By `split` we mean one of `train.500`, `train.360`, `train.100`, `validation`, `test`. For Librispeech, you'll have to specific a config (either `other` or `clean`) though:\r\n\r\n```py\r\ndatasets.load_dataset(\"librispeech_asr\", \"clean\")\r\n```\r\n\r\nshould work and give you all splits (being \"train\", \"test\", ...) for the clean config of the dataset.\r\n","If you need both `\"clean\"` and `\"other\"` I think you'll have to do concatenate them as follows: \r\n\r\n```py\r\nfrom datasets import concatenate_datasets, load_dataset\r\n\r\nother = load_dataset(\"librispeech_asr\", \"other\")\r\nclean = load_dataset(\"librispeech_asr\", \"clean\")\r\n\r\nlibrispeech = concatenate_datasets([other, clean])\r\n```\r\n\r\nSee https:\/\/huggingface.co\/docs\/datasets\/v2.1.0\/en\/process#concatenate","Downloading one split would be:\r\n\r\n```py\r\nfrom datasets import load_dataset\r\n\r\nother = load_dataset(\"librispeech_asr\", \"other\", split=\"train.500\")\r\n```\r\n\r\n\r\n","cc @lhoestq FYI maybe the docs can be improved here","Ah thanks. But wouldn't it be easier\/nicer (and more canonical) to just make it in a way that simply `load_dataset(\"librispeech_asr\")` works?","Pinging @lhoestq here, think this could make sense! Not sure however how the dictionary would then look like","Would it make sense to have `clean` as the default config ?\r\n\r\nAlso I think `load_dataset(\"librispeech_asr\")` should have raised you an error that says that you need to specify a config\r\n\r\nI also opened a PR to improve the doc: https:\/\/github.com\/huggingface\/datasets\/pull\/4183","> Would it make sense to have `clean` as the default config ?\r\n\r\nI think a user would expect that the default would give you the full dataset.\r\n\r\n> Also I think `load_dataset(\"librispeech_asr\")` should have raised you an error that says that you need to specify a config\r\n\r\nIt does raise an error, but this error confused me because I did not understand why I needed a config, or why I could not simply download the whole dataset, which is what people usually do with Librispeech.\r\n","+1 for @albertz. Also think lots of people download the whole dataset (`\"clean\"` + `\"other\"`) for Librispeech.\r\n\r\nThink there are also some people though who:\r\n- a) Don't have the memory to store the whole dataset\r\n- b) Just want to evaluate on one of the two configs","Ok ! Adding the \"all\" configuration would do the job then, thanks ! In the \"all\" configuration we can merge all the train.xxx splits into one \"train\" split, or keep them separate depending on what's the most practical to use (probably put everything in \"train\" no ?)","I'm not too familiar with how to work with HuggingFace datasets, but people often do some curriculum learning scheme, where they start with train.100, later go over to train.100 + train.360, and then later use the whole train (960h). It would be good if this is easily possible.\r\n","Hey @albertz, \r\n\r\nopened a PR here. Think by adding the \"subdataset\" class to each split \"train\", \"dev\", \"other\" as shown here: https:\/\/github.com\/huggingface\/datasets\/pull\/4184\/files#r853272727 it should be easily possible (e.g. with the filter function https:\/\/huggingface.co\/docs\/datasets\/v2.1.0\/en\/package_reference\/main_classes#datasets.Dataset.filter )","But also since everything is cached one could also just do:\r\n\r\n```python\r\nload_dataset(\"librispeech\", \"clean\", \"train.100\")\r\nload_dataset(\"librispeech\", \"clean\", \"train.100+train.360\")\r\nload_dataset(\"librispeech\" \"all\", \"train\") \r\n```"],"created_at":1650357948000,"updated_at":1650386186000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\nThe dataset librispeech_asr (standard Librispeech) fails to load.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndatasets.load_dataset(\"librispeech_asr\")\r\n```\r\n\r\n## Expected results\r\nIt should download and prepare the whole dataset (all subsets).\r\n\r\nIn [the doc](https:\/\/huggingface.co\/datasets\/librispeech_asr), it says it has two configurations (clean and other).\r\nHowever, the dataset doc says that not specifying `split` should just load the whole dataset, which is what I want.\r\n\r\nAlso, in case of this specific dataset, this is also the standard what the community uses. When you look at any publications with results on Librispeech, they always use the whole train dataset for training.\r\n\r\n## Actual results\r\n```\r\n...\r\n  File \"\/home\/az\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/librispeech_asr\/1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c\/librispeech_asr.py\", line 119, in LibrispeechASR._split_generators\r\n    line: archive_path = dl_manager.download(_DL_URLS[self.config.name])\r\n    locals:\r\n      archive_path = <not found>\r\n      dl_manager = <local> <datasets.utils.download_manager.DownloadManager object at 0x7fc07b426160>\r\n      dl_manager.download = <local> <bound method DownloadManager.download of <datasets.utils.download_manager.DownloadManager object at 0x7fc07b426160>>\r\n      _DL_URLS = <global> {'clean': {'dev': 'http:\/\/www.openslr.org\/resources\/12\/dev-clean.tar.gz', 'test': 'http:\/\/www.openslr.org\/resources\/12\/test-clean.tar.gz', 'train.100': 'http:\/\/www.openslr.org\/resources\/12\/train-clean-100.tar.gz', 'train.360': 'http:\/\/www.openslr.org\/resources\/12\/train-clean-360.tar.gz'}, 'other'...\r\n      self = <local> <datasets_modules.datasets.librispeech_asr.1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c.librispeech_asr.LibrispeechASR object at 0x7fc12a633310>\r\n      self.config = <local> BuilderConfig(name='default', version=0.0.0, data_dir='\/home\/az\/i6\/setups\/2022-03-20--sis\/work\/i6_core\/datasets\/huggingface\/DownloadAndPrepareHuggingFaceDatasetJob.TV6Nwm6dFReF\/output\/data_dir', data_files=None, description=None)\r\n      self.config.name = <local> 'default', len = 7\r\nKeyError: 'default'\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.1.0\r\n- Platform: Linux-5.4.0-107-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.9\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.4.2\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4179\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4179\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4178","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4178\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4178\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4178\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4178","id":1207787073,"node_id":"PR_kwDODunzps42ZfFN","number":4178,"title":"[feat] Add ImageNet dataset","user":{"login":"apsdehal","id":3616806,"node_id":"MDQ6VXNlcjM2MTY4MDY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3616806?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/apsdehal","html_url":"https:\/\/github.com\/apsdehal","followers_url":"https:\/\/api.github.com\/users\/apsdehal\/followers","following_url":"https:\/\/api.github.com\/users\/apsdehal\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/apsdehal\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/apsdehal\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/apsdehal\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/apsdehal\/orgs","repos_url":"https:\/\/api.github.com\/users\/apsdehal\/repos","events_url":"https:\/\/api.github.com\/users\/apsdehal\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/apsdehal\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Thanks for the comments. I believe I have addressed all of them and also decreased the size of the dummy data file, so it should be ready for a re-review. I also made a change to allow adding synset mapping and valprep script in config in case we add ImageNet 21k some time later. ","@lhoestq I have updated the PR to address all of the review comments."],"created_at":1650348095000,"updated_at":1651268639000,"closed_at":1651268228000,"author_association":"MEMBER","active_lock_reason":null,"body":"To use the dataset download the tar file\r\n[imagenet_object_localization_patched2019.tar.gz](https:\/\/www.kaggle.com\/competitions\/imagenet-object-localization-challenge\/data?select=imagenet_object_localization_patched2019.tar.gz) from Kaggle and then point the datasets library to it by using:\r\n\r\n```py\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"imagenet\",\r\ndata_dir=\"\/path\/to\/imagenet_object_localization_patched2019.tar.gz\")\r\n```\r\n\r\nCurrently train and validation splits are supported.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4178\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4178\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4178","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4178","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4178.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4178.patch","merged_at":1651268228000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4177","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4177\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4177\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4177\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4177","id":1207535920,"node_id":"PR_kwDODunzps42Yxca","number":4177,"title":"Adding missing subsets to the `SemEval-2018 Task 1` dataset","user":{"login":"micahcarroll","id":11460267,"node_id":"MDQ6VXNlcjExNDYwMjY3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/11460267?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/micahcarroll","html_url":"https:\/\/github.com\/micahcarroll","followers_url":"https:\/\/api.github.com\/users\/micahcarroll\/followers","following_url":"https:\/\/api.github.com\/users\/micahcarroll\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/micahcarroll\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/micahcarroll\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/micahcarroll\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/micahcarroll\/orgs","repos_url":"https:\/\/api.github.com\/users\/micahcarroll\/repos","events_url":"https:\/\/api.github.com\/users\/micahcarroll\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/micahcarroll\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1650322770000,"updated_at":1651095857000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"This dataset for the [1st task of SemEval-2018](https:\/\/competitions.codalab.org\/competitions\/17751) competition was missing all subtasks except for subtask 5. I added another two subtasks (subtask 1 and 2), which are each comprised of 12 additional data subsets: for each language in En, Es, Ar, there are 4 datasets, broken down by emotions (anger, fear, joy, sadness).\r\n\r\n## Remaining questions\r\n\r\nI wasn't able to find any documentation about how one should make PRs to modify datasets. Because of that, I just did my best to integrate the new data into the code, and tested locally that this worked. I'm sorry if I'm not respecting your contributing guidelines \u2013 if they are documented somewhere, I'd appreciate if you could send a pointer!\r\n\r\nNot sure how `dataset_infos.json` and `dummy` should be updated. My understanding is that they were automatically generated at the time of the original dataset creation?","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4177\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4177\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4177","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4177","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4177.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4177.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4176","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4176\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4176\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4176\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4176","id":1206515563,"node_id":"I_kwDODunzps5H6fdr","number":4176,"title":"Very slow between two operations","user":{"login":"yananchen1989","id":26405281,"node_id":"MDQ6VXNlcjI2NDA1Mjgx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/26405281?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yananchen1989","html_url":"https:\/\/github.com\/yananchen1989","followers_url":"https:\/\/api.github.com\/users\/yananchen1989\/followers","following_url":"https:\/\/api.github.com\/users\/yananchen1989\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yananchen1989\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yananchen1989\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yananchen1989\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yananchen1989\/orgs","repos_url":"https:\/\/api.github.com\/users\/yananchen1989\/repos","events_url":"https:\/\/api.github.com\/users\/yananchen1989\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yananchen1989\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1650239549000,"updated_at":1650240180000,"closed_at":1650240180000,"author_association":"NONE","active_lock_reason":null,"body":"Hello, in the processing stage, I use two operations. The first one : map + filter, is very fast and it uses the full cores, while the socond step is very slow and did not use full cores. \r\n\r\nAlso, there is a significant lag between them.  Am I missing something ?\r\n\r\n\r\n\r\n ```\r\nraw_datasets = raw_datasets.map(split_func, \r\n                batched=False,\r\n                num_proc=args.preprocessing_num_workers,\r\n                load_from_cache_file=not args.overwrite_cache, \r\n                desc = \"running split para ==>\")\\\r\n                .filter(lambda example: example['text1']!='' and example['text2']!='', \r\n                    num_proc=args.preprocessing_num_workers, desc=\"filtering ==>\")\r\n\r\n\r\n    processed_datasets = raw_datasets.map(\r\n        preprocess_function,\r\n        batched=True, \r\n        num_proc=args.preprocessing_num_workers,\r\n        remove_columns=column_names,\r\n        load_from_cache_file=not args.overwrite_cache,\r\n        desc=\"Running tokenizer on dataset===>\",\r\n    )\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4176\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4176\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4175","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4175\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4175\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4175\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4175","id":1205589842,"node_id":"PR_kwDODunzps42SqF-","number":4175,"title":"Add WIT Dataset","user":{"login":"thomasw21","id":24695242,"node_id":"MDQ6VXNlcjI0Njk1MjQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/24695242?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/thomasw21","html_url":"https:\/\/github.com\/thomasw21","followers_url":"https:\/\/api.github.com\/users\/thomasw21\/followers","following_url":"https:\/\/api.github.com\/users\/thomasw21\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/thomasw21\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/thomasw21\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/thomasw21\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/thomasw21\/orgs","repos_url":"https:\/\/api.github.com\/users\/thomasw21\/repos","events_url":"https:\/\/api.github.com\/users\/thomasw21\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/thomasw21\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Hi! Coming in late with some context.\r\n\r\nThere are two versions of the WIT dataset:\r\n1. The original source dataset managed by Wikimedia. It has more information, raw image representations, and each row corresponds to an image linked to all of its captions wherever it happens in Wikipedia (in multiple languages)\r\n2. The Google version, corresponding to the data script in this PR, which duplicates image instances and requires the user to download the images themselves from the provided URL (note that a basic implementation will have them download the same picture several time. @thomasw21 using our download manager instead of `urllib` could help with that, but it wouldn't be required if people had access to the first version)\r\n\r\nThe Wikimedia folks were really interested in us hosting a ready-to-go streaming version of this dataset where users don't have to download the version themselves, which is why we have the pre-processed versions on an HF bucket, with the raw images and a pre-computed embedding (don't remember the model, we can keep it ). That's the data script currently in https:\/\/github.com\/huggingface\/datasets\/pull\/2981 . It's nearly ready to go, the one thing we should do is move the raw data from our HF google Cloud bucket to the Hub.\r\n\r\nHow do you want to move forward? IMO the best way would be to have a WIT dataset under the Wikimedia org with both configurations, but it depends on everyone's timelines","Okay after offline discussion. We'll improve this versions and push it to the hub under `google` namespace. \r\n\r\n> which duplicates image instances and requires the user to download the images themselves from the provided URL (note that a basic implementation will have them download the same picture several time. @thomasw21 using our download manager instead of urllib could help with that, but it wouldn't be required if people had access to the first version)\r\n\r\nAh interesting wasn't aware of this duplication issue, concretely it'll just mean that our dataset in bigger than expected ... I think this should be handled after this loading script (though I have to figure our how to spawn a dl_manager).\r\n\r\n> The Wikimedia folks were really interested in us hosting a ready-to-go streaming version of this dataset where users don't have to download the version themselves, which is why we have the pre-processed versions on an HF bucket, with the raw images and a pre-computed embedding (don't remember the model, we can keep it ). That's the data script currently in https:\/\/github.com\/huggingface\/datasets\/pull\/2981 . It's nearly ready to go, the one thing we should do is move the raw data from our HF google Cloud bucket to the Hub.\r\n\r\nSimilarly a script will be written and pushed to `wikimedia` organisation.","@mariosasko can you make one last review concerning the text description changes? Then I'll handle putting it under `google` namespace and close this PR.","Looks all good now. Great job! ","Closing as this has been migrated to the hub under `google` namespace: https:\/\/huggingface.co\/datasets\/google\/wit"],"created_at":1650030152000,"updated_at":1651502041000,"closed_at":1651501601000,"author_association":"MEMBER","active_lock_reason":null,"body":"closes #2981 #2810\r\n\r\n@nateraw @hassiahk I've listed you guys as co-author as you've contributed previously to this dataset","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4175\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4175\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4175","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4175","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4175.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4175.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4174","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4174\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4174\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4174\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4174","id":1205575941,"node_id":"PR_kwDODunzps42SnJS","number":4174,"title":"Fix when map function modifies input in-place","user":{"login":"thomasw21","id":24695242,"node_id":"MDQ6VXNlcjI0Njk1MjQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/24695242?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/thomasw21","html_url":"https:\/\/github.com\/thomasw21","followers_url":"https:\/\/api.github.com\/users\/thomasw21\/followers","following_url":"https:\/\/api.github.com\/users\/thomasw21\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/thomasw21\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/thomasw21\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/thomasw21\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/thomasw21\/orgs","repos_url":"https:\/\/api.github.com\/users\/thomasw21\/repos","events_url":"https:\/\/api.github.com\/users\/thomasw21\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/thomasw21\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1650028995000,"updated_at":1650034327000,"closed_at":1650033958000,"author_association":"MEMBER","active_lock_reason":null,"body":"When `function` modifies input in-place, the guarantee that columns in `remove_columns` are contained in `input` doesn't hold true anymore. Therefore we need to relax way we pop elements by checking if that column exists.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4174\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4174\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4174","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4174","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4174.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4174.patch","merged_at":1650033958000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4173","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4173\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4173\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4173\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4173","id":1204657114,"node_id":"PR_kwDODunzps42Ppnd","number":4173,"title":"Stream private zipped images","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","oops looks like some tests are failing sorry, will fix them tomorrow\r\n\r\nEDIT: not today but asap hopefully","cc @mariosasko this is ready for review, let me know what you think !"],"created_at":1649949307000,"updated_at":1651759554000,"closed_at":1651759115000,"author_association":"MEMBER","active_lock_reason":null,"body":"As mentioned in https:\/\/github.com\/huggingface\/datasets\/issues\/4139 it's currently not possible to stream private\/gated zipped images from the Hub.\r\n\r\nThis is because `Image.decode_example` does not handle authentication. Indeed decoding requires to access and download the file from the private repository.\r\n\r\nIn this PR I added authentication to `Image.decode_example` via a `token_per_repo_id` optional argument. I first wanted to just pass `use_auth_token` but a single `Image` instance can be responsible of decoding images from a combination of several datasets together (from `interleave_datasets` for example). Therefore I just used a dictionary `repo_id` -> `token` instead.\r\n\r\nI'm getting the `repo_id` from the dataset builder (I replaced the `namepace` attribute with `repo_id`)\r\n\r\nI did the same for `Audio.decode_example`\r\n\r\ncc @SBrandeis @severo ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4173\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4173\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4173","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4173","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4173.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4173.patch","merged_at":1651759115000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4172","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4172\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4172\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4172\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4172","id":1204433160,"node_id":"PR_kwDODunzps42O7LW","number":4172,"title":"Update assin2 dataset_infos.json","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649937186000,"updated_at":1650034062000,"closed_at":1650033682000,"author_association":"MEMBER","active_lock_reason":null,"body":"Following comments in https:\/\/github.com\/huggingface\/datasets\/issues\/4003 we found that it was outdated and casing an error when loading the dataset","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4172\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4172\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4172","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4172","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4172.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4172.patch","merged_at":1650033682000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4170","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4170\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4170\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4170\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4170","id":1204413620,"node_id":"PR_kwDODunzps42O2-L","number":4170,"title":"to_tf_dataset rewrite","user":{"login":"Rocketknight1","id":12866554,"node_id":"MDQ6VXNlcjEyODY2NTU0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12866554?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Rocketknight1","html_url":"https:\/\/github.com\/Rocketknight1","followers_url":"https:\/\/api.github.com\/users\/Rocketknight1\/followers","following_url":"https:\/\/api.github.com\/users\/Rocketknight1\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Rocketknight1\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Rocketknight1\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Rocketknight1\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Rocketknight1\/orgs","repos_url":"https:\/\/api.github.com\/users\/Rocketknight1\/repos","events_url":"https:\/\/api.github.com\/users\/Rocketknight1\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Rocketknight1\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4170). All of your documentation changes will be reflected on that endpoint.","[Magic is now banned](https:\/\/www.youtube.com\/watch?v=WIn58XoY728#t=36s) by decree of @sgugger. This is honestly much cleaner, and the functionality will make much more sense in `transformers` anyway!","@gante I renamed the default collator to `minimal_tf_collate_fn`!","@lhoestq @sgugger @gante \r\n\r\nI think this should now be ready, it looks good in testing! I'll try a few more notebooks today and tomorrow to be sure before I merge. Key changes are:\r\n\r\n- No column autodetection magic (will make a separate PR to add this as a `transformers` function)\r\n- Drops non-numerical features automatically (this is more of a 'DataLoader' method, we'll have a separate method to expose 'raw' datasets to `tf.data`)\r\n- Better autodetection of numerical features.\r\n- Shouldn't randomly crash mid-function :skull: \r\n\r\nWe definitely have some questions still to resolve about how to handle making a 'DataLoader' dataset versus a 'raw' dataset - see [the Notion doc](https:\/\/www.notion.so\/huggingface2\/Splitting-to_tf_dataset-c2e0773c4bec484384064b30ed634383) if you're interested. Still, since this PR is just fixes\/improvements to an existing method which never supported non-numerical features anyway, we can merge it before we've resolved those issues, and then think about how to name and split things afterwards.","P.S. I'll take out the region comments at the end before I merge, I promise! They're just helpful while I'm editing it","+1 for the tests\r\n\r\n> Drops non-numerical features automatically\r\n\r\nCan you give more details on how this work and the rationale as well ? This is not explained in the docs\r\n\r\nAlso why are you adding `error_on_missing` and `auto_fix_label_names ` ? The rationale is not clear to me. In particular I think it is sensible enough to expect users to not ask columns that don't exist, and to rename a label column when required.","@lhoestq I rewrote those parts - they were causing some other issues too! `error_on_missing` and `auto_fix_label_names` have been removed. The new logic is to simply drop (before batch collation) all columns the user doesn't ask for, but not to raise errors if the user asked for columns not in the dataset, as they may be added by the collator. Hopefully this cleans it up and matches the documentation better!"],"created_at":1649935858000,"updated_at":1651827230000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"This PR rewrites almost all of `to_tf_dataset()`, which makes it kind of hard to list all the changes, but the most critical ones are:\r\n\r\n- Much better stability and no more dropping unexpected column names (Sorry @NielsRogge)\r\n- Doesn't clobber custom transforms on the data (Sorry @NielsRogge again)\r\n- Much better handling of the situation when the `collate_fn` adds columns that aren't in the dataset.\r\n- Better inference of shapes and data types\r\n- Lots of hacky special-casing code removed\r\n- Can return string columns (as `tf.String`)\r\n- Most arguments have default values, calling the method should be much simpler\r\n- ~~Can accept a `model` argument and only return columns that are valid inputs to that model~~\r\n- Drops the `dummy_labels` argument - this was a workaround for Keras issues that have been resolved by changes in `transformers`. Also remove it from tests and the Overview notebook.\r\n\r\nI still have a couple of TODOs remaining and some testing to do, so don't merge yet, but it should be mostly ready for review at this point!","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4170\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4170\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4170","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4170","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4170.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4170.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4169","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4169\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4169\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4169\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4169","id":1203995869,"node_id":"I_kwDODunzps5Hw4Td","number":4169,"title":"Timit_asr dataset cannot be previewed recently","user":{"login":"YingLi001","id":75192317,"node_id":"MDQ6VXNlcjc1MTkyMzE3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/75192317?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/YingLi001","html_url":"https:\/\/github.com\/YingLi001","followers_url":"https:\/\/api.github.com\/users\/YingLi001\/followers","following_url":"https:\/\/api.github.com\/users\/YingLi001\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/YingLi001\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/YingLi001\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/YingLi001\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/YingLi001\/orgs","repos_url":"https:\/\/api.github.com\/users\/YingLi001\/repos","events_url":"https:\/\/api.github.com\/users\/YingLi001\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/YingLi001\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Thanks for reporting. The bug has already been detected, and we hope to fix it soon.","TIMIT is now a dataset that requires manual download, see #4145 \r\n\r\nTherefore it might take a bit more time to fix it","> TIMIT is now a dataset that requires manual download, see #4145\r\n> \r\n> Therefore it might take a bit more time to fix it\r\n\r\nThank you for your quickly response. Exactly, I also found the manual download issue in the morning. But when I used *list_datasets()* to check the available datasets, *'timit_asr'* is still in the list. So I am a little bit confused. If *'timit_asr'* need to be manually downloaded, does that mean we can **not** automatically download it **any more** in the future?","Yes exactly. If you try to load the dataset it will ask you to download it manually first, and to pass the downloaded and extracted data like `load_dataset(\"timir_asr\", data_dir=\"path\/to\/extracted\/data\")`\r\n\r\nThe URL we were using was coming from a host that doesn't have the permission to redistribute the data, and the dataset owners (LDC) notified us about it."],"created_at":1649906911000,"updated_at":1651853211000,"closed_at":1651853211000,"author_association":"NONE","active_lock_reason":null,"body":"## Dataset viewer issue for '*timit_asr*'\r\n\r\n**Link:** *https:\/\/huggingface.co\/datasets\/timit_asr*\r\n\r\nIssue: The timit-asr dataset cannot be previewed recently.\r\n\r\nAm I the one who added this dataset ? Yes-No\r\nNo","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4169\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4169\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4168","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4168\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4168\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4168\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4168","id":1203867540,"node_id":"PR_kwDODunzps42NL6F","number":4168,"title":"Add code examples to API docs","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","> Do you think it is clearer to make every code example fully reproducible so when users copy the code they can actually run it and get an output? This seems quite repetitive - maybe even unnecessary - but it is definitely clearer.\r\n\r\nI think it's ok to be repetitive to get more clarity. Many users come from `transformers` and may have little experience with some processing methods (especially torch users).\r\n\r\n> Should we showcase a function with more than one parameter to highlight different use-cases (it's pretty basic right now, but I'd be happy to add more)?\r\n\r\nMaybe let's do it case by case, depending on whether there are parameters that are likely to be used often ?\r\n\r\n> For the class_encode_column function, let me know if there is a simpler dataset with fewer columns (currently using winograd_wsc) so it is easier for users to see what changed.\r\n\r\nYou can try with `boolq`, it has a boolean column that can be converted to labels\r\n\r\n> Where possible, I try to show the input before and the output after using a function like flatten for example. Do you think this is too much and just showing the usage (ie, >>> ds.flatten()) will be sufficient?\r\n\r\nNo I don't think it's too much, it's nice this way thanks :)","Updated each code example so they are fully reproducible (where applicable)! The next step will be to identify some functions where we can show off some parameters that are useful or commonly used. Some useful parameters can be:\r\n\r\n- use `map(batched=True)` to process batches of examples.\r\n- set a seed in `shuffle`.\r\n- set `shuffle` and `seed` in `train_test_split`.\r\n\r\nLet me know if you think of anything else related to the functions in `arrow_dataset.py`!","Cool thanks ! I think you can also do `num_proc` for `map`"],"created_at":1649891018000,"updated_at":1651085617000,"closed_at":1651085314000,"author_association":"MEMBER","active_lock_reason":null,"body":"This PR adds code examples for functions related to the base Datasets class to highlight usage. Most of the examples use the `rotten_tomatoes` dataset since it is nice and small. Several things I would appreciate feedback on:\r\n\r\n- Do you think it is clearer to make every code example fully reproducible so when users copy the code they can actually run it and get an output? This seems quite repetitive - maybe even unnecessary - but it is definitely clearer. Personally, I think we might be able to get away with not including this since users probably want to try the function on their own dataset. For example:\r\n\r\n   ```py\r\n   >>> from datasets import load_dataset\r\n   >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\r\n   >>> code example goes here\r\n   ```\r\n\r\n- Should we showcase a function with more than one parameter to highlight different use-cases (it's pretty basic right now, but I'd be happy to add more)?\r\n- For the `class_encode_column` function, let me know if there is a simpler dataset with fewer columns (currently using `winograd_wsc`) so it is easier for users to see what changed.\r\n- Where possible, I try to show the input before and the output after using a function like `flatten` for example. Do you think this is too much and just showing the usage (ie, `>>> ds.flatten()`) will be sufficient?\r\n\r\nThanks :)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4168\/reactions","total_count":2,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":2,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4168\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4168","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4168","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4168.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4168.patch","merged_at":1651085314000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4167","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4167\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4167\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4167\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4167","id":1203761614,"node_id":"PR_kwDODunzps42M1O5","number":4167,"title":"Avoid rate limit in update hub repositories","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["I also set GIT_LFS_SKIP_SMUDGE=1 to speed up git clones","_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649881937000,"updated_at":1649883401000,"closed_at":1649883032000,"author_association":"MEMBER","active_lock_reason":null,"body":"use http.extraHeader to avoid rate limit","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4167\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4167\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4167","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4167","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4167.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4167.patch","merged_at":1649883032000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4166","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4166\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4166\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4166\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4166","id":1203758004,"node_id":"PR_kwDODunzps42M0dS","number":4166,"title":"Fix exact match","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649881686000,"updated_at":1651580611000,"closed_at":1651580187000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Clarify docs and add clarifying example to the exact_match metric","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4166\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4166\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4166","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4166","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4166.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4166.patch","merged_at":1651580187000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4165","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4165\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4165\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4165\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4165","id":1203730187,"node_id":"PR_kwDODunzps42MubF","number":4165,"title":"Fix google bleu typos, examples","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649879994000,"updated_at":1651580632000,"closed_at":1651580204000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4165\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4165\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4165","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4165","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4165.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4165.patch","merged_at":1651580204000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4164","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4164\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4164\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4164\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4164","id":1203661346,"node_id":"PR_kwDODunzps42MfxX","number":4164,"title":"Fix duplicate key in multi_news","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649875704000,"updated_at":1649883856000,"closed_at":1649883482000,"author_association":"MEMBER","active_lock_reason":null,"body":"To merge after this job succeeded: https:\/\/github.com\/huggingface\/datasets\/runs\/6012207928","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4164\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4164\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4164","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4164","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4164.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4164.patch","merged_at":1649883482000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4163","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4163\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4163\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4163\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4163","id":1203539268,"node_id":"I_kwDODunzps5HvI1E","number":4163,"title":"Optional Content Warning for Datasets","user":{"login":"TristanThrush","id":20826878,"node_id":"MDQ6VXNlcjIwODI2ODc4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/20826878?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/TristanThrush","html_url":"https:\/\/github.com\/TristanThrush","followers_url":"https:\/\/api.github.com\/users\/TristanThrush\/followers","following_url":"https:\/\/api.github.com\/users\/TristanThrush\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/TristanThrush\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/TristanThrush\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/TristanThrush\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/TristanThrush\/orgs","repos_url":"https:\/\/api.github.com\/users\/TristanThrush\/repos","events_url":"https:\/\/api.github.com\/users\/TristanThrush\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/TristanThrush\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi! You can use the `extra_gated_prompt` YAML field in a dataset card for displaying custom messages\/warnings that the user must accept before gaining access to the actual dataset. This option also keeps the viewer hidden until the user agrees to terms. "],"created_at":1649867881000,"updated_at":1651493058000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Is your feature request related to a problem? Please describe.**\r\nA clear and concise description of what the problem is.\r\n\r\nWe now have hate speech datasets on the hub, like this one: https:\/\/huggingface.co\/datasets\/HannahRoseKirk\/HatemojiBuild\r\n\r\nI'm wondering if there is an option to select a content warning message that appears before the dataset preview? Otherwise, people immediately see hate speech when clicking on this dataset.\r\n\r\n**Describe the solution you'd like**\r\nA clear and concise description of what you want to happen.\r\n\r\nImplementation of a content warning message that separates users from the dataset preview until they click out of the warning.\r\n\r\n**Describe alternatives you've considered**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\nPossibly just a way to remove the dataset preview completely? I think I like the content warning option better, though.\r\n\r\n**Additional context**\r\nAdd any other context about the feature request here.\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4163\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4163\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4162","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4162\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4162\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4162\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4162","id":1203421909,"node_id":"PR_kwDODunzps42LtGO","number":4162,"title":"Add Conceptual 12M","user":{"login":"thomasw21","id":24695242,"node_id":"MDQ6VXNlcjI0Njk1MjQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/24695242?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/thomasw21","html_url":"https:\/\/github.com\/thomasw21","followers_url":"https:\/\/api.github.com\/users\/thomasw21\/followers","following_url":"https:\/\/api.github.com\/users\/thomasw21\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/thomasw21\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/thomasw21\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/thomasw21\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/thomasw21\/orgs","repos_url":"https:\/\/api.github.com\/users\/thomasw21\/repos","events_url":"https:\/\/api.github.com\/users\/thomasw21\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/thomasw21\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Looks like your dummy_data.zip file is not in the right location ;)\r\ndatasets\/datasets\/conceptual_12m\/dummy\/default\/0.0.0\/dummy_data.zip\r\n->\r\ndatasets\/conceptual_12m\/dummy\/default\/0.0.0\/dummy_data.zip"],"created_at":1649861843000,"updated_at":1650010381000,"closed_at":1650009985000,"author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4162\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4162\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4162","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4162","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4162.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4162.patch","merged_at":1650009985000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4161","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4161\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4161\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4161\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4161","id":1203230485,"node_id":"PR_kwDODunzps42LEhi","number":4161,"title":"Add Visual Genome","user":{"login":"thomasw21","id":24695242,"node_id":"MDQ6VXNlcjI0Njk1MjQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/24695242?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/thomasw21","html_url":"https:\/\/github.com\/thomasw21","followers_url":"https:\/\/api.github.com\/users\/thomasw21\/followers","following_url":"https:\/\/api.github.com\/users\/thomasw21\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/thomasw21\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/thomasw21\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/thomasw21\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/thomasw21\/orgs","repos_url":"https:\/\/api.github.com\/users\/thomasw21\/repos","events_url":"https:\/\/api.github.com\/users\/thomasw21\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/thomasw21\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Hum there seems to be some issues with tasks in test:\r\n - some tasks don't fit anything in `tasks.json`. Do I remove them in `task_categories`?\r\n - some tasks should exist, typically `visual-question-answering` (https:\/\/github.com\/huggingface\/datasets\/blame\/9f2ff14673cac1f1ad56d80221a793f5938b68c7\/src\/datasets\/utils\/resources\/tasks.json#L195) yet the exception is failing on me. I'm guessing it's because my `master` is not up-to-date. However this means that the testing only tests my branch instead of the one merged with master?\r\n \r\n cc @mariosasko @lhoestq ","> some tasks don't fit anything in tasks.json. Do I remove them in task_categories?\r\n\r\nYou can keep them, but add `other-` as a prefix to those tasks to make the CI ignore it\r\n\r\n> some tasks should exist, typically visual-question-answering (https:\/\/github.com\/huggingface\/datasets\/blame\/9f2ff14673cac1f1ad56d80221a793f5938b68c7\/src\/datasets\/utils\/resources\/tasks.json#L195) yet the exception is failing on me. I'm guessing it's because my master is not up-to-date. However this means that the testing only tests my branch instead of the one merged with master?\r\n\r\nFeel free to merge upstream\/master into your branch ;)\r\n\r\nEDIT: actually I just noticed you've already done this, thanks !","After offline discussions: will keep that image essentially it's necessary as I have a mapping that creates a mapping between url and local path (images are downloaded via a zip file) and dummy data needs to store that dummy image. The issue is when I read an annotation, I get a url, compute the local path, and basically I assume the local path exists since I've extracted all the images ... This isn't true if dummy data doesn't have all the images, so instead I've added a script that \"fixes\" the dummy data after using the CLI, it essentially adds the dummy image in the zip corresponding to the url."],"created_at":1649852724000,"updated_at":1650555769000,"closed_at":1650546532000,"author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4161\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4161\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4161","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4161","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4161.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4161.patch","merged_at":1650546532000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4160","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4160\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4160\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4160\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4160","id":1202845874,"node_id":"I_kwDODunzps5Hsfiy","number":4160,"title":"RGBA images not showing","user":{"login":"cceyda","id":15624271,"node_id":"MDQ6VXNlcjE1NjI0Mjcx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/15624271?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/cceyda","html_url":"https:\/\/github.com\/cceyda","followers_url":"https:\/\/api.github.com\/users\/cceyda\/followers","following_url":"https:\/\/api.github.com\/users\/cceyda\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/cceyda\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/cceyda\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/cceyda\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/cceyda\/orgs","repos_url":"https:\/\/api.github.com\/users\/cceyda\/repos","events_url":"https:\/\/api.github.com\/users\/cceyda\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/cceyda\/received_events","type":"User","site_admin":false},"labels":[{"id":3470211881,"node_id":"LA_kwDODunzps7O1zsp","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer","name":"dataset-viewer","color":"E5583E","default":false,"description":"Related to the dataset viewer on huggingface.co"},{"id":4030246674,"node_id":"LA_kwDODunzps7wOK8S","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer-rgba-images","name":"dataset-viewer-rgba-images","color":"6C5FC0","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},"assignees":[{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Thanks for reporting. It's a known issue, and we hope to fix it soon."],"created_at":1649833163000,"updated_at":1649833660000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"## Dataset viewer issue for ceyda\/smithsonian_butterflies_transparent\r\n\r\n[**Link:** *link to the dataset viewer page*](https:\/\/huggingface.co\/datasets\/ceyda\/smithsonian_butterflies_transparent)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/15624271\/163117683-e91edb28-41bf-43d9-b371-5c62e14f40c9.png)\r\n\r\nAm I the one who added this dataset ? Yes\r\n\r\n\ud83d\udc49  More of a general issue of 'RGBA' png images not being supported \r\n(the dataset itself is just for the huggan sprint and not that important, consider it just an example)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4160\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4160\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4159","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4159\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4159\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4159\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4159","id":1202522153,"node_id":"PR_kwDODunzps42Izmd","number":4159,"title":"Add `TruthfulQA` dataset","user":{"login":"jon-tow","id":41410219,"node_id":"MDQ6VXNlcjQxNDEwMjE5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/41410219?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jon-tow","html_url":"https:\/\/github.com\/jon-tow","followers_url":"https:\/\/api.github.com\/users\/jon-tow\/followers","following_url":"https:\/\/api.github.com\/users\/jon-tow\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jon-tow\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jon-tow\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jon-tow\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jon-tow\/orgs","repos_url":"https:\/\/api.github.com\/users\/jon-tow\/repos","events_url":"https:\/\/api.github.com\/users\/jon-tow\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jon-tow\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4159). All of your documentation changes will be reflected on that endpoint."],"created_at":1649805544000,"updated_at":1649806237000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4159\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4159\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4159","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4159","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4159.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4159.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4158","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4158\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4158\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4158\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4158","id":1202376843,"node_id":"PR_kwDODunzps42ITg3","number":4158,"title":"Add AUC ROC Metric","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649796808000,"updated_at":1651002110000,"closed_at":1651001722000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4158\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4158\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4158","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4158","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4158.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4158.patch","merged_at":1651001722000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4157","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4157\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4157\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4157\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4157","id":1202239622,"node_id":"PR_kwDODunzps42H2Wf","number":4157,"title":"Fix formatting in BLEU metric card","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649788191000,"updated_at":1649860225000,"closed_at":1649859394000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #4148 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4157\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4157\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4157","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4157","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4157.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4157.patch","merged_at":1649859394000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4156","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4156\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4156\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4156\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4156","id":1202220531,"node_id":"PR_kwDODunzps42HySw","number":4156,"title":"Adding STSb-TR dataset","user":{"login":"figenfikri","id":12762065,"node_id":"MDQ6VXNlcjEyNzYyMDY1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12762065?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/figenfikri","html_url":"https:\/\/github.com\/figenfikri","followers_url":"https:\/\/api.github.com\/users\/figenfikri\/followers","following_url":"https:\/\/api.github.com\/users\/figenfikri\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/figenfikri\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/figenfikri\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/figenfikri\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/figenfikri\/orgs","repos_url":"https:\/\/api.github.com\/users\/figenfikri\/repos","events_url":"https:\/\/api.github.com\/users\/figenfikri\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/figenfikri\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1649787005000,"updated_at":1649787155000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Semantic Textual Similarity benchmark Turkish (STSb-TR) dataset introduced in our paper [Semantic Similarity Based Evaluation for Abstractive News Summarization](https:\/\/aclanthology.org\/2021.gem-1.3.pdf) added.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4156\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4156\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4156","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4156","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4156.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4156.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4155","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4155\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4155\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4155\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4155","id":1202183608,"node_id":"PR_kwDODunzps42Hqam","number":4155,"title":"Make HANS dataset streamable","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649784853000,"updated_at":1649851426000,"closed_at":1649851055000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #4133 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4155\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4155\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4155","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4155","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4155.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4155.patch","merged_at":1649851054000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4154","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4154\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4154\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4154\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4154","id":1202145721,"node_id":"PR_kwDODunzps42Hh14","number":4154,"title":"Generate tasks.json taxonomy from `huggingface_hub`","user":{"login":"julien-c","id":326577,"node_id":"MDQ6VXNlcjMyNjU3Nw==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/326577?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/julien-c","html_url":"https:\/\/github.com\/julien-c","followers_url":"https:\/\/api.github.com\/users\/julien-c\/followers","following_url":"https:\/\/api.github.com\/users\/julien-c\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/julien-c\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/julien-c\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/julien-c\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/julien-c\/orgs","repos_url":"https:\/\/api.github.com\/users\/julien-c\/repos","events_url":"https:\/\/api.github.com\/users\/julien-c\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/julien-c\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Ok recomputed the json file, this should be ready to review now! @lhoestq ","Note: the generated JSON from `hf\/hub-docs` can be found in the output of a GitHub Action run on that repo, for instance in https:\/\/github.com\/huggingface\/hub-docs\/runs\/6006686983?check_suite_focus=true\r\n\r\n(click on \"Run export-tasks script\")","Should we not add the tasks with hideInDatasets?","yes, probably true \u2013 i'll change that in a PR in `hub-docs`","Yes that's good :) feel free to merge","thanks to the both of you!"],"created_at":1649783566000,"updated_at":1649932352000,"closed_at":1649931973000,"author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4154\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4154\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4154","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4154","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4154.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4154.patch","merged_at":1649931973000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4153","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4153\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4153\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4153\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4153","id":1202040506,"node_id":"PR_kwDODunzps42HLA8","number":4153,"title":"Adding Text-based NP Enrichment (TNE) dataset","user":{"login":"yanaiela","id":8031035,"node_id":"MDQ6VXNlcjgwMzEwMzU=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8031035?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yanaiela","html_url":"https:\/\/github.com\/yanaiela","followers_url":"https:\/\/api.github.com\/users\/yanaiela\/followers","following_url":"https:\/\/api.github.com\/users\/yanaiela\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yanaiela\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yanaiela\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yanaiela\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yanaiela\/orgs","repos_url":"https:\/\/api.github.com\/users\/yanaiela\/repos","events_url":"https:\/\/api.github.com\/users\/yanaiela\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yanaiela\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hey @lhoestq, can you please have a look? \ud83d\ude4f","Great, thanks again @lhoestq! I think we're good to go now","Done"],"created_at":1649778423000,"updated_at":1651586748000,"closed_at":1651586748000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Added the [TNE](https:\/\/github.com\/yanaiela\/TNE) dataset to the library","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4153\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4153\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4153","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4153","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4153.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4153.patch","merged_at":1651586748000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4152","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4152\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4152\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4152\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4152","id":1202034115,"node_id":"I_kwDODunzps5HpZXD","number":4152,"title":"ArrayND error in pyarrow 5","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Where do we bump the required pyarrow version? Any inputs on how I fix this issue? ","We need to bump it in `setup.py` as well as update some CI job to use pyarrow 6 instead of 5 in `.circleci\/config.yaml` and `.github\/workflows\/benchmarks.yaml`"],"created_at":1649778100000,"updated_at":1651656586000,"closed_at":1651656586000,"author_association":"MEMBER","active_lock_reason":null,"body":"As found in https:\/\/github.com\/huggingface\/datasets\/pull\/3903, The ArrayND features fail on pyarrow 5:\r\n```python\r\nimport pyarrow as pa\r\nfrom datasets import Array2D\r\nfrom datasets.table import cast_array_to_feature\r\n\r\narr = pa.array([[[0]]])\r\nfeature_type = Array2D(shape=(1, 1), dtype=\"int64\")\r\ncast_array_to_feature(arr, feature_type)\r\n```\r\nraises\r\n```python\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-8-04610f9fa78c> in <module>\r\n----> 1 cast_array_to_feature(pa.array([[[0]]]), Array2D(shape=(1, 1), dtype=\"int32\"))\r\n\r\n~\/Desktop\/hf\/datasets\/src\/datasets\/table.py in wrapper(array, *args, **kwargs)\r\n   1672             return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n   1673         else:\r\n-> 1674             return func(array, *args, **kwargs)\r\n   1675 \r\n   1676     return wrapper\r\n\r\n~\/Desktop\/hf\/datasets\/src\/datasets\/table.py in cast_array_to_feature(array, feature, allow_number_to_str)\r\n   1806         return array_cast(array, get_nested_type(feature), allow_number_to_str=allow_number_to_str)\r\n   1807     elif not isinstance(feature, (Sequence, dict, list, tuple)):\r\n-> 1808         return array_cast(array, feature(), allow_number_to_str=allow_number_to_str)\r\n   1809     raise TypeError(f\"Couldn't cast array of type\\n{array.type}\\nto\\n{feature}\")\r\n   1810 \r\n\r\n~\/Desktop\/hf\/datasets\/src\/datasets\/table.py in wrapper(array, *args, **kwargs)\r\n   1672             return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n   1673         else:\r\n-> 1674             return func(array, *args, **kwargs)\r\n   1675 \r\n   1676     return wrapper\r\n\r\n~\/Desktop\/hf\/datasets\/src\/datasets\/table.py in array_cast(array, pa_type, allow_number_to_str)\r\n   1705         array = array.storage\r\n   1706     if isinstance(pa_type, pa.ExtensionType):\r\n-> 1707         return pa_type.wrap_array(array)\r\n   1708     elif pa.types.is_struct(array.type):\r\n   1709         if pa.types.is_struct(pa_type) and (\r\n\r\nAttributeError: 'Array2DExtensionType' object has no attribute 'wrap_array'\r\n```\r\n\r\nThe thing is that `cast_array_to_feature` is called when writing an Arrow file, so creating an Arrow dataset using any ArrayND type currently fails.\r\n\r\n`wrap_array` has been added in pyarrow 6, so we can either bump the required pyarrow version or fix this for pyarrow 5","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4152\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4152\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4151","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4151\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4151\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4151\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4151","id":1201837999,"node_id":"PR_kwDODunzps42GgLu","number":4151,"title":"Add missing label for emotion description","user":{"login":"lijiazheng99","id":44396506,"node_id":"MDQ6VXNlcjQ0Mzk2NTA2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/44396506?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lijiazheng99","html_url":"https:\/\/github.com\/lijiazheng99","followers_url":"https:\/\/api.github.com\/users\/lijiazheng99\/followers","following_url":"https:\/\/api.github.com\/users\/lijiazheng99\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lijiazheng99\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lijiazheng99\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lijiazheng99\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lijiazheng99\/orgs","repos_url":"https:\/\/api.github.com\/users\/lijiazheng99\/repos","events_url":"https:\/\/api.github.com\/users\/lijiazheng99\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lijiazheng99\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1649769457000,"updated_at":1649771930000,"closed_at":1649771930000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4151\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4151\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4151","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4151","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4151.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4151.patch","merged_at":1649771930000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4150","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4150\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4150\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4150\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4150","id":1201689730,"node_id":"I_kwDODunzps5HoFSC","number":4150,"title":"Inconsistent splits generation for datasets without loading script (packaged dataset puts everything into a single split)","user":{"login":"polinaeterna","id":16348744,"node_id":"MDQ6VXNlcjE2MzQ4NzQ0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/16348744?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/polinaeterna","html_url":"https:\/\/github.com\/polinaeterna","followers_url":"https:\/\/api.github.com\/users\/polinaeterna\/followers","following_url":"https:\/\/api.github.com\/users\/polinaeterna\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/polinaeterna\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/polinaeterna\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/polinaeterna\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/polinaeterna\/orgs","repos_url":"https:\/\/api.github.com\/users\/polinaeterna\/repos","events_url":"https:\/\/api.github.com\/users\/polinaeterna\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/polinaeterna\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1649762155000,"updated_at":1651179764000,"closed_at":1651179764000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"## Describe the bug\r\nSplits for dataset loaders without scripts are prepared inconsistently. I think it might be confusing for users.\r\n\r\n## Steps to reproduce the bug\r\n* If you load a packaged datasets from Hub, it infers splits from directory structure \/ filenames (check out the data [here](https:\/\/huggingface.co\/datasets\/nateraw\/test-imagefolder-dataset)):\r\n```python\r\nds = load_dataset(\"nateraw\/test-imagefolder-dataset\")\r\nprint(ds)\r\n### Output:\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['image', 'label'],\r\n        num_rows: 6\r\n    })\r\n    test: Dataset({\r\n        features: ['image', 'label'],\r\n        num_rows: 4\r\n    })\r\n})\r\n```\r\n* If you do the same from locally stored data specifying only directory path you'll get the same:\r\n```python\r\nds = load_dataset(\"\/path\/to\/local\/data\/test-imagefolder-dataset\")\r\nprint(ds)\r\n### Output:\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['image', 'label'],\r\n        num_rows: 6\r\n    })\r\n    test: Dataset({\r\n        features: ['image', 'label'],\r\n        num_rows: 4\r\n    })\r\n})\r\n```\r\n* However, if you explicitely specify package name (like `imagefolder`, `csv`, `json`), all the data is put into a single split:\r\n```python\r\nds = load_dataset(\"imagefolder\", data_dir=\"\/path\/to\/local\/data\/test-imagefolder-dataset\")\r\nprint(ds)\r\n### Output:\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['image', 'label'],\r\n        num_rows: 10\r\n    })\r\n})\r\n```\r\n\r\n## Expected results\r\nFor `load_dataset(\"imagefolder\", data_dir=\"\/path\/to\/local\/data\/test-imagefolder-dataset\")` I expect the same output as of the two first options.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4150\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4150\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4149","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4149\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4149\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4149\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4149","id":1201389221,"node_id":"I_kwDODunzps5Hm76l","number":4149,"title":"load_dataset for winoground returning decoding error","user":{"login":"odellus","id":4686956,"node_id":"MDQ6VXNlcjQ2ODY5NTY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/4686956?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/odellus","html_url":"https:\/\/github.com\/odellus","followers_url":"https:\/\/api.github.com\/users\/odellus\/followers","following_url":"https:\/\/api.github.com\/users\/odellus\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/odellus\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/odellus\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/odellus\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/odellus\/orgs","repos_url":"https:\/\/api.github.com\/users\/odellus\/repos","events_url":"https:\/\/api.github.com\/users\/odellus\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/odellus\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["I thought I had fixed it with this after some helpful hints from @severo\r\n```python\r\nimport datasets \r\ntoken = 'hf_XXXXX'\r\ndataset = datasets.load_dataset(\r\n    'facebook\/winoground', \r\n    name='facebook--winoground', \r\n    split='train', \r\n    streaming=True,\r\n    use_auth_token=token,\r\n)\r\n```\r\nbut I found out that wasn't the case\r\n```python\r\n[x for x in dataset]\r\n...\r\nClientResponseError: 401, message='Unauthorized', url=URL('https:\/\/huggingface.co\/datasets\/facebook\/winoground\/resolve\/a86a60456fbbd242e9a744199071a6bd3e7fd9de\/examples.jsonl')\r\n```","Hi ! This dataset structure (image + labels in a JSON file) is not supported yet, though we're adding support for this in  in #4069 \r\n\r\nThe following structure will be supported soon:\r\n```\r\nmetadata.json\r\nimages\/\r\n     image0.png\r\n     image1.png\r\n    ...\r\n```\r\nWhere `metadata.json` is a JSON Lines file with labels or other metadata, and each line must have a \"file_name\" field with the name of the image file.\r\n\r\nFor the moment are only supported:\r\n- JSON files only\r\n- image files only\r\n\r\nSince this dataset is a mix of the two, at the moment it fails trying to read the images as JSON.\r\n\r\nTherefore to be able to load this dataset we need to wait for the new structure to be supported (very soon ^^), or add a dataset script in the repository that reads both the JSON and the images cc @TristanThrush \r\n","We'll also investigate the issue with the streaming download manager in https:\/\/github.com\/huggingface\/datasets\/issues\/4139 ;) thanks for reporting","Are there any updates on this?","In the meantime, anyone can always download the images.zip and examples.jsonl files directly from huggingface.co - let me know if anyone has issues with that.","I mirrored the files at https:\/\/huggingface.co\/datasets\/facebook\/winoground in a folder on my local machine `winground`\r\nand when I tried\r\n```python\r\nimport datasets\r\nds = datasets.load_from_disk('.\/winoground')\r\n```\r\nI get the following error\r\n```python\r\n--------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\nInput In [2], in <cell line: 1>()\r\n----> 1 ds = datasets.load_from_disk('.\/winoground')\r\n\r\nFile ~\/.local\/lib\/python3.8\/site-packages\/datasets\/load.py:1759, in load_from_disk(dataset_path, fs, keep_in_memory)\r\n   1757     return DatasetDict.load_from_disk(dataset_path, fs, keep_in_memory=keep_in_memory)\r\n   1758 else:\r\n-> 1759     raise FileNotFoundError(\r\n   1760         f\"Directory {dataset_path} is neither a dataset directory nor a dataset dict directory.\"\r\n   1761     )\r\n\r\nFileNotFoundError: Directory .\/winoground is neither a dataset directory nor a dataset dict directory.\r\n```\r\nso still some work to be done on the backend imo.","Note that `load_from_disk` is the function that reloads an Arrow dataset saved with `my_dataset.save_to_disk`.\r\n\r\nOnce we do support images with metadata you'll be able to use `load_dataset(\"facebook\/winoground\")` directly (or `load_dataset(\".\/winoground\")` of you've cloned the winoground repository locally).","Apologies for the delay. I added a custom dataset loading script for winoground. It should work now, with an auth token:\r\n\r\n`examples = load_dataset('facebook\/winoground', use_auth_token=<your auth token>)`\r\n\r\nLet me know if there are any issues","Adding the dataset loading script definitely didn't take as long as I thought it would \ud83d\ude05","killer"],"created_at":1649751376000,"updated_at":1651707638000,"closed_at":1651707638000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"## Describe the bug\r\nI am trying to use datasets to load winoground and I'm getting a JSON decoding error.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ntoken = 'hf_XXXXX' # my HF access token\r\ndatasets = load_dataset('facebook\/winoground', use_auth_token=token)\r\n```\r\n\r\n## Expected results\r\nI downloaded images.zip and examples.jsonl manually. I was expecting to have some trouble decoding json so I didn't use jsonlines but instead was able to get a complete set of 400 examples by doing\r\n```python\r\nimport json\r\n\r\nwith open('examples.jsonl', 'r') as f:\r\n    examples = f.read().split('\\n')\r\n\r\n# Thinking this would error if the JSON is not utf-8 encoded\r\njson_data = [json.loads(x) for x in examples]\r\nprint(json_data[-1])\r\n```\r\nand I see\r\n```python\r\n{'caption_0': 'someone is overdoing it',\r\n 'caption_1': 'someone is doing it over',\r\n 'collapsed_tag': 'Relation',\r\n 'id': 399,\r\n 'image_0': 'ex_399_img_0',\r\n 'image_1': 'ex_399_img_1',\r\n 'num_main_preds': 1,\r\n 'secondary_tag': 'Morpheme-Level',\r\n 'tag': 'Scope, Preposition'}\r\n\r\n```\r\nso I'm not sure what's going on here honestly. The file `examples.jsonl` doesn't have non-UTF-8 encoded text.\r\n\r\n## Actual results\r\nDuring the split operation after downloading, datasets encounters an error in the JSON ([trace](https:\/\/gist.github.com\/odellus\/e55d390ca203386bf551f38e0c63a46b) abbreviated for brevity).\r\n```\r\ndatasets\/packaged_modules\/json\/json.py:144 in Json._generate_tables(self, files)\r\n...\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\r\n```\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.4\r\n- Platform: Linux-5.13.0-39-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 7.0.0\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4149\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4149\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4148","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4148\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4148\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4148\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4148","id":1201169242,"node_id":"I_kwDODunzps5HmGNa","number":4148,"title":"fix confusing bleu metric example","user":{"login":"aizawa-naoki","id":6253193,"node_id":"MDQ6VXNlcjYyNTMxOTM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/6253193?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/aizawa-naoki","html_url":"https:\/\/github.com\/aizawa-naoki","followers_url":"https:\/\/api.github.com\/users\/aizawa-naoki\/followers","following_url":"https:\/\/api.github.com\/users\/aizawa-naoki\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/aizawa-naoki\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/aizawa-naoki\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/aizawa-naoki\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/aizawa-naoki\/orgs","repos_url":"https:\/\/api.github.com\/users\/aizawa-naoki\/repos","events_url":"https:\/\/api.github.com\/users\/aizawa-naoki\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/aizawa-naoki\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1649744306000,"updated_at":1649859394000,"closed_at":1649859394000,"author_association":"NONE","active_lock_reason":null,"body":"**Is your feature request related to a problem? Please describe.**\r\nI would like to see the example in \"Metric Card for BLEU\" changed.\r\nThe 0th element in the predictions list is not closed in square brackets, and the 1st list is missing a comma.\r\nThe BLEU score are calculated correctly, but it is difficult to understand, so it would be helpful if you could correct this.\r\n```\r\n>> predictions = [\r\n...     [\"hello\", \"there\", \"general\", \"kenobi\",       # <- no closing square bracket.\r\n...     [\"foo\", \"bar\" \"foobar\"]                              # <- no comma between \"bar\" and \"foobar\"\r\n... ]\r\n>>> references = [\r\n...     [[\"hello\", \"there\", \"general\", \"kenobi\"]],\r\n...     [[\"foo\", \"bar\", \"foobar\"]]\r\n... ]\r\n>>> bleu = datasets.load_metric(\"bleu\")\r\n>>> results = bleu.compute(predictions=predictions, references=references)\r\n>>> print(results)\r\n{'bleu': 0.6370964381207871, ...\r\n```\r\n\r\n**Describe the solution you'd like**\r\n```\r\n>> predictions = [\r\n...     [\"hello\", \"there\", \"general\", \"kenobi\",       # <- no closing square bracket.\r\n...     [\"foo\", \"bar\" \"foobar\"]                              # <- no comma between \"bar\" and \"foobar\"\r\n... ]\r\n# and\r\n>>> print(results)\r\n{'bleu':1.0, ...\r\n```\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4148\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4148\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4147","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4147\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4147\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4147\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4147","id":1200756008,"node_id":"PR_kwDODunzps42CtPl","number":4147,"title":"Adjust path to datasets tutorial in How-To","user":{"login":"NimaBoscarino","id":6765188,"node_id":"MDQ6VXNlcjY3NjUxODg=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/6765188?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/NimaBoscarino","html_url":"https:\/\/github.com\/NimaBoscarino","followers_url":"https:\/\/api.github.com\/users\/NimaBoscarino\/followers","following_url":"https:\/\/api.github.com\/users\/NimaBoscarino\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/NimaBoscarino\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/NimaBoscarino\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/NimaBoscarino\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/NimaBoscarino\/orgs","repos_url":"https:\/\/api.github.com\/users\/NimaBoscarino\/repos","events_url":"https:\/\/api.github.com\/users\/NimaBoscarino\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/NimaBoscarino\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649726434000,"updated_at":1649752344000,"closed_at":1649751962000,"author_association":"MEMBER","active_lock_reason":null,"body":"The link in the How-To overview page to the Datasets tutorials is currently broken. This is just a small adjustment to make it match the format used in https:\/\/github.com\/huggingface\/datasets\/blob\/master\/docs\/source\/tutorial.md.\r\n\r\n(Edit to add: The link in the PR deployment (https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4147\/en\/how_to) is also broken since it's actually hardcoded to `master` and not dynamic to the branch name, but other links seem to behave similarly.)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4147\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4147\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4147","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4147","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4147.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4147.patch","merged_at":1649751962000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4146","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4146\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4146\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4146\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4146","id":1200215789,"node_id":"I_kwDODunzps5Hidbt","number":4146,"title":"SAMSum dataset viewer not working","user":{"login":"aakashnegi10","id":39906333,"node_id":"MDQ6VXNlcjM5OTA2MzMz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/39906333?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/aakashnegi10","html_url":"https:\/\/github.com\/aakashnegi10","followers_url":"https:\/\/api.github.com\/users\/aakashnegi10\/followers","following_url":"https:\/\/api.github.com\/users\/aakashnegi10\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/aakashnegi10\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/aakashnegi10\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/aakashnegi10\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/aakashnegi10\/orgs","repos_url":"https:\/\/api.github.com\/users\/aakashnegi10\/repos","events_url":"https:\/\/api.github.com\/users\/aakashnegi10\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/aakashnegi10\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["https:\/\/huggingface.co\/datasets\/samsum\r\n\r\n```\r\nStatus code:   400\r\nException:     ValueError\r\nMessage:       Cannot seek streaming HTTP file\r\n```","Currently, only the datasets that can be streamed support the dataset viewer. Maybe @lhoestq @albertvillanova or @mariosasko could give more details about why the dataset cannot be streamed.","It looks like the host (https:\/\/arxiv.org) doesn't allow HTTP Range requests, which is what we use to stream data.\r\n\r\nThis can be fix if we host the data ourselves, which is ok since the dataset is under CC BY-NC-ND 4.0"],"created_at":1649694177000,"updated_at":1651249569000,"closed_at":1651249569000,"author_association":"NONE","active_lock_reason":null,"body":"## Dataset viewer issue for '*name of the dataset*'\r\n\r\n**Link:** *link to the dataset viewer page*\r\n\r\n*short description of the issue*\r\n\r\nAm I the one who added this dataset ? Yes-No\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4146\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4146\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4145","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4145\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4145\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4145\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4145","id":1200209781,"node_id":"PR_kwDODunzps42A6Rt","number":4145,"title":"Redirect TIMIT download from LDC","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["CI is failing because some tags are outdated, but they're fixed in #4067 ","_The documentation is not available anymore as the PR was closed or merged._","We may do a release pretty soon (today ?), let me know if it's fine to include it in the new release","Fine to include this change!"],"created_at":1649693875000,"updated_at":1649864371000,"closed_at":1649863984000,"author_association":"MEMBER","active_lock_reason":null,"body":"LDC data is protected under US copyright laws and under various legal agreements between the Linguistic Data Consortium\/the University of Pennsylvania and data providers which prohibit redistribution of that data by anyone other than LDC. Similarly, LDC's membership agreements, non-member user agreement and various corpus-specific license agreements specifically state that users cannot publish, retransmit, disclose, copy, reproduce or redistribute LDC databases to others outside their organizations.\r\n\r\nLDC explicitly asked us to remove the download script for the TIMIT dataset. In this PR I remove all means to download the dataset, and redirect users to download the data from https:\/\/catalog.ldc.upenn.edu\/LDC93S1 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4145\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4145\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4145","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4145","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4145.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4145.patch","merged_at":1649863983000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4144","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4144\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4144\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4144\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4144","id":1200016983,"node_id":"PR_kwDODunzps42ARmu","number":4144,"title":"Fix splits in local packaged modules, local datasets without script and hub datasets without script","user":{"login":"polinaeterna","id":16348744,"node_id":"MDQ6VXNlcjE2MzQ4NzQ0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/16348744?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/polinaeterna","html_url":"https:\/\/github.com\/polinaeterna","followers_url":"https:\/\/api.github.com\/users\/polinaeterna\/followers","following_url":"https:\/\/api.github.com\/users\/polinaeterna\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/polinaeterna\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/polinaeterna\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/polinaeterna\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/polinaeterna\/orgs","repos_url":"https:\/\/api.github.com\/users\/polinaeterna\/repos","events_url":"https:\/\/api.github.com\/users\/polinaeterna\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/polinaeterna\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","Thanks !\r\nI'm in favor of this change, even though it's a breaking change:\r\n\r\nif you had a dataset\r\n```\r\ndata\/\r\n  train.csv\r\n  test.csv\r\n```\r\n\r\nthen running this code would now return both train and test splits:\r\n```python\r\nload_dataset(\"csv\", data_dir=\"data\/\")\r\n```\r\nwhereas right now it returns only a train split with the data from both CSV files.\r\n\r\nIn my opinion it's ok do do this breaking change because:\r\n- it makes this behavior consistent with `load_dataset(\"path\/to\/data\")` that also returns both splits: data_files resolution must be the same\r\n- I don't expect too many affected users (unless people really wanted to group train and test images in the train split on purpose ?) compared to the many new users to come (especially with #4069 )\r\n- this usage will become more and more common as we add packaged builder and imagefolder\/audiofolder usage grows, so it may be better to do this change early\r\n\r\nLet me know if you think this is acceptable @mariosasko @albertvillanova or not, and if you think we need to first have a warning for some time before switching to this new behavior","Also, if people really want to put train and test, say, images in a single train split they could do \r\n`load_dataset(\"imagefolder\", data_files={\"train\": \"\/path\/to\/data\/**})`. Probably (arguably :)), if this is a more counterintuitive case, then it should require manual files specification, not a default one (in which we expect that users do want to infer splits from filenames \/ dir structure but currently they have to pass smth like `{\"train\": \"\/path\/to\/data\/train*\", \"test\": \"\/path\/to\/data\/test*\"}` explicitly as `data_files`)  ","I also like this change, and  I don't think we even need a warning during the transition period, considering I've been asked several times since the release of `imagefolder` why splits are not correctly inferred if the directory structure is as follows:\r\n```\r\ndata_dir\r\n    train\r\n        label_a\r\n            0.jpg\r\n            ...\r\n        label_b \r\n            0.jpg\r\n            ...\r\n    test\r\n        label_a\r\n            0.jpg\r\n            ...\r\n        label_b \r\n            0.jpg\r\n            ...\r\n```","Cool ! Feel free to add a test (maybe something similar to `test_PackagedDatasetModuleFactory_with_data_dir` but with a data_dir that contains several splits) and mark this PR as ready for review then @polinaeterna :)","@lhoestq @mariosasko do you think it's a good idea to do the same with `HubDatasetModuleFactoryWithoutScript` and `LocalDatasetModuleFactoryWithoutScript` (see the latest change). If we agree on the current change, doing \r\n```python\r\nds = load_dataset(\"polinaeterna\/jsonl_test\", data_dir=\"data\/\")\r\n```\r\non dataset with the following structure:\r\n```\r\ntrain.jsonl\r\ntest.jsonl\r\ndata\/\r\n   train.jsonl\r\n   test.jsonl\r\n```\r\nwill result in having two splits from files under `data\/` dir in specified repo, while master version returns a single train split. \r\nThe same would be for local dataset without script if doing smth like:\r\n```python\r\nds = load_dataset(\"\/home\/polina\/workspace\/repos\/jsonl_test\", data_dir=\"\/home\/polina\/workspace\/repos\/jsonl_test\/data\")\r\n```\r\n(though I'm not sure I understand this use case :D)\r\nLet me know if you think we should preserve the same logic for all factories or if I should roll back this change.","@lhoestq to test passing subdirectory (`base_path`) to data_files functions and methods, I extended the temporary test directory with data so that it contains subdirectory. Because of that the number of files in this directory increased, so I had to change some numbers and patterns to account for this change - [907ddf0](https:\/\/github.com\/huggingface\/datasets\/pull\/4144\/commits\/907ddf09d3afece5afbae18675c859d6e453f2bf)\r\n\r\nDo you think it's ok? Another option is to create another tmp dir and do all the checks inside it. "],"created_at":1649685453000,"updated_at":1651223534000,"closed_at":1651179765000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"fixes #4150\r\n\r\nI suggest to infer splits structure from files when `data_dir` is passed with `get_patterns_locally`, analogous to what's done in `LocalDatasetModuleFactoryWithoutScript` with `self.path`, instead of generating files with `data_dir\/**` patterns and putting them all into a single default (train) split.\r\n\r\nI would also suggest to align `HubDatasetModuleFactoryWithoutScript` and `LocalDatasetModuleFactoryWithoutScript` with this logic (remove `data_files = os.path.join(data_dir, \"**\")`). It's not reflected in the current code now as I'd like to discuss it cause I might be unaware of some use cases. @lhoestq @mariosasko @albertvillanova  WDYT?","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4144\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4144\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4144","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4144","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4144.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4144.patch","merged_at":1651179764000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4143","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4143\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4143\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4143\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4143","id":1199937961,"node_id":"I_kwDODunzps5HhZmp","number":4143,"title":"Unable to download `Wikepedia` 20220301.en version","user":{"login":"beyondguo","id":37113676,"node_id":"MDQ6VXNlcjM3MTEzNjc2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/37113676?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/beyondguo","html_url":"https:\/\/github.com\/beyondguo","followers_url":"https:\/\/api.github.com\/users\/beyondguo\/followers","following_url":"https:\/\/api.github.com\/users\/beyondguo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/beyondguo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/beyondguo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/beyondguo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/beyondguo\/orgs","repos_url":"https:\/\/api.github.com\/users\/beyondguo\/repos","events_url":"https:\/\/api.github.com\/users\/beyondguo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/beyondguo\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi! We've recently updated the Wikipedia script, so these changes are only available on master and can be fetched as follows:\r\n```python\r\ndataset_wikipedia = load_dataset(\"wikipedia\", \"20220301.en\", revision=\"master\")\r\n```","Hi, how can I load the previous \"20200501.en\" version of wikipedia which had been downloaded to the default path? Thanks!"],"created_at":1649682014000,"updated_at":1650967453000,"closed_at":1650560654000,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\n\r\nUnable to download `Wikepedia` dataset, 20220301.en version\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n!pip install apache_beam mwparserfromhell\r\ndataset_wikipedia = load_dataset(\"wikipedia\", \"20220301.en\")\r\n```\r\n\r\n## Actual results\r\n```\r\nValueError: BuilderConfig 20220301.en not found. \r\nAvailable: ['20200501.aa', '20200501.ab', '20200501.ace', '20200501.ady', '20200501.af', '20200501.ak', '20200501.als', '20200501.am', '20200501.an', '20200501.ang', '20200501.ar', '20200501.arc', '20200501.arz', '20200501.as', '20200501.ast', '20200501.atj', '20200501.av', '20200501.ay', '20200501.az', '20200501.azb', '20200501.ba', '20200501.bar', '20200501.bat-smg', '20200501.bcl', '20200501.be', '20200501.be-x-old', '20200501.bg', '20200501.bh', '20200501.bi', '20200501.bjn', '20200501.bm', '20200501.bn', '20200501.bo', '20200501.bpy', '20200501.br', '20200501.bs', '20200501.bug', '20200501.bxr', '20200501.ca', '20200501.cbk-zam', '20200501.cdo', '20200501.ce', '20200501.ceb', '20200501.ch', '20200501.cho', '20200501.chr', '20200501.chy', '20200501.ckb', '20200501.co', '20200501.cr', '20200501.crh', '20200501.cs', '20200501.csb', '20200501.cu', '20200501.cv', '20200501.cy', '20200501.da', '20200501.de', '20200501.din', '20200501.diq', '20200501.dsb', '20200501.dty', '20200501.dv', '20200501.dz', '20200501.ee', '20200501.el', '20200501.eml', '20200501.en', '20200501.eo', '20200501.es', '20200501.et', '20200501.eu', '20200501.ext', '20200501.fa', '20200501.ff', '20200501.fi', '20200501.fiu-vro', '20200501.fj', '20200501.fo', '20200501.fr', '20200501.frp', '20200501.frr', '20200501.fur', '20200501.fy', '20200501.ga', '20200501.gag', '20200501.gan', '20200501.gd', '20200501.gl', '20200501.glk', '20200501.gn', '20200501.gom', '20200501.gor', '20200501.got', '20200501.gu', '20200501.gv', '20200501.ha', '20200501.hak', '20200501.haw', '20200501.he', '20200501.hi', '20200501.hif', '20200501.ho', '20200501.hr', '20200501.hsb', '20200501.ht', '20200501.hu', '20200501.hy', '20200501.ia', '20200501.id', '20200501.ie', '20200501.ig', '20200501.ii', '20200501.ik', '20200501.ilo', '20200501.inh', '20200501.io', '20200501.is', '20200501.it', '20200501.iu', '20200501.ja', '20200501.jam', '20200501.jbo', '20200501.jv', '20200501.ka', '20200501.kaa', '20200501.kab', '20200501.kbd', '20200501.kbp', '20200501.kg', '20200501.ki', '20200501.kj', '20200501.kk', '20200501.kl', '20200501.km', '20200501.kn', '20200501.ko', '20200501.koi', '20200501.krc', '20200501.ks', '20200501.ksh', '20200501.ku', '20200501.kv', '20200501.kw', '20200501.ky', '20200501.la', '20200501.lad', '20200501.lb', '20200501.lbe', '20200501.lez', '20200501.lfn', '20200501.lg', '20200501.li', '20200501.lij', '20200501.lmo', '20200501.ln', '20200501.lo', '20200501.lrc', '20200501.lt', '20200501.ltg', '20200501.lv', '20200501.mai', '20200501.map-bms', '20200501.mdf', '20200501.mg', '20200501.mh', '20200501.mhr', '20200501.mi', '20200501.min', '20200501.mk', '20200501.ml', '20200501.mn', '20200501.mr', '20200501.mrj', '20200501.ms', '20200501.mt', '20200501.mus', '20200501.mwl', '20200501.my', '20200501.myv', '20200501.mzn', '20200501.na', '20200501.nah', '20200501.nap', '20200501.nds', '20200501.nds-nl', '20200501.ne', '20200501.new', '20200501.ng', '20200501.nl', '20200501.nn', '20200501.no', '20200501.nov', '20200501.nrm', '20200501.nso', '20200501.nv', '20200501.ny', '20200501.oc', '20200501.olo', '20200501.om', '20200501.or', '20200501.os', '20200501.pa', '20200501.pag', '20200501.pam', '20200501.pap', '20200501.pcd', '20200501.pdc', '20200501.pfl', '20200501.pi', '20200501.pih', '20200501.pl', '20200501.pms', '20200501.pnb', '20200501.pnt', '20200501.ps', '20200501.pt', '20200501.qu', '20200501.rm', '20200501.rmy', '20200501.rn', '20200501.ro', '20200501.roa-rup', '20200501.roa-tara', '20200501.ru', '20200501.rue', '20200501.rw', '20200501.sa', '20200501.sah', '20200501.sat', '20200501.sc', '20200501.scn', '20200501.sco', '20200501.sd', '20200501.se', '20200501.sg', '20200501.sh', '20200501.si', '20200501.simple', '20200501.sk', '20200501.sl', '20200501.sm', '20200501.sn', '20200501.so', '20200501.sq', '20200501.sr', '20200501.srn', '20200501.ss', '20200501.st', '20200501.stq', '20200501.su', '20200501.sv', '20200501.sw', '20200501.szl', '20200501.ta', '20200501.tcy', '20200501.te', '20200501.tet', '20200501.tg', '20200501.th', '20200501.ti', '20200501.tk', '20200501.tl', '20200501.tn', '20200501.to', '20200501.tpi', '20200501.tr', '20200501.ts', '20200501.tt', '20200501.tum', '20200501.tw', '20200501.ty', '20200501.tyv', '20200501.udm', '20200501.ug', '20200501.uk', '20200501.ur', '20200501.uz', '20200501.ve', '20200501.vec', '20200501.vep', '20200501.vi', '20200501.vls', '20200501.vo', '20200501.wa', '20200501.war', '20200501.wo', '20200501.wuu', '20200501.xal', '20200501.xh', '20200501.xmf', '20200501.yi', '20200501.yo', '20200501.za', '20200501.zea', '20200501.zh', '20200501.zh-classical', '20200501.zh-min-nan', '20200501.zh-yue', '20200501.zu']\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0\r\n- Platform: Ubuntu\r\n- Python version: 3.6\r\n- PyArrow version: 6.0.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4143\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4143\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4142","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4142\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4142\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4142\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4142","id":1199794750,"node_id":"I_kwDODunzps5Hg2o-","number":4142,"title":"Add ObjectFolder 2.0 dataset","user":{"login":"osanseviero","id":7246357,"node_id":"MDQ6VXNlcjcyNDYzNTc=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/7246357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/osanseviero","html_url":"https:\/\/github.com\/osanseviero","followers_url":"https:\/\/api.github.com\/users\/osanseviero\/followers","following_url":"https:\/\/api.github.com\/users\/osanseviero\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/osanseviero\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/osanseviero\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/osanseviero\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/osanseviero\/orgs","repos_url":"https:\/\/api.github.com\/users\/osanseviero\/repos","events_url":"https:\/\/api.github.com\/users\/osanseviero\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/osanseviero\/received_events","type":"User","site_admin":false},"labels":[{"id":2067376369,"node_id":"MDU6TGFiZWwyMDY3Mzc2MzY5","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20request","name":"dataset request","color":"e99695","default":false,"description":"Requesting to add a new dataset"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1649674671000,"updated_at":1649674671000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"## Adding a Dataset\r\n- **Name:** ObjectFolder 2.0\r\n- **Description:** ObjectFolder 2.0 is a dataset of 1,000 objects in the form of implicit representations. It contains 1,000 Object Files each containing the complete multisensory profile for an object instance.\r\n- **Paper:** [*link to the dataset paper if available*](https:\/\/arxiv.org\/abs\/2204.02389)\r\n- **Data:** https:\/\/github.com\/rhgao\/ObjectFolder\r\n\r\nInstructions to add a new dataset can be found [here](https:\/\/github.com\/huggingface\/datasets\/blob\/master\/ADD_NEW_DATASET.md).\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4142\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4142\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4141","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4141\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4141\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4141\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4141","id":1199610885,"node_id":"I_kwDODunzps5HgJwF","number":4141,"title":"Why is the dataset not visible under the dataset preview section?","user":{"login":"Nid989","id":75028682,"node_id":"MDQ6VXNlcjc1MDI4Njgy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/75028682?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Nid989","html_url":"https:\/\/github.com\/Nid989","followers_url":"https:\/\/api.github.com\/users\/Nid989\/followers","following_url":"https:\/\/api.github.com\/users\/Nid989\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Nid989\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Nid989\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Nid989\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Nid989\/orgs","repos_url":"https:\/\/api.github.com\/users\/Nid989\/repos","events_url":"https:\/\/api.github.com\/users\/Nid989\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Nid989\/received_events","type":"User","site_admin":false},"labels":[{"id":3470211881,"node_id":"LA_kwDODunzps7O1zsp","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer","name":"dataset-viewer","color":"E5583E","default":false,"description":"Related to the dataset viewer on huggingface.co"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1649666202000,"updated_at":1649703332000,"closed_at":1649696989000,"author_association":"NONE","active_lock_reason":null,"body":"## Dataset viewer issue for '*name of the dataset*'\r\n\r\n**Link:** *link to the dataset viewer page*\r\n\r\n*short description of the issue*\r\n\r\nAm I the one who added this dataset ? Yes-No\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4141\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4141\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4140","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4140\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4140\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4140\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4140","id":1199492356,"node_id":"I_kwDODunzps5Hfs0E","number":4140,"title":"Error loading arxiv data set","user":{"login":"yjqiu","id":5383918,"node_id":"MDQ6VXNlcjUzODM5MTg=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/5383918?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yjqiu","html_url":"https:\/\/github.com\/yjqiu","followers_url":"https:\/\/api.github.com\/users\/yjqiu\/followers","following_url":"https:\/\/api.github.com\/users\/yjqiu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yjqiu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yjqiu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yjqiu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yjqiu\/orgs","repos_url":"https:\/\/api.github.com\/users\/yjqiu\/repos","events_url":"https:\/\/api.github.com\/users\/yjqiu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yjqiu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi! I think this error may be related to using an older version of the library. I was able to load the dataset without any issues using the latest version of `datasets`.  Can you upgrade to the latest version of `datasets` and try again? :)","Hi! As @stevhliu suggested, to fix the issue, update the lib to the newest version with:\r\n```\r\npip install -U datasets\r\n```\r\nand download the dataset as follows:\r\n```python\r\nfrom datasets import load_dataset\r\ndset =  load_dataset('scientific_papers', 'arxiv', download_mode=\"force_redownload\")\r\n```","Thanks for the quick response! It works now. The problem is that I used nlp. load_dataset instead of datasets. load_dataset."],"created_at":1649660794000,"updated_at":1649780648000,"closed_at":1649780648000,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\nA clear and concise description of what the bug is.\r\n\r\nI met the error below when loading arxiv dataset via `nlp.load_dataset('scientific_papers', 'arxiv',)`. \r\n```\r\nTraceback (most recent call last):\r\n  File \"scripts\/summarization.py\", line 354, in <module>\r\n    main(args)\r\n  File \"scripts\/summarization.py\", line 306, in main\r\n    model.hf_datasets = nlp.load_dataset('scientific_papers', 'arxiv')\r\n  File \"\/opt\/conda\/envs\/longformer\/lib\/python3.7\/site-packages\/nlp\/load.py\", line 549, in load_dataset\r\n    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,\r\n  File \"\/opt\/conda\/envs\/longformer\/lib\/python3.7\/site-packages\/nlp\/builder.py\", line 463, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"\/opt\/conda\/envs\/longformer\/lib\/python3.7\/site-packages\/nlp\/builder.py\", line 522, in _download_and_prepare\r\n    self.info.download_checksums, dl_manager.get_recorded_sizes_checksums(), \"dataset source files\"\r\n  File \"\/opt\/conda\/envs\/longformer\/lib\/python3.7\/site-packages\/nlp\/utils\/info_utils.py\", line 38, in verify_checksums\r\n    raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\nnlp.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https:\/\/drive.google.com\/uc?id=1b3rmCSIoh6VhD4HKWjI4HOW-cSwcwbeC&export=download', 'https:\/\/drive.google.com\/uc?id=1lvsqvsFi3W-pE1SqNZI0s8NR9rC1tsja&export=download']\r\n```\r\n\r\nI then tried to ignore verification steps by `ignore_verifications=True` and there is another error.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/opt\/conda\/envs\/longformer\/lib\/python3.7\/site-packages\/nlp\/builder.py\", line 537, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"\/opt\/conda\/envs\/longformer\/lib\/python3.7\/site-packages\/nlp\/builder.py\", line 810, in _prepare_split\r\n    for key, record in utils.tqdm(generator, unit=\" examples\", total=split_info.num_examples, leave=False):\r\n  File \"\/opt\/conda\/envs\/longformer\/lib\/python3.7\/site-packages\/tqdm\/std.py\", line 1195, in __iter__\r\n    for obj in iterable:\r\n  File \"\/opt\/conda\/envs\/longformer\/lib\/python3.7\/site-packages\/nlp\/datasets\/scientific_papers\/9e4f2cfe3d8494e9f34a84ce49c3214605b4b52a3d8eb199104430d04c52cc12\/scientific_papers.py\", line 108, in _generate_examples\r\n    with open(path, encoding=\"utf-8\") as f:\r\nNotADirectoryError: [Errno 20] Not a directory: '\/home\/username\/.cache\/huggingface\/datasets\/downloads\/c0deae7af7d9c87f25dfadf621f7126f708d7dcac6d353c7564883084a000076\/arxiv-dataset\/train.txt'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"scripts\/summarization.py\", line 354, in <module>\r\n    main(args)\r\n  File \"scripts\/summarization.py\", line 306, in main\r\n    model.hf_datasets = nlp.load_dataset('scientific_papers', 'arxiv', ignore_verifications=True)\r\n  File \"\/opt\/conda\/envs\/longformer\/lib\/python3.7\/site-packages\/nlp\/load.py\", line 549, in load_dataset\r\n    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,\r\n  File \"\/opt\/conda\/envs\/longformer\/lib\/python3.7\/site-packages\/nlp\/builder.py\", line 463, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"\/opt\/conda\/envs\/longformer\/lib\/python3.7\/site-packages\/nlp\/builder.py\", line 539, in _download_and_prepare\r\n    raise OSError(\"Cannot find data file. \" + (self.manual_download_instructions or \"\"))\r\nOSError: Cannot find data file.\r\n```\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:\r\n- Platform:\r\n- Python version:\r\n- PyArrow version:\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4140\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4140\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4139","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4139\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4139\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4139\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4139","id":1199443822,"node_id":"I_kwDODunzps5Hfg9u","number":4139,"title":"Dataset viewer issue for Winoground","user":{"login":"alcinos","id":7438704,"node_id":"MDQ6VXNlcjc0Mzg3MDQ=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/7438704?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alcinos","html_url":"https:\/\/github.com\/alcinos","followers_url":"https:\/\/api.github.com\/users\/alcinos\/followers","following_url":"https:\/\/api.github.com\/users\/alcinos\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alcinos\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alcinos\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alcinos\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alcinos\/orgs","repos_url":"https:\/\/api.github.com\/users\/alcinos\/repos","events_url":"https:\/\/api.github.com\/users\/alcinos\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alcinos\/received_events","type":"User","site_admin":false},"labels":[{"id":3470211881,"node_id":"LA_kwDODunzps7O1zsp","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer","name":"dataset-viewer","color":"E5583E","default":false,"description":"Related to the dataset viewer on huggingface.co"},{"id":4030248571,"node_id":"LA_kwDODunzps7wOLZ7","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer-gated","name":"dataset-viewer-gated","color":"51F745","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},"assignees":[{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},{"login":"SBrandeis","id":33657802,"node_id":"MDQ6VXNlcjMzNjU3ODAy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/33657802?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/SBrandeis","html_url":"https:\/\/github.com\/SBrandeis","followers_url":"https:\/\/api.github.com\/users\/SBrandeis\/followers","following_url":"https:\/\/api.github.com\/users\/SBrandeis\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/SBrandeis\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/SBrandeis\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/SBrandeis\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/SBrandeis\/orgs","repos_url":"https:\/\/api.github.com\/users\/SBrandeis\/repos","events_url":"https:\/\/api.github.com\/users\/SBrandeis\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/SBrandeis\/received_events","type":"User","site_admin":false},{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["related (same dataset): https:\/\/github.com\/huggingface\/datasets\/issues\/4149. But the issue is different. Looking at it","I thought this issue was related to the error I was seeing, but upon consideration I'd think the dataset viewer would return a 500 (unable to create the split like me) or a 404 (unable to load split b\/c it was never created) error if it was having the issue I was seeing in #4149. 401 message makes it look like dataset viewer isn't passing through the identity of the user who has signed the licensing agreement when making the request to GET [examples.jsonl](https:\/\/huggingface.co\/datasets\/facebook\/winoground\/resolve\/a86a60456fbbd242e9a744199071a6bd3e7fd9de\/examples.jsonl).","Pinging @SBrandeis, as it seems related to gated datasets and access tokens.","To replicate:\r\n\r\n```python\r\n>>> import datasets\r\n>>> dataset= datasets.load_dataset('facebook\/winoground', name='facebook--winoground', split='train', use_auth_token=\"hf_app_...\", streaming=True)\r\n>>> next(iter(dataset))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/iterable_dataset.py\", line 497, in __iter__\r\n    for key, example in self._iter():\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/iterable_dataset.py\", line 494, in _iter\r\n    yield from ex_iterable\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/iterable_dataset.py\", line 87, in __iter__\r\n    yield from self.generate_examples_fn(**self.kwargs)\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/iterable_dataset.py\", line 439, in wrapper\r\n    for key, table in generate_tables_fn(**kwargs):\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/packaged_modules\/json\/json.py\", line 85, in _generate_tables\r\n    for file_idx, file in enumerate(files):\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/utils\/streaming_download_manager.py\", line 679, in __iter__\r\n    yield from self.generator(*self.args, **self.kwargs)\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/utils\/streaming_download_manager.py\", line 731, in _iter_from_urlpaths\r\n    for dirpath, _, filenames in xwalk(urlpath, use_auth_token=use_auth_token):\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/utils\/streaming_download_manager.py\", line 623, in xwalk\r\n    for dirpath, dirnames, filenames in fs.walk(main_hop):\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/fsspec\/spec.py\", line 372, in walk\r\n    listing = self.ls(path, detail=True, **kwargs)\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/fsspec\/asyn.py\", line 85, in wrapper\r\n    return sync(self.loop, func, *args, **kwargs)\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/fsspec\/asyn.py\", line 65, in sync\r\n    raise return_result\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/fsspec\/asyn.py\", line 25, in _runner\r\n    result[0] = await coro\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/fsspec\/implementations\/http.py\", line 196, in _ls\r\n    out = await self._ls_real(url, detail=detail, **kwargs)\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/fsspec\/implementations\/http.py\", line 150, in _ls_real\r\n    self._raise_not_found_for_status(r, url)\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/fsspec\/implementations\/http.py\", line 208, in _raise_not_found_for_status\r\n    response.raise_for_status()\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/aiohttp\/client_reqrep.py\", line 1004, in raise_for_status\r\n    raise ClientResponseError(\r\naiohttp.client_exceptions.ClientResponseError: 401, message='Unauthorized', url=URL('https:\/\/huggingface.co\/datasets\/facebook\/winoground\/resolve\/a86a60456fbbd242e9a744199071a6bd3e7fd9de\/examples.jsonl')\r\n```\r\n\r\n*edited to fix `use_token` -> `use_auth_token`, thx @odellus*","~~Using your command to replicate and changing `use_token` to `use_auth_token` fixes the problem I was seeing in #4149.~~\r\nNevermind it gave me an iterator to a method returning the same 401s. Changing `use_token` to `use_auth_token` does not fix the issue.","After investigation with @severo , we found a potential culprit: https:\/\/github.com\/huggingface\/datasets\/blob\/3cd0a009a43f9f174056d70bfa2ca32216181926\/src\/datasets\/utils\/streaming_download_manager.py#L610-L624\r\n\r\nThe streaming manager does not seem to pass `use_auth_token` to `fsspec` when streaming and not iterating content of a zip archive\r\n\r\ncc @albertvillanova @lhoestq ","I was able to reproduce it on a private dataset, let me work on a fix","Hey @lhoestq, Thanks for working on a fix! Any plans to merge #4173 into master? ","Thanks for the heads up, I still need to fix some tests that are failing in the CI before merging ;)","The fix has been merged, we'll do a new release soon, and update the dataset viewer"],"created_at":1649657501000,"updated_at":1651830191000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"## Dataset viewer issue for 'Winoground'\r\n\r\n**Link:** [*link to the dataset viewer page*](https:\/\/huggingface.co\/datasets\/facebook\/winoground\/viewer\/facebook--winoground\/train)\r\n\r\n*short description of the issue*\r\nGetting 401, message='Unauthorized'\r\nThe dataset is subject to authorization, but I can access the files from the interface, so I assume I'm granted to access it. I'd assume the permission somehow doesn't propagate to the dataset viewer tool.\r\n\r\nAm I the one who added this dataset ? No\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4139\/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4139\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4138","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4138\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4138\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4138\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4138","id":1199291730,"node_id":"I_kwDODunzps5He71S","number":4138,"title":"Incorrect Russian filenames encoding after extraction by datasets.DownloadManager.download_and_extract()","user":{"login":"MalakhovIlyaPavlovich","id":55381086,"node_id":"MDQ6VXNlcjU1MzgxMDg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/55381086?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/MalakhovIlyaPavlovich","html_url":"https:\/\/github.com\/MalakhovIlyaPavlovich","followers_url":"https:\/\/api.github.com\/users\/MalakhovIlyaPavlovich\/followers","following_url":"https:\/\/api.github.com\/users\/MalakhovIlyaPavlovich\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/MalakhovIlyaPavlovich\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/MalakhovIlyaPavlovich\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/MalakhovIlyaPavlovich\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/MalakhovIlyaPavlovich\/orgs","repos_url":"https:\/\/api.github.com\/users\/MalakhovIlyaPavlovich\/repos","events_url":"https:\/\/api.github.com\/users\/MalakhovIlyaPavlovich\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/MalakhovIlyaPavlovich\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["To reproduce:\r\n\r\n```python\r\n>>> import datasets\r\n>>> datasets.get_dataset_split_names('MalakhovIlya\/RuREBus', config_name='raw_txt')\r\nTraceback (most recent call last):\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/inspect.py\", line 280, in get_dataset_config_info\r\n    for split_generator in builder._split_generators(\r\n  File \"\/home\/slesage\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/MalakhovIlya--RuREBus\/21046f5f1a0cf91187d68c30918d78d934ec7113ec435e146776d4f28f12c4ed\/RuREBus.py\", line 101, in _split_generators\r\n    decode_file_names(folder)\r\n  File \"\/home\/slesage\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/MalakhovIlya--RuREBus\/21046f5f1a0cf91187d68c30918d78d934ec7113ec435e146776d4f28f12c4ed\/RuREBus.py\", line 26, in decode_file_names\r\n    for root, dirs, files in os.walk(folder, topdown=False):\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/streaming.py\", line 66, in wrapper\r\n    return function(*args, use_auth_token=use_auth_token, **kwargs)\r\nTypeError: xwalk() got an unexpected keyword argument 'topdown'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/inspect.py\", line 323, in get_dataset_split_names\r\n    info = get_dataset_config_info(\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/inspect.py\", line 285, in get_dataset_config_info\r\n    raise SplitsNotFoundError(\"The split names could not be parsed from the dataset config.\") from err\r\ndatasets.inspect.SplitsNotFoundError: The split names could not be parsed from the dataset config.\r\n```\r\n\r\nIt's not related to the dataset viewer. Maybe @albertvillanova or @lhoestq could help more on this issue.","Hi! This issue stems from the fact that `xwalk`, which is a streamable version of `os.walk`, doesn't support the `topdown` param due to `fsspec`'s `walk` also not supporting it, so fixing this issue could be tricky.  \r\n\r\n@MalakhovIlyaPavlovich You can avoid the error by tweaking your data processing and not using this param. (and `Path.rename`, which also cannot be streamed) ","@mariosasko thank you for your reply. I couldn't reproduce error showed by @severo either on Ubuntu 20.04.3 LTS, Windows 10 and Google Colab environments. But trying to avoid using os.walk(topdown=False) and Path.rename(), In _split_generators I replaced\r\n```\r\ndef decode_file_names(folder):\r\n    for root, dirs, files in os.walk(folder, topdown=False):\r\n        root = Path(root)\r\n        for file in files:\r\n            old_name = root \/ Path(file)\r\n            new_name = root \/ Path(\r\n                file.encode('cp437').decode('cp866'))\r\n            old_name.rename(new_name)\r\n        for dir in dirs:\r\n            old_name = root \/ Path(dir)\r\n            new_name = root \/ Path(dir.encode('cp437').decode('cp866'))\r\n            old_name.rename(new_name)\r\n\r\nfolder = dl_manager.download_and_extract(self._RAW_TXT_URLS)['raw_txt']\r\ndecode_file_names(folder)\r\n```\r\nby\r\n```\r\ndef extract(zip_file_path):\r\n    p = Path(zip_file_path)\r\n    dest_dir = str(p.parent \/ 'extracted' \/ p.stem)\r\n    os.makedirs(dest_dir, exist_ok=True)\r\n    with zipfile.ZipFile(zip_file_path) as archive:\r\n        for file_info in tqdm(archive.infolist(), desc='Extracting'):\r\n            filename = file_info.filename.encode('cp437').decode('cp866')\r\n            target = os.path.join(dest_dir, *filename.split('\/'))\r\n            os.makedirs(os.path.dirname(target), exist_ok=True)\r\n            if not file_info.is_dir():\r\n                with archive.open(file_info) as source, open(target, 'wb') as dest:\r\n                    shutil.copyfileobj(source, dest)\r\n    return dest_dir\r\n\r\nzip_file = dl_manager.download(self._RAW_TXT_URLS)['raw_txt']\r\nif not is_url(zip_file):\r\n    folder = extract(zip_file)\r\nelse:\r\n    folder = None\r\n```\r\nand now everything works well except data viewer for \"raw_txt\" subset: dataset preview on hub shows \"No data.\". As far as I understand dl_manager.download returns original URL when we are calling datasets.get_dataset_split_names and my suspicions are that dataset viewer can do smth similar. I couldn't find information about how it works. I would be very grateful, if you could tell me how to fix this)","This is what I get when I try to stream the `raw_txt` subset:\r\n```python\r\n>>> dset = load_dataset(\"MalakhovIlya\/RuREBus\", \"raw_txt\", split=\"raw_txt\", streaming=True)\r\n>>> next(iter(dset))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nStopIteration\r\n```\r\nSo there is a bug in your script.","streaming=True helped me to find solution. I fixed\r\n```\r\ndef extract(zip_file_path):\r\n    p = Path(zip_file_path)\r\n    dest_dir = str(p.parent \/ 'extracted' \/ p.stem)\r\n    os.makedirs(dest_dir, exist_ok=True)\r\n    with zipfile.ZipFile(zip_file_path) as archive:\r\n        for file_info in tqdm(archive.infolist(), desc='Extracting'):\r\n            filename = file_info.filename.encode('cp437').decode('cp866')\r\n            target = os.path.join(dest_dir, *filename.split('\/'))\r\n            os.makedirs(os.path.dirname(target), exist_ok=True)\r\n            if not file_info.is_dir():\r\n                with archive.open(file_info) as source, open(target, 'wb') as dest:\r\n                    shutil.copyfileobj(source, dest)\r\n    return dest_dir\r\n\r\nzip_file = dl_manager.download(self._RAW_TXT_URLS)['raw_txt']\r\nfolder = extract(zip_file)\r\n```\r\nby \r\n```\r\nfolder = dl_manager.download_and_extract(self._RAW_TXT_URLS)['raw_txt']\r\npath = os.path.join(folder, 'MED_txt\/unparsed_txt')\r\nfor root, dirs, files in os.walk(path):\r\n    decoded_root_name = Path(root).name.encode('cp437').decode('cp866')\r\n```\r\n@mariosasko thank you for your help :)"],"created_at":1649642833000,"updated_at":1650338146000,"closed_at":1650123989000,"author_association":"NONE","active_lock_reason":null,"body":"## Dataset viewer issue for 'MalakhovIlya\/RuREBus'\r\n\r\n**Link:** https:\/\/huggingface.co\/datasets\/MalakhovIlya\/RuREBus\r\n\r\n**Description**\r\nUsing os.walk(topdown=False) in DatasetBuilder causes following error:\r\nStatus code:   400\r\nException:     TypeError\r\nMessage:       xwalk() got an unexpected keyword argument 'topdown'\r\nCouldn't find where \"xwalk\" come from. How can I fix this?\r\n\r\nAm I the one who added this dataset ? Yes\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4138\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4138\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4137","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4137\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4137\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4137\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4137","id":1199000453,"node_id":"PR_kwDODunzps419D6A","number":4137,"title":"Add single dataset citations for TweetEval","user":{"login":"gchhablani","id":29076344,"node_id":"MDQ6VXNlcjI5MDc2MzQ0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/29076344?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/gchhablani","html_url":"https:\/\/github.com\/gchhablani","followers_url":"https:\/\/api.github.com\/users\/gchhablani\/followers","following_url":"https:\/\/api.github.com\/users\/gchhablani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/gchhablani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/gchhablani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/gchhablani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/gchhablani\/orgs","repos_url":"https:\/\/api.github.com\/users\/gchhablani\/repos","events_url":"https:\/\/api.github.com\/users\/gchhablani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/gchhablani\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","The `test_dataset_cards` method is failing with the error:\r\n\r\n```\r\nif error_messages:\r\n>           raise ValueError(\"\\n\".join(error_messages))\r\nE           ValueError: The following issues have been found in the dataset cards:\r\nE           YAML tags:\r\nE           The following typing errors are found: {'annotations_creators': \"(Expected `typing.List` with length > 0. Found value of type: `<class 'list'>`, with length: 0.\\n)\\nOR\\n(Expected `typing.Dict` with length > 0. Found value of type: `<class 'list'>`, with length: 0.\\n)\"}\r\n```\r\n\r\nAdding `found` as annotation creators."],"created_at":1649591514000,"updated_at":1649750242000,"closed_at":1649749875000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"This PR adds single data citations as per request of the original creators of the TweetEval dataset.\r\n\r\nThis is a recent email from the creator:\r\n\r\n> Could I ask you a favor? Would you be able to add at the end of the README the citations of the single datasets as well? You can just copy our readme maybe? https:\/\/github.com\/cardiffnlp\/tweeteval#citing-tweeteval\r\n(just to be sure that the creator of the single datasets also get credits when tweeteval is used)\r\n\r\nPlease let me know if this looks okay or if any changes are needed.\r\n\r\nThanks,\r\nGunjan\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4137\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4137\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4137","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4137","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4137.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4137.patch","merged_at":1649749875000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4135","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4135\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4135\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4135\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4135","id":1198307610,"node_id":"PR_kwDODunzps416-Rn","number":4135,"title":"Support streaming xtreme dataset for PAN-X config","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649485188000,"updated_at":1651826380000,"closed_at":1649660054000,"author_association":"MEMBER","active_lock_reason":null,"body":"Support streaming xtreme dataset for PAN-X config.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4135\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4135\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4135","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4135","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4135.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4135.patch","merged_at":1649660054000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4134","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4134\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4134\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4134\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4134","id":1197937146,"node_id":"I_kwDODunzps5HZxH6","number":4134,"title":"ELI5 supporting documents","user":{"login":"Slayer-007","id":69015896,"node_id":"MDQ6VXNlcjY5MDE1ODk2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/69015896?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Slayer-007","html_url":"https:\/\/github.com\/Slayer-007","followers_url":"https:\/\/api.github.com\/users\/Slayer-007\/followers","following_url":"https:\/\/api.github.com\/users\/Slayer-007\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Slayer-007\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Slayer-007\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Slayer-007\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Slayer-007\/orgs","repos_url":"https:\/\/api.github.com\/users\/Slayer-007\/repos","events_url":"https:\/\/api.github.com\/users\/Slayer-007\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Slayer-007\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892912,"node_id":"MDU6TGFiZWwxOTM1ODkyOTEy","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/question","name":"question","color":"d876e3","default":true,"description":"Further information is requested"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi ! Please post your question on the [forum](https:\/\/discuss.huggingface.co\/), more people will be able to help you there ;)"],"created_at":1649460987000,"updated_at":1649857966000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"if i am using dense search to create supporting documents for eli5 how much time it will take bcz i read somewhere that it takes about 18 hrs??","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4134\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4134\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4133","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4133\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4133\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4133\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4133","id":1197830623,"node_id":"I_kwDODunzps5HZXHf","number":4133,"title":"HANS dataset preview broken","user":{"login":"pietrolesci","id":61748653,"node_id":"MDQ6VXNlcjYxNzQ4NjUz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/61748653?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/pietrolesci","html_url":"https:\/\/github.com\/pietrolesci","followers_url":"https:\/\/api.github.com\/users\/pietrolesci\/followers","following_url":"https:\/\/api.github.com\/users\/pietrolesci\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/pietrolesci\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/pietrolesci\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/pietrolesci\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/pietrolesci\/orgs","repos_url":"https:\/\/api.github.com\/users\/pietrolesci\/repos","events_url":"https:\/\/api.github.com\/users\/pietrolesci\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/pietrolesci\/received_events","type":"User","site_admin":false},"labels":[{"id":3287858981,"node_id":"MDU6TGFiZWwzMjg3ODU4OTgx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/streaming","name":"streaming","color":"fef2c0","default":false,"description":""}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The dataset cannot be loaded, be it in normal or streaming mode.\r\n\r\n```python\r\n>>> import datasets\r\n>>> dataset=datasets.load_dataset(\"hans\", split=\"train\", streaming=True)\r\n>>> next(iter(dataset))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/iterable_dataset.py\", line 497, in __iter__\r\n    for key, example in self._iter():\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/iterable_dataset.py\", line 494, in _iter\r\n    yield from ex_iterable\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/iterable_dataset.py\", line 87, in __iter__\r\n    yield from self.generate_examples_fn(**self.kwargs)\r\n  File \"\/home\/slesage\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/hans\/1bbcb735c482acd54f2e118074b59cfd2bf5f7a5a285d4d540d1e632216672ac\/hans.py\", line 121, in _generate_examples\r\n    for idx, line in enumerate(open(filepath, \"rb\")):\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/fsspec\/spec.py\", line 1595, in __next__\r\n    out = self.readline()\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/fsspec\/spec.py\", line 1592, in readline\r\n    return self.readuntil(b\"\\n\")\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/fsspec\/spec.py\", line 1581, in readuntil\r\n    self.seek(start + found + len(char))\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/fsspec\/implementations\/http.py\", line 676, in seek\r\n    raise ValueError(\"Cannot seek streaming HTTP file\")\r\nValueError: Cannot seek streaming HTTP file\r\n>>> dataset=datasets.load_dataset(\"hans\", split=\"train\", streaming=False)\r\nDownloading and preparing dataset hans\/plain_text (download: 29.51 MiB, generated: 30.34 MiB, post-processed: Unknown size, total: 59.85 MiB) to \/home\/slesage\/.cache\/huggingface\/datasets\/hans\/plain_text\/1.0.0\/1bbcb735c482acd54f2e118074b59cfd2bf5f7a5a285d4d540d1e632216672ac...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/load.py\", line 1687, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/builder.py\", line 605, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/builder.py\", line 1104, in _download_and_prepare\r\n    super()._download_and_prepare(dl_manager, verify_infos, check_duplicate_keys=verify_infos)\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/builder.py\", line 694, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/datasets\/builder.py\", line 1087, in _prepare_split\r\n    for key, record in logging.tqdm(\r\n  File \"\/home\/slesage\/hf\/datasets-preview-backend\/.venv\/lib\/python3.9\/site-packages\/tqdm\/std.py\", line 1180, in __iter__\r\n    for obj in iterable:\r\n  File \"\/home\/slesage\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/hans\/1bbcb735c482acd54f2e118074b59cfd2bf5f7a5a285d4d540d1e632216672ac\/hans.py\", line 121, in _generate_examples\r\n    for idx, line in enumerate(open(filepath, \"rb\")):\r\nValueError: readline of closed file\r\n```\r\n\r\n","Hi! I've opened a PR that should make this dataset stremable. You can test it as follows:\r\n```python\r\nfrom datasets import load_dataset\r\ndset = load_dataset(\"hans\", split=\"train\", streaming=True, revision=\"49decd29839c792ecc24ac88f861cbdec30c1c40\")\r\n```\r\n\r\n@severo The current script doesn't throw an error in normal mode (only in streaming mode) on my local machine or in Colab. Can you update your installation of `datasets` and see if that fixes the issue?","Thanks for this. It works well, thanks! The dataset viewer is using https:\/\/github.com\/huggingface\/datasets\/releases\/tag\/2.0.0, I'm eager to upgrade to 2.0.1 \ud83d\ude09"],"created_at":1649451975000,"updated_at":1649851054000,"closed_at":1649851054000,"author_association":"NONE","active_lock_reason":null,"body":"## Dataset viewer issue for '*hans*'\r\n\r\n**Link:** [https:\/\/huggingface.co\/datasets\/hans](https:\/\/huggingface.co\/datasets\/hans)\r\n\r\nHANS dataset preview is broken with error 400\r\n\r\nAm I the one who added this dataset ? No\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4133\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4133\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4132","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4132\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4132\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4132\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4132","id":1197661720,"node_id":"PR_kwDODunzps41460R","number":4132,"title":"Support streaming xtreme dataset for PAWS-X config","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649442332000,"updated_at":1651826382000,"closed_at":1649451764000,"author_association":"MEMBER","active_lock_reason":null,"body":"Support streaming xtreme dataset for PAWS-X config.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4132\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4132\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4132","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4132","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4132.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4132.patch","merged_at":1649451764000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4131","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4131\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4131\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4131\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4131","id":1197472249,"node_id":"PR_kwDODunzps414Zt1","number":4131,"title":"Support streaming xtreme dataset for udpos config","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649431849000,"updated_at":1651826386000,"closed_at":1649435287000,"author_association":"MEMBER","active_lock_reason":null,"body":"Support streaming xtreme dataset for udpos config.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4131\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4131\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4131","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4131","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4131.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4131.patch","merged_at":1649435287000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4130","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4130\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4130\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4130\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4130","id":1197456857,"node_id":"PR_kwDODunzps414Wqx","number":4130,"title":"Add SBU Captions Photo Dataset","user":{"login":"thomasw21","id":24695242,"node_id":"MDQ6VXNlcjI0Njk1MjQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/24695242?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/thomasw21","html_url":"https:\/\/github.com\/thomasw21","followers_url":"https:\/\/api.github.com\/users\/thomasw21\/followers","following_url":"https:\/\/api.github.com\/users\/thomasw21\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/thomasw21\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/thomasw21\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/thomasw21\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/thomasw21\/orgs","repos_url":"https:\/\/api.github.com\/users\/thomasw21\/repos","events_url":"https:\/\/api.github.com\/users\/thomasw21\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/thomasw21\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649431059000,"updated_at":1649760451000,"closed_at":1649760089000,"author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4130\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4130\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4130","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4130","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4130.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4130.patch","merged_at":1649760089000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4129","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4129\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4129\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4129\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4129","id":1197376796,"node_id":"I_kwDODunzps5HXoUc","number":4129,"title":"dataset metadata for reproducibility","user":{"login":"nbroad1881","id":24982805,"node_id":"MDQ6VXNlcjI0OTgyODA1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/24982805?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nbroad1881","html_url":"https:\/\/github.com\/nbroad1881","followers_url":"https:\/\/api.github.com\/users\/nbroad1881\/followers","following_url":"https:\/\/api.github.com\/users\/nbroad1881\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nbroad1881\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nbroad1881\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nbroad1881\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nbroad1881\/orgs","repos_url":"https:\/\/api.github.com\/users\/nbroad1881\/repos","events_url":"https:\/\/api.github.com\/users\/nbroad1881\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nbroad1881\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1649427448000,"updated_at":1649427448000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"When pulling a dataset from the hub, it would be useful to have some metadata about the specific dataset and version that is used. The metadata could then be passed to the `Trainer` which could then be saved to a model card. This is useful for people who run many experiments on different versions (commits\/branches) of the same dataset. \r\n\r\nThe dataset could have a list of \u201csource datasets\u201d metadata and ignore what happens to them before arriving in the Trainer (i.e. ignore mapping, filtering, etc.).\r\n\r\nHere is a basic representation (made by @lhoestq )\r\n```python\r\n>>> from datasets import load_dataset\r\n>>> \r\n>>> my_dataset = load_dataset(...)[\"train\"]\r\n>>> my_dataset = my_dataset.map(...)\r\n>>> \r\n>>> my_dataset.sources\r\n[HFHubDataset(repo_id=..., revision=..., arguments={...})]\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4129\/reactions","total_count":4,"+1":4,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4129\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4128","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4128\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4128\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4128\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4128","id":1197326311,"node_id":"PR_kwDODunzps4138I6","number":4128,"title":"More robust `cast_to_python_objects` in `TypedSequence`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649424815000,"updated_at":1649858861000,"closed_at":1649858476000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Adds a fallback to run an expensive version of `cast_to_python_objects` which exhaustively checks entire lists to avoid the `ArrowInvalid: Could not convert` error in `TypedSequence`. Currently, this error can happen in situations where only some images are decoded in `map`, in which case `cast_to_python_objects` fails to recognize that it needs to cast `PIL.Image` objects if they are not at the beginning of the sequence and stops after the first image dictionary (e.g., if `data` is `[{\"bytes\": None, \"path\": \"some path\"}, PIL.Image(), ...]`)\r\n\r\nFix #4124","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4128\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4128\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4128","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4128","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4128.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4128.patch","merged_at":1649858476000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4127","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4127\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4127\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4127\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4127","id":1197297756,"node_id":"PR_kwDODunzps4132EN","number":4127,"title":"Add configs with processed data in medical_dialog dataset","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649423296000,"updated_at":1651826390000,"closed_at":1649434851000,"author_association":"MEMBER","active_lock_reason":null,"body":"There exist processed data files that do not require parsing the raw data files (which can take long time).\r\n\r\nFix #4122.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4127\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4127\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4127","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4127","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4127.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4127.patch","merged_at":1649434851000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4126","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4126\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4126\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4126\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4126","id":1196665194,"node_id":"I_kwDODunzps5HU6lq","number":4126,"title":"dataset viewer issue for common_voice","user":{"login":"laphang","id":24724502,"node_id":"MDQ6VXNlcjI0NzI0NTAy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/24724502?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/laphang","html_url":"https:\/\/github.com\/laphang","followers_url":"https:\/\/api.github.com\/users\/laphang\/followers","following_url":"https:\/\/api.github.com\/users\/laphang\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/laphang\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/laphang\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/laphang\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/laphang\/orgs","repos_url":"https:\/\/api.github.com\/users\/laphang\/repos","events_url":"https:\/\/api.github.com\/users\/laphang\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/laphang\/received_events","type":"User","site_admin":false},"labels":[{"id":3470211881,"node_id":"LA_kwDODunzps7O1zsp","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer","name":"dataset-viewer","color":"E5583E","default":false,"description":"Related to the dataset viewer on huggingface.co"},{"id":4027368468,"node_id":"LA_kwDODunzps7wDMQU","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/audio_column","name":"audio_column","color":"F83ACF","default":false,"description":""}],"state":"closed","locked":false,"assignee":{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},"assignees":[{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Yes, it's a known issue, and we expect to fix it soon.","Fixed.\r\n\r\n<img width=\"1393\" alt=\"Capture d\u2019e\u0301cran 2022-04-25 a\u0300 15 42 05\" src=\"https:\/\/user-images.githubusercontent.com\/1676121\/165101176-d729d85b-efff-45a8-bad1-b69223edba5f.png\">\r\n"],"created_at":1649374468000,"updated_at":1650894137000,"closed_at":1650894136000,"author_association":"NONE","active_lock_reason":null,"body":"## Dataset viewer issue for 'common_voice'\r\n\r\n**Link:** https:\/\/huggingface.co\/datasets\/common_voice\r\n\r\nServer Error\r\nStatus code:   400\r\nException:     TypeError\r\nMessage:       __init__() got an unexpected keyword argument 'audio_column'\r\n\r\nAm I the one who added this dataset ? No\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4126\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4126\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4125","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4125\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4125\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4125\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4125","id":1196633936,"node_id":"PR_kwDODunzps411qeR","number":4125,"title":"BIG-bench","user":{"login":"andersjohanandreassen","id":43357549,"node_id":"MDQ6VXNlcjQzMzU3NTQ5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/43357549?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/andersjohanandreassen","html_url":"https:\/\/github.com\/andersjohanandreassen","followers_url":"https:\/\/api.github.com\/users\/andersjohanandreassen\/followers","following_url":"https:\/\/api.github.com\/users\/andersjohanandreassen\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/andersjohanandreassen\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/andersjohanandreassen\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/andersjohanandreassen\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/andersjohanandreassen\/orgs","repos_url":"https:\/\/api.github.com\/users\/andersjohanandreassen\/repos","events_url":"https:\/\/api.github.com\/users\/andersjohanandreassen\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/andersjohanandreassen\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["> It looks like the CI is failing on windows because our windows CI is unable to clone the bigbench repository (maybe it has to do with filenames that are longer than 256 characters, which windows don't like). Could the smaller installation of bigbench via pip solve this issue ?\r\n> Otherwise we can see how to remove this limitation in our windows CI.\r\n\r\nI'm not sure.\r\nIf it's git's fault that it can't handle the long filenames, it will possibly be resolved by the pip install. If it's an issue with windows not liking long filenames after it's installed, then it will not be resolved.\r\nI don't have a windows computer to try it on, but I might be able to tweek this PR and do an experiment to find out. \r\nWe're waiting for a quota increase for the pip install (https:\/\/github.com\/pypa\/pypi-support\/issues\/1782). It's been pending for 2-3 weeks, and I don't have an estimate for when it will be resolved. \r\n\r\n>Regarding the dummy data zip files, I think we can just keep datasets\/bigbench\/dummy\/abstract_narrative_understanding\/1.0.0\/dummy_data.zip and remove all the other ones. We just require to have at least one dummy_data.zip file.\r\n\r\nSounds great. I will trim that down. ","Do you know what are the other tests dependencies that have conflicts with bigbench ? I can try to split the CI to end up with a compatible list of test dependencies","Hi @lhoestq,\r\n\r\nI haven't played with eliminating requirements form the test dependencies, and I've been trying to resolve this by modifying the bigbench repo to become compatible. \r\nIn the original bigbench repo, the version requirements were strict, and specifically it had a datasets==1.17.0 requirement which was causing trouble. \r\nI'm working on PR https:\/\/github.com\/google\/BIG-bench\/pull\/766 to get some more flexible versions that might be compatible with the test dependencies in  HF\/datasets.\r\nWe're somewhat flexible in modifying these version numbers if we can figure out what the exact conflict is. \r\n\r\nI've spent some time experimenting with different versions, but I don't have a very efficient way of doing this debugging on my work computer (which for some reason doesn't produce the same sets of errors running python 3.9 instead of 3.6 or 3.7 in the tests). \r\nIt currently fails at \r\n> The conflict is caused by:\r\n>    bert-score 0.3.6 depends on matplotlib\r\n>    big-bench 0.0.1 depends on matplotlib<4.0 and >=3.5.1\r\n\r\nwhich doesn't seem like it can be the real issue. \r\n\r\nIf you have any advice for how to resolve these conflicts, that would be greatly appreciated!","Hi again @lhoestq, \r\nAfter some more or less random guessing of conflicting packages, I've managed to find a configuration that seems to be compatible with HF\/datasets. \r\n\r\nThe errors went away after removing version limits on matplotlib and scipy, and loosening numpy from 1.19 -> 1.17 in the bigbench requirements. \r\n\r\nI might do some more tweaking to see if it lets me set some minimal limits on matplotlib and scipy, but I think we at least can move forward.\r\n\r\nThe WIN tests are still failing, now because of \r\n\r\n> Did not find path entry C:\\tools\\miniconda3\\bin\r\n>C:\\tools\\miniconda3\\envs\\py37\\python.exe: No module named pytest\r\n\r\nI have no way of debugging this locally, and unless there's some way to get more verbose logs, I don't know why it's not finding pytest. Would you be able to take a quick look? \r\n\r\nUpdate: Actually, I see it's still failing because of the long filenames. So perhaps the pytest error is just because the previous steps failed. ","One more update on the WIN errors. \r\nI think all the long filenames are in files in the github repo that does not need to be included. \r\nWe will try to remove them .","Hi ! The remaining error seems to be a `UnicodeDecodeError` from `setup.py`. I think you can fix your setup.py:\r\n```diff\r\n-     with open(os.path.join(os.path.dirname(__file__), fname)) as f:\r\n+     with open(os.path.join(os.path.dirname(__file__), fname), encoding=\"utf-8\") as f:\r\n```\r\nIndeed on windows, when you `open` a file it doesn't always use \"utf-8\" by default","Hi @lhoestq, \r\nThe dependency issues seems to now be resolved \ud83c\udf89 \r\n\r\nNow, the WIN tests are failing at\r\n> ERROR tests\/test_arrow_dataset.py::test_dummy_dataset_serialize_s3 - botocore...\r\n> ERROR tests\/test_dataset_dict.py::test_dummy_dataset_serialize_s3 - botocore...\r\n\r\nIs this testing the dummy dataset that's added in bigbench? If so, I might need some help getting the right format in.\r\n\r\nThe error message I'm seeing is \r\n> raise EndpointConnectionError(endpoint_url=request.url, error=e)\r\n> E           botocore.exceptions.EndpointConnectionError: Could not connect to the endpoint URL: \"http:\/\/127.0.0.1:5555\/test\"\r\n\r\nWhich seems unrelated, but perhaps the real issue is somewhere I'm not seeing? ","Woohoo awesome !\r\n\r\nLet me check the CI error","Can you try to re-run the CI, just in case CircleCI messed up ?","Hi @lhoestq, \r\nRerunning did not seem to solve the problem. \r\nThe `test_dummy_dataset_serialize_s3` error still seems to remain.","Hi again @lhoestq, \r\nI'm not sure if this is informative or not in terms of debugging, but I deleted the dummy data and the errors for windows still fail and the others still pass. \r\nDo you have any idea what could be causing this error on windows?","The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4125). All of your documentation changes will be reflected on that endpoint.","Now the last question: let's have the dataset under`google\/bigbench` @andersjohanandreassen ?\r\n\r\nI think it would be nicer, this way you and anyone in your team can update the dataset card whevener you want without going through a github PR. You just need to join the https:\/\/huggingface.co\/google page using your google email :)"],"created_at":1649370810000,"updated_at":1651847223000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"This PR adds all BIG-bench json tasks to huggingface\/datasets.  ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4125\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4125\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4125","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4125","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4125.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4125.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4124","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4124\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4124\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4124\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4124","id":1196469842,"node_id":"I_kwDODunzps5HUK5S","number":4124,"title":"Image decoding often fails when  transforming Image datasets","user":{"login":"RafayAK","id":17025191,"node_id":"MDQ6VXNlcjE3MDI1MTkx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17025191?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/RafayAK","html_url":"https:\/\/github.com\/RafayAK","followers_url":"https:\/\/api.github.com\/users\/RafayAK\/followers","following_url":"https:\/\/api.github.com\/users\/RafayAK\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/RafayAK\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/RafayAK\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/RafayAK\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/RafayAK\/orgs","repos_url":"https:\/\/api.github.com\/users\/RafayAK\/repos","events_url":"https:\/\/api.github.com\/users\/RafayAK\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/RafayAK\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["A quick hack I have found is that we can call the image first before running the transforms and it makes sure the image is decoded before being passed on.\r\n\r\nFor this I just needed to add `example['img'] = example['img']` to the top of my `generate_flipped_data` function, defined above, so that image decode in invoked.\r\n\r\nAfter this minor change this function works:\r\n```python\r\ndef generate_flipped_data(example, p=0.5):\r\n    \"\"\"\r\n    A Dataset mapping functions that transforms some of the image up-side-down.\r\n    If the probability value (p) is 0.5 approximately half the images will be flipped upside-down\r\n    Args:\r\n        example: An example from the dataset containing a Python dictionary with \"img\" and \"is_flipped\" key-value pair\r\n        p: probability of flipping the image up-side-down, Default 0.5\r\n\r\n    Returns:\r\n        example: A Dataset object\r\n\r\n    \"\"\"\r\n    example['img'] = example['img']  # <<< This is the only change\r\n    if rng.random() > p:  # the flip the image and set is_flipped column to 1\r\n        example['img'] = example['img'].transpose(\r\n            1)  # ImageOps.flip(example['img'])  #example['img'].transpose(Image.FLIP_TOP_BOTTOM)\r\n        example['is_flipped'] = 1\r\n\r\n    return example\r\n```","Hi @RafayAK, thanks for reporting.\r\n\r\nCurrent implementation of the Image feature performs the decoding only if the \"img\" field is accessed by the mapped function.\r\n\r\nIn your original `generate_flipped_data` function:\r\n- it only accesses the \"img\" field (and thus performs decoding) if `rng.random() > p`;\r\n- on the other hand, for the cases where `rng.random() <= p`, the \"img\" field is not accessed and thus no decoding is performed for those examples\r\n\r\nBy adding the code line `example['img'] = example['img']`, you make sure the \"img\" field is accessed in all cases, and the decoding is done for all examples.\r\n\r\nAlso note that there is a little bug in your implementation: `p` is not the probability of flipping, but the probability of not-flipping; the larger is `p`, the smaller is the probability of flipping.\r\n\r\nSome refactoring (fixing also `p`):\r\n```python\r\ndef generate_flipped_data(example, p=0.5):\r\n    \"\"\"\r\n    A Dataset mapping functions that transforms some of the image up-side-down.\r\n    If the probability value (p) is 0.5 approximately half the images will be flipped upside-down.\r\n\r\n    Args:\r\n        example: An example from the dataset containing a Python dictionary with \"img\" and \"is_flipped\" key-value pair\r\n        p: probability of flipping the image up-side-down, Default 0.5\r\n\r\n    Returns:\r\n        example: A Dataset object\r\n\r\n    \"\"\"\r\n    do_flip = rng.random() < p  # Note the \"<\" sign here instead of \">\"\r\n    example['img'] = example['img'].transpose(1) if do_flip else example['img']  # Note \"img\" is always accessed\r\n    example['is_flipped'] = 1 if do_flip else 0\r\n    return example","@albertvillanova Thanks for letting me know this is intended behavior. The docs are severely lacking on this, if I hadn't posted this here I would have never found out how I'm actually supposed to modify images in a Dataset object.","@albertvillanova Secondly if you check the error message it shows that around 1999 images were successfully created, I'm pretty sure some of them were also flipped during the process. Back to my main contention, sometimes the decoding takes place other times it fails. \r\n\r\nI suppose to run `map` on any dataset all the examples should be invoked even if on some of them we end up doing nothing, is that right?","Hi @RafayAK! I've opened a PR with the fix, which adds a fallback to reattempt casting to PyArrow format with a more robust (but more expensive) procedure if the first attempt fails. Feel free to test it by installing `datasets` from the PR branch with the following command:\r\n```\r\npip install git+https:\/\/github.com\/huggingface\/datasets.git@fix-4124\r\n```","@mariosasko I'll try this right away and report back.","@mariosasko Thanks a lot for looking into this, now the `map` function at least behaves as one would expect a function to behave. \r\n\r\nLooking forward to exploring Hugging Face more and even contributing \ud83d\ude03.\r\n\r\n```bash\r\n $ conda list | grep datasets\r\ndatasets                  2.0.1.dev0               pypi_0    pypi\r\n\r\n```\r\n\r\n```python\r\ndef preprocess_data(dataset):\r\n    \"\"\"\r\n    Helper funtion to pre-process HuggingFace Cifar-100 Dataset to remove fine_label and coarse_label columns and\r\n    add is_flipped column\r\n    Args:\r\n        dataset: HuggingFace CIFAR-100 Dataset Object\r\n\r\n    Returns:\r\n        new_dataset: A Dataset object with \"img\" and \"is_flipped\" columns only\r\n\r\n    \"\"\"\r\n    # remove fine_label and coarse_label columns\r\n    new_dataset = dataset.remove_columns(['fine_label', 'coarse_label'])\r\n    # add the column for is_flipped\r\n    new_dataset = new_dataset.add_column(name=\"is_flipped\", column=np.zeros((len(new_dataset)), dtype=np.uint8))\r\n\r\n    return new_dataset\r\n\r\n\r\ndef generate_flipped_data(example, p=0.5):\r\n    \"\"\"\r\n    A Dataset mapping functions that transforms some of the image up-side-down.\r\n    If the probability value (p) is 0.5 approximately half the images will be flipped upside-down\r\n    Args:\r\n        example: An example from the dataset containing a Python dictionary with \"img\" and \"is_flipped\" key-value pair\r\n        p: probability of flipping the image up-side-down, Default 0.5\r\n\r\n    Returns:\r\n        example: A Dataset object\r\n\r\n    \"\"\"\r\n    # example['img'] = example['img']\r\n    if rng.random() > p:  # the flip the image and set is_flipped column to 1\r\n        example['img'] = example['img'].transpose(\r\n            1)  # ImageOps.flip(example['img'])  #example['img'].transpose(Image.FLIP_TOP_BOTTOM)\r\n        example['is_flipped'] = 1\r\n\r\n    return example\r\n\r\nmy_test = preprocess_data(test_dataset)\r\nmy_test = my_test.map(generate_flipped_data)\r\n```\r\n\r\nThe output now show the function was applied successfully:\r\n``` bash\r\n\/home\/rafay\/anaconda3\/envs\/pytorch_new\/bin\/python \/home\/rafay\/Documents\/you_only_live_once\/upside_down_detector\/create_dataset.py\r\nDownloading builder script: 5.61kB [00:00, 3.16MB\/s]                   \r\nDownloading metadata: 4.21kB [00:00, 2.56MB\/s]                   \r\nReusing dataset cifar100 (\/home\/rafay\/.cache\/huggingface\/datasets\/cifar100\/cifar100\/1.0.0\/f365c8b725c23e8f0f8d725c3641234d9331cd2f62919d1381d1baa5b3ba3142)\r\nReusing dataset cifar100 (\/home\/rafay\/.cache\/huggingface\/datasets\/cifar100\/cifar100\/1.0.0\/f365c8b725c23e8f0f8d725c3641234d9331cd2f62919d1381d1baa5b3ba3142)\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000\/10000 [00:01<00:00, 5149.15ex\/s]\r\n```\r\n"],"created_at":1649359045000,"updated_at":1649858476000,"closed_at":1649858476000,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\nWhen transforming\/modifying images in an image dataset using the `map` function the PIL images often fail to decode in time for the image transforms, causing errors.\r\n\r\nUsing a debugger it is easy to see what the problem is, the Image decode invocation does not take place and the resulting image passed around is still raw bytes:\r\n```\r\n[{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00 \\x00\\x00\\x00 \\x08\\x02\\x00\\x00\\x00\\xfc\\x18\\xed\\xa3\\x00\\x00\\x08\\x02IDATx\\x9cEVIs[\\xc7\\x11\\xeemf\\xde\\x82\\x8d\\x80\\x08\\x89\"\\xb5V\\\\\\xb6\\x94(\\xe5\\x9f\\x90\\xca5\\x7f$\\xa7T\\xe5\\x9f&9\\xd9\\x8a\\\\.\\xdb\\xa4$J\\xa4\\x00\\x02x\\xc0{\\xb3t\\xe7\\x00\\xca\\x99\\xd3\\\\f\\xba\\xba\\xbf\\xa5?|\\xfa\\xf4\\xa2\\xeb\\xba\\xedv\\xa3f^\\xf8\\xd5\\x0bY\\xb6\\x10\\xb3\\xaaDq\\xcd\\x83\\x87\\xdf5\\xf3gZ\\x1a\\x04\\x0f\\xa0fp\\xfa\\xe0\\xd4\\x07?\\x9dN\\xc4\\xb1\\x99\\xfd\\xf2\\xcb\/\\x97\\x97\\x97H\\xa2\\xaaf\\x16\\x82\\xaf\\xeb\\xca{\\xbf\\xd9l.\\xdf\\x7f\\xfa\\xcb_\\xff&\\x88\\x08\\x00\\x80H\\xc0\\x80@.;\\x0f\\x8c@#v\\xe3\\xe5\\xfc\\xd1\\x9f\\xee6q\\xbf\\xdf\\xa6\\x14\\'\\x93\\xf1\\xc3\\xe5\\xe3\\xd1x\\x14c\\x8c1\\xa5\\x1c\\x9dsM\\xd3\\xb4\\xed\\x08\\x89SJ)\\xa5\\xedv\\xbb^\\xafNO\\x97D\\x84Hf .... \r\n```\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset, Dataset\r\nimport numpy as np\r\n# seeded NumPy random number generator for reprodducinble results.\r\nrng = np.random.default_rng(seed=0)\r\n\r\ntest_dataset = load_dataset('cifar100', split=\"test\")\r\n\r\ndef preprocess_data(dataset):\r\n    \"\"\"\r\n    Helper function to pre-process HuggingFace Cifar-100 Dataset to remove fine_label and coarse_label columns and\r\n    add is_flipped column\r\n    Args:\r\n        dataset: HuggingFace CIFAR-100 Dataset Object\r\n\r\n    Returns:\r\n        new_dataset: A Dataset object with \"img\" and \"is_flipped\" columns only\r\n\r\n    \"\"\"\r\n    # remove fine_label and coarse_label columns\r\n    new_dataset = dataset.remove_columns(['fine_label', 'coarse_label'])\r\n    # add the column for is_flipped\r\n    new_dataset = new_dataset.add_column(name=\"is_flipped\", column=np.zeros((len(new_dataset)), dtype=np.uint8))\r\n\r\n    return new_dataset\r\n\r\n\r\ndef generate_flipped_data(example, p=0.5):\r\n    \"\"\"\r\n    A Dataset mapping function that transforms some of the images up-side-down.\r\n    If the probability value (p) is 0.5 approximately half the images will be flipped upside-down\r\n    Args:\r\n        example: An example from the dataset containing a Python dictionary with \"img\" and \"is_flipped\" key-value pair\r\n        p: the probability of flipping the image up-side-down, Default 0.5\r\n\r\n    Returns:\r\n        example: A Dataset object\r\n\r\n    \"\"\"\r\n    # example['img'] = example['img']\r\n    if rng.random() > p:  # the flip the image and set is_flipped column to 1\r\n        example['img'] = example['img'].transpose(\r\n            1)  # ImageOps.flip(example['img'])  #example['img'].transpose(Image.FLIP_TOP_BOTTOM)\r\n        example['is_flipped'] = 1\r\n\r\n    return example\r\n\r\nmy_test = preprocess_data(test_dataset)\r\nmy_test = my_test.map(generate_flipped_data)\r\n\r\n```\r\n\r\n## Expected results\r\nThe dataset should be transformed without problems.\r\n\r\n## Actual results\r\n```\r\n\/home\/rafay\/anaconda3\/envs\/pytorch_new\/bin\/python \/home\/rafay\/Documents\/you_only_live_once\/upside_down_detector\/create_dataset.py\r\nReusing dataset cifar100 (\/home\/rafay\/.cache\/huggingface\/datasets\/cifar100\/cifar100\/1.0.0\/f365c8b725c23e8f0f8d725c3641234d9331cd2f62919d1381d1baa5b3ba3142)\r\nReusing dataset cifar100 (\/home\/rafay\/.cache\/huggingface\/datasets\/cifar100\/cifar100\/1.0.0\/f365c8b725c23e8f0f8d725c3641234d9331cd2f62919d1381d1baa5b3ba3142)\r\n 20%|\u2588\u2589        | 1999\/10000 [00:00<00:01, 5560.44ex\/s]\r\nTraceback (most recent call last):\r\n  File \"\/home\/rafay\/anaconda3\/envs\/pytorch_new\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 2326, in _map_single\r\n    writer.write(example)\r\n  File \"\/home\/rafay\/anaconda3\/envs\/pytorch_new\/lib\/python3.10\/site-packages\/datasets\/arrow_writer.py\", line 441, in write\r\n    self.write_examples_on_file()\r\n  File \"\/home\/rafay\/anaconda3\/envs\/pytorch_new\/lib\/python3.10\/site-packages\/datasets\/arrow_writer.py\", line 399, in write_examples_on_file\r\n    self.write_batch(batch_examples=batch_examples)\r\n  File \"\/home\/rafay\/anaconda3\/envs\/pytorch_new\/lib\/python3.10\/site-packages\/datasets\/arrow_writer.py\", line 492, in write_batch\r\n    arrays.append(pa.array(typed_sequence))\r\n  File \"pyarrow\/array.pxi\", line 230, in pyarrow.lib.array\r\n  File \"pyarrow\/array.pxi\", line 110, in pyarrow.lib._handle_arrow_array_protocol\r\n  File \"\/home\/rafay\/anaconda3\/envs\/pytorch_new\/lib\/python3.10\/site-packages\/datasets\/arrow_writer.py\", line 185, in __arrow_array__\r\n    out = pa.array(cast_to_python_objects(data, only_1d_for_numpy=True))\r\n  File \"pyarrow\/array.pxi\", line 316, in pyarrow.lib.array\r\n  File \"pyarrow\/array.pxi\", line 39, in pyarrow.lib._sequence_to_array\r\n  File \"pyarrow\/error.pxi\", line 143, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow\/error.pxi\", line 99, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Could not convert <PIL.Image.Image image mode=RGB size=32x32 at 0x7F56AEE61DE0> with type Image: did not recognize Python value type when inferring an Arrow data type\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/home\/rafay\/Documents\/you_only_live_once\/upside_down_detector\/create_dataset.py\", line 55, in <module>\r\n    my_test = my_test.map(generate_flipped_data)\r\n  File \"\/home\/rafay\/anaconda3\/envs\/pytorch_new\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 1953, in map\r\n    return self._map_single(\r\n  File \"\/home\/rafay\/anaconda3\/envs\/pytorch_new\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 519, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"\/home\/rafay\/anaconda3\/envs\/pytorch_new\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 486, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"\/home\/rafay\/anaconda3\/envs\/pytorch_new\/lib\/python3.10\/site-packages\/datasets\/fingerprint.py\", line 458, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"\/home\/rafay\/anaconda3\/envs\/pytorch_new\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 2360, in _map_single\r\n    writer.finalize()\r\n  File \"\/home\/rafay\/anaconda3\/envs\/pytorch_new\/lib\/python3.10\/site-packages\/datasets\/arrow_writer.py\", line 522, in finalize\r\n    self.write_examples_on_file()\r\n  File \"\/home\/rafay\/anaconda3\/envs\/pytorch_new\/lib\/python3.10\/site-packages\/datasets\/arrow_writer.py\", line 399, in write_examples_on_file\r\n    self.write_batch(batch_examples=batch_examples)\r\n  File \"\/home\/rafay\/anaconda3\/envs\/pytorch_new\/lib\/python3.10\/site-packages\/datasets\/arrow_writer.py\", line 492, in write_batch\r\n    arrays.append(pa.array(typed_sequence))\r\n  File \"pyarrow\/array.pxi\", line 230, in pyarrow.lib.array\r\n  File \"pyarrow\/array.pxi\", line 110, in pyarrow.lib._handle_arrow_array_protocol\r\n  File \"\/home\/rafay\/anaconda3\/envs\/pytorch_new\/lib\/python3.10\/site-packages\/datasets\/arrow_writer.py\", line 185, in __arrow_array__\r\n    out = pa.array(cast_to_python_objects(data, only_1d_for_numpy=True))\r\n  File \"pyarrow\/array.pxi\", line 316, in pyarrow.lib.array\r\n  File \"pyarrow\/array.pxi\", line 39, in pyarrow.lib._sequence_to_array\r\n  File \"pyarrow\/error.pxi\", line 143, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow\/error.pxi\", line 99, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Could not convert <PIL.Image.Image image mode=RGB size=32x32 at 0x7F56AEE61DE0> with type Image: did not recognize Python value type when inferring an Arrow data type\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0\r\n- Platform: Linux(Fedora 35)\r\n- Python version: 3.10\r\n- PyArrow version: 7.0.0\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4124\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4124\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4123","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4123\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4123\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4123\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4123","id":1196367512,"node_id":"I_kwDODunzps5HTx6Y","number":4123,"title":"Building C4 takes forever","user":{"login":"StellaAthena","id":15899312,"node_id":"MDQ6VXNlcjE1ODk5MzEy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/15899312?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/StellaAthena","html_url":"https:\/\/github.com\/StellaAthena","followers_url":"https:\/\/api.github.com\/users\/StellaAthena\/followers","following_url":"https:\/\/api.github.com\/users\/StellaAthena\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/StellaAthena\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/StellaAthena\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/StellaAthena\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/StellaAthena\/orgs","repos_url":"https:\/\/api.github.com\/users\/StellaAthena\/repos","events_url":"https:\/\/api.github.com\/users\/StellaAthena\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/StellaAthena\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi @StellaAthena, thanks for reporting.\r\n\r\nPlease note, that our `datasets` library performs several operations in order to load a dataset, among them:\r\n- it downloads all the required files: for C4 \"en\", 378.69 GB of JSON GZIPped files\r\n- it parses their content to generate the dataset\r\n- it caches the dataset in an Arrow file: for C4 \"en\", this file size is 1.87 TB\r\n- it memory-maps the Arrow file\r\n\r\nIf it suits your use case, you might load this dataset in streaming mode:\r\n- no Arrow file is generated\r\n- you can iterate over elements immediately (no need to wait to download all the entire files)\r\n\r\n```python\r\nIn [45]: from datasets import load_dataset\r\n    ...: ds = load_dataset(\"c4\", \"en\", split=\"train\", streaming=True)\r\n    ...: for item in ds:\r\n    ...:     print(item)\r\n    ...:     break\r\n    ...: \r\n{'text': 'Beginners BBQ Class Taking Place in Missoula!\\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.', 'timestamp': '2019-04-25T12:57:54Z', 'url': 'https:\/\/klyq.com\/beginners-bbq-class-taking-place-in-missoula\/'}\r\n```\r\nI hope this is useful for your use case."],"created_at":1649353290000,"updated_at":1649424139000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\nC4-en is a 300 GB dataset. However, when I try to download it through the hub it takes over _six hours_ to generate the train\/test split from the downloaded files. This is an absurd amount of time and an unnecessary waste of resources.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nc4 = datasets.load(\"c4\", \"en\")\r\n```\r\n\r\n## Expected results\r\nI would like to be able to download pre-split data.\r\n\r\n## Environment info\r\n- `datasets` version: 2.0.0\r\n- Platform: Linux-5.13.0-35-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.1\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4123\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4123\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4122","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4122\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4122\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4122\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4122","id":1196095072,"node_id":"I_kwDODunzps5HSvZg","number":4122,"title":"medical_dialog zh has very slow _generate_examples","user":{"login":"nbroad1881","id":24982805,"node_id":"MDQ6VXNlcjI0OTgyODA1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/24982805?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nbroad1881","html_url":"https:\/\/github.com\/nbroad1881","followers_url":"https:\/\/api.github.com\/users\/nbroad1881\/followers","following_url":"https:\/\/api.github.com\/users\/nbroad1881\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nbroad1881\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nbroad1881\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nbroad1881\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nbroad1881\/orgs","repos_url":"https:\/\/api.github.com\/users\/nbroad1881\/repos","events_url":"https:\/\/api.github.com\/users\/nbroad1881\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nbroad1881\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi @nbroad1881, thanks for reporting.\r\n\r\nLet me have a look to try to improve its performance. ","Thanks @nbroad1881  for reporting! I don't recall it taking so long. I will also have a look at this. \r\n@albertvillanova please let me know if I am doing something unnecessary or time consuming.","Hi @nbroad1881 and @vrindaprabhu,\r\n\r\nAs a workaround for the performance of the parsing of the raw data files (this could be addressed in a subsequent PR), I have found that there are also processed data files, that do not require parsing. I have added these as new configurations `processed.en` and `processed.zh`:\r\n```python\r\nds = load_dataset(\"medical_dialog\", \"processed.zh\")\r\n```"],"created_at":1649340051000,"updated_at":1649434851000,"closed_at":1649434851000,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\nAfter downloading the files from Google Drive, `load_dataset(\"medical_dialog\", \"zh\", data_dir=\".\/\")` takes an unreasonable amount of time. Generating the train\/test split for 33% of the dataset takes over 4.5 hours.\r\n\r\n## Steps to reproduce the bug\r\nThe easiest way I've found to download files from Google Drive is to use `gdown` and use Google Colab because the download speeds will be very high due to the fact that they are both in Google Cloud.\r\n\r\n```python\r\nfile_ids = [\r\n        \"1AnKxGEuzjeQsDHHqL3NqI_aplq2hVL_E\",\r\n        \"1tt7weAT1SZknzRFyLXOT2fizceUUVRXX\",\r\n        \"1A64VBbsQ_z8wZ2LDox586JIyyO6mIwWc\",\r\n        \"1AKntx-ECnrxjB07B6BlVZcFRS4YPTB-J\",\r\n        \"1xUk8AAua_x27bHUr-vNoAuhEAjTxOvsu\",\r\n        \"1ezKTfe7BgqVN5o-8Vdtr9iAF0IueCSjP\",\r\n        \"1tA7bSOxR1RRNqZst8cShzhuNHnayUf7c\",\r\n        \"1pA3bCFA5nZDhsQutqsJcH3d712giFb0S\",\r\n        \"1pTLFMdN1A3ro-KYghk4w4sMz6aGaMOdU\",\r\n        \"1dUSnG0nUPq9TEQyHd6ZWvaxO0OpxVjXD\",\r\n        \"1UfCH05nuWiIPbDZxQzHHGAHyMh8dmPQH\",\r\n]\r\nfor i in file_ids:\r\n    url = f\"https:\/\/drive.google.com\/uc?id={i}\"\r\n    !gdown $url\r\n\r\n\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"medical_dialog\", \"zh\", data_dir=\".\/\")\r\n```\r\n\r\n## Expected results\r\nFaster load time\r\n\r\n## Actual results\r\n`Generating train split: 33%: 625519\/1921127 [4:31:03<31:39:20, 11.37 examples\/s]`\r\n\r\n## Environment info\r\n- `datasets` version: 2.0.0\r\n- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.3.5\r\n\r\n@vrindaprabhu , could you take a look at this since you implemented it? I think the `_generate_examples` function might need to be rewritten","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4122\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4122\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4121","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4121\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4121\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4121\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4121","id":1196000018,"node_id":"I_kwDODunzps5HSYMS","number":4121,"title":"datasets.load_metric can not load a local metirc","user":{"login":"Gare-Ng","id":51749469,"node_id":"MDQ6VXNlcjUxNzQ5NDY5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/51749469?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Gare-Ng","html_url":"https:\/\/github.com\/Gare-Ng","followers_url":"https:\/\/api.github.com\/users\/Gare-Ng\/followers","following_url":"https:\/\/api.github.com\/users\/Gare-Ng\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Gare-Ng\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Gare-Ng\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Gare-Ng\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Gare-Ng\/orgs","repos_url":"https:\/\/api.github.com\/users\/Gare-Ng\/repos","events_url":"https:\/\/api.github.com\/users\/Gare-Ng\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Gare-Ng\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1649335736000,"updated_at":1649339607000,"closed_at":1649339607000,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\nNo matter how I hard try to tell load_metric that I want to load a local metric file, it still continues to fetch things on the Internet. And unfortunately it says 'ConnectionError: Couldn't reach'. However I can download this file without connectionerror and tell load_metric its local directory. And it comes back where it begins...\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nmetric = load_metric(path=r'C:\\Users\\Gare\\PycharmProjects\\Gare\\blue\\bleu.py')\r\n    ConnectionError: Couldn't reach https:\/\/github.com\/tensorflow\/nmt\/raw\/master\/nmt\/scripts\/bleu.py\r\n\r\nmetric = load_metric(path='bleu')\r\n    ConnectionError: Couldn't reach https:\/\/raw.githubusercontent.com\/huggingface\/datasets\/1.12.1\/metrics\/bleu\/bleu.py\r\n\r\nmetric = load_metric(path='.\/blue\/bleu.py')\r\n    ConnectionError: Couldn't reach https:\/\/github.com\/tensorflow\/nmt\/raw\/master\/nmt\/scripts\/bleu.py\r\n```\r\n\r\n## Expected results\r\nI do read the docs [here](https:\/\/huggingface.co\/docs\/datasets\/package_reference\/loading_methods#datasets.load_metric). There are no other parameters that help function to distinguish from local and online file but path. As what I code above, it should load from local.\r\n\r\n## Actual results\r\n\r\n> metric = load_metric(path=r'C:\\Users\\Gare\\PycharmProjects\\Gare\\blue\\bleu.py')\r\n\r\n> ~\\AppData\\Local\\Temp\\ipykernel_19636\\1855752034.py in <module>\r\n----> 1 metric = load_metric(path=r'C:\\Users\\Gare\\PycharmProjects\\Gare\\blue\\bleu.py')\r\nD:\\Program Files\\Anaconda\\envs\\Gare\\lib\\site-packages\\datasets\\load.py in load_metric(path, config_name, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, script_version, **metric_init_kwargs)\r\n    817         if data_files is None and data_dir is not None:\r\n    818             data_files = os.path.join(data_dir, \"**\")\r\n--> 819 \r\n    820         self.name = name\r\n    821         self.revision = revision\r\nD:\\Program Files\\Anaconda\\envs\\Gare\\lib\\site-packages\\datasets\\load.py in prepare_module(path, script_version, download_config, download_mode, dataset, force_local_path, dynamic_modules_path, return_resolved_file_path, return_associated_base_path, data_files, **download_kwargs)\r\n    639         self,\r\n    640         path: str,\r\n--> 641         download_config: Optional[DownloadConfig] = None,\r\n    642         download_mode: Optional[DownloadMode] = None,\r\n    643         dynamic_modules_path: Optional[str] = None,\r\nD:\\Program Files\\Anaconda\\envs\\Gare\\lib\\site-packages\\datasets\\utils\\file_utils.py in cached_path(url_or_filename, download_config, **download_kwargs)\r\n    297             token = hf_api.HfFolder.get_token()\r\n    298         if token:\r\n--> 299             headers[\"authorization\"] = f\"Bearer {token}\"\r\n    300     return headers\r\n    301 \r\nD:\\Program Files\\Anaconda\\envs\\Gare\\lib\\site-packages\\datasets\\utils\\file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token)\r\n    604             def _resumable_file_manager():\r\n    605                 with open(incomplete_path, \"a+b\") as f:\r\n--> 606                     yield f\r\n    607 \r\n    608             temp_file_manager = _resumable_file_manager\r\nConnectionError: Couldn't reach https:\/\/github.com\/tensorflow\/nmt\/raw\/master\/nmt\/scripts\/bleu.py\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0\r\n- Platform: Windows-10-10.0.22000-SP0\r\n- Python version: 3.7.13\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.3.4\r\n\r\nAny advice would be appreciated.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4121\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4121\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4120","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4120\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4120\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4120\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4120","id":1195887430,"node_id":"I_kwDODunzps5HR8tG","number":4120,"title":"Representing dictionaries (json) objects as features","user":{"login":"yanaiela","id":8031035,"node_id":"MDQ6VXNlcjgwMzEwMzU=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8031035?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yanaiela","html_url":"https:\/\/github.com\/yanaiela","followers_url":"https:\/\/api.github.com\/users\/yanaiela\/followers","following_url":"https:\/\/api.github.com\/users\/yanaiela\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yanaiela\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yanaiela\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yanaiela\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yanaiela\/orgs","repos_url":"https:\/\/api.github.com\/users\/yanaiela\/repos","events_url":"https:\/\/api.github.com\/users\/yanaiela\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yanaiela\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1649329661000,"updated_at":1649329661000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"In the process of adding a new dataset to the hub, I stumbled upon the inability to represent dictionaries that contain different key names, unknown in advance (and may differ between samples), original asked in the [forum](https:\/\/discuss.huggingface.co\/t\/representing-nested-dictionary-with-different-keys\/16442).\r\n\r\nFor instance:\r\n\r\n```\r\nsample1 = {\"nps\": {\r\n  \"a\": {\"id\": 0, \"text\": \"text1\"},\r\n  \"b\": {\"id\": 1, \"text\": \"text2\"},\r\n}}\r\nsample2 = {\"nps\": {\r\n  \"a\": {\"id\": 0, \"text\": \"text1\"},\r\n  \"b\": {\"id\": 1, \"text\": \"text2\"},\r\n  \"c\": {\"id\": 2, \"text\": \"text3\"},\r\n}}\r\nsample3 = {\"nps\": {\r\n  \"a\": {\"id\": 0, \"text\": \"text1\"},\r\n  \"b\": {\"id\": 1, \"text\": \"text2\"},\r\n  \"c\": {\"id\": 2, \"text\": \"text3\"},\r\n  \"d\": {\"id\": 3, \"text\": \"text4\"},\r\n}}\r\n```\r\n\r\nthe `nps` field cannot be represented as a Feature while maintaining its original structure.\r\n@lhoestq suggested to add JSON as a new feature type, which will solve this problem.\r\n\r\n\r\nIt seems like an alternative solution would be to change the original data format, which isn't an optimal solution in my case. Moreover, JSON is a common structure, that will likely to be useful in future datasets as well.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4120\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4120\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4119","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4119\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4119\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4119\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4119","id":1195641298,"node_id":"PR_kwDODunzps41yXHF","number":4119,"title":"Hotfix failing CI tests on Windows","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649317126000,"updated_at":1649324844000,"closed_at":1649318233000,"author_association":"MEMBER","active_lock_reason":null,"body":"This PR makes a hotfix for our CI Windows tests: https:\/\/app.circleci.com\/pipelines\/github\/huggingface\/datasets\/11092\/workflows\/9cfdb1dd-0fec-4fe0-8122-5f533192ebdc\/jobs\/67414\r\n\r\nFix #4118\r\n\r\nI guess this issue is related to this PR:\r\n- huggingface\/huggingface_hub#815","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4119\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4119\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4119","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4119","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4119.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4119.patch","merged_at":1649318233000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4118","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4118\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4118\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4118\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4118","id":1195638944,"node_id":"I_kwDODunzps5HRACg","number":4118,"title":"Failing CI tests on Windows","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":[],"created_at":1649316985000,"updated_at":1649318233000,"closed_at":1649318233000,"author_association":"MEMBER","active_lock_reason":null,"body":"## Describe the bug\r\nOur CI Windows tests are failing from yesterday: https:\/\/app.circleci.com\/pipelines\/github\/huggingface\/datasets\/11092\/workflows\/9cfdb1dd-0fec-4fe0-8122-5f533192ebdc\/jobs\/67414\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4118\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4118\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4117","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4117\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4117\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4117\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4117","id":1195552406,"node_id":"I_kwDODunzps5HQq6W","number":4117,"title":"AttributeError: module 'huggingface_hub' has no attribute 'hf_api'","user":{"login":"arymbe","id":4567991,"node_id":"MDQ6VXNlcjQ1Njc5OTE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/4567991?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/arymbe","html_url":"https:\/\/github.com\/arymbe","followers_url":"https:\/\/api.github.com\/users\/arymbe\/followers","following_url":"https:\/\/api.github.com\/users\/arymbe\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/arymbe\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/arymbe\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/arymbe\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/arymbe\/orgs","repos_url":"https:\/\/api.github.com\/users\/arymbe\/repos","events_url":"https:\/\/api.github.com\/users\/arymbe\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/arymbe\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi @arymbe, thanks for reporting.\r\n\r\nUnfortunately, I'm not able to reproduce your problem.\r\n\r\nCould you please write the complete stack trace? That way we will be able to see which package originates the exception.","Hello, thank you for your fast replied. this is the complete error that I got\r\n\r\n---------------------------------------------------------------------------\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n\r\n---------------------------------------------------------------------------\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n\r\nInput In [27], in <module>\r\n----> 1 from datasets import load_dataset\r\n\r\nvenv\/lib\/python3.8\/site-packages\/datasets\/__init__.py:39, in <module>\r\n     37 from .arrow_dataset import Dataset, concatenate_datasets\r\n     38 from .arrow_reader import ReadInstruction\r\n---> 39 from .builder import ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\r\n     40 from .combine import interleave_datasets\r\n     41 from .dataset_dict import DatasetDict, IterableDatasetDict\r\n\r\nvenv\/lib\/python3.8\/site-packages\/datasets\/builder.py:40, in <module>\r\n     32 from .arrow_reader import (\r\n     33     HF_GCP_BASE_URL,\r\n     34     ArrowReader,\r\n   (...)\r\n     37     ReadInstruction,\r\n     38 )\r\n     39 from .arrow_writer import ArrowWriter, BeamWriter\r\n---> 40 from .data_files import DataFilesDict, sanitize_patterns\r\n     41 from .dataset_dict import DatasetDict, IterableDatasetDict\r\n     42 from .features import Features\r\n\r\nvenv\/lib\/python3.8\/site-packages\/datasets\/data_files.py:297, in <module>\r\n    292     except FileNotFoundError:\r\n    293         raise FileNotFoundError(f\"The directory at {base_path} doesn't contain any data file\") from None\r\n    296 def _resolve_single_pattern_in_dataset_repository(\r\n--> 297     dataset_info: huggingface_hub.hf_api.DatasetInfo,\r\n    298     pattern: str,\r\n    299     allowed_extensions: Optional[list] = None,\r\n    300 ) -> List[PurePath]:\r\n    301     data_files_ignore = FILES_TO_IGNORE\r\n    302     fs = HfFileSystem(repo_info=dataset_info)\r\n\r\nAttributeError: module 'huggingface_hub' has no attribute 'hf_api'","This is weird... It is long ago that the package `huggingface_hub` has a submodule called `hf_api`.\r\n\r\nMaybe you have a problem with your installed `huggingface_hub`...\r\n\r\nCould you please try to update it?\r\n```shell\r\npip install -U huggingface_hub\r\n```","Yap, I've updated several times. Then, I've tried numeral combination of datasets and huggingface_hub versions. However, I think your point is right that there is a problem with my huggingface_hub installation. I'll try another way to find the solution. I'll update it later when I get the solution. Thank you :)","I'm sorry I can't reproduce your problem.\r\n\r\nMaybe you could try to create a new Python virtual environment and install all dependencies there from scratch. You can use either:\r\n- Python venv: https:\/\/docs.python.org\/3\/library\/venv.html\r\n- or conda venv (if you are using conda): https:\/\/docs.conda.io\/projects\/conda\/en\/latest\/user-guide\/tasks\/manage-environments.html"],"created_at":1649310756000,"updated_at":1650382595000,"closed_at":1650382595000,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\nCould you help me please. I got this following error.\r\n\r\nAttributeError: module 'huggingface_hub' has no attribute 'hf_api'\r\n\r\n## Steps to reproduce the bug\r\nwhen I imported the datasets\r\n\r\n# Sample code to reproduce the bug\r\nfrom datasets import list_datasets, load_dataset, list_metrics, load_metric\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0\r\n- Platform: macOS-12.3-x86_64-i386-64bit\r\n- Python version: 3.8.9\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.3.5\r\n- Huggingface-hub: 0.5.0\r\n- Transformers: 4.18.0\r\n\r\nThank you in advance.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4117\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4117\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4116","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4116\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4116\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4116\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4116","id":1194926459,"node_id":"PR_kwDODunzps41wCEO","number":4116,"title":"Pretty print dataset info files","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["maybe just do it from now on no? (i.e. not for existing `dataset_infos.json` files)","_The documentation is not available anymore as the PR was closed or merged._","> maybe just do it from now on no? (i.e. not for existing dataset_infos.json files)\r\n\r\nYes, or do this only for datasets created with `push_to_hub` to (always) keep the GH datasets small? \r\n","yep sounds good too on my side! ","I reverted the change to avoid the size increase and added the `pretty_print` flag, which pretty-prints the JSON, and that flag is only True for datasets created with `push_to_hub`.  "],"created_at":1649266848000,"updated_at":1649417281000,"closed_at":1649416913000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Adds indentation to the `dataset_infos.json` file when saving for nicer diffs.\r\n\r\n(suggested by @julien-c)\r\n\r\nThis PR also updates the info files of the GH datasets. Note that this change adds more than **10 MB** to the repo size (the total file size before the change: 29.672298 MB, after: 41.666475 MB), so I'm not sure this change is a good idea.\r\n\r\n`src\/datasets\/info.py` is the only relevant file for reviewers.\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4116\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4116\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4116","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4116","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4116.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4116.patch","merged_at":1649416913000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4115","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4115\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4115\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4115\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4115","id":1194907555,"node_id":"I_kwDODunzps5HONej","number":4115,"title":"ImageFolder add option to ignore some folders like '.ipynb_checkpoints'","user":{"login":"cceyda","id":15624271,"node_id":"MDQ6VXNlcjE1NjI0Mjcx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/15624271?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/cceyda","html_url":"https:\/\/github.com\/cceyda","followers_url":"https:\/\/api.github.com\/users\/cceyda\/followers","following_url":"https:\/\/api.github.com\/users\/cceyda\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/cceyda\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/cceyda\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/cceyda\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/cceyda\/orgs","repos_url":"https:\/\/api.github.com\/users\/cceyda\/repos","events_url":"https:\/\/api.github.com\/users\/cceyda\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/cceyda\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Maybe it would be nice to ignore private dirs like this one (ones starting with `.`) by default. \r\n\r\nCC @mariosasko ","Maybe we can add a `ignore_hidden_files` flag to the builder configs of our packaged loaders (to be consistent across all of them), wdyt @lhoestq @albertvillanova? ","I think they should always ignore them actually ! Not sure if adding a flag would be helpful","@lhoestq But what if the user explicitly requests those files via regex?\r\n\r\n`glob.glob` ignores hidden files (files starting with \".\") by default unless they are explicitly requested, but fsspec's `glob` doesn't follow this behavior, which is probably a bug, so maybe we can raise an issue or open a PR in their repo?","> @lhoestq But what if the user explicitly requests those files via regex?\r\n\r\nUsually hidden files are meant to be ignored. If they are data files, they must be placed outside a hidden directory in the first place right ? I think it's more sensible to explain this than adding a flag.\r\n\r\n> glob.glob ignores hidden files (files starting with \".\") by default unless they are explicitly requested, but fsspec's glob doesn't follow this behavior, which is probably a bug, so maybe we can raise an issue or open a PR in their repo?\r\n\r\nAfter globbing using `fsspec`, we already ignore files that start with a `.` in `_resolve_single_pattern_locally` and `_resolve_single_pattern_in_dataset_repository`, I guess we can just account for parent directories as well ?\r\n\r\nWe could open an issue on `fsspec` but I think they won't change this since it's an important breaking change for them."],"created_at":1649266183000,"updated_at":1650991182000,"closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Is your feature request related to a problem? Please describe.**\r\nI sometimes like to peek at the dataset images from jupyterlab. thus '.ipynb_checkpoints' folder appears where my dataset is and (just realized) leads to accidental duplicate image additions. I think this is an easy enough thing to miss especially if the dataset is very large.\r\n\r\n**Describe the solution you'd like**\r\nmaybe have an option `ignore` or something .gitignore style\r\n`dataset = load_dataset(\"imagefolder\", data_dir=\".\/data\/original\", ignore=\"regex?\")`\r\n\r\n**Describe alternatives you've considered**\r\nCould filter out manually\r\n\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4115\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4115\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4114","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4114\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4114\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4114\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4114","id":1194855345,"node_id":"I_kwDODunzps5HOAux","number":4114,"title":"Allow downloading just some columns of a dataset","user":{"login":"osanseviero","id":7246357,"node_id":"MDQ6VXNlcjcyNDYzNTc=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/7246357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/osanseviero","html_url":"https:\/\/github.com\/osanseviero","followers_url":"https:\/\/api.github.com\/users\/osanseviero\/followers","following_url":"https:\/\/api.github.com\/users\/osanseviero\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/osanseviero\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/osanseviero\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/osanseviero\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/osanseviero\/orgs","repos_url":"https:\/\/api.github.com\/users\/osanseviero\/repos","events_url":"https:\/\/api.github.com\/users\/osanseviero\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/osanseviero\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["In the general case you can\u2019t always reduce the quantity of data to download, since you can\u2019t parse CSV or JSON data without downloading the whole files right ? ^^ However we could explore this case-by-case I guess","Actually for csv pandas has `usecols` which allows loading a subset of columns in a more efficient way afaik, but yes, you're right this might be more complex than I thought."],"created_at":1649263126000,"updated_at":1649318186000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Is your feature request related to a problem? Please describe.**\r\nSome people are interested in doing label analysis of a CV dataset without downloading all the images. Downloading the whole dataset does not always makes sense for this kind of use case\r\n\r\n**Describe the solution you'd like**\r\nBe able to just download some columns of a dataset, such as doing\r\n```python\r\nload_dataset(\"huggan\/wikiart\",columns=[\"artist\", \"genre\"])\r\n```\r\n\r\nAlthough this might make things a bit complicated in terms of local caching of datasets.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4114\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4114\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4113","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4113\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4113\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4113\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4113","id":1194843532,"node_id":"I_kwDODunzps5HN92M","number":4113,"title":"Multiprocessing with FileLock fails in python 3.9","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1649262429000,"updated_at":1649262429000,"closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"On python 3.9, this code hangs:\r\n```python\r\nfrom multiprocessing import Pool\r\nfrom filelock import FileLock\r\n\r\n\r\ndef run(i):\r\n    print(f\"got the lock in multi process [{i}]\")\r\n\r\n\r\nwith FileLock(\"tmp.lock\"):\r\n    with Pool(2) as pool:\r\n        pool.map(run, range(2))\r\n\r\n```\r\n\r\nThis is because the subprocesses try to acquire the lock from the main process for some reason. This is not the case in older versions of python.\r\n\r\nThis can cause many issues in python 3.9. In particular, we use multiprocessing to fetch data files when you load a dataset (as long as there are >16 data files). Therefore `imagefolder` hangs, and I expect any dataset that needs to download >16 files to hang as well.\r\n\r\nLet's see if we can fix this and have a CI that runs on 3.9.\r\n\r\ncc @mariosasko @julien-c ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4113\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4113\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4112","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4112\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4112\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4112\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4112","id":1194752765,"node_id":"I_kwDODunzps5HNnr9","number":4112,"title":"ImageFolder with Grayscale images dataset","user":{"login":"ChainYo","id":50595514,"node_id":"MDQ6VXNlcjUwNTk1NTE0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/50595514?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ChainYo","html_url":"https:\/\/github.com\/ChainYo","followers_url":"https:\/\/api.github.com\/users\/ChainYo\/followers","following_url":"https:\/\/api.github.com\/users\/ChainYo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ChainYo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ChainYo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ChainYo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ChainYo\/orgs","repos_url":"https:\/\/api.github.com\/users\/ChainYo\/repos","events_url":"https:\/\/api.github.com\/users\/ChainYo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ChainYo\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi! Replacing:\r\n```python\r\ntransformed_dataset = dataset.with_transform(transforms)\r\ntransformed_dataset.set_format(type=\"torch\", device=\"cuda\")\r\n```\r\n\r\nwith:\r\n```python\r\ndef transform_func(examples):\r\n    examples[\"image\"] = [transforms(img).to(\"cuda\") for img in examples[\"image\"]]\r\n    return examples\r\n\r\ntransformed_dataset = dataset.with_transform(transform_func)\r\n```\r\nshould fix the issue. `datasets` doesn't support chaining of transforms (you can think of `set_format`\/`with_format` as a predefined transform func for `set_transform`\/`with_transforms`), so the last transform (in your case, `set_format`) takes precedence over the previous ones (in your case `with_format`). And the PyTorch formatter is not supported by the Image feature, hence the error (adding support for that is on our short-term roadmap).","Ok thanks a lot for the code snippet!\r\n\r\nI love the way `datasets` is easy to use but it made it really long to pre-process all the images (400.000 in my case) before training anything. `ImageFolder` from pytorch is faster in my case but force me to have the images on my local machine.\r\n\r\nI don't know how to speed up the process without switching to `ImageFolder` :smile: ","You can pass `ignore_verifications=True` in `load_dataset` to skip checksum verification, which takes a lot of time if the number of files is large. We will consider making this the default behavior."],"created_at":1649257800000,"updated_at":1650622913000,"closed_at":1650622912000,"author_association":"NONE","active_lock_reason":null,"body":"Hi, I'm facing a problem with a grayscale images dataset I have uploaded [here](https:\/\/huggingface.co\/datasets\/ChainYo\/rvl-cdip) (RVL-CDIP)\r\n\r\nI'm getting an error while I want to use images for training a model with PyTorch DataLoader. Here is the full traceback:\r\n\r\n```bash\r\nAttributeError: Caught AttributeError in DataLoader worker process 0.\r\nOriginal Traceback (most recent call last):\r\n  File \"\/home\/chainyo\/miniconda3\/envs\/gan-bird\/lib\/python3.8\/site-packages\/torch\/utils\/data\/_utils\/worker.py\", line 287, in _worker_loop\r\n    data = fetcher.fetch(index)\r\n  File \"\/home\/chainyo\/miniconda3\/envs\/gan-bird\/lib\/python3.8\/site-packages\/torch\/utils\/data\/_utils\/fetch.py\", line 49, in fetch\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"\/home\/chainyo\/miniconda3\/envs\/gan-bird\/lib\/python3.8\/site-packages\/torch\/utils\/data\/_utils\/fetch.py\", line 49, in <listcomp>\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"\/home\/chainyo\/miniconda3\/envs\/gan-bird\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py\", line 1765, in __getitem__\r\n    return self._getitem(\r\n  File \"\/home\/chainyo\/miniconda3\/envs\/gan-bird\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py\", line 1750, in _getitem\r\n    formatted_output = format_table(\r\n  File \"\/home\/chainyo\/miniconda3\/envs\/gan-bird\/lib\/python3.8\/site-packages\/datasets\/formatting\/formatting.py\", line 532, in format_table\r\n    return formatter(pa_table, query_type=query_type)\r\n  File \"\/home\/chainyo\/miniconda3\/envs\/gan-bird\/lib\/python3.8\/site-packages\/datasets\/formatting\/formatting.py\", line 281, in __call__\r\n    return self.format_row(pa_table)\r\n  File \"\/home\/chainyo\/miniconda3\/envs\/gan-bird\/lib\/python3.8\/site-packages\/datasets\/formatting\/torch_formatter.py\", line 58, in format_row\r\n    return self.recursive_tensorize(row)\r\n  File \"\/home\/chainyo\/miniconda3\/envs\/gan-bird\/lib\/python3.8\/site-packages\/datasets\/formatting\/torch_formatter.py\", line 54, in recursive_tensorize\r\n    return map_nested(self._recursive_tensorize, data_struct, map_list=False)\r\n  File \"\/home\/chainyo\/miniconda3\/envs\/gan-bird\/lib\/python3.8\/site-packages\/datasets\/utils\/py_utils.py\", line 314, in map_nested\r\n    mapped = [\r\n  File \"\/home\/chainyo\/miniconda3\/envs\/gan-bird\/lib\/python3.8\/site-packages\/datasets\/utils\/py_utils.py\", line 315, in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True, None))\r\n  File \"\/home\/chainyo\/miniconda3\/envs\/gan-bird\/lib\/python3.8\/site-packages\/datasets\/utils\/py_utils.py\", line 267, in _single_map_nested\r\n    return {k: _single_map_nested((function, v, types, None, True, None)) for k, v in pbar}\r\n  File \"\/home\/chainyo\/miniconda3\/envs\/gan-bird\/lib\/python3.8\/site-packages\/datasets\/utils\/py_utils.py\", line 267, in <dictcomp>\r\n    return {k: _single_map_nested((function, v, types, None, True, None)) for k, v in pbar}\r\n  File \"\/home\/chainyo\/miniconda3\/envs\/gan-bird\/lib\/python3.8\/site-packages\/datasets\/utils\/py_utils.py\", line 251, in _single_map_nested\r\n    return function(data_struct)\r\n  File \"\/home\/chainyo\/miniconda3\/envs\/gan-bird\/lib\/python3.8\/site-packages\/datasets\/formatting\/torch_formatter.py\", line 51, in _recursive_tensorize\r\n    return self._tensorize(data_struct)\r\n  File \"\/home\/chainyo\/miniconda3\/envs\/gan-bird\/lib\/python3.8\/site-packages\/datasets\/formatting\/torch_formatter.py\", line 38, in _tensorize\r\n    if np.issubdtype(value.dtype, np.integer):\r\nAttributeError: 'bytes' object has no attribute 'dtype'\r\n```\r\n\r\nI don't really understand why the image is still a bytes object while I used transformations on it. Here the code I used to upload the dataset (and it worked well):\r\n\r\n```python\r\ntrain_dataset = load_dataset(\"imagefolder\", data_dir=\"data\/train\")\r\ntrain_dataset = train_dataset[\"train\"]\r\ntest_dataset = load_dataset(\"imagefolder\", data_dir=\"data\/test\")\r\ntest_dataset = test_dataset[\"train\"]\r\nval_dataset = load_dataset(\"imagefolder\", data_dir=\"data\/val\")\r\nval_dataset = val_dataset[\"train\"]\r\n\r\ndataset = DatasetDict({\r\n    \"train\": train_dataset,\r\n    \"val\": val_dataset,\r\n    \"test\": test_dataset\r\n})\r\ndataset.push_to_hub(\"ChainYo\/rvl-cdip\")\r\n```\r\n\r\nNow here is the code I am using to get the dataset and prepare it for training:\r\n\r\n```python\r\nimg_size = 512\r\nbatch_size = 128\r\nnormalize = [(0.5), (0.5)]\r\ndata_dir = \"ChainYo\/rvl-cdip\"\r\n\r\ndataset = load_dataset(data_dir, split=\"train\")\r\n\r\ntransforms = transforms.Compose([\r\n        transforms.Resize(img_size), \r\n        transforms.CenterCrop(img_size), \r\n        transforms.ToTensor(), \r\n        transforms.Normalize(*normalize)\r\n])\r\n\r\ntransformed_dataset = dataset.with_transform(transforms)\r\ntransformed_dataset.set_format(type=\"torch\", device=\"cuda\")\r\n\r\ntrain_dataloader = torch.utils.data.DataLoader(\r\n    transformed_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True\r\n)\r\n```\r\n\r\nBut this get me the error above. I don't understand why it's doing this kind of weird thing?\r\nDo I need to map something on the dataset? Something like this:\r\n\r\n```python\r\nlabels = dataset.features[\"label\"].names\r\nnum_labels = dataset.features[\"label\"].num_classes\r\n\r\n\r\ndef preprocess_data(examples):\r\n    images = [ex.convert(\"RGB\") for ex in examples[\"image\"]]\r\n    labels = [ex for ex in examples[\"label\"]]\r\n    return {\"images\": images, \"labels\": labels}\r\n\r\n\r\nfeatures = Features({\r\n    \"images\": Image(decode=True, id=None),\r\n    \"labels\": ClassLabel(num_classes=num_labels, names=labels)\r\n})\r\n\r\n\r\ndecoded_dataset = dataset.map(preprocess_data, remove_columns=dataset.column_names, features=features, batched=True, batch_size=100)\r\n```\r\n\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4112\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4112\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4111","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4111\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4111\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4111\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4111","id":1194660699,"node_id":"PR_kwDODunzps41vJCt","number":4111,"title":"Update security policy","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649253591000,"updated_at":1649324790000,"closed_at":1649324427000,"author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4111\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4111\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4111","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4111","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4111.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4111.patch","merged_at":1649324427000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4110","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4110\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4110\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4110\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4110","id":1194581375,"node_id":"PR_kwDODunzps41u4Je","number":4110,"title":"Matthews Correlation Metric Card","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649249975000,"updated_at":1651585397000,"closed_at":1651584973000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4110\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4110\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4110","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4110","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4110.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4110.patch","merged_at":1651584972000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4109","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4109\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4109\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4109\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4109","id":1194579257,"node_id":"PR_kwDODunzps41u3sm","number":4109,"title":"Add Spearmanr Metric Card","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","changes made! @lhoestq let me know what you think ","The CI fail is unrelated to this PR and fixed on master, feel free to merge :)"],"created_at":1649249873000,"updated_at":1651596626000,"closed_at":1651596217000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4109\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4109\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4109","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4109","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4109.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4109.patch","merged_at":1651596217000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4108","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4108\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4108\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4108\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4108","id":1194578584,"node_id":"PR_kwDODunzps41u3j2","number":4108,"title":"Perplexity Speedup","user":{"login":"emibaylor","id":27527747,"node_id":"MDQ6VXNlcjI3NTI3NzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27527747?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emibaylor","html_url":"https:\/\/github.com\/emibaylor","followers_url":"https:\/\/api.github.com\/users\/emibaylor\/followers","following_url":"https:\/\/api.github.com\/users\/emibaylor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emibaylor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emibaylor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emibaylor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emibaylor\/orgs","repos_url":"https:\/\/api.github.com\/users\/emibaylor\/repos","events_url":"https:\/\/api.github.com\/users\/emibaylor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emibaylor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["WRT the high values, can you add some unit tests with some [string, model] pairs and their resulting perplexity code, and @TristanThrush can run the same pairs through his version of the code?","_The documentation is not available anymore as the PR was closed or merged._","I thought that the perplexity metric should output the average perplexity value of all the strings that it gets as input (not a perplexity value per string, as the new version does).\r\n@lhoestq , @TristanThrush thoughts?","> I thought that the perplexity metric should output the average perplexity value of all the strings that it gets as input (not a perplexity value per string, as the new version does). @lhoestq , @TristanThrush thoughts?\r\n\r\nI support this change from Emi. If we have a perplexity function that loads GPT2 and then returns an average over all of the strings, then it is impossible to get multiple perplexities of a batch of strings efficiently. If we have this new perplexity function that is built for batching, then it is possible to get a batch of perplexities efficiently and you can still compute the average efficiently afterwards.","Thanks a lot for working on this @emibaylor @TristanThrush :)\r\n\r\nFor consistency with the other metrics, I think it's nice if we return the mean perplexity. Though I agree that having the separate perplexities per sample can also be useful. What do you think about returning both ?\r\n```python\r\nreturn {\"perplexities\": ppls, \"mean_perplexity\": np.mean(ppls)}\r\n```\r\nwe're also doing this for the COMET metric.","> Thanks a lot for working on this @emibaylor @TristanThrush :)\r\n> \r\n> For consistency with the other metrics, I think it's nice if we return the mean perplexity. Though I agree that having the separate perplexities per sample can also be useful. What do you think about returning both ?\r\n> \r\n> ```python\r\n> return {\"perplexities\": ppls, \"mean_perplexity\": np.mean(ppls)}\r\n> ```\r\n> \r\n> we're also doing this for the COMET metric.\r\n\r\nThanks! Sounds great to me.","The CI fail is unrelated to your PR and has been fixed on master, feel free to merge the master branch into your PR to fix the CI ;)"],"created_at":1649249841000,"updated_at":1650459654000,"closed_at":1650459282000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"This PR makes necessary changes to perplexity such that:\r\n- it runs much faster (via batching)\r\n- it throws an error when input is empty, or when input is one word without <BOS> token\r\n- it adds the option to add a <BOS> token\r\n\r\nIssues:\r\n- The values returned are extremely high, and I'm worried they aren't correct. Even if they are correct, they are sometimes returned as `inf`, which is not very useful (see [comment below](https:\/\/github.com\/huggingface\/datasets\/pull\/4108#discussion_r843931094) for some of the output values). \r\n  - If the values are not correct, can you help me find the error?\r\n  - If the values are correct, it might be worth it to measure something like perplexity per word, which would allow us to get actual values for the larger perplexities, instead of just `inf`\r\n\r\nFuture:\r\n- `stride` is not currently implemented here. I have some thoughts on how to make it happen with batching, but I think it would be better to get another set of eyes to look at any possible errors causing such large values now rather than later.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4108\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4108\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4108","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4108","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4108.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4108.patch","merged_at":1650459282000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4107","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4107\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4107\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4107\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4107","id":1194484885,"node_id":"I_kwDODunzps5HMmSV","number":4107,"title":"Unable to view the dataset and loading the same dataset throws the error - ArrowInvalid: Exceeded maximum rows","user":{"login":"Pavithree","id":23344465,"node_id":"MDQ6VXNlcjIzMzQ0NDY1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/23344465?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Pavithree","html_url":"https:\/\/github.com\/Pavithree","followers_url":"https:\/\/api.github.com\/users\/Pavithree\/followers","following_url":"https:\/\/api.github.com\/users\/Pavithree\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Pavithree\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Pavithree\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Pavithree\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Pavithree\/orgs","repos_url":"https:\/\/api.github.com\/users\/Pavithree\/repos","events_url":"https:\/\/api.github.com\/users\/Pavithree\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Pavithree\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Thanks for reporting. I'm looking at it","  It's not related to the dataset viewer in itself. I can replicate the error with:\r\n\r\n```\r\n>>> import datasets as ds\r\n>>> d = ds.load_dataset('Pavithree\/explainLikeImFive')\r\nUsing custom data configuration Pavithree--explainLikeImFive-b68b6d8112cd8a51\r\nDownloading and preparing dataset json\/Pavithree--explainLikeImFive to \/home\/slesage\/.cache\/huggingface\/datasets\/json\/Pavithree--explainLikeImFive-b68b6d8112cd8a51\/0.0.0\/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 305M\/305M [00:03<00:00, 98.6MB\/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17.9M\/17.9M [00:00<00:00, 75.7MB\/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11.9M\/11.9M [00:00<00:00, 70.6MB\/s]\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3\/3 [00:05<00:00,  1.92s\/it]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3\/3 [00:00<00:00, 1948.42it\/s]\r\nFailed to read file '\/home\/slesage\/.cache\/huggingface\/datasets\/downloads\/5fee9c8819754df277aee6f252e4db6897d785231c21938407b8862ca871d246' with error <class 'pyarrow.lib.ArrowInvalid'>: Exceeded maximum rows\r\nTraceback (most recent call last):\r\n  File \"\/home\/slesage\/hf\/datasets\/src\/datasets\/packaged_modules\/json\/json.py\", line 144, in _generate_tables\r\n    dataset = json.load(f)\r\n  File \"\/home\/slesage\/.pyenv\/versions\/3.8.11\/lib\/python3.8\/json\/__init__.py\", line 293, in load\r\n    return loads(fp.read(),\r\n  File \"\/home\/slesage\/.pyenv\/versions\/3.8.11\/lib\/python3.8\/json\/__init__.py\", line 357, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"\/home\/slesage\/.pyenv\/versions\/3.8.11\/lib\/python3.8\/json\/decoder.py\", line 340, in decode\r\n    raise JSONDecodeError(\"Extra data\", s, end)\r\njson.decoder.JSONDecodeError: Extra data: line 1 column 916 (char 915)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/slesage\/hf\/datasets\/src\/datasets\/load.py\", line 1691, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/home\/slesage\/hf\/datasets\/src\/datasets\/builder.py\", line 605, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/home\/slesage\/hf\/datasets\/src\/datasets\/builder.py\", line 694, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"\/home\/slesage\/hf\/datasets\/src\/datasets\/builder.py\", line 1151, in _prepare_split\r\n    for key, table in logging.tqdm(\r\n  File \"\/home\/slesage\/.pyenv\/versions\/datasets\/lib\/python3.8\/site-packages\/tqdm\/std.py\", line 1168, in __iter__\r\n    for obj in iterable:\r\n  File \"\/home\/slesage\/hf\/datasets\/src\/datasets\/packaged_modules\/json\/json.py\", line 146, in _generate_tables\r\n    raise e\r\n  File \"\/home\/slesage\/hf\/datasets\/src\/datasets\/packaged_modules\/json\/json.py\", line 122, in _generate_tables\r\n    pa_table = paj.read_json(\r\n  File \"pyarrow\/_json.pyx\", line 246, in pyarrow._json.read_json\r\n  File \"pyarrow\/error.pxi\", line 143, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow\/error.pxi\", line 99, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Exceeded maximum rows\r\n```\r\n\r\ncc @lhoestq @albertvillanova @mariosasko ","It seems that train.json is not a valid JSON Lines file: it has several JSON objects in the first line (the 915th character in the first line starts a new object, and there's no \"\\n\")\r\n\r\nYou need to have one JSON object per line","I'm closing this issue.\r\n\r\n@Pavithree, please, feel free to re-open it if fixing the JSON file does not solve it.","Thank you! that fixes the issue."],"created_at":1649245035000,"updated_at":1649401987000,"closed_at":1649255995000,"author_association":"NONE","active_lock_reason":null,"body":"## Dataset viewer issue - -ArrowInvalid: Exceeded maximum rows\r\n\r\n**Link:** *https:\/\/huggingface.co\/datasets\/Pavithree\/explainLikeImFive*\r\n\r\n*This is the subset of original eli5 dataset  https:\/\/huggingface.co\/datasets\/vblagoje\/lfqa. I just filtered the data samples which belongs to one particular subreddit thread. However, the dataset preview for train split  returns  the below mentioned error:\r\nStatus code:   400\r\nException:     ArrowInvalid\r\nMessage:       Exceeded maximum rows\r\nWhen I try to load the same dataset it returns ArrowInvalid: Exceeded maximum rows error*\r\n\r\nAm I the one who added this dataset ? Yes \r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4107\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4107\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4106","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4106\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4106\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4106\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4106","id":1194393892,"node_id":"PR_kwDODunzps41uPpa","number":4106,"title":"Support huggingface_hub 0.5","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Looks like GH actions is not able to resolve `huggingface_hub` 0.5.0, I'm investivating","_The documentation is not available anymore as the PR was closed or merged._","I'm glad to see changes in `huggingface_hub` are simplifying code here.","seems to supersede #4102, feel free to close mine :)","maybe just cherry-pick the docstring fix","I think I've found the issue:\r\n- https:\/\/github.com\/huggingface\/huggingface_hub\/pull\/790","Good catch, `huggingface_hub` doesn't support python 3.6 anymore indeed, therefore we should keep support for 0.4.0. I'm reverting the requirement version bump for now.\r\n\r\nWe can update the requirement once we drop support for python 3.6 in `datasets`","@lhoestq, I've opened this PR on `huggingface_hub`: \r\n- https:\/\/github.com\/huggingface\/huggingface_hub\/pull\/823\r\n\r\nIs there any strong reason why `huggingface_hub` no longer supports Python 3.6? ","I think `datasets` can drop support for 3.6 soon. But for now maybe let's keep support for 0.4.0, python 3.6 users are not affected by https:\/\/github.com\/huggingface\/datasets\/issues\/4105 anyway.\r\n\r\n`huggingface_hub` doesn't not have to support 3.6 again just for the CI IMO","@lhoestq I commented on the PR, that IMO it is not a good practice to drop support for Python 3.6 without a previous deprecation cycle.","Re-added support for older versions. I ended up checking `huggingface_hub` version to use the old, deprecated API for <0.5.0","I find it good practice to have all dependency version related code in a single file so that when you decide to remove support for an old version of a dependency it's easy to find and remove them, hence suggesting `utils\/_fixes.py` in https:\/\/github.com\/huggingface\/datasets\/issues\/4105#issuecomment-1090041204","good idea, thanks !","I used your suggestion @adrinjalali , I just replace the try\/except with a check on the version of `huggingface_hub`"],"created_at":1649240125000,"updated_at":1649413723000,"closed_at":1649413343000,"author_association":"MEMBER","active_lock_reason":null,"body":"Following https:\/\/github.com\/huggingface\/datasets\/issues\/4105\r\n\r\n`huggingface_hub` deprecated some parameters in `HfApi` in 0.5. This PR updates all the calls to HfApi to remove all the deprecations, <s>and I set the `hugginface_hub` requirement to `>=0.5.0`<\/s>\r\n\r\ncc @adrinjalali @LysandreJik ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4106\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4106\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4106","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4106","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4106.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4106.patch","merged_at":1649413343000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4105","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4105\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4105\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4105\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4105","id":1194297119,"node_id":"I_kwDODunzps5HL4cf","number":4105,"title":"push to hub fails with huggingface-hub 0.5.0","user":{"login":"frascuchon","id":2518789,"node_id":"MDQ6VXNlcjI1MTg3ODk=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/2518789?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/frascuchon","html_url":"https:\/\/github.com\/frascuchon","followers_url":"https:\/\/api.github.com\/users\/frascuchon\/followers","following_url":"https:\/\/api.github.com\/users\/frascuchon\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/frascuchon\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/frascuchon\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/frascuchon\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/frascuchon\/orgs","repos_url":"https:\/\/api.github.com\/users\/frascuchon\/repos","events_url":"https:\/\/api.github.com\/users\/frascuchon\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/frascuchon\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi ! Indeed there was a breaking change in `huggingface_hub` 0.5.0 in `HfApi.create_repo`, which is called here in `datasets` by passing the org name in both the `repo_id` and the `organization` arguments:\r\n\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/2230f7f7d7fbaf102cff356f5a8f3bd1561bea43\/src\/datasets\/arrow_dataset.py#L3363-L3369\r\n\r\nI think we should fix that in `huggingface_hub`, will keep you posted. In the meantime please use `huggingface_hub` 0.4.0","I'll be sending a fix for this later today on the `huggingface_hub` side.\r\n\r\nThe error would be converted to a `FutureWarning` if `datasets` uses kwargs instead of positional, for example here: \r\n\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/2230f7f7d7fbaf102cff356f5a8f3bd1561bea43\/src\/datasets\/arrow_dataset.py#L3363-L3369\r\n\r\nto be:\r\n\r\n``` python\r\n            api.create_repo(\r\n                name=dataset_name,\r\n                token=token,\r\n                repo_type=\"dataset\",\r\n                organization=organization,\r\n                private=private,\r\n            )\r\n```\r\n\r\nBut `name` and `organization` are deprecated in `huggingface_hub=0.5`, and people should pass `repo_id='org\/name` instead. Note that `repo_id` was introduced in 0.5 and if `datasets` wants to support older `huggingface_hub` versions (which I encourage it to do), there needs to be a helper function to do that. It can be something like:\r\n\r\n\r\n```python\r\ndef create_repo(\r\n    client,\r\n    name: str,\r\n    token: Optional[str] = None,\r\n    organization: Optional[str] = None,\r\n    private: Optional[bool] = None,\r\n    repo_type: Optional[str] = None,\r\n    exist_ok: Optional[bool] = False,\r\n    space_sdk: Optional[str] = None,\r\n) -> str:\r\n    try:\r\n        return client.create_repo(\r\n            repo_id=f\"{organization}\/{name}\",\r\n            token=token,\r\n            private=private,\r\n            repo_type=repo_type,\r\n            exist_ok=exist_ok,\r\n            space_sdk=space_sdk,\r\n        )\r\n    except TypeError:\r\n        return client.create_repo(\r\n            name=name,\r\n            organization=organization,\r\n            token=token,\r\n            private=private,\r\n            repo_type=repo_type,\r\n            exist_ok=exist_ok,\r\n            space_sdk=space_sdk,\r\n        )\r\n```\r\n\r\nin a `utils\/_fixes.py` kinda file and and be used internally.\r\n\r\nI'll be sending a patch to `huggingface_hub` to convert the error reported in this issue to a `FutureWarning`.","PR with the hotfix on the `huggingface_hub` side: https:\/\/github.com\/huggingface\/huggingface_hub\/pull\/822","We can definitely change `push_to_hub` to use `repo_id` in `datasets` and require `huggingface_hub>=0.5.0`.\r\n\r\nLet me open a PR :)","`huggingface_hub` 0.5.1 just got released with a fix, feel free to update `huggingface_hub` ;)"],"created_at":1649235597000,"updated_at":1649860247000,"closed_at":1649860247000,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\n`ds.push_to_hub` is failing when updating a dataset in the form \"org_id\/repo_id\"\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset(\"rubrix\/news_test\")\r\nds.push_to_hub(\"<your-user>\/news_test\", token=\"<your-token>\")\r\n```\r\n\r\n## Expected results\r\nThe dataset is successfully uploaded\r\n\r\n## Actual results\r\nAn error validation is raised:\r\n\r\n```bash\r\nif repo_id and (name or organization):\r\n>           raise ValueError(\r\n                \"Only pass `repo_id` and leave deprecated `name` and \"\r\n                \"`organization` to be None.\"\r\nE               ValueError: Only pass `repo_id` and leave deprecated `name` and `organization` to be None.\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.1\r\n- `huggingface-hub`: 0.5\r\n- Platform: macOS\r\n- Python version: 3.8.12\r\n- PyArrow version: 6.0.0\r\n\r\ncc @adrinjalali \r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4105\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4105\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4104","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4104\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4104\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4104\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4104","id":1194072966,"node_id":"I_kwDODunzps5HLBuG","number":4104,"title":"Add time series data - stock market","user":{"login":"INF800","id":45640029,"node_id":"MDQ6VXNlcjQ1NjQwMDI5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/45640029?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/INF800","html_url":"https:\/\/github.com\/INF800","followers_url":"https:\/\/api.github.com\/users\/INF800\/followers","following_url":"https:\/\/api.github.com\/users\/INF800\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/INF800\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/INF800\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/INF800\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/INF800\/orgs","repos_url":"https:\/\/api.github.com\/users\/INF800\/repos","events_url":"https:\/\/api.github.com\/users\/INF800\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/INF800\/received_events","type":"User","site_admin":false},"labels":[{"id":2067376369,"node_id":"MDU6TGFiZWwyMDY3Mzc2MzY5","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20request","name":"dataset request","color":"e99695","default":false,"description":"Requesting to add a new dataset"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Can I use instructions present in below link for time series dataset as well? \r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/master\/ADD_NEW_DATASET.md ","cc'ing @kashif and @NielsRogge for visibility!","@INF800 happy to add this dataset! I will try to set a PR by the end of the day... if you can kindly point me to the dataset? Also, note we have a bunch of time series datasets checked in e.g. `electricity_load_diagrams` or `monash_tsf`, and ideally this dataset could also be in a similar format. ","Thankyou. This is how raw data looks like before cleaning for an individual stocks:\r\n\r\n1. https:\/\/github.com\/INF800\/marktech\/tree\/raw-data\/f\/data\/raw\r\n2. https:\/\/github.com\/INF800\/marktech\/tree\/raw-data\/t\/data\/raw\r\n3. https:\/\/github.com\/INF800\/marktech\/tree\/raw-data\/rdfn\/data\/raw\r\n4. https:\/\/github.com\/INF800\/marktech\/tree\/raw-data\/irbt\/data\/raw\r\n5. https:\/\/github.com\/INF800\/marktech\/tree\/raw-data\/hll\/data\/raw\r\n6. https:\/\/github.com\/INF800\/marktech\/tree\/raw-data\/infy\/data\/raw\r\n7. https:\/\/github.com\/INF800\/marktech\/tree\/raw-data\/reli\/data\/raw\r\n8. https:\/\/github.com\/INF800\/marktech\/tree\/raw-data\/hdbk\/data\/raw\r\n\r\n> Scraping is automated using GitHub Actions. So, everyday we will see a new file added in the above links.\r\n\r\nI can rewrite the cleaning scripts to make sure it fits HF dataset standards. (P.S I am very much new to HF dataset)\r\n\r\nThe data set above can be converted into univariate regression \/ multivariate regression \/ sequence to sequence generation dataset etc. So, do we have some kind of transformation modules that will read the dataset as some type of dataset (`GenericTimeData`) and convert it to other possible dataset relating to a specific ML task. **By having this kind of transformation module, I only have to add data once** and use transformation module whenever necessary\r\n\r\nAdditionally, having some kind of versioning for the dataset will be really helpful because it will keep on updating - especially time series datasets ","thanks @INF800 I'll have a look. I believe it should be possible to incorporate this into the time-series format.","Referencing https:\/\/github.com\/qingsongedu\/time-series-transformers-review","@INF800 yes I am aware of the review repository and paper which is more or less a collection of abstracts etc. I am working on a unified library of implementations of these papers together with datasets to be then able to compare\/contrast and build upon the research etc. but I am not ready to share them publicly just yet.\r\n\r\nIn any case regarding your dataset at the moment its seems from looking at the csv files, its mixture of textual and numerical data, sometimes in the same column etc. As you know, for time series models we would need just numeric data so I would need your help in disambiguating the dataset you have collected and also perhaps starting with just numerical data to start with... \r\n\r\nDo you think you can make a version with just numerical data?","> @INF800 yes I am aware of the review repository and paper which is more or less a collection of abstracts etc. I am working on a unified library of implementations of these papers together with datasets to be then able to compare\/contrast and build upon the research etc. but I am not ready to share them publicly just yet.\r\n> \r\n> In any case regarding your dataset at the moment its seems from looking at the csv files, its mixture of textual and numerical data, sometimes in the same column etc. As you know, for time series models we would need just numeric data so I would need your help in disambiguating the dataset you have collected and also perhaps starting with just numerical data to start with...\r\n> \r\n> Do you think you can make a version with just numerical data?\r\n\r\nWill share the numeric data and conversion script within end of this week. \r\n\r\nI am on a business trip currently - it is in my desktop."],"created_at":1649224018000,"updated_at":1649668030000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"## Adding a Time Series Dataset\r\n- **Name:** 2min ticker data for stock market \r\n- **Description:** 8 stocks' data collected for 1month post ukraine-russia war. 4 NSE stocks and 4 NASDAQ stocks. Along with technical indicators (additional features) as shown in below image\r\n- **Data:** Collected by myself from investing.com\r\n- **Motivation:** Test applicability of transformer based model on stock market \/ time series problem\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/45640029\/161904077-52fe97cb-3720-4e3f-98ee-7f6720a056e2.png)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4104\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4104\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4103","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4103\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4103\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4103\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4103","id":1193987104,"node_id":"PR_kwDODunzps41s3T4","number":4103,"title":"Add the `GSM8K` dataset","user":{"login":"jon-tow","id":41410219,"node_id":"MDQ6VXNlcjQxNDEwMjE5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/41410219?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jon-tow","html_url":"https:\/\/github.com\/jon-tow","followers_url":"https:\/\/api.github.com\/users\/jon-tow\/followers","following_url":"https:\/\/api.github.com\/users\/jon-tow\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jon-tow\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jon-tow\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jon-tow\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jon-tow\/orgs","repos_url":"https:\/\/api.github.com\/users\/jon-tow\/repos","events_url":"https:\/\/api.github.com\/users\/jon-tow\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jon-tow\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","The CI is failing because it's outdated, but the task tags are updated on `master`, merging :)"],"created_at":1649218072000,"updated_at":1649777908000,"closed_at":1649758876000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4103\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4103\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4103","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4103","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4103.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4103.patch","merged_at":1649758876000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4102","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4102\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4102\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4102\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4102","id":1193616722,"node_id":"PR_kwDODunzps41roGx","number":4102,"title":"[hub] Fix `api.create_repo` call?","user":{"login":"julien-c","id":326577,"node_id":"MDQ6VXNlcjMyNjU3Nw==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/326577?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/julien-c","html_url":"https:\/\/github.com\/julien-c","followers_url":"https:\/\/api.github.com\/users\/julien-c\/followers","following_url":"https:\/\/api.github.com\/users\/julien-c\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/julien-c\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/julien-c\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/julien-c\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/julien-c\/orgs","repos_url":"https:\/\/api.github.com\/users\/julien-c\/repos","events_url":"https:\/\/api.github.com\/users\/julien-c\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/julien-c\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4102). All of your documentation changes will be reflected on that endpoint.","Closing in favor of https:\/\/github.com\/huggingface\/datasets\/pull\/4106"],"created_at":1649186512000,"updated_at":1649752906000,"closed_at":1649752906000,"author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4102\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4102\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4102","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4102","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4102.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4102.patch","merged_at":null},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4101","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4101\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4101\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4101\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4101","id":1193399204,"node_id":"I_kwDODunzps5HIdOk","number":4101,"title":"How can I download only the train and test split for full numbers using load_dataset()? ","user":{"login":"Nakkhatra","id":64383902,"node_id":"MDQ6VXNlcjY0MzgzOTAy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/64383902?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Nakkhatra","html_url":"https:\/\/github.com\/Nakkhatra","followers_url":"https:\/\/api.github.com\/users\/Nakkhatra\/followers","following_url":"https:\/\/api.github.com\/users\/Nakkhatra\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Nakkhatra\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Nakkhatra\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Nakkhatra\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Nakkhatra\/orgs","repos_url":"https:\/\/api.github.com\/users\/Nakkhatra\/repos","events_url":"https:\/\/api.github.com\/users\/Nakkhatra\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Nakkhatra\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi! Can you please specify the full name of the dataset? IIRC `full_numbers` is one of the configs of the `svhn` dataset, and its generation is slow due to data being stored in binary Matlab files. Even if you specify a specific split, `datasets` downloads all of them, but we plan to fix that soon and only download the requested split.\r\n\r\nIf you are in a hurry, download the `svhn` script [here](`https:\/\/huggingface.co\/datasets\/svhn\/blob\/main\/svhn.py`), remove [this code](https:\/\/huggingface.co\/datasets\/svhn\/blob\/main\/svhn.py#L155-L162), and run:\r\n```python\r\nfrom datasets import load_dataset\r\ndset = load_dataset(\"path\/to\/your\/local\/script.py\", \"full_numbers\")\r\n```\r\n\r\nAnd to make loading easier in Colab, you can create a dataset repo on the Hub and upload the script there. Or push the script to Google Drive and mount the drive in Colab."],"created_at":1649174415000,"updated_at":1649250541000,"closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"How can I download only the train and test split for full numbers using load_dataset()? \r\n\r\nI do not need the extra split and it will take 40 mins just to download in Colab. I have very short time in hand. Please help.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4101\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4101\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4100","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4100\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4100\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4100\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4100","id":1193393959,"node_id":"PR_kwDODunzps41q4ce","number":4100,"title":"Improve RedCaps dataset card","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","I find this preprocessing a bit too specific to add it as a method to `datasets` as it's only useful in the context of CV (and we support multiple modalities). However, I agree it would be great to move this code to another lib to avoid code duplication. Maybe we should create a package with preprocessing functions\/transforms for this purpose?"],"created_at":1649174234000,"updated_at":1649858934000,"closed_at":1649858546000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"This PR modifies the RedCaps card to:\r\n* fix the formatting of the Point of Contact fields on the Hub\r\n* speed up the image fetching logic (aligns it with the [img2dataset](https:\/\/github.com\/rom1504\/img2dataset) tool) and make it more robust (return None if **any** exception is thrown)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4100\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4100\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4100","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4100","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4100.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4100.patch","merged_at":1649858546000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4099","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4099\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4099\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4099\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4099","id":1193253768,"node_id":"I_kwDODunzps5HH5uI","number":4099,"title":"UnicodeDecodeError: 'ascii' codec can't decode byte 0xe5 in position 213: ordinal not in range(128)","user":{"login":"andreybond","id":20210017,"node_id":"MDQ6VXNlcjIwMjEwMDE3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/20210017?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/andreybond","html_url":"https:\/\/github.com\/andreybond","followers_url":"https:\/\/api.github.com\/users\/andreybond\/followers","following_url":"https:\/\/api.github.com\/users\/andreybond\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/andreybond\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/andreybond\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/andreybond\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/andreybond\/orgs","repos_url":"https:\/\/api.github.com\/users\/andreybond\/repos","events_url":"https:\/\/api.github.com\/users\/andreybond\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/andreybond\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi @andreybond, thanks for reporting.\r\n\r\nUnfortunately, I'm not able to able to reproduce your issue:\r\n```python\r\nIn [4]: from datasets import load_dataset\r\n   ...: datasets = load_dataset(\"nielsr\/XFUN\", \"xfun.ja\")\r\n\r\nIn [5]: datasets\r\nOut[5]: \r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['id', 'input_ids', 'bbox', 'labels', 'image', 'entities', 'relations'],\r\n        num_rows: 194\r\n    })\r\n    validation: Dataset({\r\n        features: ['id', 'input_ids', 'bbox', 'labels', 'image', 'entities', 'relations'],\r\n        num_rows: 71\r\n    })\r\n})\r\n```\r\n\r\nThe only reason I can imagine this issue may arise is if your default encoding is not \"UTF-8\" (and it is ASCII instead). This is usually the case on Windows machines; but you say your environment is a Linux machine. Maybe you change your machine default encoding?\r\n\r\nCould you please check this?\r\n```python\r\nIn [6]: import sys\r\n\r\nIn [7]: sys.getdefaultencoding()\r\nOut[7]: 'utf-8'\r\n```","I opened a PR in the original dataset loading script:\r\n-  microsoft\/unilm#677\r\n\r\nand fixed the corresponding dataset script on the Hub:\r\n- https:\/\/huggingface.co\/datasets\/nielsr\/XFUN\/commit\/73ba5e026621e05fb756ae0f267eb49971f70ebd","import sys\r\nsys.getdefaultencoding()\r\n\r\nreturned: 'utf-8'\r\n\r\n---------------------\r\n\r\nI've just cloned master branch - your fix works! Thank you!"],"created_at":1649169758000,"updated_at":1649227064000,"closed_at":1649226954000,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\nError \"UnicodeDecodeError: 'ascii' codec can't decode byte 0xe5 in position 213: ordinal not in range(128)\" is thrown when downloading dataset.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset \r\ndatasets = load_dataset(\"nielsr\/XFUN\", \"xfun.ja\")\r\n```\r\n\r\n## Expected results\r\nDataset should be downloaded without exceptions\r\n\r\n## Actual results\r\nStack trace (for the second-time execution):\r\nDownloading and preparing dataset xfun\/xfun.ja to \/root\/.cache\/huggingface\/datasets\/nielsr___xfun\/xfun.ja\/0.0.0\/e06e948b673d1be9a390a83c05c10e49438bf03dd85ae9a4fe06f8747a724477...\r\nDownloading data files: 100%\r\n2\/2 [00:00<00:00, 88.48it\/s]\r\nExtracting data files: 100%\r\n2\/2 [00:00<00:00, 79.60it\/s]\r\n\r\nUnicodeDecodeErrorTraceback (most recent call last)\r\n<ipython-input-31-79c26bd1109c> in <module>\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 datasets = load_dataset(\"nielsr\/XFUN\", \"xfun.ja\")\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/datasets\/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/datasets\/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n    604         )\r\n    605 \r\n--> 606         # By default, return all splits\r\n    607         if split is None:\r\n    608             split = {s: s for s in self.info.splits}\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/datasets\/builder.py in _download_and_prepare(self, dl_manager, verify_infos)\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/datasets\/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    692         Args:\r\n    693             split: `datasets.Split` which subset of the data to read.\r\n--> 694 \r\n    695         Returns:\r\n    696             `Dataset`\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/datasets\/builder.py in _prepare_split(self, split_generator, check_duplicate_keys)\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tqdm\/notebook.py in __iter__(self)\r\n    252         if not self.disable:\r\n    253             self.display(check_delay=False)\r\n--> 254 \r\n    255     def __iter__(self):\r\n    256         try:\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tqdm\/std.py in __iter__(self)\r\n   1183             for obj in iterable:\r\n   1184                 yield obj\r\n-> 1185             return\r\n   1186 \r\n   1187         mininterval = self.mininterval\r\n\r\n~\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/nielsr--XFUN\/e06e948b673d1be9a390a83c05c10e49438bf03dd85ae9a4fe06f8747a724477\/XFUN.py in _generate_examples(self, filepaths)\r\n    140             logger.info(\"Generating examples from = %s\", filepath)\r\n    141             with open(filepath[0], \"r\") as f:\r\n--> 142                 data = json.load(f)\r\n    143 \r\n    144             for doc in data[\"documents\"]:\r\n\r\n\/usr\/lib\/python3.6\/json\/__init__.py in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\r\n    294 \r\n    295     \"\"\"\r\n--> 296     return loads(fp.read(),\r\n    297         cls=cls, object_hook=object_hook,\r\n    298         parse_float=parse_float, parse_int=parse_int,\r\n\r\n\/usr\/lib\/python3.6\/encodings\/ascii.py in decode(self, input, final)\r\n     24 class IncrementalDecoder(codecs.IncrementalDecoder):\r\n     25     def decode(self, input, final=False):\r\n---> 26         return codecs.ascii_decode(input, self.errors)[0]\r\n     27 \r\n     28 class StreamWriter(Codec,codecs.StreamWriter):\r\n\r\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xe5 in position 213: ordinal not in range(128)\r\n\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0 (but reproduced with many previous versions)\r\n- Platform: Docker: Linux da5b74136d6b 5.3.0-1031-azure #32~18.04.1-Ubuntu SMP Mon Jun 22 15:27:23 UTC 2020 x86_64 x86_64 x86_64 GNU\/Linux ; Base docker image is : huggingface\/transformers-pytorch-cpu\r\n- Python version: 3.6.9\r\n- PyArrow version:   6.0.1\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4099\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4099\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4098","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4098\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4098\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4098\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4098","id":1193245522,"node_id":"PR_kwDODunzps41qXjo","number":4098,"title":"Proposing WikiSplit metric card","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","A quick Github tip ;) To avoid running N times the CI, you can push all the changes at once: go to Files Changed tab, and on each suggestion there's a \"add to commit batch\" and then you can do one commit for all the suggestions you want to approve ;)"],"created_at":1649169394000,"updated_at":1649173717000,"closed_at":1649173348000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Pinging @lhoestq to ensure that my distinction between the dataset and the metric are clear :sweat_smile:","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4098\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4098\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4098","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4098","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4098.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4098.patch","merged_at":1649173348000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4097","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4097\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4097\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4097\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4097","id":1193205751,"node_id":"PR_kwDODunzps41qPEu","number":4097,"title":"Updating FrugalScore metric card","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649167764000,"updated_at":1649171255000,"closed_at":1649170906000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"removing duplicate paragraph","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4097\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4097\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4097","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4097","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4097.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4097.patch","merged_at":1649170906000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4096","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4096\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4096\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4096\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4096","id":1193165229,"node_id":"I_kwDODunzps5HHkGt","number":4096,"title":"Add support for streaming Zarr stores for hosted datasets","user":{"login":"jacobbieker","id":7170359,"node_id":"MDQ6VXNlcjcxNzAzNTk=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/7170359?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jacobbieker","html_url":"https:\/\/github.com\/jacobbieker","followers_url":"https:\/\/api.github.com\/users\/jacobbieker\/followers","following_url":"https:\/\/api.github.com\/users\/jacobbieker\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jacobbieker\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jacobbieker\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jacobbieker\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jacobbieker\/orgs","repos_url":"https:\/\/api.github.com\/users\/jacobbieker\/repos","events_url":"https:\/\/api.github.com\/users\/jacobbieker\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jacobbieker\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi @jacobbieker, thanks for your request and study of possible alternatives.\r\n\r\nWe are very interested in finding a way to make `datasets` useful to you.\r\n\r\nLooking at the Zarr docs, I saw that among its storage alternatives, there is the ZIP file format: https:\/\/zarr.readthedocs.io\/en\/stable\/api\/storage.html#zarr.storage.ZipStore\r\n\r\nThis might be convenient for many reasons:\r\n- On the one hand, we avoid the Git issue with huge number of small files: chunks files are compressed into a single ZIP file\r\n- On the other hand, the ZIP file format is specially suited for streaming data because it allows random access to its component files (i.e. it supports random access to its chunks)\r\n\r\nAnyway, I think that a Python loading script will be necessary: you need to implement additional logic to select certain chunks (based on date or other criteria).\r\n\r\nPlease, let me know if this makes sense to you.","Ah okay, I missed the option of zip files for zarr, I'll try that with our repos and see if it works! Thanks a lot!","Hi @jacobbieker, does the Zarr ZipStore work for your use case?","Hi,\r\n\r\nYes, it seems to! I got it working for https:\/\/huggingface.co\/datasets\/openclimatefix\/mrms thanks for the help! ","On behalf of the Zarr developers, let me say THANK YOU for working to support Zarr on HF! \ud83d\ude4f  Zarr is a 100% open-source and community driven project (fiscally sponsored by NumFocus). We see it as an ideal format for ML training datasets, particularly in scientific domains.\r\n\r\nI think the solution of zipping the Zarr store is a reasonable way to balance the constraints of Git LFS with the structure of Zarr.\r\n\r\nIt would be amazing to get something on the [Hugging Face Datasets Docs](https:\/\/huggingface.co\/docs\/datasets\/index) about how to best work with Zarr. Let me know if there's a way I could help with that effort.","Also just noting here that I was able to lazily open @jacobbieker's dataset over the internet from HF hub \ud83d\ude80 !\r\n\r\n```python\r\nimport xarray as xr\r\nurl = \"https:\/\/huggingface.co\/datasets\/openclimatefix\/mrms\/resolve\/main\/data\/2016_001.zarr.zip\"\r\nzip_url = 'zip:\/\/\/::' + url\r\nds = xr.open_dataset(zip_url, engine='zarr', chunks={})\r\n```\r\n\r\n<img width=\"740\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/1197350\/164508663-bc75cdc0-734d-44f4-9562-2877ecfdf433.png\">\r\n","However, I wasn't able to get streaming working using the Datasets api:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset(\"openclimatefix\/mrms\", streaming=True, split='train')\r\nitem = next(iter(ds))\r\n```\r\n\r\n<details>\r\n<summary>FileNotFoundError traceback<\/summary>\r\n\r\n```\r\nNo config specified, defaulting to: mrms\/2021\r\nzip:\/\/::https:\/\/huggingface.co\/datasets\/openclimatefix\/mrms\/resolve\/main\/data\/2016_001.zarr.zip\r\ndata\/2016_001.zarr.zip\r\nzip:\/\/2016_001.zarr.zip::https:\/\/huggingface.co\/datasets\/openclimatefix\/mrms\/resolve\/main\/data\/2016_001.zarr.zip\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\nInput In [1], in <cell line: 3>()\r\n      1 from datasets import load_dataset\r\n      2 ds = load_dataset(\"openclimatefix\/mrms\", streaming=True, split='train')\r\n----> 3 item = next(iter(ds))\r\n\r\nFile \/opt\/miniconda3\/envs\/hugginface\/lib\/python3.9\/site-packages\/datasets\/iterable_dataset.py:497, in IterableDataset.__iter__(self)\r\n    496 def __iter__(self):\r\n--> 497     for key, example in self._iter():\r\n    498         if self.features:\r\n    499             # we encode the example for ClassLabel feature types for example\r\n    500             encoded_example = self.features.encode_example(example)\r\n\r\nFile \/opt\/miniconda3\/envs\/hugginface\/lib\/python3.9\/site-packages\/datasets\/iterable_dataset.py:494, in IterableDataset._iter(self)\r\n    492 else:\r\n    493     ex_iterable = self._ex_iterable\r\n--> 494 yield from ex_iterable\r\n\r\nFile \/opt\/miniconda3\/envs\/hugginface\/lib\/python3.9\/site-packages\/datasets\/iterable_dataset.py:87, in ExamplesIterable.__iter__(self)\r\n     86 def __iter__(self):\r\n---> 87     yield from self.generate_examples_fn(**self.kwargs)\r\n\r\nFile ~\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/openclimatefix--mrms\/2a6f697014d7eb3caf586ca137d47ca38785ae2fe36248611b021f8248b59936\/mrms.py:150, in MRMS._generate_examples(self, filepath, split)\r\n    147 filepath = \"[https:\/\/huggingface.co\/datasets\/openclimatefix\/mrms\/resolve\/main\/data\/2016_001.zarr.zip](https:\/\/huggingface.co\/datasets\/openclimatefix\/mrms\/resolve\/main\/data\/2016_001.zarr.zip%3C\/span%3E%3Cspan) style=\"color:rgb(175,0,0)\">\"\r\n    148 # TODO: This method handles input defined in _split_generators to yield (key, example) tuples from the dataset.\r\n    149 # The `key` is for legacy reasons (tfds) and is not important in itself, but must be unique for each example.\r\n--> 150 with zarr.storage.FSStore(fsspec.open(\"zip::\" + filepath, mode='r'), mode='r') as store:\r\n    151     data = xr.open_zarr(store)\r\n    152     for key, row in enumerate(data[\"time\"].values):\r\n\r\nFile \/opt\/miniconda3\/envs\/hugginface\/lib\/python3.9\/site-packages\/zarr\/storage.py:1120, in FSStore.__init__(self, url, normalize_keys, key_separator, mode, exceptions, dimension_separator, **storage_options)\r\n   1117 import fsspec\r\n   1118 self.normalize_keys = normalize_keys\r\n-> 1120 protocol, _ = fsspec.core.split_protocol(url)\r\n   1121 # set auto_mkdir to True for local file system\r\n   1122 if protocol in (None, \"file\") and not storage_options.get(\"auto_mkdir\"):\r\n\r\nFile \/opt\/miniconda3\/envs\/hugginface\/lib\/python3.9\/site-packages\/fsspec\/core.py:514, in split_protocol(urlpath)\r\n    512 def split_protocol(urlpath):\r\n    513     \"\"\"Return protocol, path pair\"\"\"\r\n--> 514     urlpath = stringify_path(urlpath)\r\n    515     if \":\/\/\" in urlpath:\r\n    516         protocol, path = urlpath.split(\":\/\/\", 1)\r\n\r\nFile \/opt\/miniconda3\/envs\/hugginface\/lib\/python3.9\/site-packages\/fsspec\/utils.py:315, in stringify_path(filepath)\r\n    313     return filepath\r\n    314 elif hasattr(filepath, \"__fspath__\"):\r\n--> 315     return filepath.__fspath__()\r\n    316 elif isinstance(filepath, pathlib.Path):\r\n    317     return str(filepath)\r\n\r\nFile \/opt\/miniconda3\/envs\/hugginface\/lib\/python3.9\/site-packages\/fsspec\/core.py:98, in OpenFile.__fspath__(self)\r\n     96 def __fspath__(self):\r\n     97     # may raise if cannot be resolved to local file\r\n---> 98     return self.open().__fspath__()\r\n\r\nFile \/opt\/miniconda3\/envs\/hugginface\/lib\/python3.9\/site-packages\/fsspec\/core.py:140, in OpenFile.open(self)\r\n    132 def open(self):\r\n    133     \"\"\"Materialise this as a real open file without context\r\n    134 \r\n    135     The file should be explicitly closed to avoid enclosed file\r\n   (...)\r\n    138     been deleted; but a with-context is better style.\r\n    139     \"\"\"\r\n--> 140     out = self.__enter__()\r\n    141     closer = out.close\r\n    142     fobjects = self.fobjects.copy()[:-1]\r\n\r\nFile \/opt\/miniconda3\/envs\/hugginface\/lib\/python3.9\/site-packages\/fsspec\/core.py:103, in OpenFile.__enter__(self)\r\n    100 def __enter__(self):\r\n    101     mode = self.mode.replace(\"t\", \"\").replace(\"b\", \"\") + \"b\"\r\n--> 103     f = self.fs.open(self.path, mode=mode)\r\n    105     self.fobjects = [f]\r\n    107     if self.compression is not None:\r\n\r\nFile \/opt\/miniconda3\/envs\/hugginface\/lib\/python3.9\/site-packages\/fsspec\/spec.py:1009, in AbstractFileSystem.open(self, path, mode, block_size, cache_options, compression, **kwargs)\r\n   1007 else:\r\n   1008     ac = kwargs.pop(\"autocommit\", not self._intrans)\r\n-> 1009     f = self._open(\r\n   1010         path,\r\n   1011         mode=mode,\r\n   1012         block_size=block_size,\r\n   1013         autocommit=ac,\r\n   1014         cache_options=cache_options,\r\n   1015         **kwargs,\r\n   1016     )\r\n   1017     if compression is not None:\r\n   1018         from fsspec.compression import compr\r\n\r\nFile \/opt\/miniconda3\/envs\/hugginface\/lib\/python3.9\/site-packages\/fsspec\/implementations\/zip.py:96, in ZipFileSystem._open(self, path, mode, block_size, autocommit, cache_options, **kwargs)\r\n     94 if mode != \"rb\":\r\n     95     raise NotImplementedError\r\n---> 96 info = self.info(path)\r\n     97 out = self.zip.open(path, \"r\")\r\n     98 out.size = info[\"size\"]\r\n\r\nFile \/opt\/miniconda3\/envs\/hugginface\/lib\/python3.9\/site-packages\/fsspec\/archive.py:42, in AbstractArchiveFileSystem.info(self, path, **kwargs)\r\n     40     return self.dir_cache[path + \"\/\"]\r\n     41 else:\r\n---> 42     raise FileNotFoundError(path)\r\n\r\nFileNotFoundError:\r\n```\r\n\r\n<\/details>\r\n\r\nIs this a bug? Or am I just doing it wrong...","I'm still messing around with that dataset, so the data might have moved. I currently have each year of MRMS precipitation rate data as it's own zarr, but as they are quite large (on order of 100GB each) I'm working to split them into single days, and as such they are still being moved around, I was just trying to get a proof of concept working originally. ","I've mostly finished rearranging the data now and uploading some more, so this works now:\r\n```python\r\nimport datasets\r\nds = datasets.load_dataset(\"openclimatefix\/mrms\", streaming=True, split=\"train\")\r\nitem = next(iter(ds))\r\nprint(item.keys())\r\nprint(item[\"timestamp\"])\r\n```\r\n\r\nThe MRMS data now goes most of 2016-2022, with quite a few gaps I'm working on filling in"],"created_at":1649165912000,"updated_at":1650873852000,"closed_at":1650528778000,"author_association":"NONE","active_lock_reason":null,"body":"**Is your feature request related to a problem? Please describe.**\r\nLots of geospatial data is stored in the Zarr format. This format works well for n-dimensional data and coordinates, and can have good compression. Unfortunately, HF datasets doesn't support streaming in data in Zarr format as far as I can tell. Zarr stores are designed to be easily streamed in from cloud storage, especially with xarray and fsspec. Since geospatial data tends to be very large, and on the order of TBs of data or 10's of TBs of data for a single dataset, it can be difficult to store the dataset locally for users. Just adding Zarr stores with HF git doesn't work well (see https:\/\/github.com\/huggingface\/datasets\/issues\/3823) as Zarr splits the data into lots of small chunks for fast loading, and that doesn't work well with git. I've somewhat gotten around that issue by tarring each Zarr store and uploading them as a single file, which seems to be working (see https:\/\/huggingface.co\/datasets\/openclimatefix\/gfs-reforecast for example data files, although the script isn't written yet). This does mean that streaming doesn't quite work though. On the other hand, in https:\/\/huggingface.co\/datasets\/openclimatefix\/eumetsat_uk_hrv we stream in a Zarr store from a public GCP bucket quite easily. \r\n\r\n**Describe the solution you'd like**\r\nA way to upload Zarr stores for hosted datasets so that we can stream it with xarray and fsspec. \r\n\r\n**Describe alternatives you've considered**\r\nTarring each Zarr store individually and just extracting them in the dataset script -> Downside this is a lot of data that probably doesn't fit locally for a lot of potential users.\r\nPre-prepare examples in a format like Parquet -> Would use a lot more storage, and a lot less flexibility, in the eumetsat_uk_hrv, we use the one Zarr store for multiple different configurations.\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4096\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4096\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4095","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4095\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4095\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4095\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4095","id":1192573353,"node_id":"PR_kwDODunzps41oIFI","number":4095,"title":"fix typo in rename_column error message","user":{"login":"hunterlang","id":680821,"node_id":"MDQ6VXNlcjY4MDgyMQ==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/680821?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/hunterlang","html_url":"https:\/\/github.com\/hunterlang","followers_url":"https:\/\/api.github.com\/users\/hunterlang\/followers","following_url":"https:\/\/api.github.com\/users\/hunterlang\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/hunterlang\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/hunterlang\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/hunterlang\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/hunterlang\/orgs","repos_url":"https:\/\/api.github.com\/users\/hunterlang\/repos","events_url":"https:\/\/api.github.com\/users\/hunterlang\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/hunterlang\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["The docs for this PR live [here](https:\/\/moon-ci-docs.huggingface.co\/docs\/datasets\/pr_4095). All of your documentation changes will be reflected on that endpoint."],"created_at":1649130956000,"updated_at":1649148886000,"closed_at":1649148353000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"I feel bad submitting such a tiny change as a PR but it confused me today \ud83d\ude04 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4095\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4095\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4095","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4095","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4095.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4095.patch","merged_at":1649148353000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4094","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4094\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4094\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4094\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4094","id":1192534414,"node_id":"I_kwDODunzps5HFKGO","number":4094,"title":"Helo Mayfrends","user":{"login":"Budigming","id":102933353,"node_id":"U_kgDOBiKjaQ","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/102933353?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Budigming","html_url":"https:\/\/github.com\/Budigming","followers_url":"https:\/\/api.github.com\/users\/Budigming\/followers","following_url":"https:\/\/api.github.com\/users\/Budigming\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Budigming\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Budigming\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Budigming\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Budigming\/orgs","repos_url":"https:\/\/api.github.com\/users\/Budigming\/repos","events_url":"https:\/\/api.github.com\/users\/Budigming\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Budigming\/received_events","type":"User","site_admin":false},"labels":[{"id":2067376369,"node_id":"MDU6TGFiZWwyMDY3Mzc2MzY5","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20request","name":"dataset request","color":"e99695","default":false,"description":"Requesting to add a new dataset"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":[],"created_at":1649126577000,"updated_at":1649143002000,"closed_at":1649143002000,"author_association":"NONE","active_lock_reason":null,"body":"## Adding a Dataset\r\n- **Name:** *name of the dataset*\r\n- **Description:** *short description of the dataset (or link to social media or blog post)*\r\n- **Paper:** *link to the dataset paper if available*\r\n- **Data:** *link to the Github repository or current dataset location*\r\n- **Motivation:** *what are some good reasons to have this dataset*\r\n\r\nInstructions to add a new dataset can be found [here](https:\/\/github.com\/huggingface\/datasets\/blob\/master\/ADD_NEW_DATASET.md).\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4094\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4094\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4093","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4093\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4093\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4093\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4093","id":1192523161,"node_id":"I_kwDODunzps5HFHWZ","number":4093,"title":"elena-soare\/crawled-ecommerce: missing dataset","user":{"login":"seevaratnam","id":17519354,"node_id":"MDQ6VXNlcjE3NTE5MzU0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17519354?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/seevaratnam","html_url":"https:\/\/github.com\/seevaratnam","followers_url":"https:\/\/api.github.com\/users\/seevaratnam\/followers","following_url":"https:\/\/api.github.com\/users\/seevaratnam\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/seevaratnam\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/seevaratnam\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/seevaratnam\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/seevaratnam\/orgs","repos_url":"https:\/\/api.github.com\/users\/seevaratnam\/repos","events_url":"https:\/\/api.github.com\/users\/seevaratnam\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/seevaratnam\/received_events","type":"User","site_admin":false},"labels":[{"id":3470211881,"node_id":"LA_kwDODunzps7O1zsp","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer","name":"dataset-viewer","color":"E5583E","default":false,"description":"Related to the dataset viewer on huggingface.co"}],"state":"closed","locked":false,"assignee":{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},"assignees":[{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["It's a bug! Thanks for reporting, I'm looking at it.","By the way, the error on our part is due to the huge size of every row (~90MB). The dataset viewer does not support such big dataset rows for the moment.\r\nAnyway, we're working to give a hint about this in the dataset viewer.","Fixed. See https:\/\/huggingface.co\/datasets\/elena-soare\/crawled-ecommerce\/viewer\/elena-soare--crawled-ecommerce\/train.\r\n\r\n<img width=\"1552\" alt=\"Capture d\u2019e\u0301cran 2022-04-12 a\u0300 11 23 51\" src=\"https:\/\/user-images.githubusercontent.com\/1676121\/162929722-2e2b80e2-154a-4b61-87bd-e341bd6c46e6.png\">\r\n\r\nThanks for reporting!"],"created_at":1649125519000,"updated_at":1649756093000,"closed_at":1649756093000,"author_association":"NONE","active_lock_reason":null,"body":"elena-soare\/crawled-ecommerce\r\n\r\n**Link:** *link to the dataset viewer page*\r\n\r\n*short description of the issue*\r\n\r\nAm I the one who added this dataset ? Yes-No\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4093\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4093\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4092","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4092\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4092\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4092\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4092","id":1192499903,"node_id":"PR_kwDODunzps41n40R","number":4092,"title":"Fix dataset `amazon_us_reviews` metadata - 4\/4\/2022","user":{"login":"trentonstrong","id":191985,"node_id":"MDQ6VXNlcjE5MTk4NQ==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/191985?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/trentonstrong","html_url":"https:\/\/github.com\/trentonstrong","followers_url":"https:\/\/api.github.com\/users\/trentonstrong\/followers","following_url":"https:\/\/api.github.com\/users\/trentonstrong\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/trentonstrong\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/trentonstrong\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/trentonstrong\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/trentonstrong\/orgs","repos_url":"https:\/\/api.github.com\/users\/trentonstrong\/repos","events_url":"https:\/\/api.github.com\/users\/trentonstrong\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/trentonstrong\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._","cc: @albertvillanova just FYI"],"created_at":1649122785000,"updated_at":1649421341000,"closed_at":1649420971000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fixes #4048 by running `dataset-cli test` to reprocess data and regenerate metadata. Additionally I've updated the README to include up-to-date counts for the subsets.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4092\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4092\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4092","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4092","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4092.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4092.patch","merged_at":1649420970000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4091","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4091\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4091\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4091\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4091","id":1192023855,"node_id":"I_kwDODunzps5HDNcv","number":4091,"title":"Build a Dataset One Example at a Time Without Loading All Data Into Memory","user":{"login":"aravind-tonita","id":99340348,"node_id":"U_kgDOBevQPA","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/99340348?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/aravind-tonita","html_url":"https:\/\/github.com\/aravind-tonita","followers_url":"https:\/\/api.github.com\/users\/aravind-tonita\/followers","following_url":"https:\/\/api.github.com\/users\/aravind-tonita\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/aravind-tonita\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/aravind-tonita\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/aravind-tonita\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/aravind-tonita\/orgs","repos_url":"https:\/\/api.github.com\/users\/aravind-tonita\/repos","events_url":"https:\/\/api.github.com\/users\/aravind-tonita\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/aravind-tonita\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["Hi! Yes, the problem with `add_item` is that it keeps examples in memory, so you are left with these options:\r\n* writing a dataset loading script in which you iterate over `custom_example_dict_streamer` and yield the examples (in `_generate examples`)\r\n* storing the data in a JSON\/CSV\/Parquet\/TXT file and using `Dataset.from_{format}`\r\n* using `add_item` + `save_to_disk` on smaller chunks: \r\n    ```python\r\n    from datasets import Dataset, concatenate_datasets\r\n    MAX_SAMPLES_IN_MEMORY = 1000\r\n    samples_in_dset = 0\r\n    dset = Dataset.from_dict({\"col1\": [], \"col2\": []})  # empty dataset\r\n    path_to_save_dir = \"path\/to\/save\/dir\"\r\n    num_chunks = 0\r\n    for example_dict in custom_example_dict_streamer(\"\/path\/to\/raw\/data\"):\r\n        dset = dset.add_item(example_dict)\r\n        samples_in_dset += 1\r\n        if samples_in_dset == MAX_SAMPLES_IN_MEMORY:\r\n            samples_in_dset = 0\r\n            dset.save_to_disk(f\"{path_to_save_dir}{num_chunks}\")\r\n            num_chunks =+ 1\r\n            dset = Dataset.from_dict({\"col1\": [], \"col2\": []})  # empty dataset\r\n    if samples_in_dset > 0:\r\n        dset.save_to_disk(f\"{path_to_save_dir}{num_chunks}\")\r\n        num_chunks =+ 1\r\n    loaded_dsets = []  # memory-mapped\r\n    for chunk_num in range(num_chunks):\r\n        dset = Dataset.load_from_disk(f\"{path_to_save_dir}{chunk_num}\") \r\n        loaded_dsets.append(dset)\r\n    final_dset = concatenate_datasets(dset)\r\n    ```\r\n    If you still have issues with this approach, you can try to delete unused datasets with `gc.collect()` to free some memory. ","This is really elegant, thank you @mariosasko!  I will try this."],"created_at":1649089164000,"updated_at":1650465060000,"closed_at":1650465060000,"author_association":"NONE","active_lock_reason":null,"body":"**Is your feature request related to a problem? Please describe.**\r\nI have a very large dataset stored on disk in a custom format.  I have some custom code that reads one data example at a time and yields it in the form of a dictionary.  I want to construct a `Dataset` with all examples, and then save it to disk.  I later want to load the saved `Dataset` and use it like any other HuggingFace dataset, get splits, wrap it in a PyTorch `DataLoader`, etc.  **Crucially, I do not ever want to materialize all the data in memory while building the dataset.**\r\n\r\n**Describe the solution you'd like**\r\nI would like to be able to do something like the following.  Notice how each example is read and then immediately added to the dataset.  We do not store all the data in memory when constructing the `Dataset`.  If it helps, I will know the schema of my dataset before hand.\r\n```\r\n\r\n# Initialize an empty Dataset, possibly from a known schema.\r\ndataset = Dataset()\r\n\r\n# Read in examples one by one using a custom data streamer.\r\nfor example_dict in custom_example_dict_streamer(\"\/path\/to\/raw\/data\"):\r\n\r\n    # Add this example to the dict but do not store it in memory.\r\n    dataset.add_item(example_dict)\r\n\r\n# Save the final dataset to disk as an Arrow-backed dataset.\r\ndataset.save_to_disk(\"\/path\/to\/dataset\")\r\n\r\n...\r\n\r\n# I'd like to be able to later `load_from_disk` and use the loaded Dataset\r\n# just like any other memory-mapped pyarrow-backed HuggingFace dataset...\r\nloaded_dataset = Dataset.load_from_disk(\"\/path\/to\/dataset\")\r\nloaded_dataset.set_format(type=\"torch\", columnns=[\"foo\", \"bar\", \"baz\"])\r\ndataloader = torch.utils.data.DataLoader(loaded_dataset, batch_size=16)\r\n...\r\n\r\n```\r\n\r\n**Describe alternatives you've considered**\r\nI initially tried to read all the data into memory, construct a Pandas DataFrame and then call `Dataset.from_pandas`.  This would not work as it requires storing all the data in memory.  It seems that there is an `add_item` method already -- I tried to implement something like the desired API written above, but I've not been able to initialize an empty `Dataset` (this seems to require several layers of constructing `datasets.table.Table` which requires constructing a `pyarrow.lib.Table`, etc).  I also considered writing my data to multiple sharded CSV files or JSON files and then using `from_csv` or `from_json`.  I'd prefer not to do this because (1) I'd prefer to avoid the intermediate step of creating these temp CSV\/JSON files and (2) I'm not sure if `from_csv` and `from_json` use memory-mapping.\r\n\r\nDo you have any suggestions on how I'd be able to achieve this use case?  Does something already exist to support this?  Thank you very much in advance!","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4091\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4091\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4090","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4090\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4090\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4090\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4090","id":1191956734,"node_id":"PR_kwDODunzps41mEs5","number":4090,"title":"Avoid writing empty license files","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649085817000,"updated_at":1649335605000,"closed_at":1649335243000,"author_association":"MEMBER","active_lock_reason":null,"body":"This PR avoids the creation of empty `LICENSE` files.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4090\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4090\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4090","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4090","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4090.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4090.patch","merged_at":1649335243000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4089","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4089\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4089\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4089\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4089","id":1191915196,"node_id":"PR_kwDODunzps41l7yd","number":4089,"title":"Create metric card for Frugal Score","user":{"login":"sashavor","id":14205986,"node_id":"MDQ6VXNlcjE0MjA1OTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14205986?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sashavor","html_url":"https:\/\/github.com\/sashavor","followers_url":"https:\/\/api.github.com\/users\/sashavor\/followers","following_url":"https:\/\/api.github.com\/users\/sashavor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sashavor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sashavor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sashavor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sashavor\/orgs","repos_url":"https:\/\/api.github.com\/users\/sashavor\/repos","events_url":"https:\/\/api.github.com\/users\/sashavor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sashavor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649084029000,"updated_at":1649168086000,"closed_at":1649167610000,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Proposing metric card for Frugal Score.\r\n\r\n@albertvillanova or @lhoestq  -- there are certain aspects that I'm not 100% sure on (such as how exactly the distillation between BertScore and FrugalScore is done) -- so if you find that something isn't clear, please let me know!","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4089\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4089\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4089","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4089","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4089.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4089.patch","merged_at":1649167610000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4088","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4088\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4088\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4088\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4088","id":1191901172,"node_id":"PR_kwDODunzps41l4yE","number":4088,"title":"Remove unused legacy Beam utils","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649083431000,"updated_at":1649172207000,"closed_at":1649171861000,"author_association":"MEMBER","active_lock_reason":null,"body":"This PR removes unused legacy custom `WriteToParquet`, once official Apache Beam includes the patch since version 2.22.0: \r\n- Patch PR: https:\/\/github.com\/apache\/beam\/pull\/11699\r\n- Issue: https:\/\/issues.apache.org\/jira\/browse\/BEAM-10022\r\n\r\nIn relation with:\r\n- #204","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4088\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4088\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4088","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4088","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4088.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4088.patch","merged_at":1649171861000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4087","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4087\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4087\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4087\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4087","id":1191819805,"node_id":"PR_kwDODunzps41lnfO","number":4087,"title":"Fix BeamWriter output Parquet file","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":["_The documentation is not available anymore as the PR was closed or merged._"],"created_at":1649080010000,"updated_at":1649170840000,"closed_at":1649170488000,"author_association":"MEMBER","active_lock_reason":null,"body":"Since now, the `BeamWriter` saved a Parquet file with a simplified schema, where each field value was serialized to JSON. That resulted in Parquet files larger than Arrow files.\r\n\r\nThis PR:\r\n- writes Parquet file preserving original schema and without serialization, thus avoiding serialization overhead and resulting in a smaller output file size.\r\n- fixes `parquet_to_arrow` function","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4087\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4087\/timeline","performed_via_github_app":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/4087","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4087","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4087.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/4087.patch","merged_at":1649170488000},"is_pull_request":true}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4086","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4086\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4086\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4086\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4086","id":1191373374,"node_id":"I_kwDODunzps5HAuo-","number":4086,"title":"Dataset viewer issue for McGill-NLP\/feedbackQA","user":{"login":"cslizc","id":54827718,"node_id":"MDQ6VXNlcjU0ODI3NzE4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/54827718?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/cslizc","html_url":"https:\/\/github.com\/cslizc","followers_url":"https:\/\/api.github.com\/users\/cslizc\/followers","following_url":"https:\/\/api.github.com\/users\/cslizc\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/cslizc\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/cslizc\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/cslizc\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/cslizc\/orgs","repos_url":"https:\/\/api.github.com\/users\/cslizc\/repos","events_url":"https:\/\/api.github.com\/users\/cslizc\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/cslizc\/received_events","type":"User","site_admin":false},"labels":[{"id":3470211881,"node_id":"LA_kwDODunzps7O1zsp","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer","name":"dataset-viewer","color":"E5583E","default":false,"description":"Related to the dataset viewer on huggingface.co"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Hi @cslizc, thanks for reporting.\r\n\r\nI have just forced the refresh of the corresponding cache and the preview is working now.","thank you so much"],"created_at":1649057240000,"updated_at":1649111393000,"closed_at":1649059305000,"author_association":"NONE","active_lock_reason":null,"body":"## Dataset viewer issue for '*McGill-NLP\/feedbackQA*'\r\n\r\n**Link:** *[link to the dataset viewer page](https:\/\/huggingface.co\/datasets\/McGill-NLP\/feedbackQA)*\r\n\r\n*short description of the issue*\r\nThe dataset can be loaded correctly with `load_dataset` but the preview doesn't work. Error message:\r\n\r\n```\r\nStatus code:   400\r\nException:     Status400Error\r\nMessage:       Not found. Maybe the cache is missing, or maybe the dataset does not exist.\r\n```\r\n\r\nAm I the one who added this dataset ? Yes\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4086\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4086\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4085","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4085\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4085\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4085\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/4085","id":1190621345,"node_id":"I_kwDODunzps5G93Ch","number":4085,"title":"datasets.set_progress_bar_enabled(False) not working in datasets v2","user":{"login":"virilo","id":3381112,"node_id":"MDQ6VXNlcjMzODExMTI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3381112?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/virilo","html_url":"https:\/\/github.com\/virilo","followers_url":"https:\/\/api.github.com\/users\/virilo\/followers","following_url":"https:\/\/api.github.com\/users\/virilo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/virilo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/virilo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/virilo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/virilo\/orgs","repos_url":"https:\/\/api.github.com\/users\/virilo\/repos","events_url":"https:\/\/api.github.com\/users\/virilo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/virilo\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":["Now, I can't find any reference to set_progress_bar_enabled in the code.\r\n\r\nI think it have been deleted","Hi @virilo,\r\n\r\nPlease note that since `datasets` version 2.0.0, we have aligned with `transformers` the management of the progress bar (among other things):\r\n- #3897\r\n\r\nNow, you should update your code to use `datasets.logging.disable_progress_bar`.\r\n\r\nYou have more info in our docs: [Logging methods](https:\/\/huggingface.co\/docs\/datasets\/package_reference\/logging_methods)"],"created_at":1648903210000,"updated_at":1649056499000,"closed_at":1649054674000,"author_association":"NONE","active_lock_reason":null,"body":"## Describe the bug\r\n\r\ndatasets.set_progress_bar_enabled(False) not working in datasets v2\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndatasets.set_progress_bar_enabled(False)\r\n```\r\n\r\n## Expected results\r\ndatasets not using any progress bar\r\n\r\n## Actual results\r\n\r\nAttributeError: module 'datasets' has no attribute 'set_progress_bar_enabled\r\n\r\n## Environment info\r\n\r\ndatasets version 2\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4085\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/4085\/timeline","performed_via_github_app":null,"draft":null,"pull_request":null,"is_pull_request":false}
