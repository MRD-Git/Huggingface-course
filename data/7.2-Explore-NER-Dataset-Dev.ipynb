{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec470a7",
   "metadata": {},
   "source": [
    "# The CoNLL-2003 dataset\n",
    "To load the CoNLL-2003 dataset, we use the `load_dataset()` method from the ðŸ¤— Datasets library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2af0502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/matthias/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65c5534edb34d18ae71c71c201003d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "datasets.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d36330b",
   "metadata": {},
   "source": [
    "Check the relevant (*version* of `datasets`) documentation:\n",
    "- [`DatasetDict`](https://huggingface.co/docs/datasets/v2.0.0/en/package_reference/main_classes#datasetdict[[datasets.datasetdict]])\n",
    "- [`DatasetDict`](https://huggingface.co/docs/datasets/v2.0.0/en/package_reference/main_classes#datasets.datasetdict)\n",
    "- [`Dataset`](https://huggingface.co/docs/datasets/v2.0.0/en/package_reference/main_classes#datasets.Dataset)\n",
    "\n",
    "> **This [video](https://www.youtube.com/watch?v=iY2AZYdZAr0) is very relevant!**\n",
    "\n",
    "> **Employ** `id2label` as shown in 7.2-Token_classification.ipynb!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "455a4472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shards: ['train', 'validation', 'test']\n",
      "\n",
      "train features:  \t['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags']\n",
      "train num_rows:  \t14042\n",
      "\n",
      "validation features:  \t['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags']\n",
      "validation num_rows:  \t3251\n",
      "\n",
      "test features:  \t['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags']\n",
      "test num_rows:  \t3454\n",
      "\n",
      "id  \t0\n",
      "tokens  \t['CRICKET', '-', 'LEICESTERSHIRE', 'TAKE', 'OVER', 'AT', 'TOP', 'AFTER', 'INNINGS', 'VICTORY', '.']\n",
      "pos_tags  \t[22, 8, 22, 22, 15, 22, 22, 22, 22, 21, 7]\n",
      "chunk_tags  \t[11, 0, 11, 12, 13, 11, 12, 12, 12, 12, 0]\n",
      "ner_tags  \t[0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def instance_details(datasets, shard, instance):\n",
    "    shards = list(datasets.keys())\n",
    "    print(\"dataset shards: {}\\n\".format(shards))\n",
    "    for shard_i in shards:\n",
    "        print(\"{} features:  \\t{}\".format(shard_i, list(datasets[shard_i].features.keys())))\n",
    "        print(\"{} num_rows:  \\t{}\\n\".format(shard_i, datasets[shard_i].num_rows))\n",
    "    inst = datasets[shard][instance]\n",
    "    feats = list(datasets[shard_i].features.keys())\n",
    "    for feat in feats:\n",
    "        print(\"{}  \\t{}\".format(feat, inst[feat]))\n",
    "    pass\n",
    "\n",
    "instance_details(raw_datasets, \"validation\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76c8591",
   "metadata": {},
   "source": [
    "For each of the following, see if the resulting dataset can still be used for NER as shown in part 7.2 of the HuggingFace course.\n",
    "1. Build your own version of the CoNLL-2003 dataset, omitting as many `features` as possible (at best only `tokens` and `ner_tags` remaining).\n",
    "1. Write the dataset into pandas dataframes and use those dataframes to reimport the dataset.\n",
    "1. Save the pandas dataframes into CSV files and load the dataframes from those CSV files. Then use the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13cb54c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
